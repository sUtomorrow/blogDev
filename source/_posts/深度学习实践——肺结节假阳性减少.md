---
title: 深度学习实践——肺结节假阳性减少
date: 2018-07-31 14:51:43
tags: 深度学习
categories: 工程实践
---
## 数据来源

CT扫描图像数据来源于LUNA16比赛使用的LIDC-IDRI数据集[[1]](#r1)[[2]](#r2)[[3]](#r3)，其中包括888个病人的三维肺部CT扫描数据，均保存为mhd格式。在LUNA16比赛给出的annotation.csv文件中，共有1186个结节信息，在candidate_V2.csv中，包含了用于训练肺结节假阳性减少网络的所有候选点信息，其中包括了1166个结节的共约1500条正样本记录，以及约750000条负样本记录。

## 数据预处理

因为读取mhd文件速度很慢，在实际训练之前，首先根据candidate_V2.csv从mhd文件中提取出需要用于训练的小块，单独保存为pkl文件，方便快速读取。

首先收集candidate文件信息：
```python
# dataset.py

def collect_candidates(candidate_file_path = None):
    '''
    收集candidates信息，用seriesuid作为键保存
    '''
    if candidate_file_path == None:
        # 使用默认路径
        candidate_file_path = config['LUNA16_CANDIDATE_PATH']
    candidates = {}
    candidate_file = pd.read_csv(candidate_file_path)
    for _, row in candidate_file.iterrows():
        key = row['seriesuid']
        del row['seriesuid']
        if key not in candidates:
            candidates[key] = [row]
        else:
            candidates[key].append(row)
    return candidates
```

考虑到正样本和负样本数量比例约为1:490，因此需要做数据扩充，按照LUNA16比赛使用的判定标准，样本中心点与结节中心点距离在结节半径以内的点均作为正样本，其余作为负样本，因此这里我们将按照annotation.csv文件中记录的结节信息，在结节半径内，采样一些点来扩充正样本：

```python
# dataset.py

def generate_candidates_from_annotations(annotation_path, target_dir, candidate_file_path = None, candidate_pkl_path = None):
    '''
    根据annodation.csv文件中的肺结节位置和大小信息，重新采样正样本数据
    '''
    annotation_df = pd.read_csv(annotation_path)
    annotation_dict = {}
    candidates = {}
    # 整理annotation.csv文件，以seriesuid作为键值，保存世界坐标以及毫米直径信息
    for _, row in annotation_df.iterrows():
        if row['seriesuid'] not in annotation_dict:
            annotation_dict[row['seriesuid']] = [[row['coordX'], row['coordY'], row['coordZ'], row['diameter_mm']]]
        else:
            annotation_dict[row['seriesuid']].append([row['coordX'], row['coordY'], row['coordZ'], row['diameter_mm']])
    print('annotation load end')
    if candidate_pkl_path is not None and os.path.exists(candidate_pkl_path):
        # 如果提供令了从candidate文件整理好的pkl，则直接使用
        print('use %s' % candidate_pkl_path)
        with open(candidate_pkl_path, 'rb') as fp:
            candidates = pickle.load(fp)
    elif candidate_file_path is not None and os.path.exists(candidate_file_path):
        # 不存在整理好的pkl，重新整理candidate.csv并保存为pkl
        print('use %s' % candidate_file_path)
        candidates = collect_candidates(candidate_file_path)
        with open(target_dir + 'candidate.pkl', 'wb') as fp:
            pickle.dump(candidates, fp)
            print('save %s' % (target_dir + 'candidate.pkl'))
    else:
        print('cannot found candidate file or pkl!!!')
        assert(False)
    new_candidates = deepcopy(candidates)
    total = 0
    print('candidates load end')
    for key in candidates:
        # 对于每一个seriesuid，即每一个病人
        print('%s' % key)
        for candidate in candidates[key]:
            # 对于病人的每一个候选点信息
            if candidate['class'] != 1:
                # 如果不是正样本，则跳过
                continue
            found = False
            nodule_found = None
            # 遍历病人对应的结节信息，查找候选点对应的肺结节
            for nodule_xyzd in annotation_dict[key]:
                dist = pow((nodule_xyzd[0] - candidate['coordX']), 2) + pow((nodule_xyzd[1] - candidate['coordY']), 2) + pow((nodule_xyzd[2] - candidate['coordZ']), 2)
                if dist <= (nodule_xyzd[3] / 2) ** 2:
                    # 如果候选点和结节中心点距离小于结节半径，则视为找到结节
                    found = True
                    nodule_found = nodule_xyzd
                    break
            if found == False:
                # 未找到对应的结节，输出处理
                print('cannot found nodule:')
                print(candidate)
            else:
                # 在找到的结节内部开始第二次正样本采样，并将采样点加入新的candidate信息中
                temp_xyz = []
                for x in np.arange(nodule_found[0] - nodule_found[3] / 2, nodule_found[0] + nodule_found[3] / 2, 1):
                    for y in np.arange(nodule_found[1] - nodule_found[3] / 2, nodule_found[1] + nodule_found[3] / 2, 1):
                        # 有可能加上z轴距离之后，候选点中心在结节半径之外，因此添加判断
                        if (x - nodule_found[0]) ** 2 + (y - nodule_found[1]) ** 2 + (candidate['coordZ'] - nodule_found[2]) ** 2 < (nodule_found[3] / 2) ** 2:
                            # 新采样的数据需要和原始数据点区分，这里将其class置为2
                            temp_xyz.append({'coordX': x, 'coordY': y, 'coordZ': candidate['coordZ'], 'class': 2})
                if nodule_xyzd[3] <= 10:
                    # 对于直径小于等于10毫米的结节，所有采样点都保留
                    new_candidates[key] += temp_xyz
                else:
                    # 对于直径大于10毫米的结节，随机保留一部分采样点
                    np.random.shuffle(temp_xyz)
                    new_candidates[key] += temp_xyz[: min(int(nodule_xyzd[3] * 24), 1000)]
        print('%d' % len(new_candidates[key]))
        total += len(new_candidates[key])
    # 保存新的候选点信息
    with open(target_dir + 'candidate_V3.pkl', 'wb') as fp:
        pickle.dump(new_candidates, fp)
    print(target_dir + 'candidate_V3.pkl')
    print('total candidate: %d' % total)
    return new_candidates
```

为了方便的操作mhd文件，定义一个MHDFile类，来封装对mhd文件的操作：
```python
# img_process.py

class MHDFile(object):
    '''
    封装对mhd文件的操作
    '''
    def __init__(self, mhd_path, normalize = False, gray_scale = False, gray_scale_pkl_path = None):
        self.mhd_path = mhd_path
        self.itk_img = sitk.ReadImage(self.mhd_path)
        # 使用SimpleITK库，读取mhd文件
        self.img_array = sitk.GetArrayFromImage(self.itk_img).astype(np.float32) # z, y, x
        self.spacing = np.array(list(reversed(self.itk_img.GetSpacing()))) # z, y, x
        self.direction = np.array(self.itk_img.GetDirection())
        self.origin = np.array(list(reversed(self.itk_img.GetOrigin())))    #z, y, x
        self.size_z, self.size_y, self.size_x = self.img_array.shape
        self.flip_y = False
        self.flip_x = False
        self.gray_scale = gray_scale
        if round(self.direction[0]) == -1:
            # x轴坐标需要反向
            self.origin[2] *= -1
            self.flip_x = True
            self.direction[0] = 1

        if round(self.direction[4]) == -1:
            # y轴坐标需要反向
            self.origin[1] *= -1
            self.flip_y = True
            self.direction[4] = 1

        if normalize:
            # 调整数据的均值和方差，实验证明没有效果，一般不使用
            self.img_norm(mean = -600, std = -300)
        
        if self.gray_scale:
            # 灰度直方图均衡，最好先提供经过灰度直方图均衡后的pkl（即时处理非常慢），这个预处理暂时不用
            if gray_scale_pkl_path is not None and os.path.exists(gray_scale_pkl_path):
                # 读取经过灰度直方图均衡后的img_array
                with open(gray_scale_pkl_path, 'rb') as fp:
                    self.gray_scale_array = pickle.load(fp)
            else:
                # 即时处理直方图均衡化，首先对原hu值矩阵加肺窗截断处理
                self.gray_scale_array = deepcopy(self.img_array)
                self.gray_scale_array = self.lung_windows(self.gray_scale_array)
                
                # 归一化到0~1范围，肺窗范围为-1000~400
                self.gray_scale_array += 1000
                self.gray_scale_array /= 1400
                #对每一层进行灰度自适应均衡处理，nbins参数暂时使用256,
                for i in range(len(self.gray_scale_array)):
                    self.gray_scale_array[i, :, :] = equalize_adapthist(self.gray_scale_array[i, :, :], nbins = 256)
                self.gray_scale_array = self.gray_scale_array.astype(np.float32)


    def get_cubic(self, start_zyx, end_zyx, lung_win = False, medi_win = False, gray_scale = False):
        '''
        获取一个小块
        '''
        # 如果需要纵膈窗，则小块的通道数为2
        add_dim = 2 if medi_win else 1

        # 首先按照起止坐标计算出小块的形状，并创建对应形状的零数组
        shape_zyx = np.array(end_zyx) - np.array(start_zyx)
        cubic = np.zeros(np.append(shape_zyx.astype(np.int32), [add_dim], 0), dtype = np.float32)

        # 偏移值
        shift_zyx = np.array([0, 0, 0], np.int32)
        
        # 修正边界情况，并记录偏移
        if start_zyx[0] < 0:
            shift_zyx[0] = -start_zyx[0]
            start_zyx[0] = 0
        if start_zyx[1] < 0:
            start_zyx[1] = 0
            shift_zyx[1] = -start_zyx[1]
        if start_zyx[2] < 0:
            start_zyx[2] = 0
            shift_zyx[2] = -start_zyx[2]

        if end_zyx[0] > self.size_z:
            end_zyx[0] = self.size_z
        if end_zyx[1] > self.size_y:
            end_zyx[1] = self.size_y
        if end_zyx[2] > self.size_x:
            end_zyx[2] = self.size_x
        
        shape_zyx = np.array(end_zyx, dtype = np.int32) - np.array(start_zyx, dtype = np.int32)

        if lung_win:
            # 返回肺窗截断并归一化的图像块
            cubic[shift_zyx[0] : int(shape_zyx[0]) + shift_zyx[0], shift_zyx[1] : int(shape_zyx[1]) + shift_zyx[1], shift_zyx[2] : int(shape_zyx[2]) + shift_zyx[2], 0] = (self.lung_windows(self.img_array[int(start_zyx[0]) : int(end_zyx[0]), int(start_zyx[1]) : int(end_zyx[1]), int(start_zyx[2]) : int(end_zyx[2])]) + 1000) / 1400
        else:
            # 直接返回hu值矩阵
            temp = self.img_array[int(start_zyx[0]): int(end_zyx[0]), int(start_zyx[1]): int(end_zyx[1]), int(start_zyx[2]): int(end_zyx[2])]
            max_v = np.max(temp)
            min_v = np.min(temp)
            cubic[shift_zyx[0]: int(shape_zyx[0]) + shift_zyx[0], shift_zyx[1]: int(shape_zyx[1]) + shift_zyx[1], shift_zyx[2]: int(shape_zyx[2]) + shift_zyx[2], 0] = (temp - min_v) / (max_v - min_v)
        if medi_win:
            # 返回纵膈窗截断并归一化的图像块
            cubic[shift_zyx[0] : int(shape_zyx[0]) + shift_zyx[0], shift_zyx[1] : int(shape_zyx[1]) + shift_zyx[1], shift_zyx[2] : int(shape_zyx[2]) + shift_zyx[2], 1] = (self.medi_windows(self.img_array[int(start_zyx[0]) : int(end_zyx[0]), int(start_zyx[1]) : int(end_zyx[1]), int(start_zyx[2]) : int(end_zyx[2])]) + 115) / 300
        if gray_scale:
            # 返回肺窗并归一化之后进行灰度直方图自适应拉伸的图像，
            if self.gray_scale:
                cubic[shift_zyx[0]: int(shape_zyx[0]) + shift_zyx[0], shift_zyx[1]: int(shape_zyx[1]) + shift_zyx[1], shift_zyx[2]: int(shape_zyx[2]) + shift_zyx[2], 1] = self.gray_scale_array[int(start_zyx[0]): int(end_zyx[0]), int(start_zyx[1]): int(end_zyx[1]), int(start_zyx[2]): int(end_zyx[2])]
            else:
                print('do not have gray scale array, initial with param gray_scale = True')
                assert(False)

        return cubic

    def lung_windows(self, cubic):
        # 肺窗截断
        return np.clip(cubic, -1000, 400)

    def medi_windows(self, cubic):
        # 纵膈窗截断
        return np.clip(cubic, -115, 185)

    def img_norm(self, mean = -600, std = -300):
        # 调整图像均值和方差
        ori_var = np.var(self.img_array)
        self.img_array /= np.sqrt(ori_var / std ** 2).astype(np.float32)

        ori_mean = np.mean(self.img_array)
        self.img_array -= (ori_mean - mean).astype(np.float32)

    def get_pix_coord(self, z, y, x):
        # 根据传入的世界坐标，获取对应的像素坐标
        if self.flip_x:
            x *= -1
        if self.flip_y:
            y *= -1

        z = np.rint((z - self.origin[0]) / self.spacing[0])
        y = np.rint((y - self.origin[1]) / self.spacing[1])
        x = np.rint((x - self.origin[2]) / self.spacing[2])
        return z, y, x
    
    def save_pkl(self, pkl_path, gray_scale = True):
        # 保存图像矩阵
        with open(pkl_path, 'wb') as fp:
            if gray_scale:
                pickle.dump(self.gray_scale_array, fp)
            else:
                pickle.dump(self.img_array, fp)
        print(pkl_path)
```

根据新采样的肺结节候选点，现在正样本数量大概增加为原来的100倍，正负样本比例大概为1:5，进一步的正样本数据扩充将在数据读取的时候进行，现在开始根据新的候选位置信息，打包训练、测试数据。

```python
# dataset.py

def data_prepare_from_new_candidate(target_dir = None):
    '''
    根据target_dir中的候选点信息，从原始mhd数据中打包训练用的数据，使用pickle库保存为pkl文件
    '''
    # luna16数据所在文件夹，包含subset0~subset9十个子文件夹
    luna16_dir = config['LUNA16_DIR']
    if target_dir == None:
        # 如果没有指定数据存放文件夹，则使用默认文件夹
        target_dir = config['data']['default_data_dir']
    
    if not os.path.exists(target_dir):
        # 不存在则创建
        os.mkdir(target_dir)

    if not os.path.exists(target_dir + 'candidate_V3.pkl'):
        # 如果之前没有生成新的采样点信息，则先生成并保存
        print('cannot found candidate_V3.pkl, collecting candidate info... ')
        candidates = generate_candidates_from_annotations(config['LUNA16_ANNOTATION_PATH'], target_dir,  config['LUNA16_CANDIDATE_PATH'], target_dir + 'candidate.pkl')
        with open(target_dir + 'candidate_V3.pkl', 'wb') as fp:
            pickle.dump(candidates, fp)
        print('collecting candidate info end, save as %s' % (target_dir + 'candidate_V3.pkl'))
    else:
        # 如果存在新的采样点信息，则直接读取使用
        print('found candidate_V3.pkl!!! loading candidate info...')
        with open(target_dir + 'candidate_V3.pkl', 'rb') as fp:
            candidates = pickle.load(fp)
        print('loading candidate info end!!!')

    for i in range(0, 10):
        # 数据按照fold_0~fold_9保存，对应于原始数据的subset0~subset9十个文件夹
        target_fold_dir = target_dir + 'fold_%d/' % i
        if not os.path.exists(target_fold_dir):
            os.mkdir(target_fold_dir)

        luna16_fold_dir = luna16_dir + 'subset%d/' % i
        # 子文件夹下面所有的mhd文件路径 list
        mhd_file_paths = glob(luna16_fold_dir + '*.mhd')

        for mhd_file_path in mhd_file_paths:
            # 对于每一个mhd文件，即对于每一个病人
            # mhd文件名，即patient_id，对应于候选点信息中的seriesuid
            patient_id = mhd_file_path.split('/')[-1].split('.mhd')[0]
            #该病人数据保存的路径
            target_patient_path = target_fold_dir + patient_id + '/'
            if not os.path.exists(target_patient_path):
                os.mkdir(target_patient_path)
            # 使用MHDFile类，读取和操作mhd文件
            mhd_file = MHDFile(mhd_file_path, normalize=False)

            # 当前病人对应的所有候选点
            patient_candidates = candidates[patient_id]

            patient_positive_pkl = []
            patient_negative_pkl = []
            patient_ori_positive_pkl = []

            for candidate in patient_candidates:
                # 对于每一个候选点

                # 根据候选位置中心点的世界坐标获取对应的像素坐标
                center_z, center_y, center_x = mhd_file.get_pix_coord(candidate['coordZ'], candidate['coordY'],
                                                                      candidate['coordX'])
                center = np.array([center_z, center_y, center_x])
                
                # 根据需要的数据块大小，计算起止像素坐标
                start_zyx = center - np.array(config['data']['cubic_shape'][0: 3], dtype=np.int32) // 2
                end_zyx = center + (np.array(config['data']['cubic_shape'][0: 3], dtype=np.int32) + 1) // 2

                row = deepcopy(candidate)
                
                # 获取数据，只需要肺窗，因此返回的数据的通道数为1
                row['data'] = mhd_file.get_cubic(start_zyx, end_zyx, True, False).astype(np.float32)

                # 记录中心点像素坐标
                row['pix_z'], row['pix_y'], row['pix_x'] = center_z, center_y, center_x

                if int(row['class']) == 0:
                    # 加入负样本文件
                    patient_negative_pkl.append(row)
                else:
                    # 如果class为1，则是原始候选点，如果为2则是新采样的候选点
                    if int(row['class']) == 1:
                        # 原始正样本
                        patient_ori_positive_pkl.append(row)
                    row['class'] = 1
                    #新采样的正样本
                    patient_positive_pkl.append(row)

            # 每一个病人的原始正样本、新采样的正样本以及负样本分开保存
            with open(target_patient_path + 'positive.pkl', 'wb') as fp:
                pickle.dump(patient_positive_pkl, fp)

            with open(target_patient_path + 'negative.pkl', 'wb') as fp:
                pickle.dump(patient_negative_pkl, fp)

            with open(target_patient_path + 'ori_positive.pkl', 'wb') as fp:
                pickle.dump(patient_ori_positive_pkl, fp)

            # 保存样本数量信息，方便统计
            num_info = {'positive_num': len(patient_positive_pkl), 'negative_num': len(patient_negative_pkl), 'ori_positive_num': len(patient_ori_positive_pkl)}
            with open(target_patient_path + 'num_info.pkl', 'wb') as fp:
                pickle.dump(num_info, fp)

            print('patient %s:' % target_patient_path)
            print('ori positive num: %d' % num_info['ori_positive_num'])
            print('positive num: %d' % num_info['positive_num'])
            print('negative num: %d' % num_info['negative_num'])

            del patient_positive_pkl
            del patient_negative_pkl
            del patient_ori_positive_pkl
```

## 模型结构

参考LUNA16比赛中第一名的多尺度模型[[4]](#r4)，按照论文中的模型，完全无法复现效果，因此稍微有些修改

代码中定义了FprModel类来封装整个模型：
```python
# fpr_model.py

def step_decay(epoch, lr):
    # 每隔一代，学习速率衰减
    if epoch > 0:
        lr = lr * config['train']['lr_decay']
    print("learnrate: ", lr, " epoch: ", epoch)
    return lr

# def get_froc_monitor(patient_num):
#     def froc_monitor(y_true, y_pred):
#         total = len(y_true)
#         total_P = sum(y_true)
#         total_N = (total - total_P)
#         fprs, tprs, _ = skl_metrics.roc_curve(y_true, y_pred)
#         fps = fprs * total_N / (patient_num)
#         score = 0.0
#         for val in np.interp([0.125, 0.25, 0.5, 1.0, 2.0, 4.0, 8.0], fps, tprs):
#             score += val
#         score /= 7
#         return score

class drawloss(keras.callbacks.Callback):
    '''
    自定义keras回调函数，保存训练过程中的训练集loss和验证集loss变化情况
    '''
    def __init__(self, work_dir, fold_idx):
        self.model_dir = work_dir + ('fold_%d/model/' % fold_idx)
        self.work_dir = work_dir
        self.fold_idx = fold_idx
        self.losses = []
        self.var_losses = []
        # 如果当前训练目录存在loss记录，则读取
        if os.path.exists(self.model_dir + 'loss_log.pkl'):
            with open(self.model_dir + 'loss_log.pkl', 'rb') as fp:
                loss_log = pickle.load(fp)
                self.losses = loss_log['losses']
                self.var_losses = loss_log['var_losses']

    # def on_train_begin(self, logs=None):
    #     pass
    #
    # def on_train_end(self, logs=None):
    #     pass

    def on_epoch_end(self, epoch, logs = None):
        # 每一代结束之后，保存loss记录，并绘更新练集和验证集的loss变化图像
        self.losses.append(logs['loss'])
        self.var_losses.append(logs['val_loss'])
        with open(self.model_dir + 'loss_log.pkl', 'wb') as fp:
            loss_log = {'losses': self.losses, 'var_losses': self.var_losses}
            pickle.dump(loss_log, fp)

        plt.plot(self.losses)
        plt.plot(self.var_losses)
        plt.title('model loss')
        plt.ylabel('loss')
        plt.xlabel('epoch')
        plt.legend(['train', 'val'], loc='upper left')
        plt.savefig(self.work_dir + ("fold_%d/model/" % self.fold_idx) + "train_loss.png")
        plt.close()

class FprModel(object):
    '''
    对完整模型进行封装
    '''
    def __init__(self, load_weight_path = None, train = False, learning_rate = 0.001, gpu_nums = 3):
        # 是否处于训练模式
        self.train = train
        # 使用的gpu个数
        self.gpu_nums = gpu_nums
        # 自动寻找并设置可用gpu，这个函数根据gpu状态来设置os.environ['CUDA_VISIBLE_DEVICES']的值
        set_gpus(num_gpus= self.gpu_nums, auto_growth = True)
        # 构建模型
        self.model = self.build_model(load_weight_path)
        if self.train:
            # 如果是训练状态，则进行模型编译，选择损失函数、优化函数以及观察指标
            # self.model.compile(optimizer=RMSprop(lr=learning_rate),
            #                    loss={"out_class": "binary_crossentropy"},
            #                    metrics={"out_class": [binary_accuracy, binary_crossentropy]})
            self.model.compile(optimizer=SGD(lr = learning_rate, momentum = 0.9, nesterov = True), loss={"out_class": "binary_crossentropy"}, metrics={"out_class": [binary_accuracy, binary_crossentropy]})
    
    def build_model(self, load_weight_path = None) -> Model:
        # 构建模型，本模型使用三个不同尺度的输入
        self.input_d40 = Input((26, 40, 40, 1), name="d_40")
        self.input_d30 = Input((10, 30, 30, 1), name="d_30")
        self.input_d20 = Input((6, 20, 20, 1), name="d_20")
        # 模型网络连接
        self.outputs = self.network(self.input_d40, self.input_d30, self.input_d20)
        
        # 建立keras模型，指定输入和输出
        model = Model(inputs=[self.input_d40, self.input_d30, self.input_d20], outputs = self.outputs)
        print(model.summary())

        # 多GPU分解
        model = to_multi_gpu_nodule_segmented(model, n_gpus = self.gpu_nums)
        
        # 如果指定了初始权重文件，则加载
        if load_weight_path is not None:
            print("load fpr model from: ", load_weight_path)
            print("start loading fpr model...")
            model.load_weights(load_weight_path, by_name = False)
            print("end loading fpr model")

        return model

    def network(self, input_d40, input_d30, input_d20):
        # 模型主要网络结构，三分支，每个分支均为Conv-pool-conv-conv-full结构
        l1 = Conv3D(64, (3, 5, 5), name="conv1_1", strides=(1, 1, 1), padding="same")(input_d40)
        l1 = BatchNormalization()(l1)
        l1 = PReLU()(l1)
        # l1 = Dropout(0.2)(l1)
        l1 = MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), name="pool1", padding="valid")(l1)

        l1 = Conv3D(64, (3, 5, 5), name="conv1_2", strides=(1, 1, 1), padding="same")(l1)
        l1 = BatchNormalization()(l1)
        l1 = PReLU()(l1)
        l1 = Conv3D(64, (3, 5, 5), name="conv1_3", strides=(1, 1, 1), padding="same")(l1)
        l1 = BatchNormalization()(l1)
        l1 = PReLU()(l1)
        l1 = Flatten()(l1)
        l1 = Dense(250)(l1)
        l1 = BatchNormalization()(l1)
        l1 = PReLU()(l1)
        l1 = Dropout(0.2)(l1)

        l2 = Conv3D(64, (3, 5, 5), name="conv2_1", strides=(1, 1, 1), padding="same")(input_d30)
        l2 = BatchNormalization()(l2)
        l2 = PReLU()(l2)
        # l2 = Dropout(0.2)(l2)
        # l2 = MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2), name="pool2", padding="valid")(l2)
        l2 = MaxPooling3D(pool_size=(1, 2, 2), strides=(2, 2, 2), name="pool2", padding="valid")(l2)

        l2 = Conv3D(64, (3, 5, 5), name="conv2_2", strides=(1, 1, 1), padding="same")(l2)
        l2 = BatchNormalization()(l2)
        l2 = PReLU()(l2)
        l2 = Conv3D(64, (3, 5, 5), name="conv2_3", strides=(1, 1, 1), padding="same")(l2)
        l2 = BatchNormalization()(l2)
        l2 = PReLU()(l2)
        l2 = Flatten()(l2)
        l2 = Dense(250)(l2)
        l2 = BatchNormalization()(l2)
        l2 = PReLU()(l2)
        l2 = Dropout(0.2)(l2)

        l3 = Conv3D(64, (3, 5, 5), name="conv3_1", strides=(1, 1, 1), padding="same")(input_d20)
        l3 = BatchNormalization()(l3)
        l3 = PReLU()(l3)
        # l3 = Dropout(0.2)(l3)
        l3 = MaxPooling3D(pool_size=(1, 1, 1), strides=(2, 2, 2), name="pool3", padding="valid")(l3)

        l3 = Conv3D(64, (3, 5, 5), name="conv3_2", strides=(1, 1, 1), padding="same")(l3)
        l3 = BatchNormalization()(l3)
        l3 = PReLU()(l3)
        l3 = Conv3D(64, (1, 5, 5), name="conv3_3", strides=(1, 1, 1), padding="same")(l3)
        l3 = BatchNormalization()(l3)
        l3 = PReLU()(l3)
        l3 = Flatten()(l3)
        l3 = Dense(150)(l3)
        l3 = BatchNormalization()(l3)
        l3 = PReLU()(l3)
        l3 = Dropout(0.2)(l3)

        l = concatenate(inputs=[l1, l2, l3], axis=1)
        l = Dense(200)(l)
        l = BatchNormalization()(l)
        l = PReLU()(l)
        l = Dense(2, name="out_class", activation="softmax")(l)

        return l

    def train_model(self, train_data_gen, train_data_num, eval_data_gen, eval_data_num, batch_size, work_dir = None, fold_idx = 0, model_name = 'fpr', initial_epoch = 0):
        if self.train == False:
            print('this model is not in training mode!!!')
            return
        print('train data num: %d' % train_data_num)
        print('eval data num: %d' % eval_data_num)

        # 保证batch_size大于GPU个数
        if batch_size < self.gpu_nums:
            batch_size = self.gpu_nums
        # monitor：需要监视的值；verbose：信息展示模式，0或1；save_best_only：当设置为True时，将只保存在验证集上性能最好的模型
        # 保存每一代的模型文件
        checkpoint = ModelCheckpoint(work_dir + ("fold_%d/model/" % fold_idx) + model_name  + "_e" + "{epoch:02d}-{val_loss:.4f}.hd5",
                                    monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1) #临时模型文件
        # 保存当前最好（验证集loss最低）的模型
        checkpoint_fixed_name = ModelCheckpoint(work_dir + ("fold_%d/model/" % fold_idx) + model_name  + "_best.hd5",
                                                monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)
        # 学习速率衰减回调函数
        learnrate_scheduler = LearningRateScheduler(step_decay)

        # loss记录回调函数
        draw_loss = drawloss(work_dir, fold_idx)

        # 开始训练
        self.model.fit_generator(generator = train_data_gen, steps_per_epoch = train_data_num // batch_size, epochs=20,
                            validation_data = eval_data_gen,
                            validation_steps = (eval_data_num + batch_size - 1) // batch_size,
                            callbacks=[checkpoint, checkpoint_fixed_name, learnrate_scheduler, draw_loss], initial_epoch = initial_epoch)
        # 保存最终模型
        self.model.save(work_dir + ("fold_%d/model/" % fold_idx) + model_name  + "_end.hd5")
    
    def predict(self, inputs, batch_size):
        # 模型预测
        inputs_num = inputs['d_20'].shape[0]

        if batch_size < self.gpu_nums:
            batch_size = self.gpu_nums
        
        if inputs_num < self.gpu_nums:
            # 如果送入的数据数量不足，则需要进行扩充
            d_40_inputs_shape = inputs['d_40'].shape
            add_zeros = np.zeros([self.gpu_nums - d_40_inputs_shape[0]] + list(d_40_inputs_shape[1:]))
            inputs['d_40'] = np.concatenate([inputs['d_40'], add_zeros], 0)

            d_30_inputs_shape = inputs['d_30'].shape
            add_zeros = np.zeros([self.gpu_nums - d_30_inputs_shape[0]] + list(d_30_inputs_shape[1:]))
            inputs['d_30'] = np.concatenate([inputs['d_30'], add_zeros], 0)

            d_20_inputs_shape = inputs['d_20'].shape
            add_zeros = np.zeros([self.gpu_nums - d_20_inputs_shape[0]] + list(d_20_inputs_shape[1:]))
            inputs['d_20'] = np.concatenate([inputs['d_20'], add_zeros], 0)

        preds = self.model.predict(inputs, batch_size = batch_size)

        if inputs_num < self.gpu_nums:
            # 只取真实数据
            preds = preds[ : inputs_num, : ]
        
        return preds
```

## 模型训练

首先需要为模型训练准备一个数据生成器，这里封装了一个DataSet类来进行数据生成等操作：
```python
# dataset.py

class DataSet(object):
    def __init__(self, train_dir = None):
        # 初始化，读取训练数据集、验证数据集、测试数据集的信息
        self.train_dir = train_dir
        with open(train_dir + 'train_info.pkl', 'rb') as fp:
            self.train_info = pickle.load(fp)

        with open(train_dir + 'test_info.pkl', 'rb') as fp:
            self.test_info = pickle.load(fp)

        with open(train_dir + 'eval_info.pkl', 'rb') as fp:
            self.eval_info = pickle.load(fp)

    # data_type : 'pos' or 'neg'
    # 每次生成单个数据，根据data_type判断读取正样本还是负样本
    def get_data_generator(self, patient_paths, data_type = 'pos', shuffle = False, data_augmentation = False):
        pkl_file_name = ('positive.pkl' if data_augmentation else 'ori_positive.pkl') if data_type == 'pos' else 'negative.pkl'
        while True:
            if shuffle:
                # 如果指定了shuffle为True，则每次都打乱patient_paths
                random.shuffle(patient_paths)
            for patient_path in patient_paths:
                # print('%s, path: %s' % (data_type, patient_path))
                with open(patient_path + pkl_file_name, 'rb') as fp:
                    temp_pkl_file = pickle.load(fp)
                # print('len %d' % len(temp_pkl_file))
                if shuffle:
                    random.shuffle(temp_pkl_file)
                for record in temp_pkl_file:
                    yield record

    def normalize(self, data, mean, std):
        ori_var = np.var(data)
        data /= np.sqrt(ori_var / (std ** 2)).astype(np.float32)

        ori_mean = np.mean(data)
        data -= (ori_mean - mean).astype(np.float32)

        return data

    def extract_cubic(self, record):
        # cubic = np.zeros([26, 40, 40, 1])
        # cubic[:, :, :, :] = record['data'][:, :, :, 0 : 1]
        # 暂时只使用肺窗数据
        cubic= record['data'][:, :, :, 0 : 1]
        #layer_idx = 0
        #print('start')
        #for _ in cubic:
        #    cubic[layer_idx, :, :, 1] = equalize_adapthist(cubic[layer_idx, :, :, 0])
        #    layer_idx += 1
        #print('end')
        return cubic

    def data_augmentation(self, record):
        # 即时数据增强，旋转 90、 180、 270度
        print('data augmentation')
        cubics = []

        # record['data'] = record['data'] * 1400 - 1000

        #1000 times
        # for i in range(-2, 3):
        #     for j in range(-2, 3):
        #         for k in range(-2, 3):
        cubic = record['data'][:, :, :, 0: 1]
                    # cubic = self.normalize(cubic, -600, -300)
                    # max_v = np.max(cubic)
                    # min_v = np.min(cubic)
                    # cubic -= min_v
                    # cubic /= max_v - min_v
                    # for flip_z in [-1, 1]:

        cubic_r = cubic
        cubics.append(cubic_r)
        # rotate 90
        cubic_r = np.rot90(cubic_r, axes = [1, 2])
        cubics.append(cubic_r)

        # rotate 180
        cubic_r = np.rot90(cubic_r, axes=[1, 2])
        cubics.append(cubic_r)

        # rotate 270
        cubic_r = np.rot90(cubic_r, axes=[1, 2])
        cubics.append(cubic_r)
                    # for flip_y in [-1, 1]:
                    #     for flip_x in [-1, 1]:
                    #         # cubic_f = cubic[::flip_z, ::flip_y, ::flip_x, :]
                    #         cubic_f = cubic[:, ::flip_y, ::flip_x, :]
                    #         cubics.append(cubic_f)
        print('data augmentation end')
        # print('cubic len %d' % len(cubics))
        return cubics

    def get_train_data_generator(self, batch_size, shuffle_num = 0, data_augmentation = False):
        # 每次生成batch_size数量的数据
        # 首先读取shuffle_num数量的数据，然后进行shuffle，再按照batch_size数量分批输出
        
        # 正样本使用新采样的数据，因此指定data_augmenttation参数
        pos_gen = self.get_data_generator(self.train_info['patient_paths'], 'pos', shuffle = True, data_augmentation = data_augmentation)

        # 负样本生成器
        neg_gen = self.get_data_generator(self.train_info['patient_paths'], 'neg', shuffle = True)
        shuffle_list = []
        shuffle_list_num = 0
        batch_data = {'inputs': [], 'out_class': []}
        batch_data_num = 0

        if shuffle_num < batch_size:
            shuffle_num = 2 * batch_size
        if data_augmentation:
            # 如果有数据扩充，那么shuffle_num会稍微有些变化
            print('real shuffle num: %d' % ( shuffle_num // 2 + ((shuffle_num // 2 + config['data']['positive_augmente_times'] - 1) // config['data']['positive_augmente_times']) * config['data']['positive_augmente_times']))
        else:
            print('real shuffle num: %d' % shuffle_num)
        while True:

            label = [1, 0]
            # 收集一半负样本
            while shuffle_list_num < shuffle_num // 2:
                record = next(neg_gen)
                data = {'inputs': self.extract_cubic(record), 'out_class': label}
                shuffle_list.append(data)
                shuffle_list_num += 1

            label = [0, 1]
            # 收集一半正样本
            while shuffle_list_num < shuffle_num:
                record = next(pos_gen)

                if data_augmentation:
                    cubics = self.data_augmentation(record)
                    for cubic in cubics:
                        shuffle_list.append({'inputs': cubic, 'out_class': label})
                    shuffle_list_num += len(cubics)
                else:
                    data = {'inputs': self.extract_cubic(record), 'out_class': label}
                shuffle_list.append(data)
                shuffle_list_num += 1

            random.shuffle(shuffle_list)

            # 按照batch_size，分批生成数据
            for data in shuffle_list:
                batch_data['inputs'].append(data['inputs'])
                batch_data['out_class'].append(data['out_class'])
                batch_data_num += 1
                if batch_data_num == batch_size:
                    yield {'d_40': np.array(batch_data['inputs']), 'd_30': np.array(batch_data['inputs'])[:, 8 : 18, 5 : 35, 5 : 35, :], 'd_20': np.array(batch_data['inputs'])[:, 10 : 16, 10 : 30, 10 : 30, :]}, {'out_class': np.array(batch_data['out_class'])}
                    batch_data = {'inputs': [], 'out_class': []}
                    batch_data_num = 0

            shuffle_list = []
            shuffle_list_num = 0

    def get_eval_data_generator(self, batch_size):
        # 验证集不用shuffle，而且不使用数据扩充
        pos_gen = self.get_data_generator(self.eval_info['patient_paths'], 'pos', shuffle=False)
        neg_gen = self.get_data_generator(self.eval_info['patient_paths'], 'neg', shuffle=False)
        batch_data = {'inputs': [], 'out_class': []}
        batch_data_num = 0
        while True:
            label = [0, 1]
            # 按照正样本数量，读取正样本
            for i in range(self.eval_info['ori_positive_num']):
                record = next(pos_gen)
                batch_data['inputs'].append(self.extract_cubic(record))
                batch_data['out_class'].append(label)
                batch_data_num += 1
                if batch_data_num == batch_size:
                    yield {'d_40': np.array(batch_data['inputs']), 'd_30': np.array(batch_data['inputs'])[:, 8 : 18, 5 : 35, 5 : 35, :], 'd_20': np.array(batch_data['inputs'])[:, 10 : 16, 10 : 30, 10 : 30, :]}, {'out_class': np.array(batch_data['out_class'])}
                    batch_data = {'inputs': [], 'out_class': []}
                    batch_data_num = 0

            label = [1, 0]
            # 按照负样本数量读取负样本
            for i in range(self.eval_info['negative_num']):
                record = next(neg_gen)
                batch_data['inputs'].append(self.extract_cubic(record))
                batch_data['out_class'].append(label)
                batch_data_num += 1
                if batch_data_num == batch_size:
                    yield {'d_40': np.array(batch_data['inputs']), 'd_30': np.array(batch_data['inputs'])[:, 8 : 18, 5 : 35, 5 : 35, :], 'd_20': np.array(batch_data['inputs'])[:, 10 : 16, 10 : 30, 10 : 30, :]}, {'out_class': np.array(batch_data['out_class'])}
                    batch_data = {'inputs': [], 'out_class': []}
                    batch_data_num = 0

    def get_test_data_generator(self, batch_size):
        # 获取测试集数据，并且带上样本相关的坐标信息
        batch_data = {'inputs': []}
        nodule_infos = []
        batch_data_num = 0
        if batch_size < 1:
            batch_size = 1
        for patient_path in self.test_info['patient_paths']:
            # 对于测试集中的每一个病人
            # 获取病人seriesuid即patient_id
            seriesuid = patient_path.split('/')[-2]
            print('path: %s' % (patient_path))
            # 读取该病人的原始正样本数据
            with open(patient_path + 'ori_positive.pkl', 'rb') as fp:
                temp_pkl_file = pickle.load(fp)
            for record in temp_pkl_file:
                batch_data['inputs'].append(self.extract_cubic(record))
                del record['data']
                record['label'] = record['class']
                del record['class']
                record['seriesuid'] = seriesuid
                nodule_infos.append(record)
                batch_data_num += 1
                if batch_data_num == batch_size:
                    yield {'d_40': np.array(batch_data['inputs']), 'd_30': np.array(batch_data['inputs'])[:, 8 : 18, 5 : 35, 5 : 35, :], 'd_20': np.array(batch_data['inputs'])[:, 10 : 16, 10 : 30, 10 : 30, :]}, nodule_infos
                    batch_data = {'inputs': []}
                    nodule_infos = []
                    batch_data_num = 0
            # 读取该病人的原始负样本数据
            with open(patient_path + 'negative.pkl', 'rb') as fp:
                temp_pkl_file = pickle.load(fp)
            for record in temp_pkl_file:
                batch_data['inputs'].append(self.extract_cubic(record))
                del record['data']
                record['label'] = record['class']
                del record['class']
                record['seriesuid'] = seriesuid
                nodule_infos.append(record)
                batch_data_num += 1
                if batch_data_num == batch_size:
                    yield {'d_40': np.array(batch_data['inputs']), 'd_30': np.array(batch_data['inputs'])[:, 8 : 18, 5 : 35, 5 : 35, :], 'd_20': np.array(batch_data['inputs'])[:, 10 : 16, 10 : 30, 10 : 30, :]}, nodule_infos
                    batch_data = {'inputs': []}
                    nodule_infos = []
                    batch_data_num = 0
        if batch_data_num:
            yield {'d_40': np.array(batch_data['inputs']), 'd_30': np.array(batch_data['inputs'])[:, 8 : 18, 5 : 35, 5 : 35, :], 'd_20': np.array(batch_data['inputs'])[:, 10 : 16, 10 : 30, 10 : 30, :]}, nodule_infos

    def get_train_data_num(self):
        # print(self.train_info['positive_num'])
        # 正样本数量乘上增强倍数
        return self.train_info['positive_num'] * config['data']['positive_augmente_times']+ self.train_info['negative_num']

    def get_eval_data_num(self):
        return self.eval_info['ori_positive_num'] + self.eval_info['negative_num']

    def get_test_data_num(self):
        return self.test_info['ori_positive_num'] + self.test_info['negative_num']

    def show_some_img(self, patient_path, zs, start = 0, end = 100, mode = 'pos'):
        # 随机查看一些pkl文件中的样本，用于测试数据打包代码的正确性
        if mode == 'pos':
            with open(patient_path + 'positive.pkl', 'rb') as fp:
                pkl_file = pickle.load(fp)
        elif mode == 'ori_pos':
            with open(patient_path + 'ori_positive.pkl', 'rb') as fp:
                pkl_file = pickle.load(fp)
        else:
            with open(patient_path + 'negative.pkl', 'rb') as fp:
                pkl_file = pickle.load(fp)
        target_dir = patient_path + 'example_img/'
        if not os.path.exists(target_dir):
            os.mkdir(target_dir)
        for i in range(start, end):
            # max_val = np.max(pkl_file[i]['data'])
            # min_val = np.min(pkl_file[i]['data'])
            # pkl_file[i]['data'] -= min_val
            # pkl_file[i]['data'] /= (max_val - min_val)
            for z in zs:
                cv2.imwrite(target_dir + 'img%d_%d_f.png' % (i, z), pkl_file[i]['data'][z, :, :, 0] * 255)
                cv2.imwrite(target_dir + 'img%d_%d_z.png' % (i, z), pkl_file[i]['data'][z, :, :, 1] * 255)
```

数据准备和数据生成器已经完成，接下来开始模型的训练和验证：

首先初始化训练信息，按照十折交叉验证，划分所有数据集
```python
# train.py
def init_train_dir(train_dir, data_dir, fold_num):
    data_info = {}
    # 对每一个fold里面的数据进行统计键值为fold_0~fold_9
    for data_fold in ['fold_%d' % fold_idx for fold_idx in range(0, 10)]:
        patient_paths = glob(data_dir + data_fold + '/*')
        data_info[data_fold] = {'patient_paths': [patient_path + '/' for patient_path in patient_paths], 'positive_num': 0, 'negative_num': 0, 'ori_positive_num': 0, 'patient_num': 0}
        for patient_path in data_info[data_fold]['patient_paths']:
            # 对于每个病人的文件夹，直接读取数据数量信息文件
            # if not os.path.isdir(patient_path):
            #     continue
            data_info[data_fold]['patient_num'] += 1
            with open(patient_path + 'num_info.pkl', 'rb') as fp:
                num_info_file = pickle.load(fp)
            data_info[data_fold]['positive_num'] += num_info_file['positive_num']
            data_info[data_fold]['negative_num'] += num_info_file['negative_num']
            data_info[data_fold]['ori_positive_num'] += num_info_file['ori_positive_num']

    for i in range(0, fold_num):
        # 十折，一共划分十次
        # 训练集和验证集和测试集的相关信息，patient_paths列表是所包含的所有病人的路径
        train_info = {'patient_paths': [], 'positive_num': 0, 'negative_num': 0, 'ori_positive_num': 0, 'patient_num': 0}
        eval_info = {'patient_paths': [], 'positive_num': 0, 'negative_num': 0, 'ori_positive_num': 0, 'patient_num': 0}
        test_info = {'patient_paths': [], 'positive_num': 0, 'negative_num': 0, 'ori_positive_num': 0, 'patient_num': 0}
        fold_path = train_dir + 'fold_%d/' % i
        model_save_path = fold_path + 'model/'
        if not os.path.exists(fold_path):
            os.mkdir(fold_path)

        if not os.path.exists(model_save_path):
            os.mkdir(model_save_path)

        for j in range(0, 10):
            if j == i:
                # 将当前fold的数据加入到测试集
                test_info['patient_paths'] += data_info['fold_%d' % j]['patient_paths']
                test_info['ori_positive_num'] += data_info['fold_%d' % j]['ori_positive_num']
                test_info['positive_num'] += data_info['fold_%d' % j]['positive_num']
                test_info['negative_num'] += data_info['fold_%d' % j]['negative_num']
                test_info['patient_num'] += data_info['fold_%d' % j]['patient_num']
            elif j == (i + 1) % 10:
                # 将当前fold的数据加入到验证集
                eval_info['patient_paths'] += data_info['fold_%d' % j]['patient_paths']
                eval_info['ori_positive_num'] += data_info['fold_%d' % j]['ori_positive_num']
                eval_info['positive_num'] += data_info['fold_%d' % j]['positive_num']
                eval_info['negative_num'] += data_info['fold_%d' % j]['negative_num']
                eval_info['patient_num'] += data_info['fold_%d' % j]['patient_num']
            else:
                # 将当前fold的数据加入到训练集
                #print(data_info['fold_%d' % j]['positive_num'])
                train_info['patient_paths'] += data_info['fold_%d' % j]['patient_paths']
                train_info['ori_positive_num'] += data_info['fold_%d' % j]['ori_positive_num']
                train_info['positive_num'] += data_info['fold_%d' % j]['positive_num']
                train_info['negative_num'] += data_info['fold_%d' % j]['negative_num']
                train_info['patient_num'] += data_info['fold_%d' % j]['patient_num']
        print(train_info['positive_num'])
        #if i == 1:
        #    print(test_info)
        with open(fold_path + 'train_info.pkl', 'wb') as fp:
            pickle.dump(train_info, fp)
        with open(fold_path + 'eval_info.pkl', 'wb') as fp:
            pickle.dump(eval_info, fp)
        with open(fold_path + 'test_info.pkl', 'wb') as fp:
            pickle.dump(test_info, fp)
```

训练需要的相关信息准备好之后，开始训练模型
```python
# train.py

def train_fpr_model(work_dir, fold_idx, initial_epoch = 0, batch_size = 64, initial_lr = 0.001, model_name = 'fpr', num_gpus = 3):
    train_dir = work_dir + 'fold_%d/' % fold_idx
    dataset = DataSet(train_dir = train_dir)
    model_path = None
    if initial_epoch > 0:
        # 如果要接着上次训练，那么首先查找上次训练的模型文件
        model_paths = glob(train_dir + 'model/%s_e%02d*.hd5' % (model_name, initial_epoch))
        if len(model_paths) != 1:
            print('cannot found model save file!!!!')
            assert(False)
        else:
            model_path = model_paths[0]
        # 因为有学习速率衰减，因此如果接着训练，那么需要调整初始学习速率
        initial_lr = initial_lr * (config['train']['lr_decay'] ** (initial_epoch - 1))
    else:
        initial_epoch = 0
    
    # 初始化模型
    fpr_model = FprModel(model_path, True, initial_lr, num_gpus)
    # 获取训练集数据生成器
    train_data_gen = dataset.get_train_data_generator(batch_size = batch_size, shuffle_num = 10000, data_augmentation=True)
    # 训练集样本数量
    train_data_num = dataset.get_train_data_num()
    # 获取验证集数据生成器
    eval_data_gen = dataset.get_eval_data_generator(batch_size = batch_size)
    # 验证集样本数量
    eval_data_num = dataset.get_eval_data_num()
    # 开始训练
    fpr_model.train_model(train_data_gen, train_data_num, eval_data_gen, eval_data_num, batch_size, work_dir, fold_idx = fold_idx, model_name = model_name, initial_epoch = initial_epoch)
```

<span id = 'r1'>[1]</span> Armato III, Samuel G., McLennan, Geoffrey, Bidaut, Luc, McNitt-Gray, Michael F., Meyer, Charles R., Reeves, Anthony P., … Clarke, Laurence P. (2015). Data From LIDC-IDRI. The Cancer Imaging Archive.
<span id = 'r2'>[2]</span> Armato SG III, McLennan G, Bidaut L, McNitt-Gray MF, Meyer CR, Reeves AP, Zhao B, Aberle DR, Henschke CI, Hoffman EA, Kazerooni EA, MacMahon H, van Beek EJR, Yankelevitz D, et al.:  The Lung Image Database Consortium (LIDC) and Image Database Resource Initiative (IDRI): A completed reference database of lung nodules on CT scans. Medical Physics, 38: 915--931, 2011. 
<span id = 'r3'>[3]</span> Clark K, Vendt B, Smith K, Freymann J, Kirby J, Koppel P, Moore S, Phillips S, Maffitt D, Pringle M, Tarbox L, Prior F. The Cancer Imaging Archive (TCIA): Maintaining and Operating a Public Information Repository, Journal of Digital Imaging, Volume 26, Number 6, December, 2013, pp 1045-1057. 
<span id = 'r4'>[4]</span> Dou Q, Chen H, Yu L, et al. Multi-level Contextual 3D CNNs for False Positive Reduction in Pulmonary Nodule Detection[J]. IEEE Transactions on Biomedical Engineering, 2017, 64(7):1558-1567.
