---
title: 学习笔记_2019-02
date: 2019-02-13 15:16:58
tags: [学习笔记，杂项]
mathjax: true
---

# 2019-02-13

## 支持向量机
首先复习了数学上的一个小知识，如果有一个超平面可以表达为如下线性方程：
$$
\begin{aligned}
    w^Tx+b = 0
\end{aligned}
$$
则空间中任意一点$x_0$到该超平面的距离可以表示为如下：
$$
\begin{aligned}
    r = \frac{|w^Tx_0+b|}{||w||}
\end{aligned}
$$

在分类问题的样本空间中，令$x_i$和$y_i$分别表示某个样本和其对应的标签，支持向量机的基本思想是求出一个满足如下条件的超平面：
$$
\begin{cases}
    w^Tx_i + b \ge 1 & y_i = 1\\
    w^Tx_i + b \le 1 & y_i = -1
\end{cases}
$$
示意图如下,这里z展示的样本的特征空间为2维，图中被圈出来的满足上式等号条件一个正样本和两个负样本被称为支持向量，$\gamma=\frac{2}{||w||}$称为间隔。

{% asset_img 支持向量机超平面示意.png 支持向量机超平面示意%}

找分类超平面的问题就可以转化为如下：
$$
\begin{aligned}
    &\max_{w,b}\frac{2}{||w||}\\
    &s.t.\quad y_i(w^Tx_i + b) \ge 1, i = 1,2,...,m\\
    等价为\\
    &\min_{w,b}\frac{||w||^2}{2}\\
    &s.t.\quad y_i(w^Tx_i + b) \ge 1, i = 1,2,...,m
\end{aligned}
$$
这里可以使用不等式约束下的拉格朗日乘子法转换为对偶问题进行求解。

支持向量机的基本型就是希望找到一个间隔最大的超平面，但是这里有个问题，在原始的特征空间中，样本不一定线性可分，因此引入一个函数，将原始特征空间映射到一个高维的特征空间，使得在这个高维空间中线性可分，这样即可将任何分类问题转换成线性可分的形式，但是这样也有一个问题，设映射后的特征向量为$\phi(x)$，求解超平面的过程中，会遇到${\phi(x)}^T\phi(x)$的情况，因为$\phi(x)$的维数可以为无穷，所以这里的计算无法进行，因此需要设想一个这样的函数：$k(x_i,x_j)={\phi(x_i)}^T\phi(x_j)$使得不需要通过$\phi(x)$来计算${\phi(x)}^T\phi(x)$，而是直接通过$k(x_i,x_j)$来表示，这个函数即称为核函数。

核函数可以用一个核矩阵来表示如下，任何一个对称的半正定矩阵，都可以作为核矩阵。
$$
k=\begin{bmatrix}
    k(x_1,x_1) &\cdots &k(x_1,x_j) &\cdots &k(x_1,x_n)\\
    \vdots &\ddots &\vdots &\ddots &\vdots\\
    k(x_j,x_1) &\cdots &k(x_j,x_j) &\cdots &k(x_j,x_n)\\
    \vdots &\ddots &\vdots &\ddots &\vdots\\
    k(x_n,x_1) &\cdots &k(x_n,x_j) &\cdots &k(x_n,x_n)\\
\end{bmatrix}
$$

选择一个核函数，就隐式的确定了映射的特征空间，因此核函数的选择非常重要。

但是在实际使用中，选出一个合适的特征空间恰好让这个样本线性可分非常不容易，因此将优化目标重写为：
$$
\min_{w,b}\frac{1}{2}{||w||}^2+C\sum_{i=1}^ml(y_i(w^Tx_i+b) - 1)
$$
这里的$C$为一个设定好的常数，决定模型的偏好，$l$为一种损失函数。这样的支持向量机被称为软间隔支持向量机。

# 2019-02-19
## 对拉格朗日乘子法的理解
前置知识：梯度的方向是当前等值平面的法线方向。约束函数和目标函数相切时，约束函数和目标函数的梯度方向相同。

如下图所示，其中$f(x,y)$是目标函数，$f(x,y) = d_1,f(x,y) = d_2,f(x,y) = d_3$都是目标函数的等值线，
$g(x,y) = c$是约束条件，在约束条件和等值线相切的地方，有$\nabla f = \lambda \nabla g$,这里假设$d_1 < d_2 < d_3$且所求问题为
$$
\min f(x,y) \\
s.t. g(x,y)=c
$$
那么其最优解一定出现在相切的地方，即只要求如下联立方程即可得到最优解的可能位置。

$$
\begin{cases}
    \nabla f = \lambda \nabla g \\
    g(x,y) = 0
\end{cases}
$$

但是一般看到的拉格朗日乘子法是下面的形式：
$$
\begin{aligned}
    &首先定义F(x,y,\lambda) = f(x,y) + \lambda g(x,y)\\
    &之后求解\nabla F = \mathbf{0}
\end{aligned}
$$

其实是一样的，展开$\nabla F$即可，如下：
$$
\begin{aligned}
    \begin{pmatrix}
        \frac{\partial F}{\partial x}\\
        \\
        \frac{\partial F}{\partial y}\\
        \\
        \frac{\partial F}{\partial \lambda}\\
    \end{pmatrix} &= \begin{pmatrix}
        0\\
        \\
        0\\
        \\
        0\\
    \end{pmatrix}\\
    &即\\
    \nabla f &= -\lambda \nabla g \\
    \frac{\partial F}{\partial \lambda} &= g(x,y)\\
    &= 0\\
\end{aligned}
$$

{%asset_img 约束条件和目标函数的等值线.png 约束条件和目标函数的等值线%}

上面只是单个约束时且约束为等式的情况，如果是多个等式约束，则可以写成

$F(x,y,\lambda_1, \lambda_2,...,\lambda_m) = f(x,y) + \lambda_i g_i(x,y);i = 1, 2, ..., m$

同理求解如下：
$$
\begin{cases}
    \nabla f = \lambda_i \nabla g_i &i=1,2,...m\\
    g_i(x,y) = 0&i=1,2,...m
\end{cases}
$$
也可以写成
$$
\begin{aligned}
    \begin{pmatrix}
        \frac{\partial F}{\partial x}\\
        \\
        \frac{\partial F}{\partial y}\\
        \\
        \frac{\partial F}{\partial \lambda_1}\\
        \vdots\\
        \frac{\partial F}{\partial \lambda_m}\\
    \end{pmatrix} = \begin{pmatrix}
        0\\
        \\
        0\\
        \\
        0\\
        \vdots\\
        0\\
    \end{pmatrix}
\end{aligned}
$$

如果是不等式约束，如下描述：
$$
\begin{aligned}
    \min f(x,y)\\
    s.t.\quad h(x,y) \le 0
\end{aligned}$$
可以分成两种情况讨论，第一种情况是内部解，第二种情况是边界解，如果最优解在约束条件内部，那么约束条件不起作用， 退化为无约束问题，$\nabla f = 0 且 \lambda = 0$，如果在边界上，则满足$h(x,y)=0$因此不论哪种情况，$\mu h(x,y) = 0$恒成立。

这里依旧定义一个如下形式的函数：
$$
\begin{aligned}
    F(x,y,\mu) = f(x,y) + \mu h(x,y)
\end{aligned}
$$

因此可以提出KKT条件的基本形式：
$$
\begin{aligned}
    \begin{cases}
        \begin{pmatrix}
        \frac{\partial F}{\partial x}\\
        \\
        \frac{\partial F}{\partial y}
    \end{pmatrix} &= \begin{pmatrix}
        0\\
        \\
        0
    \end{pmatrix}\\
    h(x,y) &\le 0\\
    \lambda &\ge 0 \\
    \lambda h(x,y) &= 0
    \end{cases}
\end{aligned}
$$
这里的$\lambda \ge 0$是因为要求的是$\min f(x,y)$，如果要求的是$\max f(x,y)$,则使用$\lambda \ge 0$

对于多个等式约束和多个不等式约束的情况：
$$
\begin{aligned}
    &\min f(\mathbf{x})\\
    s.t.\quad &g_j(\mathbf{x}) = 0, j=1,2,...m\\
    s.t.\quad &h_k(\mathbf{x}) \le 0, k=1,2,...n
\end{aligned}
$$
首先定义拉格朗日函数：
$$
L(\mathbf{x},\lambda_1,\lambda_2,...,\lambda_m,\mu_1,\mu_2,...,\mu_n) = f(\mathbf{x}) + \sum_{j=1}^m \lambda_j g_j(\mathbf{x}) + \sum_{k=1}^n \lambda_k h_k(\mathbf{x})
$$
这里直接给出完整的KKT条件：
$$
\begin{cases}
    \nabla_{\mathbf{x}}L = \mathbf{0}\\
    g_j(\mathbf{x})=0 \quad j=1,2,...,m\\
    h_k(\mathbf{x})\le 0 \quad k=1,2,...,n\\
    \mu_k \ge 0 \quad k=1,2,...,n\\
    \mu_k h_k(\mathbf{x})=0 \quad k=1,2,...,n
\end{cases}
$$
求解KKT条件，即可找到约束优化问题的解。

# 2019-02-20
## git的常用操作复习

查看仓库当前状态

     git status 

查看修改内容

     git diff <file>

查看最近的提交

     git log

恢复最近提交

     git reset --[soft|mixed|hard] HEAD~
     soft:仅仅修改HEAD指向
     mixed:在修改HEAD之后，修改暂存区内容
     hard:在修改暂存区内容之后，修改工作区内容
     HEAD~:等效于HEAD^，代表HEAD的上一次提交，可以使用HEAD~100代表HEAD之前100次提交
     HEAD~也可替换为commit id

如果指定恢复文件，则只从最后一次提交恢复暂存区文件。

     git reset file
     等价于 git reset --mixed HEAD file

查看版本库历史和commit id，可以用

     git log

查看版本库所有的改动，可以用

     git reflog

创建和切换分支

     git branch <branch_name> #创建分支
     git chechout <branch_name> #切换分支
     git checkout -b <branch_name> #切换分支，不存在则创建

将当前的暂存区改动压栈并隐藏

     git stash

查看当前的栈区

     git stash list

从栈顶恢复

     git stash apply # 只恢复不删除栈内容
     git stash pop # 恢复并删除栈内容

查看当前分支

     git branch

合并分支

     git merge <branch_name> #合并指定分支到当前分支

如果合并出现冲突，修改冲突文件之后，可以重新add、commit进行提交

删除分支

     git branch -d <branch_name>

关联远程分支和本地分支

     git branch --set-upstream <branch-name> <origin/branch-name>

之后就可以从远程分支pull或者push到远程分支了

关联远程服务器

     git remote add <remote-name> <remote-url>

创建远程分支

     git push <remote-name> <local-branch-name>:<remote-branch-name>

删除远程分支

     git push <remote-name> :<remote-branch-name>

# 2019-02-21

## 对l2正则化的理解

普通的损失函数可以写成如下：
$$
J(w;X,y)
$$

在权重参数上加上l2正则化项之后，损失函数如下：

$$
\begin{aligned}
    &\tilde{J} = \frac{\alpha}{2}w^Tw + J(w;X,y)\\
    &对w的求导：\\
    &\nabla_w\tilde{J} = \alpha w + \nabla_w J(w;X,y)
\end{aligned}
$$

每次的参数更新可以写成如下：

$$
\begin{aligned}
    &w \leftarrow w - \epsilon(\alpha w + \nabla_w J(w;X,y))\\
    &等价于：\\
    &w \leftarrow (1 - \epsilon\alpha) w + \epsilon\nabla_w J(w;X,y)
\end{aligned}
$$

为了分析l2正则化的作用，假定$J(w ^ \ast ; X , y)$是损失函数的极小值。
$ w^\ast$是使损失函数取得极小值的参数，先简单的对损失函数进行二次近似如下，这里没有一次项是因为在函数极小值的地方，一次导数应该为0。

$$
\begin{aligned}
    J(w;X,y) = J(w^\ast;X,y) + (w-w^\ast)^TH(w-w^\ast)
\end{aligned}
$$

对加上l2正则化项的损失函数近似形式如下：

$$
\begin{aligned}
    \hat{J}(w;X,y) = J(w^\ast;X,y) + (w-w^\ast)^TH(w-w^\ast) + \alpha\frac{1}{2}w^Tw
\end{aligned}
$$

对损失函数进行求导如下。

$$
\begin{aligned}
    \nabla_w \hat{J}(w;X,y) = \alpha w + H(w - w^\ast)
\end{aligned}
$$

当$\nabla_w\hat{J}(w;X,y) = 0$时，有如下推导：

$$
\begin{aligned}
    \alpha w + H(w - w^\ast) = 0\\
    (H + \alpha I)w = Hw^\ast\\
    w = (H + \alpha I)^{-1}Hw^\ast
\end{aligned}
$$

可见当$\alpha \rightarrow 0$时，$w \rightarrow w^\ast$，进一步，因为$H$是实对称矩阵，因此必定可以正交对角化，$H=Q \Lambda Q^T$，因此进一步推导如下：

$$
\begin{aligned}
    w &= (Q \Lambda Q^T+ \alpha I)^{-1}Q \Lambda Q^Tw^\ast\\
    &=[Q (\Lambda+ \alpha I) Q^T]^{-1}Q \Lambda Q^Tw^\ast\\
    &=Q (\Lambda+ \alpha I)^{-1}\Lambda Q^Tw^\ast
\end{aligned}
$$

其中，假设$\Lambda$如下：

$$
\begin{bmatrix}
    &\lambda_1 &0 &0 &\cdots &0\\
    &0 &\lambda_2 &0 &\cdots &0\\
    &0 &0 &\lambda_3 &\cdots &0\\
    &\vdots &\vdots &\vdots &\ddots &\vdots\\
    &0 &0 &0 &\cdots &\lambda_n\\
\end{bmatrix}
$$

则$(\Lambda+ \alpha I)^{-1}\Lambda$如下：
$$
\begin{bmatrix}
    &\frac{\lambda_1}{\lambda_1 + \alpha} &0 &0 &\cdots &0\\
    &0 &\frac{\lambda_2}{\lambda_2 + \alpha} &0 &\cdots &0\\
    &0 &0 &\frac{\lambda_3}{\lambda_3 + \alpha} &\cdots &0\\
    &\vdots &\vdots &\vdots &\ddots &\vdots\\
    &0 &0 &0 &\cdots &\frac{\lambda_n}{\lambda_n + \alpha}\\
\end{bmatrix}
$$
这就相当于在原损失函数极小值点的Hession矩阵$H$的特征向量方向上，将$w^\ast$进行了缩放，而且特征值$\lambda_i$越小的方向，$\alpha$对其影响越大，缩小得越大，即加入l2正则化项之后，模型参数倾向于对损失函数影响显著的方向偏移。

以最小二乘线性回归为例，其损失函数如下：

$$
(Xw-y)^T(Xw-y)
$$

如果加上l2正则化，则损失函数变成如下形式：

$$
(Xw-y)^T(Xw-y) + \frac{1}{2}\alpha w^Tw
$$

那么线性回归的解就从：

$$
w = (X^TX)^{-1}X^Ty
$$

变成了：
$$
w = (X^TX + \alpha I)^{-1}X^Ty
$$

其中$X$可以写成如下,其中$x_{ij}$表示第i个样本$x_i$的第j维：
$$
X=\begin{bmatrix}
    &x_{11} &x_{12} &\cdots &x_{1m}\\
    &x_{21} &x_{22} &\cdots &x_{2m}\\
    &\vdots &\vdots &\ddots &\vdots\\
    &x_{n1} &x_{n1} &\cdots &x_{nm}\\
\end{bmatrix}
$$

则$X^TX$可以表示如下：
$$
X^TX=\begin{bmatrix}
    &\sum_i^nx_{i1}x_{i1} &\sum_i^nx_{i1}x_{i2} &\cdots &\sum_i^nx_{i1}x_{im}\\
    &\sum_i^nx_{i2}x_{i1} &\sum_i^nx_{i2}x_{i2} &\cdots &\sum_i^nx_{i2}x_{im}\\
    &\vdots &\vdots &\ddots &\vdots\\
    &\sum_i^nx_{im}x_{i1} &\sum_i^nx_{im}x_{i2} &\cdots &\sum_i^nx_{im}x_{im}\\
\end{bmatrix}
$$

$X^TX$同样可以正交对角化，$X^TX = Q \Lambda Q^T$，这里的$\Lambda$对角线上的值是$X$奇异值的平方，之后的推导和上面相同，可见，加入l2正则化项之后，模型参数倾向于对损失函数影响显著的方向偏移，从而忽略掉数据中的一些干扰，增强模型泛化能力。

## 记录几个简单的矩阵求导公式
$$
\frac{\partial \beta^T X}{\partial X} = \beta\\
\quad\\
\frac{\partial X^T X}{\partial X} = X\\
\quad\\
\frac{\partial X^T A X}{\partial X} = (A+A^T)X\\
$$