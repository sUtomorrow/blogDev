---
title: 学习笔记_2019-01
date: 2019-02-13 15:06:42
tags: [学习笔记，杂项]
mathjax: true
---
# 2019-01-20

## 学习了如何在hexo的markdown中使用数学公式
首先修改next的配置文件如下：

     mathjax:
        enable: true
        per_page: true

由于开启了pre_page，因此首先需要在markdown页面中使用：

     mathjax: true

写法和latex中的公式差不多，如：
     ```latex
        $\sum_{i=0}^{n}x_i$
     ```
显示效果：$\sum_{i=0}^{n}x_i$

但是在使用中碰到了数学公式渲染不正确的问题，网上说是hexo默认的渲染包有问题，因此需要修改。
首先在博客根目录使用以下命令，卸载原来的渲染包，安装新的。

     npm uninstall hexo-renderer-marked --save
     npm install hexo-renderer-kramed --save

这里需要注意的是一定要在博客根目录下打开控制台，才会安装到博客的node_modules目录中，否则不起作用。

之后还需要修改kramed的rule文件

     修改node_modules/kramed/lib/rules/inline.js
     第11行: escape: /^\\([\\`*{}\[\]()#$+\-.!_>])/,
     替换为: escape: /^\\([`*\[\]()#$+\-.!_>])/,
     第20行: em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,
     替换为: em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,

修改之后，一切正常。语法与latex公式基本相同，详细参考[latex数学公式](https://www.luogu.org/blog/IowaBattleship/latex-gong-shi-tai-quan)

## 《Deep learning》读书笔记
### 极大似然与对数极大似然与KL散度
极大似然估计表示如下：

$$\begin{aligned}
\theta_{ML}  & = \mathop{\arg\max}_\theta p_{model}(\mathbb{X}; \theta)\\
& = \mathop{\arg\max}_\theta \prod_{i=1}^m p_{model}(x^{(i)};\theta)
\end{aligned}
$$

由于这样的连乘容易造成下溢，因此可以替换为对数极大似然估计：

$$
\begin{aligned}
\theta_{ML} & = \mathop{\arg\max}_\theta\sum_{i=1}^m\log p_{model}(x^{(i)};\theta)
\end{aligned}
$$

将训练数据看做一种经验分布$\hat p_{data}$，且因整体缩放不会影响$\mathop{\arg\max}$操作，因此后面的项可以用期望表示，对数似然估计可以用下面的式子表达：

$$
\begin{aligned}
\theta_{ML} & = \mathop{\arg\max}_\theta \mathbb{E}_{\mathbf{x}\sim \hat p_{data}}\log p_{model}(x^{(i)};\theta)
\end{aligned}
$$

这其实就相当于最小化KL散度，KL散度的定义如下：

$$
\begin{aligned}
    D_{KL}(\hat p_{data} \| p_{model}) & = \mathbb{E}_{\mathbf{x} \sim \hat p_{data}}[\log \hat p_{data}(x) - \log p_{model}(x)]
\end{aligned}
$$

其中有最后一项的期望:$-\mathbb{E}_{\mathbf{x} \sim \hat p_{data}} \log p_{model}(x)$即是负的对数似然。

# 2019-01-21

## 在vscode预览markdown时渲染数学公式
只需要安装'Markdown+Math'这个插件就OK了。

## 尝试解析keras保存的参数hd5文件
经过尝试，发现keras保存的参数文件结构如下：最上层有两个键:'optimizer_weights'和'model_weights'，其中'optimizer_weights'是优化器参数，这里不关心，第二个键有关于模型
权重的信息。

'model_weights'包含attrs属性，其下又会有三个键:'layer_names','backend','keras_version'。 
重要的是其中的'layer_names',这个下面需要包含所有层名，字节数组的形式。

'model_weights'下所有层名作为键值，每个键值都有attrs属性，attrs属性下有键值'weight_names'，包括所有的权重参数名，字节数组形式。

# 2019-01-22

## 空洞卷积(也叫膨胀卷积,Dilated Convolution)
空洞卷积的数学定义如下：
如果$F:\mathbb{Z}^2\rightarrow\mathbb{R}$是一个离散函数，定义一个变量域$\Omega_r = [-r, r]^2 \cap\mathbb{Z}^2$再定义一个大小为$(2r+1)^2$的离散卷积$k:\Omega_r\rightarrow \mathbb{R}$,那么卷积操作可以表示为：

$$
\begin{aligned}
(F \ast k)(p) = \sum_{s+t=p}F(s)k(t)
\end{aligned}
$$

空洞卷积可以表示为：

$$
\begin{aligned}
(F \ast_l k)(p) = \sum_{s+lt=p}F(s)k(t)
\end{aligned}
$$

可见，当$l$为1时，空洞卷积就是普通的卷积。

空洞卷积可以增加感受野，空洞卷积感受野示意图如下，其中(a)图为普通卷积产生的感受野示意,记为$F1$，$3 \times 3$的普通卷积感受野和卷积核大小相同，(b)图为在(a)中的$F1$基础上进行$l$等于2的空洞卷积操作，结果记为$F2$，其感受野变为$7 \times 7$，(c)图为在(b)中$F2$的基础上进行$l$等于4的空洞卷积，其感受野计算为$(4 \ast 2 + 1) \times (4 \ast 2 + 1) = (9 \times 9)$，注意这里的感受野计算是基于逐层卷积的结果，很多博客中没有说明，我看了原文才知道。
{% asset_img 空洞卷积感受野示意图.png 空洞卷积感受野示意图%}

# 2019-01-23
## DenseNet论文阅读
论文地址：[Densely Connected Convolutional Networks](https://arxiv.org/pdf/1608.06993.pdf)

DenseNet模型结构如下。

{%asset_img DenseNet模型结构.png DenseNet模型结构%}

其中$1 \times 1$卷积层称为bottleneck层，用于减少通道个数，DenseBlock由BN-ReLU-Conv($1 \times 1$)-BN-ReLU-Conv($3 \times 3$)这样的结构重复而组成，如果一个DenseBlock中每一个$3 \times 3$的卷积输出通道个数是$k$，那么作者建议设置的bottleneck层输出通道个数为$4k$，使用了bottleneck层的DenseNet称为DenseNet-B。

之后的TransitionLayer会进一步压缩模型的通道个数，其输出通道个数为$\theta m$，其中m为DenseBlock的输出通道数。而$0 < \theta \le 1$，如果$0 < \theta < 1$那么称为DenseNet-C。

作者的实验中，最前面的一个卷积层输出通道数为$2k$，设置$\theta=0.5$并且使用了bottleneck层，因此称其模型为DenseNet-BC。训练过程中，使用SGD，初始学习率为0.1在30代和60代的时候分别除以10，训练batch_size：256，一共训练90代。

论文中解释说Densenet的提出是希望解决深层网络带来的梯度消失和梯度爆炸问题，并提对深度学习模型提出了一种新的解释：传统的前向传播模型就像是一种具有状态的算法，每一层读取其前一层的状态(输入)并对其进行处理，修改并保存了认为需要保存的状态，之后传到下一层，而Resnet通过相加处理，显式的保存了前一层的状态，Densenet通过通道连接，不仅保存了前一层的状态，而且还可以加以区分，虽然连接更密集，但是Densenet的模型可以参数相比于Resnet少，因为Densenet在DenseBlock中每一层的卷积核个数可以很少，通过$k$来指定。

## ROI Pooling
ROI Pooling可以根据提供的区域位置信息，将特征图上的位置pooling到一个固定大小的输出。

以一个输出为$2 \times 2$的ROI Pooling为例。

输入为一张特征图。

{%asset_img ROI_Pooling输入.png ROI Pooling输入%}

由区域建议网络给出区域位置。

{%asset_img 区域建议网络给出的位置.png 区域建议网络给出的位置%}

将建议区域划分为$2 \times 2$的区域。

{%asset_img 按照设定的输出大小进行划分.png 按照设定的输出大小进行划分%}

在各个区域内进行Pooling操作(这里是Max Pooling)，得到最终输出。

{%asset_img ROI_Pooling输出.png ROI Pooling输出%}

## 卷积层输出的尺寸计算
$n_{out} = [\dfrac{n_{in} + 2p - k}{s}] + 1$

其中$n_{out}$表示输出的特征图的大小，$n_{in}$表示输入的特征图的大小，$p$表示padding大小，$k$表示卷积核大小，$s$表示stride大小。

## Inception Net
Inception使用了NIN(Network in Network)的思想，网络中的一层不再是单一的卷积，而是几种大小的卷积分支或者几个不同深度的分支进行同时计算，最后在通道维度连接到一起。

Inception在V1版本中(也就是GoogLeNet)使用了$5 \times 5$和$7 \times 7$大小的卷积核，以适应不同尺度的目标识别。

Inception V2版本将V1中的$5 \times 5$和$7 \times 7$卷积拆开成了小卷积的堆叠，减少了计算量的同时，增加了层数。在卷积层之后使用了BN层，添加了辅助分类层，论文中说，辅助分类层在最开始的训练过程中看不出效果，在模型收敛的时候才显现出效果。

Inception V3将V2中的辅助分类层也加上了BN。

Inception V4修改了之前inception模型中最前面的卷积部分(进入Inception Block之前)，将其中的下采样变成了两个分支，一个是步长为2的卷积，一个是池化，最终再Concatenate。

inception v2/v3论文中还提到了设计模型的一些经验原则，我的理解如下：

      1、慎用非常窄的瓶颈层，前馈神经网络中的瓶颈层会减少传播的信息量，如果瓶颈层非常窄，会导致有用信息的丢失，特征图尺寸应该从输入到输出逐渐减小，直到用来完成当前的任务(识别、分类等)。
      2、增加卷积层的卷积核个数可以丰富特征的组合，让特征图通道之间耦合程度更低，使网络加速收敛。
      3、网络开始的几层特征关联性很高，对其进行降维导致的信息损失较小，降维甚至可以加速学习。
      4、平衡网络的宽度和深度。优化网络性能可以认为是平衡每个阶段(层)的卷积核数目和网络深度。同时增加宽度和深度能够提升网络性能。

# 2019-01-25 ~ 2019-01-26
## Pandas库的排序问题
注意到了一个非常坑的地方，关于Pandas库的，例如以下代码。

```python
import pandas as pd
dict1 = {'c': [1, 4, 5, 2, 3]}
dict2 = {'c': [5, 4, 1, 2, 3]}
df1 = pd.DataFrame(dict1)
df2 = pd.DataFrame(dict2)
df1['c'] += df2['c']
print(list(df1['c']))
```
代码中创建了两个DataFrame，然后进行列求和。
这个输出如下，一切正常。

          c
      0   6
      1   8
      2   6
      3   4
      4   6

但是，如果首先对两个DataFrame排序，如下：

```python
import pandas as pd
dict1 = {'c': [1, 4, 5, 2, 3]}
dict2 = {'c': [5, 4, 1, 2, 3]}
df1 = pd.DataFrame(dict1)
df2 = pd.DataFrame(dict2)

df1 = df1.sort_values(by='c')
df2 = df2.sort_values(by='c')
print(df1)
print(df2)
df1['c'] += df2['c']
print(list(df1['c']))
```

这个时候输出就很奇怪了，如下。

         c
      0  1
      3  2
      4  3
      1  4
      2  5
         c
      2  1
      3  2
      4  3
      1  4
      0  5
      [6, 4, 6, 8, 6]

首先是打印的两个DataFrame，常规操作没有任何问题，之后进行了两列的求和，但是求和结果的顺序看不懂了，按照代码里的意思，我希望得到的结果是

      [2, 4, 6, 8, 10]

但是输出的结果是，乍一看，好像还挺顺口！！！

      [6, 4, 6, 8, 6]
这个结果，既不是排序之后相加，也不是原顺序相加(注意这个输出和不排序版本的输出也有差别)。

最后找到了正确解释：先按照对应的index相加，之后再按照df1的index顺序进行输出。

## 全连接层的反向传播算法
之前看过深度神经网络(Deep Neural Network)反向传播的推导，但是没怎么用，现在感觉快忘光了，再来详细的推导一遍。
首先假设损失函数:
$$
loss = J(a^L, y)
$$
其中$a^L$为第$L$层的输出值，且有
$$
\begin{aligned}
a^L & = \sigma(z^L)\\
z^L & = W^L a^{L-1} + b^L\\
z^L & = W^L \sigma(z^{L-1}) + b^L
\end{aligned}
$$
那么损失函数对第$L$层的权重和偏置的偏导为：
$$
\begin{aligned}
\frac{\partial J}{\partial W^L} & = \frac{\partial J}{\partial z^L}\frac{\partial z^L}{\partial W^L}\\
\frac{\partial J}{\partial b^L} & = \frac{\partial J}{\partial z^L}\frac{\partial z^L}{\partial b^L}
\end{aligned}
$$
首先要计算的是$\frac{\partial J}{\partial z^L}$，这一项与损失函数和最后一层的激活函数有关，这里不具体讨论，直接计算即可，并将其结果记为$\delta^L$，之后的$\frac{\partial z^L}{\partial W^L}$项和$\frac{\partial z^L}{\partial b^L}$的计算非常简单，其结果分别为$(a^{L-1})^T$和$\delta^L$。

到这里，第$L$层的偏导就计算完了，那么$L-1$层同理可以如下计算。

$$
\begin{aligned}
\frac{\partial J}{\partial W^{L-1}} & = \frac{\partial J}{\partial z^L}\frac{\partial z^L}{\partial z^{L-1}}\frac{\partial z^{L-1}}{\partial W^{L-1}}\\
& = \delta^L \frac{\partial z^L}{\partial z^{L-1}}(a^{L-2})^T\\
\frac{\partial J}{\partial b^{L-1}} & = \frac{\partial J}{\partial z^L}\frac{\partial z^L}{\partial z^{L-1}}\frac{\partial z^{L-1}}{\partial b^{L-1}}\\
& = \delta^L \frac{\partial z^L}{\partial z^{L-1}}
\end{aligned}
$$

推广开来，若一共有$L$层，为方便表达，定义任意层的$\delta^l$
$$
\delta^l = \begin{cases}
     \frac{\partial J}{\partial z^l}&l=L\\
     \\
     \delta^{l+1} \frac{\partial z^l}{\partial z^{l-1}}&l < L
\end{cases}
$$

则$L-n$层的计算如下：
$$
\begin{aligned}
     \frac{\partial J}{\partial W^{L-n}} & = \delta^{L-n} (a^{L-n-1})^T\\
     \frac{\partial J}{\partial b^{L-n}} & = \delta^{L-n}\\
\end{aligned}
$$

## 卷积层的反向传播
首先要明确数学上的离散卷积和卷积网络中的卷积操作(网上有人也称作互相关)有区别。

对于数学中的二维卷积$Z = K*B$，若$K$的宽度和高度分别为$W_K, H_K$，若$B$的宽度和高度分别为$W_B, H_B$，那么其表达式可以写为:
$$
\begin{aligned}
     Z &= K \ast B\\
     Z_{s,t} &= \sum_{h=0}^{H_K-1}\sum_{w=0}^{W_K-1}K_{h,w}B_{s-h,t-w}
\end{aligned}
$$

而同样条件下，卷积网络卷积操作表示如下：
$$
\begin{aligned}
     Z &= K \ast_{Conv} B\\
     Z_{s,t} &= \sum_{h=0}^{H_K-1}\sum_{w=0}^{W_K-1}K_{h,w}B_{s+h,t+w}
\end{aligned}
$$
数学中的卷积$\ast$可以看做是把$\ast_{Conv}$中的卷积核$K$旋转180度后再进行$\ast_{Conv}$操作。

一般的卷积网络中，卷积层的操作可以表示如下
$$
\begin{aligned}
     z^l_{s,t} &= b^l + \sum_{h=0}^{H_K-1}\sum_{w=0}^{W_K-1}K^l_{h,w}a^{l-1}_{s+h,t+w}\\
     a^l &= \sigma(z^l)
\end{aligned}
$$
其中$a^l$表示第$l$层输出的特征图，是一个三维张量，$W_K$和$H_K$意义同上，$K^l$表示第$l$层的卷积核，二维卷积的卷积核是一个四维张量，前面两维表示位置，后两维是一个用于映射前一层特征向量到下一层特征向量的矩阵。

如果不考虑卷积核的最后两维和特征图的最后一维，第$l$层的卷积核的偏导表示如下，其中$S$和$T$分别为第$z^l$层的高和宽。
$$
\begin{aligned}
     \frac{\partial J}{\partial K^l} &= \frac{\partial J}{\partial z^l}\frac{\partial z^l}{\partial K^l}\\
\end{aligned}
$$
写成逐像素计算的形式，如下:
$$
\begin{aligned}
     \frac{\partial J}{\partial K^l_{h,w}} &= \sum_{s=0}^{S}\sum_{t=0}^{T}\frac{\partial J}{\partial z^l_{s,t}}\frac{\partial z^l_{s,t}}{\partial K^l_{h,w}}
\end{aligned}
$$
其中的第二项如下：
$$
\begin{aligned}
     \frac{\partial z^l_{s,t}}{\partial K^l_{h,w}} &= \frac{b^l + \sum_{h^{'}=0}^{H_K-1}\sum_{w^{'}=0}^{W_K-1}K^l_{h^{'},w^{'}}a^{l-1}_{s+h^{'},t+w^{'}}}{\partial K^l_{h,w}}\\
     &=a^{l-1}_{s+h,t+w}
\end{aligned}
$$
则有：
$$
\begin{aligned}
     \frac{\partial J}{\partial K^l_{h,w}} &= \sum_{s=0}^{S}\sum_{t=0}^{T}\frac{\partial J}{\partial z^l_{s,t}}a^{l-1}_{s+h,t+w}
\end{aligned}
$$

要进一步计算，为了简化，这里和DNN中的反向传播相同，先给出$\delta^l$定义和逐层计算规则如下：

$$
\begin{aligned}
     \delta^l &= \frac{\partial J}{\partial z^l}\\
     \delta^l_{s,t} &= \frac{\partial J}{\partial z^l_{s,t}}
\end{aligned}
$$

因此可以将上面的偏导等式写成如下表示：

$$
\begin{aligned}
     \frac{\partial J}{\partial K^l_{h,w}} &= \sum_{s=0}^{S}\sum_{t=0}^{T}\delta^l_{s,t}a^{l-1}_{s+h,t+w}\\
     &=\delta^l\ast_{Conv}a^{l-1}
\end{aligned}
$$
这里的卷积操作是上面提到的卷积层的卷积(互相关)。

最后的问题就是，如何计算$\delta^l$，推导如下：

$$
\begin{aligned}
     \delta^l &= \frac{\partial J}{\partial z^l}\\
     \delta^{l-1} &= \frac{\partial J}{\partial z^l}\frac{\partial z^l}{\partial z^{l-1}}\\
     &=\delta^l\frac{\partial z^l}{\partial z^{l-1}}\\
     \delta^{l-1}_{s,t} &= \frac{\partial J}{\partial z^l_{s,t}}\\
     &=\sum_{s^{'} = 0}^{S^{'}}\sum_{t^{'} = 0}^{T^{'}}\frac{\partial J}{\partial z^l_{s{'}, t^{'}}}\frac{\partial z^l_{s{'}, t^{'}}}{\partial z^{l-1}_{s,t}}\\
     &=\sum_{s^{'} = 0}^{S^{'}}\sum_{t^{'} = 0}^{T^{'}}\delta^l_{s^{'},t^{'}}\frac{\partial z^l_{s{'}, t^{'}}}{\partial z^{l-1}_{s,t}}\\
     z^l_{s^{'},t^{'}} &= b^l + \sum_{h=0}^{H_K-1}\sum_{w=0}^{W_K-1}K^l_{h,w}a^{l-1}_{s^{'}+h,t^{'}+w}\\
     &=b^l + \sum_{h=0}^{H_K-1}\sum_{w=0}^{W_K-1}K^l_{h,w}\sigma(z^{l-1}_{s^{'}+h,t^{'}+w})\\
     \frac{\partial z^l_{s{'}, t^{'}}}{\partial z^{l-1}_{s,t}} &= K^l_{s-s^{'},t-t^{'}}\sigma^{'}(z^l_{s, t})\\
     \delta^{l-1}_{s,t} &= \sum_{s^{'} = 0}^{S^{'}}\sum_{t^{'} = 0}^{T^{'}}\delta^l_{s^{'},t^{'}}(K^l_{s-s^{'},t-t^{'}}\sigma^{'}(z^l_{s, t}))\\
     &=\sigma^{'}(z^l_{s, t})(\delta^l\ast K^l)
\end{aligned}
$$

如果考虑卷积核的最后两维的话，卷积操作中的乘法应该是向量乘法，这里不详细讨论。

## 池化层的反向传播
池化层的反向传播算法非常简单，因为池化层没有可学习的参数，所以只需要传播$\delta^l$，以便于前面的层计算梯度。

# 2019-01-27

## 无偏性
估计值的均值等于被估计的随机变量:$E(\hat{\alpha}) = \alpha$

## 渐进无偏性
渐进无偏性相比于无偏性的要求要弱一些，是指在样本数趋于无穷大的时候，估计值的期望等于被估计值:$\lim_{n\rightarrow\infty}E(\hat{\alpha}) = \alpha$

## 依概率收敛

$\lim_{n\rightarrow\infty}\mathbb{P}(|X-X_n| \ge\epsilon) = 0$

## 条件极大似然与均方误差
首先回顾下极大似然估计：极大似然估计用于估计数据的分布参数，其对数似然形式的定义为：$\sum_i\log P(x^{(i)};\theta)$。

而在线性回归问题中，给定一个输入$x$，要预测一个$y$。

即需要求出一个函数$\hat{y}(x;w)$，其参数为$w$，给定输入$x$，输出预测值。

假设训练集中样本为$x^{(i)}$，符合独立同分布(i.i.d.)条件，观测标签为$y^{(i)}$。

最小二乘法的思想是直接对$\hat{y}(x;w)$建模并学习参数$w$。
如果使用最小二乘法，学习的过程可以表示如下：
$$
\begin{aligned}
     \mathop{\arg\min}_w\sum_{i}{||y^{(i)} - \hat{y}(x^{(i)};w)||}_2
\end{aligned}
$$

从似然估计的角度，可以对$P(y|x)$建模，借助高斯分布，可以做如下定义：

$$
\begin{aligned}
     P(y|x) = \mathcal{N}(y;\hat{y}(x;w),\sigma^2)
\end{aligned}
$$

这里使用$\hat{y}(x;w)$作为均值，方差则考虑了观测标签中的噪声。

因此在线性回归问题中，使用条件极大似然的方法，给出条件对数似然的形式定义如下：

$$
\begin{aligned}
     \sum_i\log P(y^{(i)}|x^{(i)};w)
\end{aligned}
$$
可以进一步写成：
$$
\begin{aligned}
     \sum_i\log \mathcal{N}(y;\hat{y}(x;w),\sigma^2)
\end{aligned}
$$

学习的过程可以表示成如下：

$$
\begin{aligned}
     \mathop{\arg\max}_{w}\sum_i\log \mathcal{N}(y^{(i)};\hat{y}(x^{(i)};w),\sigma^2)\\
\end{aligned}
$$

展开表示：
$$
\begin{aligned}
     &\mathop{\arg\max}_{w} \sum_i\log (\frac{1}{\sqrt{2\pi}\sigma} e^{ - \frac{(y^{(i)} - \hat{y}(x^{(i)};w))^2}{2\sigma^2}})\\
     &=\mathop{\arg\max}_{w}\sum_i(-\frac{1}{2}\log2\pi - \log \sigma - \frac{(y^{(i)} - \hat{y}(x^{(i)};w))^2}{2\sigma^2})
\end{aligned}
$$

可见这里和最小二乘估计相同，依旧需要小化$\sum_i{(y^{(i)}-\hat{y}(x^{(i)};w))}^2$

因此条件似然估计和最小二乘估计其实最终得到的结果是相同的。

## hexo的一个问题
在latex公式中，如果出现如下的写法，会导致报错(虽然在vscode中预览渲染没有问题)
```latex
{(...)}^2
```
报错如下，说是表达式后面需要逗号，明显是hexo解析有问题，暂时没有解决方案，只能换一种写法，将大括号去掉(不影响公式形式)。

      INFO  Start processing
      FATAL Something's wrong. Maybe you can find the solution here: http://hexo.io/docs/troubleshooting.html
      Template render error: (unknown path) [Line 201, Column 78]parseAggregate: expected comma after expression

# 2019-01-28
## 贝叶斯统计方法和频率统计方法的理解
贝叶斯统计的视角将参数也看做一个随机变量，而频率统计的视角是将参数看做一个固定量。