---
title: 实验日志
date: 2018-08-16 15:04:50
tags: [record]
categories: 实验记录
---

## 假阳性减少

### 独立的假阳性减少
1.最初使用单个三分支卷积网络，不做等方性，三个输入大小为(26， 40， 40)、(10， 30， 30)、(6， 20， 20)，进行了十折交叉验证，FROC评分为80.5

2.(networks.py)尝试使用多模型(res、ince_res、densenet)，并使用等方性，输入大小变为(40， 40， 40)、(32， 32， 32)、(24， 24， 24)，结果求平均的方式进行LUNA16数据集上的训练，单折结果最好的是四个模型(加上之前的不做等方性的VGG模型)求平均，FROC评分为85.2
其中res模型单折FROC评分为76.4
其中den模型单折FROC评分为77.5
其中ince_res模型单折FROC评分为78.1

3.(networks_v2.py)在networks.py的基础上，将原来的不做等方性的三分支VGG网络改为做等方性，并且统一修改其卷积核大小为(3, 3, 3)，修改在network.py的基础上修改res、ince_res、den三个网络的结构，将所有模型最开始的一个(5，5，5)的大卷积核改为两个(3，3，3)的小卷积核，之后使用中心池化代替最大池化下采样，这个版本的网络只有res模型在LUNA16完整数据上进行了训练。
其中res模型单折FROC评分为80.6

4.(networks_v3.py) 在networks_v2.py的基础上，在res、ince_res、den三个网络中的块结构中的卷积块结构中添加调整通道数的(1, 1, 1)大小的卷积(其中den网络没有添加)，并使用PReLU来代替ReLU，在res模型中，还将卷积下采样改为了中心池化
其中vgg模型单折FROC评分为82.6
其中res模型单折FROC评分为 78.9
其中den模型单折FROC评分为85.3.
其中ince_res模型单折FROC评分为78.3
den模型和vgg模型的融合结果FROC评分为85.9
在densenet和vgg模型的结果上，融合hard mining训练的ince_res模型的结果进来，最终FROC可以达到87.8

5.(单分支VGG)尝试使用32大小单分支等方性的Vgg(network_v3.py)模型，训练一折，其中一次池化的Vgg，FROC评分为82.8，稍高于多分支vgg
其中两次池化的Vgg, FROC评分为80.25

6.尝试使用结节检测阶段的假阳性减少的47中最终使用的三个模型来进行单独假阳性减少的实验，最后FROC只有79左右，效果不好，应该是数据量变得太大或者正负样本变得更加不平衡导致。

7.尝试将40 30 20 大小的图片全都resize到20大小，作为三通道输入，在6的网络上进行实验，最终FROC只有77左右。

8.尝试去掉LUNA16那些不在肺分割掩码中的候选点，使用4中的den模型结构进行实验，尚未训练完，第五代FROC80。

9.尝试将空洞卷积加入之前的Vgg模型，并且使用SE结构来可学习的挑选有用的通道，在LUNA16原始数据上进行训练。

10.尝试网格搜索三个模型的集成权重，在测试集上进行步长为0.1的搜索，最好的结果是88.2，如果使用同样的步长在验证集上和训练集上进行搜索，其搜索出的权重用于预测测试集的话不可能超过这个值了。


### 基于结节检测的假阳性减少
1.使用单独的三分支VGG网络，不做等方性变换，输入大小为(26， 40， 40)、(10， 30， 30)、(6， 20， 20)，在结节检测的结果上进行训练，FROC评分只有60左右

2.使用三分支VGG网路，数据输入经过等方性处理，输入大小变为(40， 40， 40)、(32， 32， 32)、(24， 24， 24)，同时将卷积核大小修改为(3， 3， 3)，在结节检测的结果上进行训练，FROC评分80.4

3.(networks_v2.py)尝试修改ince_res、densenet、res三个模型中的前两个卷积层，将单个大核卷积换成多个3 * 3 * 3的小核卷积，并使用中心池化代替最大池化下采样，在结节检测的结果上划分训练集和测试集。
最终最好结果的是三个模型求平均，FROC评分87.2
其中vgg模型单折FROC评分为80.4
其中res模型单折FROC评分为81.2
其中den模型单折FROC评分为82.15
其中ince_res模型单折FROC评分为83.8

4.(networks_v3.py)尝试按照原论文简化ince_res、densenet、res三个模型的结构，并使用PReLU代替原来的ReLU激活函数，并在一些块中添加1 * 1 * 1的降维卷积(其中den网络没有添加)，增加深度同时减少计算量，在res模型中，还将卷积下采样改为了中心池化，尚未开始训练。

5.使用最新结节检测结果(tpr：93， fps： 9)进行假阳性模型的训练，正负样本均扩充108倍，总训练样本约12万。
其中densenet第一折结果FROC评分最高85.2，使用旋转增强测试集后取平均84.5
其中ince_res第一折结果FROC评分最高84.2，使用旋转增强测试集后取平均84.3
其中res第一折结果FROC评分最高83.8，使用旋转增强测试集后取平均84.9
Den和ince_res的最后一折融合后最高为86.0， 使用旋转增强测试集后取平均86.0

6.使用结节检测结果(tpr:97， fps:44)进行假阳性减少模型训练，正样本扩充108倍，负样本过采样，总训练样本30000左右
其中den第一折结果FROC评分最高75.9
其中ince_res第一折结果FROC评分最高65.2
尝试使用单分支Den模型，第一折结果FROC评分76.4

7.使用结节检测结果(tpr:97， fps:44)进行假阳性减少模型训练，正样本扩充108倍，负样本扩充4倍，然后正样本过采样，总训练样本30000左右

8.查看第五种尝试中的dense的结果，将测试集预测的结果可视化，查看预测概率比较低的正样本，其中大部分大小较小，者颜色比较暗，或者与血管粘连，而在预测概率比较高的负样本中，大部分图片中心是颜色较亮的小点或者与血管粘连的暗块，尚无解决办法.

9.使用结节检测结果(tpr：93， fps： 9)，尝试截取24大小的数据块，然后缩放到40大小，正负样本都使用旋转扩充四倍，使用三分支densenet模型，训练十代，FROC:70.9

10.使用结节检测结果(tpr：93， fps： 9)，正负样本均扩充108倍，使用单分支densenet模型模型，但是深度比之前的多分支densenet模型更深，卷积核个数比之前的多分支densenet模型更多。

11.使用结节检测结果(tpr：93， fps： 9)，加上训练集中的正样本，以平衡数据，正负样本均扩充108倍，使用network_v3中的densenet模型进行训练。学习速率衰减加快，之前是0.95/epoch，现在是0.9/epoch。
densenet最好结果为FROC:85.8，相比于之前的85.2要高一些

12.在11的基础上，使用focal loss代替二值交叉熵进行训练，densenet最好结果为FROC:85.2

13.在11的基础上，修改三分支的densenet,将其中的40大小的分支去掉，使用24和32大小的双分支网络进行训练。densenet模型的最好效果为：81.1

14.在11的基础上，修改三分支的densenet,将其中的24大小的分支去掉，使用40和32大小的双分支网络进行训练。densenet模型的最好效果为：84.7

15.在11的基础上，修改三分支的densenet,将其中的32大小的分支去掉，使用24和40大小的双分支网络进行训练。densenet模型的最好效果为：84.3

15.在11的基础上，修改三分支的densenet,将Dropout层去掉，densenet模型的最好效果为：82.9

16.将三分支的Densenet模型分成三个模型，大小分别为10 * 10 * 10，20 * 20 * 20，40 * 40 * 40, 将结节按照直径分成0~5、5~10、10~30三类，分别作为三种模型的正样本，首先训练20的模型，然后10和40的模型以20的模型为基础，进行微调，最终预测时，结节概率取三者最大值。这里有个问题在于三个模型因输入大小不同，因此PReLU层和最后的全连接层参数不能兼容，只能舍弃，所以还尝试用ReLU代替PReLU。使用PReLU的三个模型合在一起的结果为82.5，使用ReLU的结果为83.56

17.在11的基础上，将数据预处理中的肺窗改为-1200 ~ 600(之前是-1000 ~ 400), 尝试用三分支densenet模型进行训练,结果最高只有83.2

18.尝试调试DSSD模型，使用之前训练好的DSSD预测训练集病人的所有不包含结节的层，将结节预测概率大于0.8的作为hard negative样本(之前的训练集样本只在结节周围采样)，尝试将这种样本加入训练，但是效果很差。

19.分析了预检测的结果，发现即使按照LUNA16的标准，预检测的TPR也可以达到95%以上，因此可以考虑跳过DSSD模型，直接在预检测的基础上进行假阳性减少步骤。

20.在19的基础上，首先尝试使用预检测tpr：97%,fps:103的检测结果，在这上面划分训练集、验证集和测试集(7:1:2)，肺窗使用-1200~600，数据预处理做等方性、归一化，减均值操作。正样本数据增强27*4倍，负样本数据增强27倍，使用三分支densenet进行训练，训练了4代，每一代的FROC都在77左右，效果不好，停止训练。

21.在20的基础上，将肺窗改回原来的-1000~400,训练densenet模型,训练了4代，FROC最好为第三代80.9，第四代又降为70，所以停止训练。最高FROC比20中的结果要好，因此实验表明-1000~400的肺窗更适合用于训练和预测。

22.在21的基础上，发现LUNA16的评估过程中，图片块的正中心是否在结节半径内，对评估结果有非常大的影响，而Densenet在全连接层之前使用的全局平均池化层会丢失位置信息，因此尝试将全局平均池化层改为全连接层，训练Densenet模型，训练了5代，第4代FROC最高只有78.9。

23.在22的基础上，尝试不增强负样本，正样本只做旋转增强，训练Densenet模型，训练了6代，第6代FROC最高只有74，尝试使用vgg模型，训练了10代，最高FROC为第七代80

24.从训练过程看，模型在训练集上表现非常好（vgg第十代可以达到99.6的正确率），但是在验证集和测试集上则表现一般（验证集正确率只有92、93左右），因此怀疑模型的泛化能力不足，模型中已经有BatchNormalization和Dropout层可以提高模型泛化能力，进一步还可以尝试添加权重衰减的策略。在22的vgg的基础上，为分支网络的卷积层和全连接层的权重和偏置参数都添加0.01的l2权重正则化项，训练过程中，验证集loss总体趋于稳定，但是如果只看二值交叉熵loss(不包括权重衰减值),还是在一定值(0.2~0.3左右)波动，同时，训练集的正确率在9代左右，依旧上升到99.6以上，但是验证集正确率跟不上(只有94左右)，其中第7代FROC最高为77.7

25.从24的结果上看，训练集还是有些过拟合，考虑加大权重衰减的惩罚，将0.01改为0.1，同时网上查资料发现偏置项上没有使用权重衰减的必要，因此去掉偏置参数上的l2权重正则化项，但是效果没有明显的提升，原因不明。

26.尝试使用预检测第二阶段tpr：95%,fps:23的检测结果进行训练，和之前的训练过程对比，发现一种现象，就是训练样本中，负样本越少，训练过程越容易过拟合，最终结果越差。最终结果也和候选点个数相关，检测阶段提供的候选点个数越多，最终结果越不好。但是我们需要在检测结果上进行训练和预测，这两个相互矛盾，不可能同时满足。

27.尝试使用21中的数据(负样本多)来训练，之后在26中的数据(总的候选点少)的基础上进行预测，这个结果(densenet单模型84.9)和之前加上DSSD模型的结果(densenet单模型85.2)的总体效果相差不大，如果这样做的话，应该可以去掉DSSD部分，最终的系统也可以更快而且占用内存更少。

28.从26的分析上看，既然不能同时满足负样本多且假阳性少的条件，那么可以尝试在训练和预测阶段采用不同的数据，由于预检测是两个阶段，第一个阶段结果(tpr:98, fps:103)负样本较多，可以用于训练。第二个阶段结果(tpr:95,fps:23)假阳性少，可以用于预测，因此根据这个方法进行试验，训练densenet和inception resnet模型，其中densenet模型独立FROC：84.9(低于之前用DSSD的结果进行的实验，FROC:85.4)，inception resnet模型独立FROC:84.1(之前用DSSD的结果进行的实验，FROC:84.5)，两个模型融合后的结果FROC:85.2(低于之前用DSSD的结果进行的实验，FROC:86.5)，最终结果比之前低了1.3个百分点，但是由于去掉了中间一个DSSD的步骤，可以使得整个系统的速度变快，同时消耗内存和显存将大大减少。

29.随机掩码的尝试，在之前训练的模型的基础上，进行随机掩码的微调，取28中densenet最好的第3代的模型，在这个基础上，减小学习速率为0.0002，每代衰减0.7(之前训练的初始速率是0.001，每代衰减0.9)微调，改变数据的输入方法，首先读取n个(实验中n=3000)数据，其中一半是正样本，一半是负样本，一个batch的数据，用如下选择方法，首先取一个样本s1，如果s1是正样本，那么再随机取一个负样本s2(如果s1是负样本，那么随机取一个正样本吗作为s2)，按照正样本的直径信息，从s2中取小块，覆盖s1中的相应位置，并将这个样本作为一个新样本，如果s1是正的，则这个样本标签为负，s1和新样本在同一个batch里面输入进行训练，微调了两代之后得到FROC为88.5的结果，之后的几代，FROC有所下降，但稳定在86以上。

30.尝试使用29中的方法在inception resnet模型和vgg模型上面进行试验,前面不加随机掩码训练出的模型，效果正常，inception resnet模型FROC:84.1，vgg模型FROC:84.5,但是添加随机掩码微调后，两个模型的效果不升反降，随机掩码的策略貌似只有在Densenet模型上面产生效果。

31.尝试在原来的Resnet模型中加入squeeze excitatioin结构，将原来的Resnet模型变成se-resnet模型，通过和28相同的训练和预测方式，se-resnet模型第五代FROC：87.1。尝试将这个结果和之前densenetFROC：88.5的结果融合，最后得到FROC：89.8，由于一次意外的文件覆盖，这个densenet88.5的模型文件已经没了，但结果还在。尝试重新训练：首先用0.001的初始学习速率，0.9的衰减速率，在普通数据上进行训练，第三代得到FROC:85.1，之后使用0.0001的初始学习速率，0.9的衰减速率，用随机掩码的数据微调到15代，得到86.6的结果, 调过很多参数，均不能重现之前的88.5，但是这个86.6的结果和se-resnet模型的结果融合之后，可以达到FROC:89.4，和之前只相差0.4个百分点，可以接受。

32.在31的基础上，尝试用随机掩码来微调se-resnet模型，但是最终结果不升反降。

33.31中的是将squeeze excitatioin结构放在Add之后的模型， 按照senet原论文，尝试将se-resnet的squeeze excitatioin结构放到Add操作之前，作为se-resnet_v2模型，用0.001的初始学习速率，0.9的衰减速率，进行训练，第六代得到最好结果FROC:84.2，和之前的se-resnet模型的效果差别有些大。

34.尝试使用31中的se结构来进行Densenet的改造，作为se-densenet模型，用0.001的初始学习速率，0.9的衰减速率，进行训练，第9代结果的FROC最高：85.1，和不用se结构的Densenet最好的结果相差不多，对Densenet加se结构的做法没有什么效果。

35.尝试按照33中的方法进行Densenet的改造，作为se-densenet_v2模型，用0.001的初始学习速率，0.9的衰减速率，进行训练，第10代结果的FROC最高：85.9，v2版本的se结构对Densenet有一定效果。

36.尝试将随机掩码加入到常规的训练集增强之中，随机掩码的概率为0.2，不再仅仅使用随机掩码微调，同时尝试将随机掩码的半径向外扩充5个像素，使用Dense模型进行训练，lr:0.001，lr_decay:0.9，样本个数设置为15乘以负样本个数，正样本仅annotation中的样本才增强108倍，负样本仅仅只做平移增强，使用预检测第一阶段(tpr:98, fps:103)的结果进行训练，在预检测第二阶段(tpr:95, fps:20)的结果上进行预测，训练了五代，FROC在80左右，效果不好。

37.在36的基础上，将随机掩码概率改为0.05，训练了4代，FROC在80左右，效果不好。

38.在37的基础上，去掉随机掩码时的5个像素的扩展，Densenet第七代FROC可以达到86.6，尝试将这个结果和之前se-resnet模型的结果进行融合，最终FROC88.7，效果不是很理想。

39.尝试在Densenet模型中加入中心裁剪，尝试将中心裁剪的输出和转换层的输出连接，使用和37同样的训练方式进行训练，第8代FROC最高为:85.8。

40.尝试使用DenseBtnet模型，在38的数据增强方式的基础上，按照DenseBtnet论文中的训练方式，使用Adam(beta_1=0.9,beta_2=0.99)作为优化器，训练样本个数设置为2*negative_num per epoch，初始学习速率为0.001，学习速率每代衰减为0.9，卷积层使用kernel_initializer为he_normal，使用kernel_regularizer为l2(0.0008)，结果非常差，FROC只有80左右。

41.尝试将有中心裁剪结构的Densenet中的bottleneck去掉，然后将模型换成(40,40,40)大小输入的单分支模型，训练了20代，发现第20代最好，FROC为88.1，但是将这个结果融合进之前的89.4的结果，FROC反而下降到89.1，尝试单独和se-resnet模型的87.1的结果融合，结果只有88.8。最终没有提高, 但是将第18代FROC87.7的结果和之前89.4的结果融合发现FROC达到89.7已经接近之前的最好结果。

42.尝试将networks_v4.py中的原始Densenet的bottleneck去掉，使用和38中相同的方式进行训练，训练了18代，发现第17代FROC最高为83.6，不如38中的效果好。

43.尝试在se-resnet模型的基础上加上centercrop结构，并且改成只用40大小的单分支模型，第15代FROC可以达到89.8，但是尝试和之前的结果融合的时候发现用第12代88.0的结果和之前41中FROC88.1的结果融合，效果最好，FROC可以达到90.8。

44.尝试直接在输入上做centercrop，作为centercrop模型的v2版本，将原来的SE-resnet_centercop模型修改为SE-resnet_centercop_v2使用和之前相同的数据扩充和训练方式进行训练，训练了13代，效果始终只有FROC:84左右，没有之前的模型效果好。

45.尝试将之前合并结果达到90.8的两个单分支模型，合并为一个双分支模型进行训练，使用和之前相同的数据扩充和训练方式进行训练，训练了18代，测试集FROC最高只有86.8，没有之前分开训练的模型效果好。

46.尝试在之前的inception_resnet模型上面进行centercrop改造，使用(40,40,40)输入大小的单分支结构，第6代FROC87.7，尝试将这个结果和之前90.8的结果进行合并，最终得到FROC91.1，有一定提高。

47.在第二折上训练之前最好的三个模型，并将结果和第一折的结果合并起来一起测试，最终FROC为92.4。


### 使用HardMining的假阳性减少
1.networks_v2.py中的ince_res网络，首先随机采样40000正样本，随机选出150000负样本，然后正样本旋转扩充为四倍作为初始训练数据，每次迭代后，学习速率回到最初学习速率继续训练
初始训练结果FROC：76.7
第一次迭代后训练结果FROC：74.3

2.networks_v3.py中的网络，首先根据annotation.csv随机采样40000正样本，并从candidate_v2.csv中随机选出150000负样本作为初始训练样本，训练时正样本旋转扩充为四倍，每次迭代，在训练集中加入正负比例为1:4的错误样本(不够则补)，每次迭代后，学习速率接着上次训练最终的速率继续衰减
ince_res初始训练结果FROC：78.3
ince_res第一次迭代后训练结果FROC：81.2
den初始训练结果FROC：79.3
den第一次迭代后训练结果FROC：79.2
res初始训练结果FROC：74.1
res第一次迭代后训练结果FROC：77.3

3.使用新的hard mining策略，先划分四份负样本数据，训练四个模型，再用这四个模型选出用于hard mining第一代的数据
使用ince_res模型进行测试
其中第一次迭代FROC：62.1
第二次迭代FROC：53
第三次迭代FROC：74.2

4.首先使用所有的数据，训练出一个模型，然后使用这个模型预测训练集，挑选出数量相等的正负样本进行迭代训练。迭代的初始速率降为0.000028(0.001 * 0.7 ^ 10)