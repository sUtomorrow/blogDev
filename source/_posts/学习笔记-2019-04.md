---
title: 学习笔记-2019-04
date: 2019-04-06 20:05:26
tags: [学习笔记，杂项]
mathjax: true
---
# 2019-04-06
## 对于随机批梯度下降理解
- 对于batch size：从n个训练数据对真实数据分布进行估计，其标准差可以写为$\sigma/\sqrt{n}$，因此加大batch size对模型的学习效果的提升不是线性的。
- 对于数据：随机批梯度下降，可以减少计算的冗余度，因为很多数据其求出的梯度基本相同，最坏的情况就是训练集中每个数据都是相同的，如果使用原始的梯度下降算法，对于整个训练集进行梯度计算，最后得到的梯度和根据单个样本计算的梯度相同，存在大量计算冗余，随机批梯度下降对这种问题有一定缓解作用。
- 小的batch size：小的batch size 可以起到正则化的作用，有可能是因为小的batch size在估计梯度的过程中，引入了一定地噪声，从而可以使得模型的泛化误差减小，但是小的batch size 需要更小的学习速率和更多的学习步数。
- 对于优化算法：不同的优化算法对于batch size有不一样的要求，因为不同的优化算法对采样误差的敏感程度不同，可能一些优化算法需要从样本中提取的信息很难从少量的样本中估计出来，因此需要设置batch size为一个较大值才会有较好效果。仅仅直接使用梯度$g$的优化算法往往有较好的鲁棒性，并一般可以适应100这种小的batch size。

## 病态矩阵与条件数理解
对于矩阵方程如下：
$$
\begin{aligned}
    A x = b
\end{aligned}
$$
若对于一个较小的样本观察误差$\Delta b$，使得$A(x+\Delta x)=b + \Delta b$，$\Delta x$如果比$\Delta b$大很多，则称矩阵A是一个病态矩阵，一个典型的病态矩阵如下：
$$
\begin{bmatrix}
    1000 &1000\\
    0 &0.001
\end{bmatrix}
$$
其中两个列向量的相关性非常大，夹角非常小，表示的特征太过相似，若$b$是第一个列向量方向的单位向量,则求出的解为$[n, 0]^T$，若$b$稍微偏差一点，变到了第二个列向量的方向，则求出来的解变为$[0\ 1]^T$，两次解的差距非常大，仅仅因为$b$上有非常小的偏移。

对于某种矩阵范数$||A||$以及某种向量范数$||b||$，若:
$$
A(x+\Delta x)=b + \Delta b
$$
则
$$
A\Delta x = \Delta b
$$
即
$$
\Delta x = A^{-1}\Delta b
$$ 
则有
$$
||\Delta x|| \le ||A^{-1}||\cdot||\Delta b||
$$
同时有
$$
||A|| \cdot ||x|| \ge ||b||
$$
因此
$$
\frac{||\Delta x||}{||A|| \cdot ||x||} \le \frac{||A^{-1}||\cdot||\Delta b||}{||b||}
$$
$$
\frac{||\Delta x||}{||x||} \le (||A^{-1}||\cdot||A||)\frac{||\Delta b||}{||b||}
$$
这里将$||A^{-1}||\cdot||A||$称为矩阵A的条件数，对于不同的范数，条件数各有不同，但都反映了矩阵A的病态程度，条件数越大，矩阵呈现更加明显的病态特征。

# 2019-04-11
## SGD中的Momentum方法
momentum方法主要针对两个问题：
- 参数空间存在条件数较大的病态Hessian矩阵。
- 随机梯度下降对梯度的估计存在偏差。
$$
v = \alpha v - \epsilon \nabla_\theta(\frac{1}{m}\sum_{i=1}^{m}L(f(x^{(i)};\theta), y^{(i)}))\\
\theta \leftarrow \theta + v
$$
如果每次梯度都是$g$，那么最终的$v$会趋近于$\frac{\epsilon g}{1 - \alpha}$

## SGD中的Nesterov Momentum方法
$$
v = \alpha v - \epsilon \nabla_\theta(\frac{1}{m}\sum_{i=1}^{m}L(f(x^{(i)};\theta + \alpha v), y^{(i)}))\\
\theta \leftarrow \theta + v
$$

## 网络权重随机初始化的原因
如果两个计算单元具有相同的输入和激活函数，如果初始化相同，那么这两个计算单元在优化过程中很可能一直同步，最终出现冗余单元的情况，即两个计算单元计算的是相同的函数，重复计算没有意义。