---
title: 学习笔记-2019-04
date: 2019-04-06 20:05:26
tags: [学习笔记，杂项]
mathjax: true
---
# 2019-04-06
## 对于随机批梯度下降理解
- 对于batch size：从n个训练数据对真实数据分布进行估计，其标准差可以写为$\sigma/\sqrt{n}$，因此加大batch size对模型的学习效果的提升不是线性的。
- 对于数据：随机批梯度下降，可以减少计算的冗余度，因为很多数据其求出的梯度基本相同，最坏的情况就是训练集中每个数据都是相同的，如果使用原始的梯度下降算法，对于整个训练集进行梯度计算，最后得到的梯度和根据单个样本计算的梯度相同，存在大量计算冗余，随机批梯度下降对这种问题有一定缓解作用。
- 小的batch size：小的batch size 可以起到正则化的作用，有可能是因为小的batch size在估计梯度的过程中，引入了一定地噪声，从而可以使得模型的泛化误差减小，但是小的batch size 需要更小的学习速率和更多的学习步数。
- 对于优化算法：不同的优化算法对于batch size有不一样的要求，因为不同的优化算法对采样误差的敏感程度不同，可能一些优化算法需要从样本中提取的信息很难从少量的样本中估计出来，因此需要设置batch size为一个较大值才会有较好效果。仅仅直接使用梯度$g$的优化算法往往有较好的鲁棒性，并一般可以适应100这种小的batch size。

## 病态矩阵与条件数理解
对于矩阵方程如下：
$$
\begin{aligned}
    A x = b
\end{aligned}
$$
若对于一个较小的样本观察误差$\Delta b$，使得$A(x+\Delta x)=b + \Delta b$，$\Delta x$如果比$\Delta b$大很多，则称矩阵A是一个病态矩阵，一个典型的病态矩阵如下：
$$
\begin{bmatrix}
    1000 &1000\\
    0 &0.001
\end{bmatrix}
$$
其中两个列向量的相关性非常大，夹角非常小，表示的特征太过相似，若$b$是第一个列向量方向的单位向量,则求出的解为$[n, 0]^T$，若$b$稍微偏差一点，变到了第二个列向量的方向，则求出来的解变为$[0\ 1]^T$，两次解的差距非常大，仅仅因为$b$上有非常小的偏移。

对于某种矩阵范数$||A||$以及某种向量范数$||b||$，若:
$$
A(x+\Delta x)=b + \Delta b
$$
则
$$
A\Delta x = \Delta b
$$
即
$$
\Delta x = A^{-1}\Delta b
$$ 
则有
$$
||\Delta x|| \le ||A^{-1}||\cdot||\Delta b||
$$
同时有
$$
||A|| \cdot ||x|| \ge ||b||
$$
因此
$$
\frac{||\Delta x||}{||A|| \cdot ||x||} \le \frac{||A^{-1}||\cdot||\Delta b||}{||b||}
$$
$$
\frac{||\Delta x||}{||x||} \le (||A^{-1}||\cdot||A||)\frac{||\Delta b||}{||b||}
$$
这里将$||A^{-1}||\cdot||A||$称为矩阵A的条件数，对于不同的范数，条件数各有不同，但都反映了矩阵A的病态程度，条件数越大，矩阵呈现更加明显的病态特征。

# 2019-04-11
## SGD中的Momentum方法
momentum方法主要针对两个问题：
- 参数空间存在条件数较大的病态Hessian矩阵。
- 随机梯度下降对梯度的估计存在偏差。
$$
v = \alpha v - \epsilon \nabla_\theta(\frac{1}{m}\sum_{i=1}^{m}L(f(x^{(i)};\theta), y^{(i)}))\\
\theta \leftarrow \theta + v
$$
如果每次梯度都是$g$，那么最终的$v$会趋近于$\frac{\epsilon g}{1 - \alpha}$

## SGD中的Nesterov Momentum方法
$$
v = \alpha v - \epsilon \nabla_\theta(\frac{1}{m}\sum_{i=1}^{m}L(f(x^{(i)};\theta + \alpha v), y^{(i)}))\\
\theta \leftarrow \theta + v
$$

## 网络权重随机初始化的原因
如果两个计算单元具有相同的输入和激活函数，如果初始化相同，那么这两个计算单元在优化过程中很可能一直同步，最终出现冗余单元的情况，即两个计算单元计算的是相同的函数，重复计算没有意义。

# 2019-04-12

## AdaGrad优化算法
Global learning rate $\epsilon$

Initial parameter $\theta$

Small constant $\delta$

Initial variable $r = 0$

batch size $m$

$A \odot B$: hadamard product of $A$ and $B$

每次计算迭代过程如下：
$$
\begin{aligned}
g &\leftarrow \frac{1}{m} \nabla_\theta\sum_iL(f(x^{(i)}; \theta), y^{(i)})\\
r &\leftarrow r + g \odot g\\
\Delta \theta &\leftarrow -\frac{\epsilon}{\delta + \sqrt{r}} \odot g\\
\theta &\leftarrow \theta + \Delta \theta    
\end{aligned}
$$
此算法主要用于凸函数的优化问题，在梯度较小的地方可以加大参数移动速度，但是我觉得这个算法不好，因为对梯度的方向进行了更改，而且$r$项一直增大，会使得学习过程很快趋于停滞。

## RMSProp优化算法
Global learning rate $\epsilon$

decay rate $\rho$

Initial parameter $\theta$

Small constant $\delta$

Initial variable $r = 0$

batch size $m$

$A \odot B$: hadamard product of $A$ and $B$

每次计算迭代过程如下：
$$
\begin{aligned}
g &\leftarrow \frac{1}{m} \nabla_\theta\sum_iL(f(x^{(i)}; \theta), y^{(i)})\\
r &\leftarrow \rho r + (1 - \rho)g \odot g\\
\Delta \theta &\leftarrow -\frac{\epsilon}{\sqrt{\delta + r}} \odot g\\
\theta &\leftarrow \theta + \Delta \theta    
\end{aligned}
$$
在AdaGrad算法的基础上，通过衰减参数$\rho$可以避免学习过程趋于停滞。

## 带有Nesterov Momentum 的RMSProp优化算法
Global learning rate $\epsilon$

decay rate $\rho$

momentum coefficent $\alpha$

Initial parameter $\theta$

Small constant $\delta$

initial velocity $v$

Initial variable $r = 0$

batch size $m$

$A \odot B$: hadamard product of $A$ and $B$

每次计算迭代过程如下：
$$
\begin{aligned}
\tilde{\theta} &\leftarrow \theta + \alpha v\\
g &\leftarrow \frac{1}{m} \nabla_{\tilde{\theta}}\sum_iL(f(x^{(i)}; \tilde{\theta}), y^{(i)})\\
r &\leftarrow \rho r + (1 - \rho)g \odot g\\
v &\leftarrow \alpha v -\frac{\epsilon}{\sqrt{r}} \odot g\\
\theta &\leftarrow \theta + v
\end{aligned}
$$
这里计算了Nesterov动量项以应对可能的梯度估计偏差和参数空间病态Hessian矩阵问题。

## Adam优化算法

Step size $\epsilon$

Exponential decay rates for moment estimates $\rho_1$ $\rho_2$

Small constant $\delta$

Initial parameter $\theta$

Initialize 1st and 2nd moment variables $s=0$ $r=0$

Initialize time step $t=0$

$A \odot B$: hadamard product of $A$ and $B$

每次计算迭代过程如下:

$$
\begin{aligned}
    g &\leftarrow \frac{1}{m} \nabla_\theta\sum_iL(f(x^{(i)}; \theta), y^{(i)})\\
    t &\leftarrow t + 1\\
    s &\leftarrow \rho_1 s + (1-\rho_1) g\\
    r &\leftarrow \rho_2 r + (1-\rho_2) g \odot g\\
    \hat{s} &\leftarrow \frac{s}{1-\rho_1^t}\\
    \hat{r} &\leftarrow \frac{r}{1 - \rho_2^t}\\
    \Delta_\theta &\leftarrow -\epsilon \frac{\hat{s}}{\sqrt{\hat{r}} + \delta}\\
    \theta &\leftarrow \theta + \Delta_\theta
\end{aligned}
$$

## 牛顿法
首先通过二阶估计来表达损失函数：
$$
\begin{aligned}
    \hat{J}(\theta) &\approx J(\theta_0) + (\theta - \theta_0)^T\nabla_\theta J(\theta_0) + \frac{1}{2}(\theta - \theta_0)^TH(\theta - \theta_0)\\
\end{aligned}
$$
其一阶导数：
$$
\begin{aligned}
\frac{\partial \hat{J}}{\partial \theta} &= (\nabla_\theta J(\theta_0))^T + \frac{1}{2}(H + H^T)(\theta - \theta_0)\\
&= \nabla_\theta J(\theta_0) + H(\theta - \theta_0)
\end{aligned}
$$
对于凸函数，当一阶导数为0时，有可以直接得到最优解：
$$
\nabla_\theta J(\theta_0) + H(\theta - \theta_0) = 0\\
\theta = \theta_0 - H^{-1}\nabla_\theta J(\theta_0)
$$
对于非凸函数，牛顿法不能有效的收敛。

# 2019-04-27
## 论文：Feature Selective Anchor-Free Module for Single-Shot Object Detection
这篇论文对retinanet进行了改进，在原始的anchor-based分支上，新增了一个anchor-free分支，目前一阶段的目标检测大部分都是基于先验框进行，就是对于所谓的anchor(锚点)，事先定义好anchor box的大小、比例、个数，模型的回归输出作为基于anchor box的偏移，之后将目标检测框映射分配到anchor box上，为anchor box指定优化目标进行模型的训练，而inference过程是将和先验anchor box一起计算得到最终的predict box.