---
title: 学习笔记-2020-04
date: 2020-04-17 17:27:30
tags: [学习笔记，杂项]
mathjax: true
---
# 2020-04-17

## 方阵的一些理解

### 方阵的特征分解和方阵的相似关系

特征分解可以表示为$A = Pdiag(\lambda)P^{-1}$，其中$\lambda$是A的特征值组成的对角矩阵，$P$是$A$的特征向量作为列向量组成的矩阵。

方阵和向量的乘法，例如$Ax$，其中$x$在标准正交基下的坐标可以表示为$(x_1\ x_2\ x_3\ ...\ x_n)$，可以看做一个对$x$的映射，而$Ax = Pdiag(\lambda)P^{-1}x$，则可以理解为首先将x从标准正交基下的坐标转换到以$A$的特征向量作为基的坐标（$P^{-1}x$），然后在此坐标下进行缩放（$diag(\lambda)P^{-1}x$），最后再将坐标转换回标准正交基的坐标（$Pdiag(\lambda)P^{-1}x$）。

方阵$A$相似与$B$即表示$A=PBP^{-1}$，也可以和方阵的特征分解类似理解，$A$可以看做在标准正交基下做的变换，$B$则是对应的在$P$的列向量作为基的情况下的相同变换，即同一个变换，在不同的基下的不同表示。

## 二次型
二次型是一种二次齐次函数的统称，可以表示为$f(x_1, x_2, ..., x_n) = a_{11}x_1^2 + a_{22}x_2^2 + ... + a_{nn}x_n^2 + 2a_{12}x_1x_2 + 2a_{13}x_1x_3 + ... +  + 2a_{n-1, n}x_1x_n$

二次型可以通过一个对称矩阵表示$f(x_1, x_2, ..., x_n)=X^T A X$，其中A因为是对称矩阵，有很多很好的分解方式，因此非常方便分析。

# 2020-04-18

## 线性SVM详细推导
首先，一个点$p \in \mathbb{R}^d$到超平面$w^Tx+b=0$的距离可以表示为$\frac{1}{||w||}|w^Tp + b|$。

对于一个两类别数据集$X\in\mathbb{R}^d, Y\in\{0, 1\}$，定义间隔$\gamma=2\min_i\frac{1}{||w||}|w^Tx_i + b|$

线性支持向量机的目标即找到一组适合的参数$(w, b)$使得

$$
\max_{w,b}\gamma = \max_{w,b} 2\min_i\frac{1}{||w||}|w^Tx_i + b|\\
s.t. \ y_i(w^Tx_i + b) > 0, i = 1,2,...,m
$$

若一组$(w^*, b^*)$是支持向量机的一个解，那么对于$\lambda > 0$，$(\lambda w^*, \lambda b^*)$也是该优化问题的一个解，因为间隔不会变化。

所以这里添加一个约束条件，让$\min_i |w^T x_i + b| = 1$，因此支持向量机的优化目标可以进一步化成如下，即支持向量机的基本型：

$$
\max_{w,b} 2\min_i\frac{1}{||w||}|w^Tx_i + b| \\
= \max_{w,b} \frac{2}{||w||}\\
= \min_{w,b} \frac{1}{2}w^T w\\
s.t. \ y_i(w^Tx_i + b) \ge 1, i = 1,2,...,m
$$

可以看出，$w^T w$是一个正定二次型，如此一来，支持向量机的可以看做一个凸二次优化问题：

$$
\min_{u} \frac{1}{2} u^T Q u + t^T u \\
s.t. \ c_i^T u \ge d_i, i = 1, 2,...,m
$$

其中$u=\begin{bmatrix}w\\b\end{bmatrix}$, $Q=\begin{bmatrix}I &\mathbf{0}\\\mathbf{0}&0\end{bmatrix}$, $t=\mathbf{0}$, $c_i = y_i\begin{bmatrix}x_i\\ 1\end{bmatrix}$, $d_i = 1$

也可以运用拉格朗日法来求解支持向量机，定义其拉格朗日函数$\mathcal{L}(w,b,\alpha) = \frac{1}{2}w^T w + \sum_{i=1}^{m} \alpha_i(1 - y_i(w^T x_i + b))$
原问题可以表示为
$$
\min_{w,b} \max_\alpha \mathcal{L}(w, b, \alpha)\\
s.t. \ \alpha_i \ge 0, i=1,2,...,m
$$

其对偶问题可以表示为:

$$
\max_\alpha \min_{w,b} \mathcal{L}(w, b, \alpha)\\
s.t. \ \alpha_i \ge 0, i=1,2,...,m
$$

其KKT条件表示为:

$$
\begin{aligned}
&1 - y_i(w^T x_i + b) \le 0 \\
&\alpha_i \ge 0 \\
&\alpha_i(1 - y_i(w^Tx_i + b)) = 0    
\end{aligned}
$$

这里对支持向量机的对偶问题进行第一步求解:$\min_{w,b} \mathcal{L}(w, b, \alpha)$，直接令一阶导数等于0：
$$
\frac{\partial\mathcal{L}}{\partial w} = 0 \Rightarrow w = \sum_{i=1}^m \alpha_i y_i x_i \\
\frac{\partial\mathcal{L}}{\partial b} = 0 \Rightarrow \sum_{i=1}^m \alpha_i y_i = 0
$$

这里可以看出，$w$仅和$\alpha_i > 0$的样本有关，而根据KKT条件，$\alpha_i > 0$的地方必须满足$(1 - y_i(w^Tx_i + b)) = 0$，即这些$x_i$在最大间隔边界上，这样的样本称为支持向量，支持向量机的解仅仅和支持向量有关。

将求得结果代入$\mathcal{L}$：
$$
\begin{aligned}
\mathcal{L} &= \frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m \alpha_i \alpha_j y_i y_j x_i^T x_j + \sum_{i=1}^m \alpha_i - \sum_{i=1}^m \alpha_i y_i(w^Tx_i + b)\\
&= \frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m \alpha_i \alpha_j y_i y_j x_i^T x_j + \sum_{i=1}^m \alpha_i - \sum_{i=1}^m \alpha_i y_iw^Tx_i\\
&= \sum_{i=1}^m \alpha_i - \frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m \alpha_i \alpha_j y_i y_j x_i^T x_j
\end{aligned}
$$

此时，再求解
$$
\max_{\alpha} \mathcal{L}(w, b, \alpha)\\
s.t. \ \alpha_i \ge 0,i=1,2,...,m\\
\ \sum_{i=1}^m \alpha_i y_i = 0
$$
这个问题的求解使用SMO(序列最小优化)算法，大致思路和坐标上升法类似，迭代进行，每次选取两个$\alpha$进行更新，同时更新参数$b$，具体步骤后面有时间再详细学习。

## SVM的核技巧
线性SVM基于一个基本假设：数据在空间$\mathbb{R}^d$中线性可分，但这个假设在实际应用中，基本不满足。

但是存在一个定理：当$d$有限时，一定存在$\hat{d}$，使得样本在空间$\mathbb{R}^{\hat{d}}$中线性可分。

因此我们可以构造一种映射：$x_i \rightarrow \phi(x_i)$，然后在这个映射的空间中使用线性SVM进行分类。

这样最终需要求解的拉格朗日函数可以写成：$\mathcal{L} = \sum_{i=1}^m \alpha_i - \frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m \alpha_i \alpha_j y_i y_j \phi(x_i)^T \phi(x_j)$，看起来很简单，只需要换一种计算方式就行，但是这里存在一个问题：$\hat{d}$可能非常大，导致计算困难。

针对上述问题，需要使用核技巧：构造一个计算复杂度为$O(d)$的$k(x_i, x_j)$使得：$k(x_i, x_j)=\phi(x_i)^T \phi(x_j)$，即我们只需要一种快速的计算内积的方式，并不关心其他运算。

核函数的选择：当数据维数$d$超过了样本数量的时候，一般选用线性核，当数据维数$d$较小，而样本量$m$中等时，可以选择RBF核，但是当数据维数$d$较小，而样本量$m$特别大时，不需要选择了，直接使用深度神经网络吧。

核函数的定义需要满足Mercer条件：核函数矩阵必须是半正定的。可以理解为内积大于等于0。

核函数有一些性质：如果$k_1$、$k_2$是核函数，那么下列函数也是核函数：

$c_1k_1(x_i, x_j) + c_2k_2(x_i, x_j), \ c1,c2 > 0$

$k_1(x_i, x_j)k_2(x_i, x_j)$

$f(x_i)k_1(x_i, x_j)f(x_j)$

# 2020-04-19

## 软间隔SVM
数据中不能总是找到线性可分的空间，而且数据存在噪声或者错误标注，这个时候我们如果按照SVM的优化方式，很可能造成过拟合的问题，因此可以允许少量分类错误出现，定义松弛变量$\epsilon_i = \begin{cases}
    0 & y_i(w^T\phi(x_i) + b) \ge 1\\
    1 - y_i(w^T\phi(x_i) + b) & y_i(w^T\phi(x_i) + b) < 1
\end{cases}$，由此定义软间隔支持向量机的基本型：
$$
\min_{w,b,\epsilon} \frac{1}{2}w^T w + C\sum_{i=1}^m \epsilon_i\\
s.t. y_i(w^T\phi(x_i) + b) \ge 1 - \epsilon_i, i = 1, 2,...,m\\
\epsilon_i \ge 0, i=1,2,...,m
$$
其中$C$是一个可调节参数，用于调节错误分类的惩罚。

软间隔支持向量机的求解方式和支持向量机类似，不过从一个约束变成了两个约束。

另外$\epsilon_i$也可以表示成$\max(0, 1 - y_i(w^T\phi(x_i) + b))$，因此软间隔支持向量机的基本型的对偶问题可以表示为$\min_{w, b} \frac{1}{m}\sum_{i=1}^m \max(0, 1 - y_i(w^T\phi(x_i) + b)) + \frac{1}{2mC}w^Tw$，其中第一项称为经验风险，度量模型对数据的拟合程度，第二项称为结构风险，度量模型的复杂程度，也可以称为正则化项。因此还衍生出一种损失函数：hinge loss：$\mathbb{l}(s) = max(0, 1-s)$。

## 凸优化P3~P4-仿射集、凸集、锥集、凸锥集、仿射组合、凸组合、凸锥组合、仿射包、凸包、凸锥包
锥：$C = \{x| \theta x \in C\}, x \in R^n,\theta \ge 0$

仿射组合：$\theta_1+\theta_2+...=1$

凸组合：$\theta_1+\theta_2+...=1,\theta_1+\theta_2+... \ge 0$

凸锥组合：$\theta_1+\theta_2+... \ge 0$

# 2020-04-20
## 凸优化P5~P6-一些凸集：超平面、半空间、球、椭球、多面体、单纯形、对称（半）（正定）矩阵
超平面：$\{x|w^Tx=b\},x \in R^n,w \in R^n, b \in R, w \ne \mathbb{0}$

半空间：$\{x|w^Tx \ge b\},x \in R^n,w \in R^n, b \in R, w \ne \mathbb{0}$或$\{x|w^Tx \le b\},x \in R^n,w \in R^n, b \in R, w \ne \mathbb{0}$

球：$B(x_c, r) = \{x |\ ||x-x_c||_2 \le r\}, x_c \in R^n, x \in R^n, r \ge 0$

椭球：$\epsilon(x_c, P) = \{x |\ (x-x_c)^T P^{-1} (x-x_c) \le 1\}, x_c \in R^n, x \in R^n, P \in S_{++}^n$

多面体：$P=\{x|a_i^T x \le b_i, c_j^T x = d_j, i = 1,2,...,m, j=1,2,...,n\}$

单纯形：$R^n$中的$k+1$个点$v_0, v_1, ..., v_k$，满足$v_1-v_0, v_2-v_0, ..., v_k-v_0$线性无关，则$v_0, v_1, ..., v_k$的单纯形为$v_0, v_1, ..., v_k$的凸包，单纯形是多面体的一种，证明的时候，可以借助"秩为$k$的矩阵$B$,可以用一个非奇异矩阵$A$转换成$\begin{bmatrix}I_k \\ \mathbb{0} \end{bmatrix}$的形式"这个定理。

# 2020-04-21

## 凸优化P7~P8-仿射函数、透视函数、线性分数函数、保凸运算
仿射函数：$f:R^n \rightarrow R^m, f(x)=Ax+b, A \in R^{(m \times n)}, b \in R^n$

缩放：$\alpha S = \{\alpha x| x \in S\}$、移位：$S+a=\{x+a|x \in S\}$是仿射变换的一种。

线性矩阵不等式的解集是凸集：$\{x|A(x) \preceq B\}$

透视函数：$P: R^{n+1} \rightarrow R^n, P(x,y) = \{\frac{x}{y}\}\ x \in R^n, y\in R_{++}$

保凸运算：
- 交集
- 集合的和：$S_1 + S_2 = \{x_1 + x_2 | x_1 \in S_1, x_2 \in S_2\}$
- 集合的笛卡尔积：$S_1 \times S_2 = \{(x_1 , x_2) | x_1 \in S_1, x_2 \in S_2\}$
- （逆）仿射函数
- 透视函数
- 线性分数函数：首先定义仿射函数$g:R^n \rightarrow R^{m+1}, g(x)=\begin{bmatrix} A \\ c^T \end{bmatrix} x + \begin{bmatrix} b \\ d \end{bmatrix}, A \in R^{m \times n}, c \in R^n, b \in R^m, d \in R$，再定义透视函数$p:R^{m+1} \rightarrow R^m$，则线性分数函数$f = p \circ g = \frac{Ax + b}{c^T x + d},\ dom f = \{x| c^T x + d > 0\}$