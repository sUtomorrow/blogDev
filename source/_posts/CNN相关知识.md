---
title: CNN相关知识
date: 2020-05-07 11:12:22
tags: [深度学习]
mathjax: true
---

使用CNN这么久了，最近抽点时间来复习一下CNN相关的一些基础内容。

# CNN中的基本操作
## 卷积层
一般的神经网络中，每一层是上层输出到该层输出的全连接，但是对于图像数据，如果使用全连接操作，参数量会变得非常大，难以计算和优化而且容易过拟合。

卷积层有一维卷积、二维卷积、三维卷积几种常见的类型，这里主要讨论二维卷积，其他的卷积只是输入的维度数量不同，并无特殊。

在Pytorch框架下的CNN实现中，卷积层的输入一般是$(B\times C\times H\times W)$的形状（其他框架例如Tensorflow，使用的输入形状是$(B\times H\times W\times C)$，只是维度顺序不同），其中$B$是代表batch size，表示不同的图片样本，$C$代表通道数量，例如原始彩色图片一般是三个通道：R、G、B，最后的$H,\ W$是特征图的像素高度和像素宽度。

卷积层的参数主要有两个：Kernel、Bias。

其中Kernel的形状是$(H_k\times W_k\times C_o\times C_i)$，$H_k, W_k$表示卷积核的大小，例如下图的卷积核大小为$3\times 3$，$C_i$表示卷积核的通道数，如果输入的特征图是彩色图像，即一个三通道图片，那么$C_i=3$即表示在每一个像素点，对其三个通道进行全连接操作，$C_o$则是对每个像素所有通道进行全连接操作的输出通道数，也可以理解为卷积核个数（一个卷积核输出一个通道）。

Bias一般是个$R^{C_o}$的向量，在卷积操作结束之后，对输出的特征图进行偏移，每个通道共用其中的一个偏置参数。

卷积操作如图所示：
{%asset_img Conv.gif 卷积操作示意%}

### 卷积层的参数量和计算量
参数量即卷积核的大小加上偏置的大小：$H_k\times W_k\times C_o\times C_i + C_o$
计算量受到步长，padding方式等影响，从输入的大小来看不好计算，但是可以根据输出的特征图大小来计算：$H' \times W' \times (H_k\times W_k\times C_o\times C_i + C_o)$

### 对卷积操作的理解
卷积操作的本质就是在特征图上使用相同的参数滑动进行局部的全连接，这样做能够减少参数量和计算量，并且因为共享权值，在进行优化时更加容易。

卷积层有一些主要特点：
- 权值共享：在图像的不同部分，使用相同的特征提取是有意义的，这也是卷积层具备平移不变性的原因。
- 稀疏连接：对于一个输出值只和输入的一部分相关。

卷积操作可以看做一种局部特征的提取，例如一些边缘检测算子，实际上就是在图片上使用相同的参数滑动进行乘法操作（匹配特征方向），将代表边缘的像素点赋予更大的像素值，从而在结果图片中将边缘表示为亮的线条，这里就是在提取局部的边缘特征并保留其位置信息，最终组成整幅图片的边缘表示。

### CNN的反向传播
这部分内容可见我的另一篇文章{% post_link CNN的反向传播 CNN的反向传播 %}

## 激活函数
激活函数一般是一个非线性的逐元素的函数，例如Sigmoid和ReLU。
### 为什么要使用激活函数
在NN中，如果不使用一些非线性函数作为激活函数，那么模型的深度将毫无意义（仿射变换之后，在进行仿射变换，依旧等效于一个仿射变换）。
在CNN中也有类似的问题，如果多层卷积堆叠，中间没有非线性处理，那么完全可以用一个大核卷积层来进行替换，虽然会导致运算量上升，但是由于小核卷积的参数共享更严重，所以多个小核卷积层的表达能力还不如一个大核卷积层。

激活函数不论是加在NN中还是CNN中，其主要目的是引入非线性操作，提升模型的表达能力。
## 池化层
池化层的本质是采样操作，用于对上一层得到的结果进行压缩，减少模型计算量，同时有助于滤除噪声。

池化层主要有两类：
- 最大池化：保留池化核范围内最大的元素，注重于纹理信息的提取、过滤噪声，更适合浅层特征。
- 平均池化：将池化核范围内的元素取平均，注重保留背景信息、混合不同空间位置的信息，更适合深层特征。
