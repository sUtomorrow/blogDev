---
title: CNN相关知识
date: 2020-05-07 11:12:22
tags: [深度学习]
mathjax: true
---

# CNN中的基本操作
## 卷积层
一般的神经网络中，每一层是上层输出到该层输出的全连接，但是对于图像数据，如果使用全连接操作，参数量会变得非常大，难以计算和优化而且容易过拟合。

卷积层有一维卷积、二维卷积、三维卷积几种常见的类型，这里主要讨论二维卷积，其他的卷积只是输入的维度数量不同，并无特殊。

在Pytorch框架下的CNN实现中，卷积层的输入一般是$(B\times C\times H\times W)$的形状（其他框架例如Tensorflow，使用的输入形状是$(B\times H\times W\times C)$，只是维度顺序不同），其中$B$是代表batch size，表示不同的图片样本，$C$代表通道数量，例如原始彩色图片一般是三个通道：R、G、B，最后的$H,\ W$是特征图的像素高度和像素宽度。

卷积层的参数主要有两个：Kernel、Bias。

其中Kernel的形状是$(H_k\times W_k\times C_o\times C_i)$，$H_k, W_k$表示卷积核的大小，例如下图的卷积核大小为$3\times 3$，$C_i$表示卷积核的通道数，如果输入的特征图是彩色图像，即一个三通道图片，那么$C_i=3$即表示在每一个像素点，对其三个通道进行全连接操作，$C_o$则是对每个像素所有通道进行全连接操作的输出通道数，也可以理解为卷积核个数（一个卷积核输出一个通道）。

Bias一般是个$R^{C_o}$的向量，在卷积操作结束之后，对输出的特征图进行偏移，每个通道共用其中的一个偏置参数。

卷积操作如图所示：
{%asset_img Conv.gif 卷积操作示意%}

### 卷积层的参数量和计算量
参数量即卷积核的大小加上偏置的大小：$H_k\times W_k\times C_o\times C_i + C_o$
计算量受到步长，padding方式等影响，从输入的大小来看不好计算，但是可以根据输出的特征图大小来计算：$H' \times W' \times (H_k\times W_k\times C_o\times C_i + C_o)$

### 对卷积操作的理解

卷积操作的本质就是在特征图上使用相同的参数滑动进行局部的全连接，这样做能够减少参数量和计算量，并且因为共享权值，在进行优化时更加容易。

卷积操作可以看做一种局部特征的提取，例如一些边缘检测算子，实际上就是在图片上使用相同的参数滑动进行乘法操作（匹配特征方向），将代表边缘的像素点赋予更大的像素值，从而在结果图片中将边缘表示为亮的线条，这里就是在提取局部的边缘特征并保留其位置信息，最终组成整幅图片的边缘表示。

### CNN的反向传播
可见我的另一篇文章{% post_link CNN的反向传播 CNN的反向传播 %}

## 激活函数

### TODO

## 池化层

### TODO