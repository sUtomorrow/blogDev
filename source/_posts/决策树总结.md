---
title: 决策树总结
date: 2020-05-29 16:57:57
tags: [机器学习]
mathjax: true
---

# 决策树基础知识

## 信息熵
如果某个事件，有$C$种可能，其中第$k$中可能状态发生概率为$p_k$，那么该事件的信息熵定义为$Ent(D) = -\sum\limits_{k=1}^C p_k\log_2p_k$，表示的是所有状态的信息的期望，以2为底数这样的定义的信息熵单位是bit。

在决策树方法中，也类似的定义信息熵：样本集合$D$中第$k$类样本所占比例为$p_k$，则信息熵定义为$Ent(D) = -\sum\limits_{k=1}^C p_k\log_2p_k$，其中$C$为类别个数，其刻画了样本集合的纯度。

## 信息增益
假设离散属性$a$有$v$个取值：$a_1, a_2, ..., a_v$，可以将当前数据集合分成$V$个子集：$D^1, D^2, ..., D^V$，那么使用属性$a$划分样本集的信息增益定义为$Gain(D,a)=Ent(D)-\sum\limits_{v=1}^V\frac{|D^v|}{|D|}Ent(D^v)$

## 增益率
信息增益的定义导致其对数量较多的$D^v$更加敏感，因此又提出了信息增益率的概念：$Gain\_ratio(D,a)=\frac{Gain(D,a)}{IV(a)}$，其中，$IV(a)=-\sum\limits_{v=1}^V \frac{|D^v|}{|D|} \log_2\frac{|D^v|}{|D|}$称为属性$a$的固有值（或者叫内在价值）。

## 基尼指数
基尼值定义为$Gini(D) = \sum\limits_{k=1}^C\sum\limits_{k' \ne k}p_k p_{k'} = 1-\sum\limits_{k=1}^Cp_k^2$，其反映了在$D$中随机抽取两个样本，属于同一类别的概率。
和信息熵增益类似，对于在属性$a$上划分出$V$个区间的操作，定义划分后的基尼指数为$Gini\_index(D, a) = \sum\limits_{v=1}^V\frac{|D^v|}{|D|}Gini(D^v)$，构造决策树时的最优划分属性选择也可以使用能够使基尼指数最小的属性。

# 决策树的构造算法
## ID3算法
每次选择一个信息增益最大的属性$a$构造节点，这个节点将数据划分为$V$个子集，然后在每个子集中进行同样的操作（不能选择已经使用过的属性），若无属性可用或者当前子集类别相同，则当前子集标记为类别数量最多的类别，停止继续划分。

## C4.5算法
由于ID3算法在选择属性时使用信息增益最大值来选取，因此这样构造的决策树偏向于先选择特征取值更多的属性，另外ID3算法还有一个问题是处理连续值属性，这两个问题在C4.5算法中都给出了解决方案。

C4.5算法则是对ID3算法的一个改进版本，其中使用增益率来进行属性选择。

### 对连续值属性的处理
在遇到连续值时，如果连续属性$a$在$D$中出现$n$个取值，则将其从小到大排序为$\begin{bmatrix}a_1, a_2, ... a_n\end{bmatrix}$，这样产生$n-1$个离散值$T_a = \{\frac{a_i + a_{i+1}}{2}|1 \le i \le n-1\}$则$Gain(D,a)= \max\limits_{t \in T_a}Gain(D, a, t)$，其中$Gain(D, a, t)$表示将$a$属性使用$t$划分为两部分，这样连续值的决策树构造和离散值类似了，不过连续值属性在构造决策树的过程中可以使用多次。

## 分类回归树(Classification And Regression Tree, CART)算法
对于分类树，CART算法使用基尼指数最小化准则进行特征选择，CART的主要思想是每次将数据划分成两个单元，例如，对于当前的数据集合$D$，根据属性$a$上的取值$s$数据划分成$D^1$和$D^2$两个子集（对于连续变量，划分数据就用大于和小于关系，对于离散变量就是用等于或不等于关系），对于这样的划分，可以得到基尼指数$Gini\_index(D, a, s) = \frac{|D^1|}{|D|}Gini(D^1) + \frac{|D^2|}{|D|}Gini(D^2)$，每次划分都选择使得$Gini\_index(D, a, s)$最小的$a$和$s$。如此循环，即得到了CART分类树。

不同于ID3和C4.5算法，CART算法还能够处理回归树。对于回归树，其样本标签是连续值，回归树的一般形式表示如下，其中$H(x)$表示回归树的预测函数，$\mathbf{1}$表示指示函数，其基本思想是根据数据的属性，将数据划分成$M$个部分$R_m, m=1,2,...,M$，每个部分对应一个输出值。
$$
H(x) = \sum\limits_{i=1}^M c_i \mathbf{1}(X \in R_i)
$$
CART回归树使用平方误差作为每个部分的损失函数，每次利用属性$a$上的取值$s$将数据划分成两个部分：$R^1(a, s)$和$R^2(a, s)$，按照下式的规则来选择每次划分的$a$和$s$：
$$
\min\limits_{a, s}[\min\limits_{c_1} \sum\limits_{x_i \in R^1(a,s)} (y_i - c_1)^2 + \min\limits_{c_2} \sum\limits_{x_j \in R^2(a,s)} (y_j - c_2)^2]
$$
其中c的值非常好计算，直接使用该数据子集对应标签的平均数即可。

## 属性缺失的处理
令$\tilde{D}$是所有没有缺失属性a的样本集合，对于样本$x$，有样本权重$w_x$，定义如下参数。
$$
\rho = \frac{\sum_{x\in \tilde{D}} w_x}{\sum_{x\in D}w_x}\\
\tilde{p}_k = \frac{\sum_{x\in \tilde{D}_k w_x}}{\sum_{x\in \tilde{D}}w_x}, (1\le k \le C)\\
\tilde{r}_v = \frac{\sum_{x\in \tilde{D}^v}w_x}{\sum_{x \in \tilde{D}} w_x}, (1 \le v \le V)
$$
显然，$\rho$表示属性无缺失样本所占比例，$\tilde{p}_k$表示属性无缺失样本中第$k$类所占比例，$\tilde{r}_v$表示属性无缺失样本中在属性$a$上取值$a^v$的样本比例。

由此推广信息增益为：
$$
\begin{aligned}
Gain(D, a) &= \rho \times Gain(\tilde{D}, a)\\
&=\rho \times (Ent(\tilde{D}) - \sum\limits_{v=1}^V \tilde{r}_v Ent(\tilde{D}^v))
\end{aligned}
$$
其中：
$$
Ent(\tilde{D}) = -\sum\limits_{k=1}^C \tilde{p}_k log_2 \tilde{p}_k
$$
这样解决了最优划分的属性选择问题，在构造子树时，如果样本$x$在属性$a$上的取值已知，那么$x$划分到相应子节点，且权重保持为$w_x$，如果属性$a$未知，则将$s$划分入所有的子节点，且权重调整为$\tilde{r}_v w_x$。

## 多变量决策树
叶节点不再针对某个属性，而是针对属性的线性组合进行划分。

# 决策树的剪枝

## 预剪枝
在生成决策树的过程中，利用验证集验证当前划分是否能提升泛化效果，如果不能，则停止继续划分，直接将当前样本集作为决策树叶节点，并指定数量最多的类别做为叶节点类别。预剪枝训练时间短，但是由于其是贪心策略，容易引起欠拟合。

## 后剪枝
在决策树生成完成之后，自底向上的对非叶节点进行考察，观察将该节点直接指定为叶节点是否能提升验证集泛化效果，如果能则将此节点的子树一起合并为一个叶节点。后剪枝的训练时间比预剪枝长很多，但是其欠拟合风险小，而且泛化性能往往优于预剪枝。
