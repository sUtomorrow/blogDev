---
title: 日常学习笔记
date: 2018-03-10 19:06:20
tags: [学习笔记，杂项]
mathjax: true
---
# 2019-01-10

之前我看《Deep Learning》这本书看不懂，上了一学期矩阵论之后，我发现现在看起来那些东西似乎不是很复杂，里面主要用到一些矩阵分解方法，非常基础，很多推导只要一步一步看下去，没有什么不能理解的，在深度学习中非常实用。

# 2019-01-20

## 学习了如何在hexo的markdown中使用数学公式
首先修改next的配置文件如下：

     mathjax:
        enable: true
        per_page: true

由于开启了pre_page，因此首先需要在markdown页面中使用：

     mathjax: true

写法和latex中的公式差不多，如：
     ```latex
        $\sum_{i=0}^{n}x_i$
     ```
显示效果：$\sum_{i=0}^{n}x_i$

但是在使用中碰到了数学公式渲染不正确的问题，网上说是hexo默认的渲染包有问题，因此需要修改。
首先在博客根目录使用以下命令，卸载原来的渲染包，安装新的。

     npm uninstall hexo-renderer-marked --save
     npm install hexo-renderer-kramed --save

这里需要注意的是一定要在博客根目录下打开控制台，才会安装到博客的node_modules目录中，否则不起作用。

之后还需要修改kramed的rule文件

     修改node_modules/kramed/lib/rules/inline.js
     第11行: escape: /^\\([\\`*{}\[\]()#$+\-.!_>])/,
     替换为: escape: /^\\([`*\[\]()#$+\-.!_>])/,
     第20行: em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,
     替换为: em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,

修改之后，一切正常。语法与latex公式基本相同，详细参考[latex数学公式](https://www.luogu.org/blog/IowaBattleship/latex-gong-shi-tai-quan)

## 《Deep learning》读书笔记
### 极大似然与对数极大似然与KL散度
极大似然估计表示如下：

$$\begin{aligned}
\theta_{ML}  & = \mathop{\arg\max}_\theta p_{model}(\mathbb{X}; \theta)\\
& = \mathop{\arg\max}_\theta \prod_{i=1}^m p_{model}(x^{(i)};\theta)
\end{aligned}
$$

由于这样的连乘容易造成下溢，因此可以替换为对数极大似然估计：

$$
\begin{aligned}
\theta_{ML} & = \mathop{\arg\max}_\theta\sum_{i=1}^m\log p_{model}(x^{(i)};\theta)
\end{aligned}
$$

将训练数据看做一种经验分布$\hat p_{data}$，且因整体缩放不会影响$\mathop{\arg\max}$操作，因此后面的项可以用期望表示，对数似然估计可以用下面的式子表达：

$$
\begin{aligned}
\theta_{ML} & = \mathop{\arg\max}_\theta \mathbb{E}_{\mathbf{x}\sim \hat p_{data}}\log p_{model}(x^{(i)};\theta)
\end{aligned}
$$

这其实就相当于最小化KL散度，KL散度的定义如下：

$$
\begin{aligned}
    D_{KL}(\hat p_{data} \| p_{model}) & = \mathbb{E}_{\mathbf{x} \sim \hat p_{data}}[\log \hat p_{data}(x) - \log p_{model}(x)]
\end{aligned}
$$

其中有最后一项的期望:$-\mathbb{E}_{\mathbf{x} \sim \hat p_{data}} \log p_{model}(x)$即是负的对数似然。

# 2019-01-21

## 在vscode预览markdown时渲染数学公式
只需要安装'Markdown+Math'这个插件就OK了。

## 尝试解析keras保存的参数hd5文件
经过尝试，发现keras保存的参数文件结构如下：最上层有两个键:'optimizer_weights'和'model_weights'，其中'optimizer_weights'是优化器参数，这里不关心，第二个键有关于模型
权重的信息。

'model_weights'包含attrs属性，其下又会有三个键:'layer_names','backend','keras_version'。 
重要的是其中的'layer_names',这个下面需要包含所有层名，字节数组的形式。

'model_weights'下所有层名作为键值，每个键值都有attrs属性，attrs属性下有键值'weight_names'，包括所有的权重参数名，字节数组形式。

# 2019-01-22

## 空洞卷积(也叫膨胀卷积,Dilated Convolution)
空洞卷积的数学定义如下：
如果$F:\mathbb{Z}^2\rightarrow\mathbb{R}$是一个离散函数，定义一个变量域$\Omega_r = [-r, r]^2 \cap\mathbb{Z}^2$再定义一个大小为$(2r+1)^2$的离散卷积$k:\Omega_r\rightarrow \mathbb{R}$,那么卷积操作可以表示为：

$$
\begin{aligned}
(F \ast k)(p) = \sum_{s+t=p}F(s)k(t)
\end{aligned}
$$

空洞卷积可以表示为：

$$
\begin{aligned}
(F \ast_l k)(p) = \sum_{s+lt=p}F(s)k(t)
\end{aligned}
$$

可见，当$l$为1时，空洞卷积就是普通的卷积。

空洞卷积可以增加感受野，空洞卷积感受野示意图如下，其中(a)图为普通卷积产生的感受野示意,记为$F1$，$3 \times 3$的普通卷积感受野和卷积核大小相同，(b)图为在(a)中的$F1$基础上进行$l$等于2的空洞卷积操作，结果记为$F2$，其感受野变为$7 \times 7$，(c)图为在(b)中$F2$的基础上进行$l$等于4的空洞卷积，其感受野计算为$(4 \ast 2 + 1) \times (4 \ast 2 + 1) = (9 \times 9)$，注意这里的感受野计算是基于逐层卷积的结果，很多博客中没有说明，我看了原文才知道。
{% asset_img 空洞卷积感受野示意图.png 空洞卷积感受野示意图%}


