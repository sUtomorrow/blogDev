---
title: 日常学习笔记
date: 2018-03-10 19:06:20
tags: [学习笔记，杂项]
mathjax: true
---
# 2019-01-10

之前我看《Deep Learning》这本书看不懂，上了一学期矩阵论之后，我发现现在看起来那些东西似乎不是很复杂，里面主要用到一些矩阵分解方法，非常基础，很多推导只要一步一步看下去，没有什么不能理解的，在深度学习中非常实用。

# 2019-01-20

## 学习了如何在hexo的markdown中使用数学公式
首先修改next的配置文件如下：

     mathjax:
        enable: true
        per_page: true

由于开启了pre_page，因此首先需要在markdown页面中使用：

     mathjax: true

写法和latex中的公式差不多，如：
     ```latex
        $\sum_{i=0}^{n}x_i$
     ```
显示效果：$\sum_{i=0}^{n}x_i$

但是在使用中碰到了数学公式渲染不正确的问题，网上说是hexo默认的渲染包有问题，因此需要修改。
首先在博客根目录使用以下命令，卸载原来的渲染包，安装新的。

     npm uninstall hexo-renderer-marked --save
     npm install hexo-renderer-kramed --save

这里需要注意的是一定要在博客根目录下打开控制台，才会安装到博客的node_modules目录中，否则不起作用。

之后还需要修改kramed的rule文件

     修改node_modules/kramed/lib/rules/inline.js
     第11行: escape: /^\\([\\`*{}\[\]()#$+\-.!_>])/,
     替换为: escape: /^\\([`*\[\]()#$+\-.!_>])/,
     第20行: em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,
     替换为: em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,

修改之后，一切正常。语法与latex公式基本相同，详细参考[latex数学公式](https://www.luogu.org/blog/IowaBattleship/latex-gong-shi-tai-quan)

## 《Deep learning》读书笔记
### 极大似然与对数极大似然与KL散度
极大似然估计表示如下：

$$\begin{aligned}
\theta_{ML}  & = \mathop{\arg\max}_\theta p_{model}(\mathbb{X}; \theta)\\
& = \mathop{\arg\max}_\theta \prod_{i=1}^m p_{model}(x^{(i)};\theta)
\end{aligned}
$$

由于这样的连乘容易造成下溢，因此可以替换为对数极大似然估计：

$$
\begin{aligned}
\theta_{ML} & = \mathop{\arg\max}_\theta\sum_{i=1}^m\log p_{model}(x^{(i)};\theta)
\end{aligned}
$$

将训练数据看做一种经验分布$\hat p_{data}$，且因整体缩放不会影响$\mathop{\arg\max}$操作，因此后面的项可以用期望表示，对数似然估计可以用下面的式子表达：

$$
\begin{aligned}
\theta_{ML} & = \mathop{\arg\max}_\theta \mathbb{E}_{\mathbf{x}\sim \hat p_{data}}\log p_{model}(x^{(i)};\theta)
\end{aligned}
$$

这其实就相当于最小化KL散度，KL散度的定义如下：

$$
\begin{aligned}
    D_{KL}(\hat p_{data} \| p_{model}) & = \mathbb{E}_{\mathbf{x} \sim \hat p_{data}}[\log \hat p_{data}(x) - \log p_{model}(x)]
\end{aligned}
$$

其中有最后一项的期望:$-\mathbb{E}_{\mathbf{x} \sim \hat p_{data}} \log p_{model}(x)$即是负的对数似然。
