---
title: 日常学习笔记
date: 2018-03-10 19:06:20
tags: [学习笔记，杂项]
mathjax: true
---
# 2019-01-10

之前我看《Deep Learning》这本书看不懂，上了一学期矩阵论之后，我发现现在看起来那些东西似乎不是很复杂，里面主要用到一些矩阵分解方法，非常基础，很多推导只要一步一步看下去，没有什么不能理解的，在深度学习中非常实用。

# 2019-01-20

## 学习了如何在hexo的markdown中使用数学公式
首先修改next的配置文件如下：

     mathjax:
        enable: true
        per_page: true

由于开启了pre_page，因此首先需要在markdown页面中使用：

     mathjax: true

写法和latex中的公式差不多，如：
     ```latex
        $\sum_{i=0}^{n}x_i$
     ```
显示效果：$\sum_{i=0}^{n}x_i$

但是在使用中碰到了数学公式渲染不正确的问题，网上说是hexo默认的渲染包有问题，因此需要修改。
首先在博客根目录使用以下命令，卸载原来的渲染包，安装新的。

     npm uninstall hexo-renderer-marked --save
     npm install hexo-renderer-kramed --save

这里需要注意的是一定要在博客根目录下打开控制台，才会安装到博客的node_modules目录中，否则不起作用。

之后还需要修改kramed的rule文件

     修改node_modules/kramed/lib/rules/inline.js
     第11行: escape: /^\\([\\`*{}\[\]()#$+\-.!_>])/,
     替换为: escape: /^\\([`*\[\]()#$+\-.!_>])/,
     第20行: em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,
     替换为: em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,

修改之后，一切正常。语法与latex公式基本相同，详细参考[latex数学公式](https://www.luogu.org/blog/IowaBattleship/latex-gong-shi-tai-quan)

## 《Deep learning》读书笔记
### 极大似然与对数极大似然与KL散度
极大似然估计表示如下：

$$\begin{aligned}
\theta_{ML}  & = \mathop{\arg\max}_\theta p_{model}(\mathbb{X}; \theta)\\
& = \mathop{\arg\max}_\theta \prod_{i=1}^m p_{model}(x^{(i)};\theta)
\end{aligned}
$$

由于这样的连乘容易造成下溢，因此可以替换为对数极大似然估计：

$$
\begin{aligned}
\theta_{ML} & = \mathop{\arg\max}_\theta\sum_{i=1}^m\log p_{model}(x^{(i)};\theta)
\end{aligned}
$$

将训练数据看做一种经验分布$\hat p_{data}$，且因整体缩放不会影响$\mathop{\arg\max}$操作，因此后面的项可以用期望表示，对数似然估计可以用下面的式子表达：

$$
\begin{aligned}
\theta_{ML} & = \mathop{\arg\max}_\theta \mathbb{E}_{\mathbf{x}\sim \hat p_{data}}\log p_{model}(x^{(i)};\theta)
\end{aligned}
$$

这其实就相当于最小化KL散度，KL散度的定义如下：

$$
\begin{aligned}
    D_{KL}(\hat p_{data} \| p_{model}) & = \mathbb{E}_{\mathbf{x} \sim \hat p_{data}}[\log \hat p_{data}(x) - \log p_{model}(x)]
\end{aligned}
$$

其中有最后一项的期望:$-\mathbb{E}_{\mathbf{x} \sim \hat p_{data}} \log p_{model}(x)$即是负的对数似然。

# 2019-01-21

## 在vscode预览markdown时渲染数学公式
只需要安装'Markdown+Math'这个插件就OK了。

## 尝试解析keras保存的参数hd5文件
经过尝试，发现keras保存的参数文件结构如下：最上层有两个键:'optimizer_weights'和'model_weights'，其中'optimizer_weights'是优化器参数，这里不关心，第二个键有关于模型
权重的信息。

'model_weights'包含attrs属性，其下又会有三个键:'layer_names','backend','keras_version'。 
重要的是其中的'layer_names',这个下面需要包含所有层名，字节数组的形式。

'model_weights'下所有层名作为键值，每个键值都有attrs属性，attrs属性下有键值'weight_names'，包括所有的权重参数名，字节数组形式。

# 2019-01-22

## 空洞卷积(也叫膨胀卷积,Dilated Convolution)
空洞卷积的数学定义如下：
如果$F:\mathbb{Z}^2\rightarrow\mathbb{R}$是一个离散函数，定义一个变量域$\Omega_r = [-r, r]^2 \cap\mathbb{Z}^2$再定义一个大小为$(2r+1)^2$的离散卷积$k:\Omega_r\rightarrow \mathbb{R}$,那么卷积操作可以表示为：

$$
\begin{aligned}
(F \ast k)(p) = \sum_{s+t=p}F(s)k(t)
\end{aligned}
$$

空洞卷积可以表示为：

$$
\begin{aligned}
(F \ast_l k)(p) = \sum_{s+lt=p}F(s)k(t)
\end{aligned}
$$

可见，当$l$为1时，空洞卷积就是普通的卷积。

空洞卷积可以增加感受野，空洞卷积感受野示意图如下，其中(a)图为普通卷积产生的感受野示意,记为$F1$，$3 \times 3$的普通卷积感受野和卷积核大小相同，(b)图为在(a)中的$F1$基础上进行$l$等于2的空洞卷积操作，结果记为$F2$，其感受野变为$7 \times 7$，(c)图为在(b)中$F2$的基础上进行$l$等于4的空洞卷积，其感受野计算为$(4 \ast 2 + 1) \times (4 \ast 2 + 1) = (9 \times 9)$，注意这里的感受野计算是基于逐层卷积的结果，很多博客中没有说明，我看了原文才知道。
{% asset_img 空洞卷积感受野示意图.png 空洞卷积感受野示意图%}

# 2019-01-23
## DenseNet论文阅读
论文地址：[Densely Connected Convolutional Networks](https://arxiv.org/pdf/1608.06993.pdf)

DenseNet模型结构如下。

{%asset_img DenseNet模型结构.png DenseNet模型结构%}

其中$1 \times 1$卷积层称为bottleneck层，用于减少通道个数，DenseBlock由BN-ReLU-Conv($1 \times 1$)-BN-ReLU-Conv($3 \times 3$)这样的结构重复而组成，如果一个DenseBlock中每一个$3 \times 3$的卷积输出通道个数是$k$，那么作者建议设置的bottleneck层输出通道个数为$4k$，使用了bottleneck层的DenseNet称为DenseNet-B。

之后的TransitionLayer会进一步压缩模型的通道个数，其输出通道个数为$\theta m$，其中m为DenseBlock的输出通道数。而$0 < \theta \le 1$，如果$0 < \theta < 1$那么称为DenseNet-C。

作者的实验中，最前面的一个卷积层输出通道数为$2k$，设置$\theta=0.5$并且使用了bottleneck层，因此称其模型为DenseNet-BC。训练过程中，使用SGD，初始学习率为0.1在30代和60代的时候分别除以10，训练batch_size：256，一共训练90代。

论文中解释说Densenet的提出是希望解决深层网络带来的梯度消失和梯度爆炸问题，并提对深度学习模型提出了一种新的解释：传统的前向传播模型就像是一种具有状态的算法，每一层读取其前一层的状态(输入)并对其进行处理，修改并保存了认为需要保存的状态，之后传到下一层，而Resnet通过相加处理，显式的保存了前一层的状态，Densenet通过通道连接，不仅保存了前一层的状态，而且还可以加以区分，虽然连接更密集，但是Densenet的模型可以参数相比于Resnet少，因为Densenet在DenseBlock中每一层的卷积核个数可以很少，通过$k$来指定。

## ROI Pooling
ROI Pooling可以根据提供的区域位置信息，将特征图上的位置pooling到一个固定大小的输出。

以一个输出为$2 \times 2$的ROI Pooling为例。

输入为一张特征图。

{%asset_img ROI_Pooling输入.png ROI Pooling输入%}

由区域建议网络给出区域位置。

{%asset_img 区域建议网络给出的位置.png 区域建议网络给出的位置%}

将建议区域划分为$2 \times 2$的区域。

{%asset_img 按照设定的输出大小进行划分.png 按照设定的输出大小进行划分%}

在各个区域内进行Pooling操作(这里是Max Pooling)，得到最终输出。

{%asset_img ROI_Pooling输出.png ROI Pooling输出%}

## 卷积层输出的尺寸计算
$n_{out} = [\dfrac{n_{in} + 2p - k}{s}] + 1$

其中$n_{out}$表示输出的特征图的大小，$n_{in}$表示输入的特征图的大小，$p$表示padding大小，$k$表示卷积核大小，$s$表示stride大小。

## Inception Net
