---
title: 日常学习笔记
date: 2018-03-10 19:06:20
tags: [学习笔记，杂项]
mathjax: true
---
# 2019-01-20

## 学习了如何在hexo的markdown中使用数学公式
首先修改next的配置文件如下：

     mathjax:
        enable: true
        per_page: true

由于开启了pre_page，因此首先需要在markdown页面中使用：

     mathjax: true

写法和latex中的公式差不多，如：
     ```latex
        $\sum_{i=0}^{n}x_i$
     ```
显示效果：$\sum_{i=0}^{n}x_i$

但是在使用中碰到了数学公式渲染不正确的问题，网上说是hexo默认的渲染包有问题，因此需要修改。
首先在博客根目录使用以下命令，卸载原来的渲染包，安装新的。

     npm uninstall hexo-renderer-marked --save
     npm install hexo-renderer-kramed --save

这里需要注意的是一定要在博客根目录下打开控制台，才会安装到博客的node_modules目录中，否则不起作用。

之后还需要修改kramed的rule文件

     修改node_modules/kramed/lib/rules/inline.js
     第11行: escape: /^\\([\\`*{}\[\]()#$+\-.!_>])/,
     替换为: escape: /^\\([`*\[\]()#$+\-.!_>])/,
     第20行: em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,
     替换为: em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,

修改之后，一切正常。语法与latex公式基本相同，详细参考[latex数学公式](https://www.luogu.org/blog/IowaBattleship/latex-gong-shi-tai-quan)

## 《Deep learning》读书笔记
### 极大似然与对数极大似然与KL散度
极大似然估计表示如下：

$$\begin{aligned}
\theta_{ML}  & = \mathop{\arg\max}_\theta p_{model}(\mathbb{X}; \theta)\\
& = \mathop{\arg\max}_\theta \prod_{i=1}^m p_{model}(x^{(i)};\theta)
\end{aligned}
$$

由于这样的连乘容易造成下溢，因此可以替换为对数极大似然估计：

$$
\begin{aligned}
\theta_{ML} & = \mathop{\arg\max}_\theta\sum_{i=1}^m\log p_{model}(x^{(i)};\theta)
\end{aligned}
$$

将训练数据看做一种经验分布$\hat p_{data}$，且因整体缩放不会影响$\mathop{\arg\max}$操作，因此后面的项可以用期望表示，对数似然估计可以用下面的式子表达：

$$
\begin{aligned}
\theta_{ML} & = \mathop{\arg\max}_\theta \mathbb{E}_{\mathbf{x}\sim \hat p_{data}}\log p_{model}(x^{(i)};\theta)
\end{aligned}
$$

这其实就相当于最小化KL散度，KL散度的定义如下：

$$
\begin{aligned}
    D_{KL}(\hat p_{data} \| p_{model}) & = \mathbb{E}_{\mathbf{x} \sim \hat p_{data}}[\log \hat p_{data}(x) - \log p_{model}(x)]
\end{aligned}
$$

其中有最后一项的期望:$-\mathbb{E}_{\mathbf{x} \sim \hat p_{data}} \log p_{model}(x)$即是负的对数似然。

# 2019-01-21

## 在vscode预览markdown时渲染数学公式
只需要安装'Markdown+Math'这个插件就OK了。

## 尝试解析keras保存的参数hd5文件
经过尝试，发现keras保存的参数文件结构如下：最上层有两个键:'optimizer_weights'和'model_weights'，其中'optimizer_weights'是优化器参数，这里不关心，第二个键有关于模型
权重的信息。

'model_weights'包含attrs属性，其下又会有三个键:'layer_names','backend','keras_version'。 
重要的是其中的'layer_names',这个下面需要包含所有层名，字节数组的形式。

'model_weights'下所有层名作为键值，每个键值都有attrs属性，attrs属性下有键值'weight_names'，包括所有的权重参数名，字节数组形式。

# 2019-01-22

## 空洞卷积(也叫膨胀卷积,Dilated Convolution)
空洞卷积的数学定义如下：
如果$F:\mathbb{Z}^2\rightarrow\mathbb{R}$是一个离散函数，定义一个变量域$\Omega_r = [-r, r]^2 \cap\mathbb{Z}^2$再定义一个大小为$(2r+1)^2$的离散卷积$k:\Omega_r\rightarrow \mathbb{R}$,那么卷积操作可以表示为：

$$
\begin{aligned}
(F \ast k)(p) = \sum_{s+t=p}F(s)k(t)
\end{aligned}
$$

空洞卷积可以表示为：

$$
\begin{aligned}
(F \ast_l k)(p) = \sum_{s+lt=p}F(s)k(t)
\end{aligned}
$$

可见，当$l$为1时，空洞卷积就是普通的卷积。

空洞卷积可以增加感受野，空洞卷积感受野示意图如下，其中(a)图为普通卷积产生的感受野示意,记为$F1$，$3 \times 3$的普通卷积感受野和卷积核大小相同，(b)图为在(a)中的$F1$基础上进行$l$等于2的空洞卷积操作，结果记为$F2$，其感受野变为$7 \times 7$，(c)图为在(b)中$F2$的基础上进行$l$等于4的空洞卷积，其感受野计算为$(4 \ast 2 + 1) \times (4 \ast 2 + 1) = (9 \times 9)$，注意这里的感受野计算是基于逐层卷积的结果，很多博客中没有说明，我看了原文才知道。
{% asset_img 空洞卷积感受野示意图.png 空洞卷积感受野示意图%}

# 2019-01-23
## DenseNet论文阅读
论文地址：[Densely Connected Convolutional Networks](https://arxiv.org/pdf/1608.06993.pdf)

DenseNet模型结构如下。

{%asset_img DenseNet模型结构.png DenseNet模型结构%}

其中$1 \times 1$卷积层称为bottleneck层，用于减少通道个数，DenseBlock由BN-ReLU-Conv($1 \times 1$)-BN-ReLU-Conv($3 \times 3$)这样的结构重复而组成，如果一个DenseBlock中每一个$3 \times 3$的卷积输出通道个数是$k$，那么作者建议设置的bottleneck层输出通道个数为$4k$，使用了bottleneck层的DenseNet称为DenseNet-B。

之后的TransitionLayer会进一步压缩模型的通道个数，其输出通道个数为$\theta m$，其中m为DenseBlock的输出通道数。而$0 < \theta \le 1$，如果$0 < \theta < 1$那么称为DenseNet-C。

作者的实验中，最前面的一个卷积层输出通道数为$2k$，设置$\theta=0.5$并且使用了bottleneck层，因此称其模型为DenseNet-BC。训练过程中，使用SGD，初始学习率为0.1在30代和60代的时候分别除以10，训练batch_size：256，一共训练90代。

论文中解释说Densenet的提出是希望解决深层网络带来的梯度消失和梯度爆炸问题，并提对深度学习模型提出了一种新的解释：传统的前向传播模型就像是一种具有状态的算法，每一层读取其前一层的状态(输入)并对其进行处理，修改并保存了认为需要保存的状态，之后传到下一层，而Resnet通过相加处理，显式的保存了前一层的状态，Densenet通过通道连接，不仅保存了前一层的状态，而且还可以加以区分，虽然连接更密集，但是Densenet的模型可以参数相比于Resnet少，因为Densenet在DenseBlock中每一层的卷积核个数可以很少，通过$k$来指定。

## ROI Pooling
ROI Pooling可以根据提供的区域位置信息，将特征图上的位置pooling到一个固定大小的输出。

以一个输出为$2 \times 2$的ROI Pooling为例。

输入为一张特征图。

{%asset_img ROI_Pooling输入.png ROI Pooling输入%}

由区域建议网络给出区域位置。

{%asset_img 区域建议网络给出的位置.png 区域建议网络给出的位置%}

将建议区域划分为$2 \times 2$的区域。

{%asset_img 按照设定的输出大小进行划分.png 按照设定的输出大小进行划分%}

在各个区域内进行Pooling操作(这里是Max Pooling)，得到最终输出。

{%asset_img ROI_Pooling输出.png ROI Pooling输出%}

## 卷积层输出的尺寸计算
$n_{out} = [\dfrac{n_{in} + 2p - k}{s}] + 1$

其中$n_{out}$表示输出的特征图的大小，$n_{in}$表示输入的特征图的大小，$p$表示padding大小，$k$表示卷积核大小，$s$表示stride大小。

## Inception Net
Inception使用了NIN(Network in Network)的思想，网络中的一层不再是单一的卷积，而是几种大小的卷积分支或者几个不同深度的分支进行同时计算，最后在通道维度连接到一起。

Inception在V1版本中(也就是GoogLeNet)使用了$5 \times 5$和$7 \times 7$大小的卷积核，以适应不同尺度的目标识别。

Inception V2版本将V1中的$5 \times 5$和$7 \times 7$卷积拆开成了小卷积的堆叠，减少了计算量的同时，增加了层数。在卷积层之后使用了BN层，添加了辅助分类层，论文中说，辅助分类层在最开始的训练过程中看不出效果，在模型收敛的时候才显现出效果。

Inception V3将V2中的辅助分类层也加上了BN。

Inception V4修改了之前inception模型中最前面的卷积部分(进入Inception Block之前)，将其中的下采样变成了两个分支，一个是步长为2的卷积，一个是池化，最终再Concatenate。

inception v2/v3论文中还提到了设计模型的一些经验原则，我的理解如下：

      1、慎用非常窄的瓶颈层，前馈神经网络中的瓶颈层会减少传播的信息量，如果瓶颈层非常窄，会导致有用信息的丢失，特征图尺寸应该从输入到输出逐渐减小，直到用来完成当前的任务(识别、分类等)。
      2、增加卷积层的卷积核个数可以丰富特征的组合，让特征图通道之间耦合程度更低，使网络加速收敛。
      3、网络开始的几层特征关联性很高，对其进行降维导致的信息损失较小，降维甚至可以加速学习。
      4、平衡网络的宽度和深度。优化网络性能可以认为是平衡每个阶段(层)的卷积核数目和网络深度。同时增加宽度和深度能够提升网络性能。

# 2019-01-25
## Pandas库的排序问题
注意到了一个非常坑的地方，关于Pandas库的，例如以下代码。

```python
import pandas as pd
dict1 = {'c': [1, 4, 5, 2, 3]}
dict2 = {'c': [5, 4, 1, 2, 3]}
df1 = pd.DataFrame(dict1)
df2 = pd.DataFrame(dict2)
df1['c'] += df2['c']
print(list(df1['c']))
```
代码中创建了两个DataFrame，然后进行列求和。
这个输出如下，一切正常。

          c
      0   6
      1   8
      2   6
      3   4
      4   6

但是，如果首先对两个DataFrame排序，如下：

```python
import pandas as pd
dict1 = {'c': [1, 4, 5, 2, 3]}
dict2 = {'c': [5, 4, 1, 2, 3]}
df1 = pd.DataFrame(dict1)
df2 = pd.DataFrame(dict2)

df1 = df1.sort_values(by='c')
df2 = df2.sort_values(by='c')
print(df1)
print(df2)
df1['c'] += df2['c']
print(list(df1['c']))
```

这个时候输出就很奇怪了，如下。

         c
      0  1
      3  2
      4  3
      1  4
      2  5
         c
      2  1
      3  2
      4  3
      1  4
      0  5
      [6, 4, 6, 8, 6]

首先是打印的两个DataFrame，常规操作没有任何问题，之后进行了两列的求和，但是求和结果的顺序看不懂了，按照代码里的意思，我希望得到的结果是

      [2, 4, 6, 8, 10]

但是输出的结果是，乍一看，好像还挺顺口！！！

      [6, 4, 6, 8, 6]
这个结果，既不是排序之后相加，也不是原顺序相加(注意这个输出和不排序版本的输出也有差别)。

最后找到了正确解释：先按照对应的index相加，之后再按照df1的index顺序进行输出。

## 全连接层的反向传播算法
之前看过深度神经网络(Deep Neural Network)反向传播的推导，但是没怎么用，现在感觉快忘光了，再来详细的推导一遍。
首先假设损失函数:
$$
loss = J(a^L, y)
$$
其中$a^L$为第$L$层的输出值，且有
$$
\begin{aligned}
a^L & = \sigma(z^L)\\
z^L & = W^L a^{L-1} + b^L\\
z^L & = W^L \sigma(z^{L-1}) + b^L
\end{aligned}
$$
那么损失函数对第$L$层的权重和偏置的偏导为：
$$
\begin{aligned}
\frac{\partial J}{\partial W^L} & = \frac{\partial J}{\partial z^L}\frac{\partial z^L}{\partial W^L}\\
\frac{\partial J}{\partial b^L} & = \frac{\partial J}{\partial z^L}\frac{\partial z^L}{\partial b^L}
\end{aligned}
$$
首先要计算的是$\frac{\partial J}{\partial z^L}$，这一项与损失函数和最后一层的激活函数有关，这里不具体讨论，直接计算即可，并将其结果记为$\delta^L$，之后的$\frac{\partial z^L}{\partial W^L}$项和$\frac{\partial z^L}{\partial b^L}$的计算非常简单，其结果分别为$(a^{L-1})^T$和$\delta^L$。

到这里，第$L$层的偏导就计算完了，那么$L-1$层同理可以如下计算。

$$
\begin{aligned}
\frac{\partial J}{\partial W^{L-1}} & = \frac{\partial J}{\partial z^L}\frac{\partial z^L}{\partial z^{L-1}}\frac{\partial z^{L-1}}{\partial W^{L-1}}\\
& = \delta^L \frac{\partial z^L}{\partial z^{L-1}}(a^{L-2})^T\\
\frac{\partial J}{\partial b^{L-1}} & = \frac{\partial J}{\partial z^L}\frac{\partial z^L}{\partial z^{L-1}}\frac{\partial z^{L-1}}{\partial b^{L-1}}\\
& = \delta^L \frac{\partial z^L}{\partial z^{L-1}}
\end{aligned}
$$

推广开来，若一共有$L$层，为方便表达，定义任意层的$\delta^l$
$$
\delta^l = \begin{cases}
     \frac{\partial J}{\partial z^l}&l=L\\
     \\
     \delta^{l+1} \frac{\partial z^l}{\partial z^{l-1}}&l < L
\end{cases}
$$

则$L-n$层的计算如下：
$$
\begin{aligned}
     \frac{\partial J}{\partial W^{L-n}} & = \delta^{L-n} (a^{L-n-1})^T\\
     \frac{\partial J}{\partial b^{L-n}} & = \delta^{L-n}\\
\end{aligned}
$$

## 卷积层的反向传播
首先要明确数学上的离散卷积和卷积网络中的卷积操作有区别。

对于数学中的二维卷积$C = K*B$，若$K$的宽度和高度分别为$W_K, H_K$，若$B$的宽度和高度分别为$W_B, H_B$:
$$
\begin{aligned}
     C &= K * B\\
     C_{s,t} &= \sum_{h=0}^{H_K-1}\sum_{w=0}^{W_K-1}K_{h,w}B_{s-h,t-w}
\end{aligned}
$$

而同样条件下，卷积网络卷积操作表示如下：
$$
\begin{aligned}
     C &= K \ast_{Conv} B\\
     C_{s,t} &= \sum_{h=0}^{H_K-1}\sum_{w=0}^{W_K-1}K_{h,w}B_{s+h,t+w}
\end{aligned}
$$
数学中的卷积'$*$'可以看做是把'$*_{Conv}$'中的卷积核$K$旋转180度后再进行'$*_{Conv}$'操作。

未完待续。。。

## 池化层的反向传播
池化层的反向传播算法非常简单，因为池化层没有可学习的参数，所以只需要传播$\delta^l$，以便于前面的层计算梯度。
