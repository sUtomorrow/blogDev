---
title: 回归方法总结
date: 2020-05-06 10:15:58
tags: [机器学习]
mathjax: true
---

在机器学习中，回归方法主要有线性回归和逻辑回归两种，其中线性回归是真的用于回归任务，而逻辑回归主要用于分类任务。

# 线性回归（Linear Regression, LR）
线性回归的表达方式很简单：$\hat{y} = x^Tw + b$，其中$x$是一个表示为$R^n$的样本，$\hat{y}$表示对样本$x$的回归值，$w,\ b$是线性回归的参数。

如果令$X=\begin{bmatrix}x_1 & 1\\ x_2 & 1\\ \vdots & \vdots \\ x_m & 1 \end{bmatrix} \in R^{m \times (n+1)}$表示所有数据的扩展矩阵，其中$m$是数据数量，$Y=\begin{bmatrix}y_1 \\ y_2 \\ \vdots \\ y_m\end{bmatrix} \in R^{m \times 1}$表示数据的标签，$W \in R^{(n+1) \times 1}$表示线性回归的参数，其中$n$是数据特征维数，则线性回归可以表示为$\hat{Y} = XW$。

如果首先对数据和标签进行了中心化，即$\sum_i x_i=0,\ \sum_i y_i=0$，那么可以令$X=\begin{bmatrix}x_1\\ x_2\\ \vdots\\ x_m \end{bmatrix} \in R^{m \times n}$表示所有数据的矩阵，其中$m$是数据数量，$W \in R^{n \times 1}$表示线性回归的参数。

## 线性回归的最小二乘法求解
如果定义线性回归的损失函数为$L = (\hat{Y} - Y)^T(\hat{Y} - Y)$，虽小化损失函数即最小化均方误差。

损失函数可以进一步表达为：
$$
\begin{aligned}
L(W) &= (XW - Y)^T (XW - Y) \\
&= W^TX^TXW - YXW - XWY + Y^TY\\
\end{aligned}
$$
求损失函数对$w$的偏导：
$$
\begin{aligned}
dL(W) &= Z^TdZ + dZ^TZ,\ Z = XW-Y\\
&=2Z^TdZ\\
&=2Z^Td(XW-Y)\\
&=2Z^TXdW\\
\frac{\partial L(W)}{\partial W} &= X^TZ\\
&=X^T(XW-Y)
\end{aligned}
$$
如果$X^TX$可逆，则令$\frac{\partial L(W)}{\partial W} = 0$可得$W = (X^TX)^{-1}X^TY$，如果$X^TX$不可逆，则可以利用梯度下降法进行求解。

## 最小二乘法线性回归的理解
如果假设数据标签服从高斯分布，则可以根据最大似然法推导得出最小二乘法，这里不再赘述。

如果首先对数据进行了中心化，则$X^TY$是表示各个属性和回归值相关性的一个向量，$X^TX$表示的是数据的协方差矩阵。

如果数据量小于特征个数（$X$中肯定有列相关的情况），那么$X^TX$不可逆，且容易导致过拟合。

如果一些属性之间存在近似线性关系甚至线性关系，即$X^TX$是个病态矩阵甚至$X^TX$中存在相关（$X^TX$不可逆，可以看做病态矩阵的一种极端情况），根据$X^TXW = X^TY$，当$X^TY$稍作变动，所求得的$W$变化会非常大，一般情况下，没有做特征工程的数据，都会存在特征之间有近似线性关系的情况，因此线性回归的一个问题是对噪声非常敏感。

如果$X^TX$不可逆，则$X^TXW = X^TY$的解不止一个，这种时候，为了防止过拟合，同时也防止对噪声数据过于敏感，可以在线性回归的损失函数上加一个正则化项，因此就有了岭回归和LASSO回归。

## 岭回归（ridge regression, RR）
岭回归在线性回归的损失函数中加入了L2正则化，即$L(W) = (XW - Y)^T (XW - Y) + \lambda W^TW$。
对该损失函数求梯度如下：
$$
\begin{aligned}
    dL(W) &= 2(XW-Y)^TXdW + 2\lambda W^TdW\\
    &=(2(XW-Y)^TX + 2\lambda W^T)dW\\
    \frac{\partial L(W)}{\partial W} &= 2X^T(XW-Y) + 2\lambda W
\end{aligned}
$$
令$\frac{\partial L(W)}{\partial W} = 0$可得$(X^TX + \lambda I)W = X^TY$

## 对岭回归的理解
这里和线性回归的区别在于$\lambda I$这一项。

由于$X^TX$是实对称矩阵，其特征分解可以表示为$X^TX = Q\Lambda Q^T$，其中$Q$是由特征向量构成的正交矩阵。

那么$X^TX + \lambda I = Q\Lambda Q^T + \lambda I = Q (\Lambda + \lambda I) Q^T$，可以看出加入了L2正则化之后，可以理解为一定程度上可以防止特征值为0，也可以理解为减小了$X^TX$为病态矩阵的影响。

## LASSO回归（Least absolute shrinkage and selection operator Regression）
类似于岭回归，LASSO回归在线性回归的损失函数中加入了L1正则化，即$L(W) = (XW - Y)^T (XW - Y) + \lambda ||W||_1$。

如果将$W$分解成$W^+,\ W^-$两部分，其中$W^+$只包含非负数，$W^-$只包含非正数，$W = W^+ - W^-$，那么$L(W^+, W^-) = (XW - Y)^T (XW - Y) + \lambda (I^n)^TW^+ + \lambda (I^n)^TW^-$

为了最小化$L(W^+, W^-)$，求导可得：
$$
\begin{aligned}
    dL(W^+, W^-) &= 2Z^TdZ + \lambda (I^n)^TdW^+ + \lambda (I^n)^TdW^-,\ Z = XW-Y\\
    &=2Z^TXdW + \lambda (I^n)^TdW^+ + \lambda (I^n)^TdW^-\\
    &=2Z^TXdW^+ - 2Z^TXdW^- + \lambda (I^n)^TdW^+ + \lambda (I^n)^TdW^-\\
    &=(\lambda (I^n)^T + 2Z^TX)dW^+ + (\lambda (I^n)^T - 2Z^TX)dW^-\\
    \frac{\partial L(W^+, W^-)}{\partial W^+} &= \lambda I^n + 2X^TZ\\
    &=\lambda I^n + 2X^T(XW-Y)\\
    \frac{\partial L(W^+, W^-)}{\partial W^-} &= \lambda I^n - 2X^TZ\\
    &=\lambda I^n - 2X^T(XW-Y)\\
\end{aligned}
$$

令导数为0：
$$
\begin{aligned}
    \lambda I^n + 2X^T(XW-Y) = 0\\
    \Rightarrow 2X^TXW = 2X^TY - \lambda I^n\\

    \lambda I^n - 2X^T(XW-Y) = 0\\
    \Rightarrow 2X^TXW = 2X^TY + \lambda I^n \\
\end{aligned}
$$
可以发现，如果不是$\lambda$为0，则两个导数不可能同时为0，所以希望通过导数为0得到损失函数极小值的办法行不通。

LASSO回归可以考虑坐标下降法或者是转换为带约束条件的优化问题来解决。

## 对LASSO回归的理解
如果是基于梯度的方法来求解LASSO回归，则可以发现，如果$W$中某个元素当前的值为负数，则其梯度中包含$-\lambda$这一项，向着梯度相反的方向更新的话，则需要加上$\alpha \lambda$这一项，其实是在让$W$中的元素趋近于0，即让参数变得稀疏，有助于模型的可解释性，同时$\alpha \lambda$这一项也倾向于使得这个元素的绝对值变小，有助于缓解过拟合。

# 逻辑回归
逻辑回归表面上是个回归方法，其实是用于二分类任务的，只不过需要回归的值变成了$ln\frac{P(y=1)}{P(y=0)}$，其中$P(y=1)$表示样本类别为1的概率，$P(y=0)$表示样本类别为0的概率，因为是二分类：$P(y=1) = 1 - P(y=0)$，所以$\frac{P(y=1)}{P(y=0)}$其实就是$\frac{P(y=1)}{1 - P(y=1)}$即类别为1的对数几率，因此逻辑回归也叫对数几率回归。

如果令$x \in R^n$表示一个样本，$w \in R^n,\ b\in R^n$表示逻辑回归的参数，其中$n$是数据特征维数，则逻辑回归可以表示为$ln\frac{P(y=1)}{P(y=0)} = z,\ z = w^Tx + b$，即使用$w^Tx + b$去回归类别为1的对数几率。

因为$ln\frac{P(y=1)}{1 - P(y=1)} = z,\ P(y=1) = 1 - P(y=0)$，所以可以得出$P(y=1) = \frac{1}{1 + e^{-z}},\ P(y=0) = \frac{1}{1+e^z}$。

其对数似然函数可以写成$\sum\limits_X ylnP(y=1) + (1-y)lnP(y=0)$，这个表达式就是负的二值交叉熵，因此极大化其对数似然，就是极小化二值交叉熵损失。

在分类CNN中线性回归的部分被替换为卷积神经网络，最后一层的激活函数使用Sigmoid或者Softmax，将回归输出转换为（多）类别概率，如果使用（二值）交叉熵损失函数，其实就是在最大化对数似然。






