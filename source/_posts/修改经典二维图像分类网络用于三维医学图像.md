---
title: 修改经典二维图像分类网络用于三维医学图像
date: 2018-08-21 19:34:16
tags: 深度学习
categories: 工程实践
---

因为需要使用一些经典网络，但是这些网络全是2D版本，因此按照自己的理解，实现了简化版的ResNet、DenseNet、InceptionResNet这几个模型的3D版本

代码使用kears实现，基本模型结构如下：

```python
# networks.py
import keras
from keras.layers import Input, Dense, BatchNormalization, Flatten, concatenate, Conv3D, MaxPooling3D, Dropout, Lambda, AveragePooling3D, ZeroPadding3D, merge
from keras.layers.core import Activation

################################DenseNet####################################
def TransLayer_3D(inputs, k_num, reduction = 0.5, dropout_rate=None):

    l = BatchNormalization()(inputs)
    l = Activation('relu')(l)
    l = Conv3D(int(k_num * reduction), kernel_size = (1, 1, 1), strides= (1, 1, 1), padding='valid')(l)

    if dropout_rate is not None:
        l = Dropout(dropout_rate)(l)

    l = AveragePooling3D(pool_size = (2, 2, 2), strides = (2, 2, 2))(l)

    return l

def DenseBlock_3D(inputs, layer_num = 6, k_num = 32, dropout_rate = None, kernel_szie = (3, 3, 3), strides = (1, 1, 1)):

    concat_l = inputs

    for layer_idx in range(0, layer_num):

        l = BatchNormalization()(concat_l)
        l = Activation('relu')(l)
        l = Conv3D(k_num * 4, kernel_size = (1, 1, 1), strides = (1, 1, 1), padding='valid')(l)

        if dropout_rate is not None:
            l = Dropout(dropout_rate)(l)

        l = BatchNormalization()(l)
        l = Activation('relu')(l)
        l = Conv3D(k_num, kernel_size = kernel_szie, strides = strides, padding = 'same')(l)

        concat_l = concatenate([l, concat_l], axis=-1)

    return l

def DenseNet_3D(inputs, block_layer_nums = [6, 12, 18, 12], k_init = 32, k_inc = 32, final_pooling_size = (6, 6, 6), dropout_rate = None):
    l = ZeroPadding3D((2, 2, 2))(inputs)
    l = Conv3D(k_init, kernel_size = (5, 5, 5), strides = (1, 1, 1), padding='valid')(l)
    l = ZeroPadding3D((1, 1, 1))(l)
    l = MaxPooling3D(pool_size = (3, 3, 3), strides = (2, 2, 2))(l)
    k_num = k_init

    down_sample = False

    for block_layer_num in block_layer_nums:
        if down_sample:
            l = TransLayer_3D(l, k_num=k_num, reduction=0.5)
        down_sample = True
        l = DenseBlock_3D(l, layer_num = block_layer_num, k_num = k_num, dropout_rate = dropout_rate)

        k_num += k_inc

    l = AveragePooling3D(pool_size = final_pooling_size, strides = final_pooling_size)(l)

    l = Flatten()(l)

    # l = Dense(2, activation = 'softmax')(l)

    return l


################################ResNet####################################

def ResBlock_3D(inputs, filer_num, layer_num = 2, short_cut_turn = False, down_sample = False):
    l = short_cut = inputs
    strides = (1, 1, 1)
    padding = 'same'
    if down_sample:
        strides = (2, 2, 2)
        padding = 'valid'
    if short_cut_turn == True:
        if down_sample:
            short_cut = ZeroPadding3D((1, 1, 1))(short_cut)
        short_cut = Conv3D(filer_num, kernel_size = (3, 3, 3), strides = strides, padding = padding)(short_cut)

    for layer_idx in range(layer_num - 1):
        if down_sample:
            l = ZeroPadding3D((1, 1, 1))(l)
        l = Conv3D(filer_num, kernel_size = (3, 3, 3), strides = strides, padding = padding)(l)

        if down_sample:
            down_sample = False
            strides = (1, 1, 1)
            padding = 'same'

        l = BatchNormalization()(l)
        l = Activation('relu')(l)

    l = Conv3D(filer_num, kernel_size=(3, 3, 3), strides=strides, padding=padding)(l)

    l = merge([l, short_cut], mode = 'sum')

    l = BatchNormalization()(l)
    l = Activation('relu')(l)

    return l

def ResNet_3D(inputs, k_init, k_nums = [32, 64, 128], res_block_nums = [4, 8, 16], final_pooling_size = (6, 6, 6)):
    l = ZeroPadding3D((2, 2, 2))(inputs)
    l = Conv3D(k_init, (5, 5, 5), strides = (1, 1, 1), padding = 'valid')(l)
    l = ZeroPadding3D((1, 1, 1))(l)
    l = MaxPooling3D(pool_size=(3, 3, 3), strides=(2, 2, 2))(l)
    l = BatchNormalization()(l)
    l = Activation('relu')(l)

    assert(len(k_nums) == len(res_block_nums))

    short_cut_turn = True
    down_sample = False

    for res_block_idx in range(len(k_nums)):

        for _ in range(res_block_nums[res_block_idx]):

            l = ResBlock_3D(l, k_nums[res_block_idx], 2, short_cut_turn, down_sample = down_sample)
            down_sample = False
            short_cut_turn = False

        down_sample = True
        short_cut_turn = True

    l = AveragePooling3D(pool_size = final_pooling_size, strides = final_pooling_size)(l)

    l = Flatten()(l)

    # l = Dense(2, activation='softmax')(l)

    return l


################################InceptionResNet####################################

def InceptionResBlock_3D(inputs, k_num):

    l1 = Conv3D(k_num, (1, 1, 1), strides=(1, 1, 1), padding="same")(inputs)

    l2 = Conv3D(k_num // 4, (1, 1, 1), strides=(1, 1, 1), padding="same")(inputs)
    l2 = BatchNormalization()(l2)
    l2 = Activation('relu')(l2)

    l2 = Conv3D(k_num, (3, 3, 3), strides=(1, 1, 1), padding="same")(l2)
    l2 = BatchNormalization()(l2)
    l2 = Activation('relu')(l2)

    l2 = Conv3D(k_num, (3, 3, 3), strides=(1, 1, 1), padding="same")(l2)
    l2 = BatchNormalization()(l2)
    l2 = Activation('relu')(l2)

    l2 = Conv3D(k_num, (3, 3, 3), strides=(1, 1, 1), padding="same")(l2)

    l3 = Conv3D(k_num // 4, (1, 1, 1), strides=(1, 1, 1), padding="same")(inputs)
    l3 = BatchNormalization()(l3)
    l3 = Activation('relu')(l3)
    l3 = Conv3D(k_num, (5, 5, 5), strides=(1, 1, 1), padding="same")(l3)
    l3 = BatchNormalization()(l3)
    l3 = Activation('relu')(l3)

    l3 = Conv3D(k_num, (5, 5, 5), strides=(1, 1, 1), padding="same")(l3)

    l = concatenate([l1, l2, l3])
    l = BatchNormalization()(l)
    l = Activation('relu')(l)

    l = Conv3D(k_num, (1, 1, 1), strides = (1, 1, 1), padding = 'same')(l)

    l = merge([l, inputs], mode = 'sum')

    l = BatchNormalization()(l)
    l = Activation('relu')(l)
    return l


def InceptionResNet_3D(inputs, k_init = 32, k_nums = [64, 128, 256], final_pooling_size = [6, 6, 6]):
    l = ZeroPadding3D((2, 2, 2))(inputs)
    l = Conv3D(k_init, (5, 5, 5), strides = (1, 1, 1), padding = 'valid')(l)
    l = ZeroPadding3D((1, 1, 1))(l)
    l = MaxPooling3D(pool_size=(3, 3, 3), strides=(2, 2, 2))(l)
    l = BatchNormalization()(l)
    l = Activation('relu')(l)

    down_sample = False

    for k_num in k_nums:

        if down_sample:
            l1 = ZeroPadding3D((2, 2, 2))(l)
            l1 = Conv3D(k_num, (5, 5, 5), strides = (2, 2, 2), padding = 'valid')(l1)

            l2 = ZeroPadding3D((1, 1, 1))(l)
            l2 = Conv3D(k_num, (3, 3, 3), strides = (2, 2, 2), padding = 'valid')(l2)
            l2 = BatchNormalization()(l2)
            l2 = Activation('relu')(l2)

            l2 = Conv3D(k_num, (3, 3, 3), padding='same')(l2)

            l3 = MaxPooling3D((2, 2, 2), strides = (2, 2, 2))(l)

            l = concatenate([l1, l2, l3])

            l = BatchNormalization()(l)
            l = Activation('relu')(l)

        l = Conv3D(k_num, (1, 1, 1), strides = (1, 1, 1), padding = 'valid')(l)
        l = BatchNormalization()(l)
        l = Activation('relu')(l)

        down_sample = True
        l = InceptionResBlock_3D(l, k_num)

    l = AveragePooling3D(final_pooling_size, final_pooling_size)(l)
    l = Flatten()(l)

    return l
```