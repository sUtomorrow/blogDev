---
title: 推荐系统学习笔记
date: 2020-08-11 21:21:06
tags:
---

# 推荐系统简介
## 基于用户的推荐系统
计算用户信息之间的相似性，对于和用户A相似的用户B，将用户A喜欢的物品推荐给用户B。
## 基于内容的推荐系统
计算物品之间的相似性，与某个物品A相似的物品B，将被推荐给喜欢物品A的用户。
## 基于协同过滤（Collaborative Filtering）的推荐系统
协同过滤是指用户行为，需要的数据表现为一个用户对物品的评分矩阵（或者点击、喜爱程度矩阵），协同过滤推荐有两种方式，一种是User-CF，对于喜欢的物品相似的用户A和用户B,将他们喜欢的其他东西推荐给对方。一种是Item-CF，如果一系列的物品都被同一个用户喜欢，则这些物品判定为相似，可以推荐给喜欢这类物品的其他用户。
## 混合推荐
实际系统中，一般是多种推荐方式一起使用，但是混合方式很多，例如加权混合、切换混合（在不同情况下，使用不同的推荐系统）、分区混合（不同方式推荐的物品放在不同位置显示）、分层混合（不同的推荐方式串行）。

# 推荐系统的评测方式
## 推荐系统的实验方法
- 离线实验，将已有数据作为划分为训练数据和测试数据，在训练数据上训练推荐算法，在测试数据上对测试算法进行测评。
- 用户调查，记录用户在推荐系统上的行为。
- 在线实验，AB测试：将用户随机分成两部分，分别使用不同的推荐系统，查看两部分用户的行为。

## 评测指标
### 预测准确度
- 评分预测的准确度，均方根误差（RMSE），或者平均绝对值误差（MAE）。
- Top-N推荐，精确率（precision=TP/(FP + TP)），召回率（Recall = TP / (TP+FN)）
### 用户满意度
### 覆盖率
关注冷门物品的推荐。
### 多样性
每次推荐的物品的多样性。
### 惊喜度
### 信任度
### 实时性
### 健壮性
### 商业目标


## 基于UGC(用户生成标签)的推荐
用户对物品的标签可以用三元组表示，（用户，物品，标签），表示为（u, i, b），用户u对物品i的兴趣公式可以表达为$p(u, i) = \sum\limits_b n_{u,b}n_{b,i}$，其中，$n_{u,b}$表示用户$u$打过$b$标签的次数，$n_{b, i}$表示物品$i$被打标签$b$的次数。

## TF-IDF
在UGC中，如果有热门标签，或者热门物品，那么UGC总是倾向于推荐热门物品或者有热门标签的物品，因此需要对热门程度做一个惩罚，例如在文章的关键字提取中，不能仅仅用词语出现的频率大小来决定关键字，因此提出了TF-IDF方法，通过计算TFIDF值的大小来决定文章的关键字，计算方式如下。

$TFIDF = TF \times IDF$，这个值越大，那么当前词就越可能是当前文档的关键词。

### 词频(Term Frenquency, TF)
表示某个词语在改文件中的出现频率，$TF_{i,j} = \frac{n_{i, j}}{n_j}$，$n_{i, j}$是在文档$j$中，词语$i$出现的次数，$n_j$表示在文档$j$中的总词数。

### 逆向文件频率（Inverse Document Frequency, IDF）
表示一个词语的重要性，可以用文档数目除以包含该文档的总数目，然后取对数，$IDF_i = \log(\frac{N+1}{N_i + 1})$，其中$N$表示所有文档的数量，$N_i$表示包括词语i的文档数量。

## TF-IDF对UGC推荐的改进
在原始的UGC推荐中，用户$u$对于物品$i$的感兴趣程度由$p(u, i) = \sum\limits_b n_{u,b}n_{b,i}$计算。为了避免热门标签和热门物品的影响，我们将$n_{u,b}$替换为$\frac{n_{u,b}}{\log(1 + n^{u}_b)}$，将$n_{b,i}$替换为$\frac{n_{b,i}}{\log(1 + n^{u}_i)}$，其中$n^{u}_b$表示有多少个用户打过$b$标签，$n^{u}_i$表示有多少个用户对物品$i$打过标签。
$$
\begin{aligned}
    p(u, i) = \sum\limits_b\frac{n_{u,b}}{\log(1 + n^{u}_b)}\frac{n_{b,i}}{\log(1 + n^{u}_i)}
\end{aligned}
$$

## 隐语义模型(LFM，Latent Factor Model)
对于行代表用户，列代表物品的一个喜爱程度矩阵$M \in R^{m \times n}$，可以通过矩阵分解的方式，分解为$M = P \times Q$，其中$P \in R^{m \times k}$，$Q \in R^{k \times n}$，$k$表示我们提取出来的隐语义特征的个数。

由于$M$将会是一个稀疏矩阵，很多位置的信息我们并没有收集到（而且这些位置正是我们要预测的），因此这里的矩阵分解不是求一个解析解，而是可以通过梯度下降的方式，损失函数定义如下：

$$
L = \lambda ||P_i.^T||^2 + \lambda ||Q._j||^2 + \sum\limits_{i,j} I_{i,j}(M_{i,j} - (P_i.\ Q._j))^2
$$

这里$I_{i,j}$是个指示函数，表示矩阵$M$中第$i$行第$j$列是否是已收集到的数据，训练时只对已收集的数据做约束，其他地方是需要预测的，$P_i.$表示矩阵$P$的第$i$行，$Q._j$表示矩阵$Q$的第$j$列，这两个用于模型的l2正则化。通过这样的方式将$P$、$Q$学习好之后，就可以用$PQ$来获取完整的$M$矩阵，即可知道每个用户对每个物品喜爱程度的预测值，可用于推荐。

## 未完待续。。。