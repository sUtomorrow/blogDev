---
title: 学习笔记-2019-06
date: 2019-06-20 19:13:39
tags: [学习笔记，杂项]
mathjax: true
---

# 2019-06-20
## 课程学习在弱监督学习中的应用
在网上看到了关于CVPR2017 WebVision图片分类竞赛冠军的技术分享，大致整理其主要思路。

这个比赛的数据非常多，但是标签可能存在问题，要求是在存在问题标签的数据中进行模型训练。

这里主要使用了一种课程学习的思想，先让模型学习简单的样本，然后再让其学习较困难的样本，循序渐进，得到最终的训练模型。

难点在于没有先验知识的情况下定义数据集或者图片的难易程度。

其基本思想是在同一个类别的图像的特征空间中，越密集的地方的图像特征越相似，表示图片类别越可能正确，根据这个思想将图片分类成不同的难易程度，分步，使用不同权重对待不同数据，进行模型训练。

这里先介绍一种样本分析方法density-distance：对于样本$P_i$，其特征$f(P_i)$，计算所有样本之间的特征距离矩阵：$D_{ij} = {||f(P_i) - f(P_j)||} ^ 2$，定义样本密度：$\rho_i = \sum_i X(D_{ij} - d_c) \quad X(d) = \begin{cases} 1 & d < 0 \\ 0 & other \end{cases}$，其中$d_c$是一个超参数， 计算每个样本的密度距离值：$\delta_i = \begin{cases}min_{j:\rho_j > \rho_i}(D_{ij}) & if \quad \exists j s.t. \rho_j > \rho_i \\ max(D_{ij}) & otherwise \end{cases}$，将$\rho_i$和$\delta_i$相乘，值越大的说明可能在聚类中心，数据标签越可能正确。

对数据进行分类的具体实现步骤如下：

- 设计课程
  - 使用所有数据($P_i$)训练一个特征提取模型：$f$
  - 对所有样本的进行特征提取：$f(P_i)$
  - 对每个类别的图片分别进行density-distance分析
  - 按照$\rho_i \times \delta_i$结果划分出三个子类，分别代表难易度不同的图片，作为先后训练的数据
- 课程训练
  - 首先使用第一类最简单的数据进行训练
  - 为第二类数据设置权重0.5，继续训练
  - 为第三类数据设置权重0.5，继续训练

# 2019-06-29
## BCE、CE两个分类损失与激活函数的关系
网上都说BCE用于二分类，CE用于多分类，我认为这里有些问题可以讨论，下面的例子基于tensorflow和keras。

BCE和CE是常用的分类损失函数，计算公式如下。
$$
\begin{aligned}
    BCE(x)_i &= -(y_i log(f(x)_i) + (1 - y_i) log(1 - f(x)_i)) \\
    BCE_{loss}(x) &= \frac{\sum_{i=0}^{C}BCE(x)_i}{C}\\
    CE_{loss}(x) &= CE(x) = \sum_{i=0}^{C}-y_i log(f(x)_i)
\end{aligned}
$$
从上面可以看出，CE是个标量，直接作为loss函数使用，而BCE计算出的是一个向量，在keras的实现中，BCE损失最终需要对每个类别求平均才能作为loss函数使用。

而对于一个batch的数据，在keras实现中，BCE和CE都是直接对每个样本求平均。
$$
\begin{aligned}
    BCE_{final} &= \frac{\sum_{b=1}^{N} BCE_{loss}(x^{(b)}}{N}\\
    CE_{final} &= \frac{\sum_{b=1}^{N} CE_{loss}(x^{(b)}}{N}
\end{aligned}
$$

对于一般的分类任务，最终都是一个全连接层变为输出向量$O$，之后再经过sigmoid或者softmax变为预测概率向量。

$$
\begin{aligned}
    O_{sigmoid} = [\frac{1}{1 + e^{-O_1}} \quad \frac{1}{1 + e^{-O_2}} \quad ... \quad \frac{1}{1 + e^{-O_c}}]\\
    O_{softmax} = [\frac{O_1}{\sum_{j = 1}^c O_j} \quad \frac{O_2}{\sum_{j = 1}^c O_j} \quad ... \quad \frac{O_c}{\sum_{j = 1}^c O_j}]
\end{aligned}
$$

这里要注意的是，如果输出的损失函数是用的sigmoid，那么输出向量中，每个元素是相互独立的，如果这个时候使用CE作为损失函数，CE只计算了$y_i$为1的地方的损失，对于$y_i$为0的地方不考虑，所以可能导致模型训练出现问题。

tensorflow作为后端的keras定义的两种损失函数见下方代码，一般使用时要注意from_logits这个参数，如果为False，则binary_crossentropy默认模型的输出是$O_{sigmoid}$而categorical_crossentropy默认模型的输出的是$O_{softmax}$BCE会尝试将$O_{sigmoid}$还原成$O$，然后调用tf.nn.sigmoid_cross_entropy_with_logits这个方法，而对于CE,因为softmax无法还原，因此keras直接自己写了个损失函数，否则会直接调用softmax_cross_entropy_with_logits这个方法。

```python
def binary_crossentropy(target, output, from_logits=False):
    """Binary crossentropy between an output tensor and a target tensor.

    # Arguments
        target: A tensor with the same shape as `output`.
        output: A tensor.
        from_logits: Whether `output` is expected to be a logits tensor.
            By default, we consider that `output`
            encodes a probability distribution.

    # Returns
        A tensor.
    """
    # Note: tf.nn.sigmoid_cross_entropy_with_logits
    # expects logits, Keras expects probabilities.
    if not from_logits:
        # transform back to logits
        _epsilon = _to_tensor(epsilon(), output.dtype.base_dtype)
        output = tf.clip_by_value(output, _epsilon, 1 - _epsilon)
        output = tf.log(output / (1 - output))

    return tf.nn.sigmoid_cross_entropy_with_logits(labels=target,
                                                   logits=output)


def categorical_crossentropy(target, output, from_logits=False, axis=-1):
    """Categorical crossentropy between an output tensor and a target tensor.

    # Arguments
        target: A tensor of the same shape as `output`.
        output: A tensor resulting from a softmax
            (unless `from_logits` is True, in which
            case `output` is expected to be the logits).
        from_logits: Boolean, whether `output` is the
            result of a softmax, or is a tensor of logits.
        axis: Int specifying the channels axis. `axis=-1`
            corresponds to data format `channels_last`,
            and `axis=1` corresponds to data format
            `channels_first`.

    # Returns
        Output tensor.

    # Raises
        ValueError: if `axis` is neither -1 nor one of
            the axes of `output`.
    """
    output_dimensions = list(range(len(output.get_shape())))
    if axis != -1 and axis not in output_dimensions:
        raise ValueError(
            '{}{}{}'.format(
                'Unexpected channels axis {}. '.format(axis),
                'Expected to be -1 or one of the axes of `output`, ',
                'which has {} dimensions.'.format(len(output.get_shape()))))
    # Note: tf.nn.softmax_cross_entropy_with_logits
    # expects logits, Keras expects probabilities.
    if not from_logits:
        # scale preds so that the class probas of each sample sum to 1
        output /= tf.reduce_sum(output, axis, True)
        # manual computation of crossentropy
        _epsilon = _to_tensor(epsilon(), output.dtype.base_dtype)
        output = tf.clip_by_value(output, _epsilon, 1. - _epsilon)
        return - tf.reduce_sum(target * tf.log(output), axis)
    else:
        return tf.nn.softmax_cross_entropy_with_logits(labels=target,
                                                       logits=output)
```

最后，总结一下就是：**BCE loss不是不能用于多分类, 但CE loss不适合单输出的分类, BCE loss最好用sigmoid激活函数，而CE loss最好用softmax函数。**
