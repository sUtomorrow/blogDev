---
title: 《机器学习》读书笔记
date: 2020-04-17 17:27:30
tags: [机器学习, 读书笔记, 杂项]
mathjax: true
---

# 线性模型

## 基于均方误差最小化来进行求解的方法称为最小二乘法

## 用最小二乘法来优化线性回归
线性回归的目标是学习函数$f(X) = Xw$，使得$f(X) \approx Y$，其中$X=\begin{bmatrix}x_1 & 1\\ x_2 & 1\\ \vdots & \vdots \\ x_m & 1 \end{bmatrix}$，$Y=\begin{bmatrix}y_1 \\ y_2 \\ \vdots \\ y_m\end{bmatrix}$，$w \in R^{(n+1) \times m}$，$n$是数据特征维数。

如果用均方误差作为损失函数$L = (Xw-Y)^T (Xw-Y)$，那么问题可以描述为

$$
\begin{aligned}
&\arg \min_{w} (Xw-Y)^T (Xw-Y)\\
\end{aligned}
$$
直接求导：
$$
\frac{\partial L}{\partial w} = \frac{\partial (w^T X^T X w - Y^TXw -XwY - Y^TY)}{\partial w} = 2X^T(Xw-Y)\\
$$
令：
$$
\begin{aligned}
&\frac{\partial L}{\partial w} = 0\\
&\Rightarrow w = (X^TX)^{-1}X^TY
\end{aligned}
$$
即得到使用均方误差的线性回归问题的解。

## 对数几率回归
对数几率回归即用线性回归去拟合对数几率$ln\frac{y}{1-y} = w^T x + b$

对数几率回归也等价于$y=\frac{1}{1+e^{-z}}, z = w^T x + b$

若将$y$视为后验概率，则$ln \frac{P(y=1|x)}{P(y=0|x)} = z, z=w^Tx + b$

显然有$P(y=1|x) = \frac{e^z}{1 + e^z}, P(y=0|x)=\frac{1}{1 + e^z}$

使用极大似然法求解对数几率回归：$\mathop{\arg\max}\limits_{w, b}\prod\limits_{i=1}^m P(y=y_i| x_i) \Rightarrow \mathop{\arg\max}\limits_{w, b}\sum\limits_{i=1}^m ln(P(y=y_i|x_i))$

令$\beta = \begin{bmatrix}w ^ T& b\end{bmatrix} ^ T \in R^{n+1}, Y=\begin{bmatrix}y_1&y_2&\dots&y_m\end{bmatrix} ^ T, X=\begin{bmatrix} x_1&x_2&\dots&x_m\\1&1&\dots&1\end{bmatrix},x_i \in R^n, X \in R^{(n+1) \times m}$，其中$m$是数据量。

使用极大似然法求解对数几率回归可以重写为：
$$
\begin{aligned}
&\mathop{\arg\max}\limits_{\beta} l(Z)\\
&Z = X^T \beta\\
&l(Z) = Y^Tln\frac{e^Z}{\mathbf{1} + e^Z} + (\mathbf{1}-Y)^Tln\frac{\mathbf{1}}{\mathbf{1}+e^Z}\\
&=Y^TZ - ln(\mathbf{1}+e^Z)
\end{aligned}
$$

使用牛顿法，第$t$次更新为$\beta^{t+1} \leftarrow \beta ^ t - (\triangledown_2l)^{-1}\frac{\partial l}{\partial \beta}$

$$
\begin{aligned}
dl &= Y^TdZ - \mathbf{1}^T\frac{e^Z}{\mathbf{1}+e^Z} \odot dZ\\
&=Y^TdZ -\mathbf{1}^T \hat{P}_1 \odot dZ, \hat{P} = \begin{bmatrix} P(y=1|x_1) & P(y=1|x_2)& \dots & P(y=1|x_m)\end{bmatrix}^T\\
&=Y^TX^Td\beta - \mathbf{1}^T \hat{P}_1 \odot (X^Td\beta)\\
&=Y^TX^Td\beta - (\mathbf{1} \odot \hat{P}_1)^TX^Td\beta\\
&=(Y^T-\hat{P}_1^T)X^Td\beta
\end{aligned}
$$

所以$\frac{\partial l}{\partial \beta} = X(Y-\hat{P}_1)$

$$
\begin{aligned}
    d(\frac{\partial l}{\partial \beta}) &= d(X(Y-\hat{P}_1))\\
    &=Xd\hat{P}_1\\
    &=Xd\frac{e^Z}{\mathbf{1}+e^Z}\\
    &=X(\frac{1}{1+e^Z}\odot\frac{e^Z}{1+e^Z} \odot dZ)\\
    &=X(\hat{P}_0 \odot \hat{P}_1 \odot (X^Td\beta))\\
    &=X diag(\hat{P}_0) diag(\hat{P}_1) X^Td\beta,\ diag(\hat{P}_0) = \begin{bmatrix}
        P(y=0|x_1)&\cdots&0\\
        \vdots&\ddots&\vdots\\
        0&\cdots&P(y=0|x_m)
    \end{bmatrix}
\end{aligned}
$$

所以$\frac{\partial^2 l}{\partial \beta \partial \beta^T} = \frac{\partial^2 l}{\partial \beta^T \partial \beta} = Xdiag(\hat{P}_1) diag(\hat{P}_0)X^T$

即如果用牛顿法来求解极大似然对数几率回归，第$t$次更新为

$$
\beta^{t+1} \leftarrow \beta ^ t - (Xdiag(\hat{P}_1) diag(\hat{P}_0)X^T)^{-1} X(Y-\hat{P}_1)
$$

## 线性判别分析
线性判别分析也叫“Fisher判别分析”，其思想非常朴素：试图找到一个低维空间（直线），可以表示为$y=w^Tx$(这个表达式中的$y$表示$x$投影到这个空间（直线）后和原点的距离)，使得当样本投影到该直线时不同类别样本的距离尽可能远，而相同类别的数据尽可能接近，预测时按照样本的投影位置对其进行分类。

以两类数据$x_1, x_2$为例，设$\mu_1,\mu_2,\Sigma_1,\Sigma_2$分别表示两类数据的均值和方差，则投影之后的均值和方差为$w\mu_1,w\mu_2,w^T\Sigma_1w,w^T\Sigma_2w$，因为直线是一维空间，所以这些均为实数，投影之后的类内距离可以用方差来衡量，这里使用$w^T\Sigma_1w + w^T\Sigma_2w$来度量投影之后的类内距离，而类间距离可以写成$||w\mu_2 - w\mu_1||_2^2$，同时考虑两种距离，给出希望最大化的目标函数如下。
$$
\begin{aligned}
J &= \frac{||w^T\mu_2 - w^T\mu_1||_2^2}{w^T\Sigma_1w + w^T\Sigma_2w}\\
&= \frac{w^T(\mu_2 - \mu_1)(\mu_2 - \mu_1)^Tw}{w^T(\Sigma_1 + \Sigma_2)w}
\end{aligned}
$$
定义类内散度矩阵$S_w = \Sigma_1 + \Sigma_2$，类间散度矩阵$S_b = (\mu_2 - \mu_1)(\mu_2 - \mu_1)^T$，上面的优化目标可以简写为如下。
$$
\begin{aligned}
    J = \frac{w^TS_bw}{w^TS_ww}
\end{aligned}
$$
这个优化目标又称为$S_b$和$S_w$的广义瑞利商，注意到分子分母中都有$w$的二次项，因此和$w$大小无关，只和w方向有关，所以优化问题可以写成下式。
$$
\begin{aligned}
\min_w& \quad-w^TS_bw\\
s.t.& \quad w^TS_ww = 1
\end{aligned}
$$
用拉格朗日乘子法进行优化，求解$S_bw = \lambda S_ww$，因$S_bw$方向和$\mu_2 - \mu_1$相同，因此令$S_bw = \lambda(\mu_2 - \mu_1)$，代入求解，可以得到$w = S_w^{-1}(\mu_2 - \mu_1)$。

将数据进行降维，使得类内散度最小，类间散度最大，本质上是有监督的降维。

## 多分类
可以将多分类问题拆解为二分类问题，拆解策略有三种：一对一（OvO）、一对其余（OvR）、多对多（MvM）

在MvM中，最常用的是纠错输出码（Error Correcting Output Codes，ECOC）,有$C_1C_2C_3...C_n$共$n$个类别，每个样本属于其中的一种，训练m个二分类器$f_1, f_2, ..., f_m$，每个分类器将一些类作为正类，另一些类作为负类，这样对于某个类别的样本，理想情况是$m$个分类器对其进行预测的输出组成的0,1串，构成一种长度为$m$的固定的类别组合串，$n$个类就有$n$种组合，但在预测时，对一个样本预测得到的输出串，可能不在$n$个类的$n$种组合中，这时，计算预测输出串和每个类别组合串的距离（海明距离或者欧式距离），将样本判定为距离最小的那个类别组合串对应的类别。

## 类别不平衡
解决办法主要有三种：
- 再缩放（再平衡），根据样本数量移动判定阈值或者缩放预测概率。
- 欠采样，将样本量过多的类别进行采样，减少该类别的样本数量，再拿去训练，但是这个方法容易丢失数据中的信息，最好是分成多个模型，每个模型使用该类别的一部分数据。
- 过采样，将样本量过少的类别样本进行重复，然后训练，但是这个方法容易严重过拟合，一个办法是用两个该类别样本进行插值，生成新的该类别样本。

# 决策树

## 信息熵
样本集合$D$中第$k$类样本所占比例为$p_k$，则信息熵定义为$Ent(D) = -\sum\limits_{k=1}^C p_k\log_2p_k$，其中$C$为类别个数。

## 信息熵增益
假设离散属性$a$有$v$个取值：$a_1, a_2, ..., a_v$，可以将当前数据集合分成$V$个子集：$D_1, D_2, ..., D^V$，那么信息熵增益定义为$Gain(D,a)=Ent(D)-\sum\limits_{v=1}^V\frac{|D^v|}{|D|}Ent(D^v)$

决策树构造过程，即每次选择一个信息熵增益最大的属性$a$，将数据划分为$V$个子集，然后在每个子集中进行同样的操作（不能选择已经使用过的属性），若无属性可用或者当前子集类别相同，则当前子集标记为类别数量最多的类别，停止继续划分。

## 增益率
信息熵增益的定义导致其对数量较多的$D^v$更加敏感，因此又提出了增益率的概念：$Gain_ratio(D,a)=\frac{Gain(D,a)}{IV(a)}$，其中，$IV(a)=-\sum\limits_{v=1}^V \frac{|D^v|}{|D|} \log_2\frac{|D^v|}{|D|}$，称为属性$a$的固有值。

## 基尼指数
基尼值定义为$Gini(D) = \sum\limits_{k=1}^C\sum\limits_{k' \ne k}p_k p_{k'} = 1-\sum\limits_{k=1}^Cp_k^2$，其反映了在$D$中随机抽取两个样本，属于同一类别的概率。
和信息熵增益类似，定义基尼指数为$Gini_index(D, a) = \sum\limits_{v=1}^V\frac{|D^v|}{|D|}Gini(D^v)$，构造决策树时的最优划分属性选择也可以使用能够使基尼指数最小的属性。

# 决策树的剪枝

## 预剪枝
在生成决策树的过程中，利用验证集验证当前划分是否能提升泛化效果，如果不能，则停止继续划分，直接将当前样本集作为决策树叶节点，并指定数量最多的类别做为叶节点类别。预剪枝训练时间短，但是由于其是贪心策略，容易引起欠拟合。

## 后剪枝
在决策树生成完成之后，自底向上的对非叶节点进行考察，观察将该节点直接指定为叶节点是否能提升验证集泛化效果，如果能则将此节点的子树一起合并为一个叶节点。后剪枝的训练时间比预剪枝长很多，但是其欠拟合风险小，而且泛化性能往往优于预剪枝。

# 包含连续值的决策树
如果连续属性$a$在$D$中出现$n$个取值，则将其从小到大排序为$\begin{bmatrix}a_1, a_2, ... a_n\end{bmatrix}$，这样产生$n-1$个离散值$T_a = \{\frac{a_i + a_{i+1}}{2}|1 \le i \le n-1\}$
则$Gain(D,a)= \max\limits_{t \in T_a}Gain(D, a, t)$，其中$Gain(D, a, t)$表示将$a$属性使用$t$划分为两部分，这样，连续值的决策树构造和离散值类似了，不过连续值属性在构造决策树的过程中可以使用多次。

# 属性缺失的处理
令$\tilde{D}$是所有没有缺失属性a的样本集合，对于样本$x$，有样本权重$w_x$，定义如下参数。
$$
\rho = \frac{\sum_{x\in \tilde{D}} w_x}{\sum_{x\in D}w_x}\\
\tilde{p}_k = \frac{\sum_{x\in \tilde{D}_k w_x}}{\sum_{x\in \tilde{D}}w_x}, (1\le k \le C)\\
\tilde{r}_v = \frac{\sum_{x\in \tilde{D}^v}w_x}{\sum_{x \in \tilde{D}} w_x}, (1 \le v \le V)
$$
显然，$\rho$表示属性无缺失样本所占比例，$\tilde{p}_k$表示属性无缺失样本中第$k$类所占比例，$\tilde{r}_v$表示属性无缺失样本中在属性$a$上取值$a^v$的样本比例。

由此推广信息增益为：
$$
\begin{aligned}
Gain(D, a) &= \rho \times Gain(\tilde{D}, a)\\
&=\rho \times (Ent(\tilde{D}) - \sum\limits_{v=1}^V \tilde{r}_v Ent(\tilde{D}^v))
\end{aligned}
$$
其中：
$$
Ent(\tilde{D}) = -\sum\limits_{k=1}^C \tilde{p}_k log_2 \tilde{p}_k
$$
这样解决了最优划分的属性选择问题，在构造子树时，如果样本$x$在属性$a$上的取值已知，那么$x$划分到相应子节点，且权重保持为$w_x$，如果属性$a$未知，则将$s$划分入所有的子节点，且权重调整为$\tilde{r}_v w_x$。

## 多变量决策树
叶节点不再针对某个属性，而是针对属性的线性组合进行划分。

# 神经网络

## 感知机
两层神经元，输入层（没有权重，直接前馈数据）和输出层，输出层是M-P神经元（阈值逻辑单元），感知机只能拟合线性可分的数据，否则其学习过程将变得震荡，难以收敛。

## BP算法
对于$l$层神经网络,输入$x \in R^n$，标签$y \in R^c$，第$i$层权重表示为$w_i \in R^{O_i \times I_i}, I_1 = n，O_l = c$，第$i$层偏移表示为$b_i \in R^{O_i}$，第$i$层激活函数表示为$\sigma_i$，一般是个逐元素函数，第$i$层输入即第$i-1$层的输出，表示为$l_{i-1},l_0 = x, a_i = w_i l_{i-1} + b_i, l_i = \sigma_i(a_i)$

loss函数记为$E(l_l, y)$，BP算法每次更新$w_i = w_i - \eta \frac{\partial E}{\partial w_i}$， $b_i = b_i - \eta \frac{\partial E}{\partial b_i}$，即让参数像梯度最小的方向前进。

首先定义$E$对$l_l$的偏导为$\frac{\partial E}{\partial l_l} = E'$，这个值由loss函数决定。

再考虑$l_i$对$l_{i-1}$的导数：
$$
\begin{aligned}
dl_i &= \sigma_i'(a_i) \odot (da_i)\\
&=diag(\sigma_i'(a_i))da_i\\
&=diag(\sigma_i'(a_i))w_idl_{i-1}\\
&=diag(\sigma_i'(a_i))w_i diag(\sigma_{i-1}'(a_{i-1}))w_{i-1}dl_{i-2}
\end{aligned}
$$

即得到：
$$
\begin{aligned}
\frac{\partial l_i}{\partial l_{i-1}} &= w_i^T diag(\sigma_i'(a_i))\\
\frac{\partial l_i}{\partial l_{i-2}} &= w_{i-1}^Tdiag(\sigma_{i-1}'(a_{i-1}))Wi^T diag(\sigma_i'(a_i))\\
&=\frac{\partial l_{i-1}}{\partial l_{i-2}} \frac{\partial l_i}{\partial l_{i-1}}
\end{aligned}

$$
又有：
$$
\begin{aligned}
dE &= E'^Tdl_l\\
&=E'^T(\frac{\partial l_l}{\partial l_{l-1}})^Tdl_{l-1}\\
&=E'^T(\frac{\partial l_l}{\partial l_{l-1}})^T(\frac{\partial l_{l-1}}{\partial l_{l-2}})^Tdl_{l-2}
\end{aligned}
$$

因此：
$$
\begin{aligned}
    \frac{\partial E}{\partial l_{l-1}} &= (\frac{\partial l_l}{\partial l_{l-1}})E'\\
\frac{\partial E}{\partial l_{l-2}} &= (\frac{\partial l_{l-1}}{\partial l_{l-2}})(\frac{\partial l_l}{\partial l_{l-1}})E'\\
&\cdots \\
\frac{\partial E}{\partial l_{l-k}} &= (\frac{\partial l_{l-k+1}}{\partial l_{l-k}})\cdots(\frac{\partial l_{l-1}}{\partial l_{l-2}})(\frac{\partial l_l}{\partial l_{l-1}})E'
\end{aligned}
$$

现在再来考虑$E$对$w_{l-k}$的导数：
$$
\begin{aligned}
    dE &= \frac{\partial E}{\partial l_{l-k}}^Tdl_{l-k}\\
    &= \frac{\partial E}{\partial l_{l-k}}^T(\sigma'_{l-k}(a_{l-k}) \odot (dw_{l-k}l_{l-k-1}))\\
    &= tr(\frac{\partial E}{\partial l_{l-k}}^Tdiag(\sigma'_{l-k}(a_{l-k}))dw_{l-k}l_{l-k-1})\\
    &=tr(l_{l-k-1}\frac{\partial E}{\partial l_{l-k}}^Tdiag(\sigma'_{l-k}(a_{l-k}))dw_{l-k})
\end{aligned}
$$
这里的变换属于标量对矩阵求导$d f = tr((\frac{\partial f}{\partial X}) ^ T dX)$，且用到了迹的一个性质：$tr(A B) = tr(B A)$，其中$A$和$B^T$大小相同

因此：
$$
\begin{aligned}
    \frac{\partial E}{\partial w_{l-k}} = diag(\sigma'_{l-k}(a_{l-k}))\frac{\partial E}{\partial l_{l-k}}l_{l-k-1}^T
\end{aligned}
$$

又考虑$E$对$b_{l-k}$的导数：
$$
\begin{aligned}
    dE &= \frac{\partial E}{\partial l_{l-k}}^Tdl_{l-k}\\
    &= \frac{\partial E}{\partial l_{l-k}}^Tdiag(\sigma'_{l-k}(a_{l-k}))db_{l-k}\\
\end{aligned}
$$
因此：
$$
\begin{aligned}
    \frac{\partial E}{\partial b_{l-k}} &= diag(\sigma'_{l-k}(a_{l-k}))\frac{\partial E}{\partial l_{l-k}}
\end{aligned}
$$

## RBF（Radial Basis Function，径向基函数）网络

RBF网络是指使用径向基函数作为隐层激活函数的单隐层前馈神经网络$\phi(x) = \sum\limits_{i=1}^q w_i\rho(x, c_i)$，常用的高斯径向基函数$\rho(x, c_i) = e^{-\beta_i||x-c_i||^2}$，其中$c_i,w_i$分别表示第$i$个神经元的中心和权重。

$c_i$可以使用随机采样或者聚类来获得，而其他参数$w_i, \beta_i$由BP算法等方法来获得。

# SVM
支持向量机的相关内容可以见我的另一篇文章{% post_link SVM学习笔记 SVM学习笔记 %}，这里不再重新做笔记。

# 贝叶斯分类器
## 条件风险
条件风险$R(c_i|x) = \sum\limits_{j=1}^N \lambda_{ij}P(c_j|x)$其中$\lambda_{ij}$表示将$j$类样本分类为$i$时的损失。

机器学习的任务是寻找一个判定准则$h:x \rightarrow y$以最小化总体风险$\min\limits_{h} R(h)=E_x[R(h(x)|x)]$，即在每个样本上选择$h^\star(x) = \mathop{\arg\max}\limits_c R(c|x)$，这样的分类器$h(x)$被称为贝叶斯最优分类器。与之对应的总体风险$R(h^\star)$被称为贝叶斯风险，$1-R(h^\star)$是分类器能达到的最好性能，即通过机器学习能产生的模型精度理论上限。

如果$\lambda_{ij} = \begin{cases}1&i \ne j\\0&i = j\end{cases}$，那么条件风险将变成$R(c|x) = 1-P(c|x)$，于是最小化分类错误率的贝叶斯最优分类器变成$h^\star(x) = \mathop{\arg\max}\limits_c P(c|x)$即最大化后验概率。

## 生成式与判别式模型
在使用贝叶斯分类器时，需要获取后验概率$P(c|x)$，但是非常难以直接获取，因此有两种方式：

第一种是直接对$P(c|x)$进行建模，称为判别式方法。

第二种是生成式模型，考虑$P(c|x) = \frac{P(x, c)}{P(x)}=\frac{P(c)P(x|c)}{p(x)}$，其中$P(c)$称为类先验概率，$P(x|c)$是样本$x$相对于类别$c$的类条件概率（似然），$P(c)$可以在样本量足够大时用各类样本出现的频率来估计（大数定律），但是$P(x|c)$非常难估计，因为这涉及到关于$x$的所有属性的联合概率，很难直接用样本出现的频率来进行估计。

## 极大似然估计
再生成式模型中，估计$P(x|c)$的一种策略是首先确定其满足某种确定的概率分布形式，假设$P(x|c)$被参数向量$\theta_c$唯一确定，因此$P(x|c)$可以被记为$P(x|\theta_c)$，概率模型的训练过程就是参数估计的过程，参数估计有两种方案：

- 参数虽然未知，但是却是客观存在的固定值（频率主义学派）
- 参数也是一种未观察到的随机变量，其本身也满足一定的分布，因此需要先假设参数服从一个先验分布，然后基于观测数据来计算参数的后验分布（贝叶斯学派）

极大似然估计属于频率主义学派，将参数当成未知的固定值来处理。首先令$D_c$表示训练集$D$的第$c$类样本集合，并假设这些样本独立同分布，因此其似然可以表示为$P(D_c|\theta_c) = \prod\limits_{x \in D_c}P(x|\theta_c)$，极大化似然，就能找到参数$\theta_c$。

似然的表达式中有连乘，容易造成下溢，因此通常使用对数似然$\log P(D_c|\theta_c) = \sum\limits_{x \in D_c} \log P(x|\theta_c)$。

像极大似然法这种参数化的方法的准确性严重依赖于对$P(x|\theta_c)$分布的假设，在实际应用中需要利用应用任务的经验知识，才能得到比较好的分类器。

## 朴素贝叶斯分类器
在生成式模型中，$P(c|x) = \frac{P(x, c)}{P(x)}=\frac{P(c)P(x|c)}{P(x)}$，这里估计$P(x|c)$的困难在于类条件概率$P(x|c)$是$x$所有属性的联合分布，难以从有限的训练样本中估计得到。

朴素贝叶斯分类器采用属性条件独立性假设：对所有已知类别，样本$x$的所有属性相互独立。

因此$P(c|x) = \frac{P(c)P(x|c)}{P(x)} = \frac{P(c)}{P(x)} \prod\limits_{i=1}^dP(x_i|c)$，其中$d$为样本的属性数量，$x_i$表示样本$x$的第$i$个属性值。

对于所有类别来说，$P(x)$相同，因此基于$\lambda_{ij} = \begin{cases}1&i \ne j\\0&i = j\end{cases}$，$h^\star(x) = \mathop{\arg\max}\limits_c P(c|x)$即最大化后验概率的朴素贝叶斯分类器就可以表达为$h_{nb}(x) = \mathop{\arg\max}\limits_c P(c)\prod\limits_{i=1}^dP(x_i|c)$

对于类先验概率，可以从训练集中使用$P(c) = \frac{|D_c|}{|D|}$估计。

对于离散属性，估计$P(x_i|c)$的方式常用$P(x_i|c) = \frac{|D_{c,x_i}|}{|D_c|}$，其中$D_{c,x_i}$表示$D_c$中第$i$个属性取值为$x_i$的样本集合。

对于连续属性，则可以使用概率密度函数，假定$P(x_i|c)$服从某种分布，然后对其进行估计。

在离散属性的处理上，有个问题是：如果训练集中某种属性在类别$c$上没有出现，或者类别$c$在训练集上没有出现，则$P(c)\prod\limits_{i=1}^dP(x_i|c)$直接就为0了，因此需要进行修正平滑处理。

常用的是拉普拉斯修正，即将$P(c) = \frac{|D_c|}{|D|}$更改为$P(c) = \frac{|D_c|+1}{|D|+N}$，其中N表示类别个数, 将$P(x_i|c) = \frac{|D_{c,x_i}|}{|D_c|}$更改为$P(x_i|c) = \frac{|D_{c,x_i}| + 1}{|D_c| + N_i}$，其中$N_i$表示第$i$个属性的可取值个数。这样可以避免因训练集样本不充分而导致的概率估计为零的问题。

## 半朴素贝叶斯分类器
由于属性条件独立性假设很难成立，因此尝试对条件独立性假设进行一定程度的放松。

例如独依赖估计（One-Dependent Estimator，ODE），即每个属性在类别之外最多依赖一个其他属性：$P(c|x)\propto P(c) \prod\limits_{i=1}^dP(x_i|c,pa_i)$，其中$pa_i$为属性$x_i$所依赖的属性，称为$x_i$的父属性。

问题的关键在于如何确定父属性，最直接的办法是假设所有属性都依赖于同一个父属性，称为超父（super-parent）由此形成了SPODE(Super-Parent ODE)方法。

# 集成学习
构建并结合多个学习器来完成学习任务。一般是先产生一组个体学习器，然后使用某种策略将他们结合。

根据个体学习器的生成方式，集成学习方法大致可以分为两类：
- 个体学习器之间存在强依赖关系，必须串行生成，例如：Boosting。
- 个体学习器之间不存在强依赖关系，可以同时并行生成，例如：Bagging和随机森林（Random Forest）

## Boosting
先训练一个基学习器，然后使用基学习器对训练样本分布进行调整，使得基学习器预测错误的样本在后续得到更多关注，然后进行下一个基学习器的训练，直到学习器数量达到指定数量。

Boosting中最著名的是AdaBoosting，可以理解为加性模型，即使用基学习器的线性组合$H(x) = \sum\limits_{t=1}^T\alpha_t h_t (x)$来最小化指数损失函数$l_{exp}(H|D) = E_{x \sim D}[e^{-f(x)H(x)}]$，其中$h_i$表示第$i$个基分类器，$f(x)$表示真实函数$f(x) = y$。

AdaBoosting只适用于二分类任务。

## Bagging
在原始数据中每次有放回的采样$m$个样本，组成一个子训练集，最终得到共$T$个子训练集，用每个子训练集训练一个基学习器，再将所有基学习器进行结合。

假设基学习器的计算复杂度为$O(m)$，Bagging的复杂度大致为$T(O(m) + O(s))$，其中$O(s)$为投票和采样的复杂度，一般非常小，而$T$是个比较小的常数，因此Bagging的训练复杂度和训练一个学习器的复杂程度同阶，非常高效。

## 随机森林（Random Forest，RF）
以决策树为基学习器的Bagging集成基础上，引入随机属性选择。在构造决策树过程中，选择最优划分属性时，可以在当前可用属性的随机子集中进行选择。

## 集成学习的结合策略
- 平均法
- 投票法
- 学习法

stacking是学习法的一个例子：先用原始训练集训练出初级学习器，然后将初级学习器的输出作为输入特征，训练一个次级学习器，用于结合初级学习器输出得到最终输出。最好在初级学习器的验证集上对次级学习器进行训练。

## 多样性
如果用$h_1,h_2,...,h_T$通过加权平均法来集成得到$H$来估计真实函数$f:R^d \rightarrow R$，则对样本$x$，定义学习器$h_i$的分歧为：$A(h_i|x) = (h(x_i) - H(x))^2$。集成的分歧表示为$\overline{A}(H|x) = \sum\limits_{i=1}^T w_i A(h_i|x)=\sum\limits_{i=1}^Tw_i(h_i(x) - H(x))^2$。分歧表示个体学习器在样本$x$上的不一致性。

而集成的泛化误差可以表示为$E=\overline{E} - \overline{A}$（$\overline{E} = \sum\limits_{i=1}^Tw_iE_i$，$\overline{A}=\sum_{i=1}^Tw_iA_i$，推导过程略过），这说明个体学习器误差越低，多样性越大，集成的效果越好。

增加个体学习器多样性的方法：
- 数据样本扰动：通常基于采样法，产生不同的数据子集。但是有些学习器对数据样本量的扰动不敏感（如线性学习器，支持向量机，朴素贝叶斯，k近邻，这些学习器称为稳定基学习器）
- 输入属性扰动：抽取属性子集来训练学习器。不适合属性较少的数据。
- 输出表示扰动：将样本的类别标记稍作改动，例如随机改变一些标记，或者进行编码调制，例如ECOC。
- 算法参数扰动：随机设置不同的参数，例如隐层神经元个数、初始连接权值等。

# 聚类

## 原型聚类
又称基于原型的聚类方法，此类算法假设聚类结构可以通过一组原型来刻画，通常先对原型进行初始化，然后对原型进行迭代更新求解。不同的原型表示、不同的求解方式就会产生不同的算法

### k均值算法
给定样本集$D=\{x_1,x_2,...,x_m\}$，聚类结果$C = \{C_1, C_2, ...,C_k\}$，定义平方误差$E = \sum\limits_{i=1}^k \sum\limits_{x\in C_i} ||x-\mu_i||^2_2$，其中$\mu_i = \frac{1}{C_i}\sum\limits_{x \in C_i}x$表示簇$C_i$的均值向量。

k均值算法针对平方误差$E$进行优化，使用贪心策略，通过迭代优化方式来进行：

1、初始化$k$个初始均值向量$\mu_1, \mu_2, ..., \mu_k$。

2、计算每个样本到$k$个均值向量的值，将每个样本划入最近的均值向量对应的簇中，得到一个划分。

3、使用划分好的簇计算新的均值向量。

4、如果没有均值向量被大幅更新或者达到了最大迭代次数，那么停止，否则从第2步继续循环。

### 学习向量量化（Learning Vector Quantization，LVQ）
和k均值算法类似，但是学习向量量化在学习过程中还利用样本的监督信息来辅助聚类：

1、初始化原型向量$\{p_1, p_2, ..., p_q\}$

2、随机选取样本$x_j$，计算$x_j$和每个原型向量的距离，并找出距离最小对应的原型向量。

3、如果$x_j$的标签和原型向量的标签相同，则使用$p=p + \eta (x_j - p)$来对选出的原型向量进行更新，否则使用$p = p - \eta (x_j - p)$来对选出的原型向量进行更新

4、达到最大迭代轮数或者更新幅度很小则停止更新，否则从第2步继续循环。

其中$\eta \in (0, 1)$表示学习速率，在迭代停止之后，对于任意样本$x$，可以将其划分到与其距离最近的原型向量所代表的的簇中。

### 高斯混合聚类（Mixture-of-Gaussian）
假设数据服从高斯混合分布$p_M(x) = \sum\limits_{i=1}^k \alpha_i p(x|\mu_i, \Sigma_i), \sum\limits_{i=1}^k \alpha_i=1$。

令随机变量$z_j,j\in \{1,2,...,k\}$表示样本$x_j$预测类别，其先验概率$p(z_j = i) = \alpha_i$，则$p_M(z_j = i|x_j) = \frac{\alpha_ip(x_i|\mu_i,\Sigma_i)}{\sum\limits_{l=1}^k \alpha_lp(x_j|\mu_l, \Sigma_l)}$

当高斯混合分布已知时，即可将样本划分成$k$个簇，其标记为$\mathop{\arg\max}\limits_i p_M(z_j = i|x_j)$

估计这样的高斯混合分布可以使用极大似然法，令对数似然$LL(p_M) = \sum\limits_{j=1}^m ln(\sum\limits_{i=1}^k \alpha_ip(x_i|\mu_i,\Sigma_i))$，使用EM算法即可求解。

## 密度聚类
### DBSCAN
密度聚类算法的代表是DBSCAN算法，使用一组邻域参数$(\epsilon, MinPts)$来刻画样本分布的紧密程度。

- $\epsilon$-邻域：$N_\epsilon(x_j) = \{x_i | x_i \in D, dist(x_j,x_i)\le \epsilon\}$，其中$D$表示全部数据集合。
- 核心对象：满足$|N_\epsilon(x_j)| \ge MinPts$的$x_j$称为核心对象，其中$|N_\epsilon(x_j)|$表示$x_j$的$\epsilon$-邻域中的样本数量。
- 密度直达：$x_j$是一个核心对象，且$x_i$在$x_j$的$\epsilon$-邻域中，则$x_i$由$x_j$密度直达。
- 密度可达：如果存在样本序列$p_1, p_2, ..., p_n, p_1 = x_j, p_n = x_i$使得$p_{m+1}$由$p_m$密度直达，则称$x_i$由$x_j$密度可达。
- 密度相连：如果存在$x_k$使得$x_j$和$x_i$均可由$x_k$密度可达，则称$x_i$和$x_j$密度相连。

基于以上概念，DBSCAN将簇定义为由密度可达关系到处的最大密度相连样本集合。

DBSCAN的聚类过程如下：

1、找到所有的核心对象。

2、循环随机取一个还没有访问过的核心对象，将其所有密度相连的样本生成一个簇并标记为已访问，如果没有未访问的核心对象，则停止循环

3、仍未访问的样本被视为噪声样本。

## 层次聚类
层次聚类方法试图在不同层次上对数据进行划分，形成树形的聚类结构。

### AGNES

AGNES是一种自底向上的层次聚类算法，首先将所有样本单独看做一个簇，然后每次迭代找到距离最近的两个簇进行合并，直到簇数量等于指定数量。

这里的关键是如何定义两个簇的距离，主要有三种方式，使用三种距离计算的AGNES分别被称为：
- 最大距离：单链接算法
- 最小距离：全链接算法
- 平均距离：均链接算法

# 降维与度量学习

## k近邻（k-Nearest Neighbor）学习
k近邻方法没有训练过程，在给定测试样本时，直接使用训练样本中与其最靠近的$k$个样本，基于这$k$个样本的信息来对测试样本进行预测。

k近邻方法是懒惰学习（lazy learning）的一个代表，而那些在训练阶段就对样本进行学习处理的方法称为急切学习（eager learning）

## 低维嵌入
大部分时候，观测到的数据是高维数据，但与学习任务密切相关的很可能是高维空间中的一个低维嵌入。

### 多维缩放（Multiple Dimensional Scaling，MDS）
多维缩放的思路是找到一个低维空间，使得在这个低维空间中的欧氏距离和原始空间中的距离相等。

假如原始空间的维度为$d$，所有数据的距离矩阵$D\in R^{m \times m}$，其中$m$为样本数量，$d_{ij}$表示样本$x_i$和$x_j$的距离，降维后的数据$z \in R^{d'}$，所有样本表示为$Z\in R^{d' \times m}$。

令$B = Z^T Z \in R^{m\times m}$是降维后的内积矩阵，有$b_{ij} = z_i^T z_j$

则$d_{ij}^2 = ||z_i||^2 + ||z_j||^2 - 2z_i^T z_j = b_{ii} + b_{jj} - 2b_{ij}$

令降维后的样本$Z$被中心化，即$\sum\limits_{i=1}^m z_i = \mathbf{0}$，可得到$\sum\limits_{i=1}^m b_{ij} = \sum\limits_{j=1}^m b_{ij} = 0$

因此：

$$
\sum\limits_{i=1}^md_{ij}^2 = tr(B) + mb_{jj}\\
\sum\limits_{j=1}^md_{ij}^2 = tr(B) + mb_{ii}\\
\sum\limits_{i=1}^m\sum\limits_{j=1}^md_{ij}^2 = 2m\ tr(B)\\
tr(B) = \sum\limits_{i=1}^m b_{ii}
$$

则有：

$$
\begin{aligned}
    b_{ij} &= \frac{b_{ii} + b_{jj} - d_{ij}^2}{2}\\
    &=\frac{1}{2m}(\sum\limits_{i=1}^md_{ij}^2) + \frac{1}{2m}(\sum\limits_{j=1}^md_{ij}^2) - \frac{1}{2m^2} \sum\limits_{i=1}^m\sum\limits_{j=1}^md_{ij}^2 - \frac{d_{ij}^2}{2}
\end{aligned}
$$

这样就能根据原来的距离矩阵求出内积矩阵$B$，再对$B$进行特征分解得到$B=V \Lambda V^T$，其中$\Lambda = diag(\lambda_1, ..., \lambda_d)$为特征值构成的对角阵，$\lambda_1 \ge \lambda_2\ge ...\ge \lambda_d$，为了降维，我们可以取其中$d'$个非零值，构成对角矩阵$\Lambda_\star$与其对应的特征向量矩阵$V_\star$，则$Z = \Lambda_\star ^{\frac{1}{2}} V_\star^T \ in R^{d' \times m}$

## 主成分分析（PCA）
如果希望使用一个超平面来对数据进行表示，那么可以从两个方面去考虑：
- 最近重构性：样本点到这个超平面的距离都足够近
- 最大可分性：样本点在这个超平面上的投影尽可能分开

但是两个方面的考虑最终都会得到PCA的等价推导，即PCA既保证了最近重构性也保证了最大可分性。

假定数据$x_i \in R^d$已经进行过中心化，即$\sum_i x_i = \mathbf{0}$，现在使用一组标准正交基对$x_i$进行投影，得到$z_i = Wx_i, W = \begin{bmatrix}w_1^T\\ w_2^T\\ \vdots\\ w_{d'}^T \end{bmatrix}\in R^{d' \times d}, w_i^T w_j = \begin{cases}1 & i=j\\ 0& i\ne j\end{cases}$，其中$z_{ij} = w_j^Tx_i$，如果使用$z_i$来还原$x_i$则得到$\hat{x}_i = \sum\limits_{j=1}^{d'}z_{ij}w_j = W^T z_i$。

如果从最近重构性来考虑，我们希望最小化$\sum\limits_{i=1}^m ||\hat{x}_i - x_i||^2_2$，即：
$$
\begin{aligned}
    &\min \sum\limits_{i=1}^m ||\hat{x}_i - x_i||^2_2\\
    &=\min \sum\limits_{i=1}^m || \sum\limits_{j=1}^{d'}z_{ij}w_j - x_i||^2_2\\
    &=\min \sum\limits_{i=1}^m || W^Tz_i - x_i||^2_2\\
    &=\min \sum\limits_{i=1}^m (z_i^TWW^Tz_i - z_i^TWx_i - x^T_iW^Tz_i + x^T_i x_i)\\
    &=\min \sum\limits_{i=1}^m (z_i^Tz_i - z_i^TWx_i - x^T_iW^Tz_i)\\
    &=\min \sum\limits_{i=1}^m tr(z_i^Tz_i - z_i^TWx_i - x^T_iW^Tz_i)\\
    &=\min \sum\limits_{i=1}^m tr(z_i^Tz_i - 2z_i^TWx_i)\\
    &=\min \sum\limits_{i=1}^m -tr(z_i^T z_i)\\
    &=\min -tr(Z^T Z),\ Z = \begin{bmatrix}z_1 &z_2 & \cdots &z_m\end{bmatrix} = WX,\ X = \begin{bmatrix}x_1 &x_2 & \cdots &x_m\end{bmatrix}\\
    &=\min -tr(X^TW^TWX)\\
    &=\min -tr(WXX^TW^T)\\
\end{aligned}
$$

因此我们需要解决的问题就是：
$$
\begin{aligned}
    \min\ & -tr(WXX^TW^T)\\
    s.t.\ & WW^T = I_{d'}
\end{aligned}
$$

另一方面，如果从最大可分性来考虑，我们希望最大化$z_i$之间的方差$\sum\limits_{i=1}^m (z_i - \frac{1}{m}\sum\limits_{j=1}^mz_j)^2$

$$
\begin{aligned}
    &\max \sum\limits_{i=1}^m ||z_i - \frac{1}{m}\sum\limits_{j=1}^mz_j||_2^2\\
    &=\max \sum\limits_{i=1}^m ||z_i - \frac{1}{m}\sum\limits_{j=1}^mWx_j||_2^2\\
    &=\max \sum\limits_{i=1}^m ||z_i - \frac{1}{m}W\sum\limits_{j=1}^mx_j||_2^2\\
    &=\max \sum\limits_{i=1}^m ||z_i||_2^2\\
    &=\max \sum\limits_{i=1}^m (z^T_iz_i)\\
    &=\max\ tr(Z^TZ)\\
    &=\max\ tr(X^TW^TWX)\\
    &=\min\ -tr(X^TW^TWX)\\
    &=\min\ -tr(WXX^TW^T)\\
\end{aligned}
$$

因此我们需要解决的问题就是：
$$
\begin{aligned}
    \min\ & -tr(WXX^TW^T)\\
    s.t.\ & WW^T = I_{d'}\\
\end{aligned}
$$

由此可见，在$\sum x_i = 0$的情况下，从两个方面得到的结果完全相同。

求解PCA可以使用拉格朗日法，首先得到拉格朗日函数$L(W) = -tr(X^TW^TWX) + \lambda (WW^T - I_{d'}), \lambda \ge 0$

$$
\begin{aligned}
    dL(W) &= -tr(WXX^TdW^T + dWXX^TW^T) + \lambda tr(dW W^T + WdW^T)\\
    &= -tr(2XX^TW^TdW - 2\lambda W^TdW)\\
    \\
    \frac{\partial dL(W)}{\partial W} &= 2\lambda W - 2WXX^T
\end{aligned}
$$

令$\frac{\partial dL(W)}{\partial W} = 0$可得$\lambda W^T = XX^TW^T$，即求出协方差矩阵$XX^T$的特征向量即可构成$W^T$，在这个过程中，可以舍弃一部分特征向量，只取特征值最大的$d'$个特征向量，即可将数据维度从$d$维缩减到$d'$维。

如果将$X$进行奇异值分解，则有$X=D\Sigma V^T$，$XX^T = D \Sigma \Sigma^T D^T$，其中$D$是$X$的左奇异矩阵，也就是$XX^T$的特征矩阵，如果令$W=D^T$可以得到$Z=D^TX=D^TD\Sigma V^T = \Sigma V^T$，因此求出$X^TX$的特征矩阵也可以求出$Z$。

## 核化线性降维

### 核主成分分析（KPCA）

PCA是一种线性降维方式，在降维时假设从高维空间到低维空间的映射是线性的，其线性映射由$W$确定，且有$\lambda W^T = XX^TW^T = (\sum\limits_{i=1}^m x_i x^T_i)W^T$，假设在高维空间$\phi(x)$中进行PCA，则有$\lambda W^T = (\sum\limits_{i=1}^m \phi(x_i) \phi(x_i)^T)W^T$，有如下推导：

$$
\begin{aligned}
    \lambda W^T &= (\sum\limits_{i=1}^m \phi(x_i) \phi(x_i)^T)W^T\\
    &= (\sum\limits_{i=1}^m \phi(x_i) \phi(x_i)^T)W^T\\
    &= \sum\limits_{i=1}^m \phi(x_i)\phi(x_i)^TW^T\\
    &= \sum\limits_{i=1}^m\phi(x_i)\alpha_i,\ \alpha_i = \phi(x_i)^TW^T\\
\end{aligned}
$$

由于$\phi$函数无法明确求出，因此引入核函数。
$$
\begin{aligned}
    \lambda \phi(x_j)^TW^T &= \phi(x_j)^T\sum\limits_{i=1}^m\phi(x_i)\alpha_i\\
    \lambda A&=KA\\
\end{aligned}
$$

其中$K$是核矩阵$K_{ij} = \phi(x_i)^T\phi(x_j)$，$A = \begin{bmatrix}\alpha_1, \alpha_2, ..., \alpha_m\end{bmatrix}$

则$z_{ij} = w_j^T \phi(x_i) = \sum\limits_{k=1}^m\alpha_{k,j}\phi(x_k)^T\phi(x_i)=\sum\limits_{k=1}^m\alpha_{k,j}K_{ki}$表示降维之后$x_i$对应向量的第$j$个分量。

其中W可以由$\phi(X)^T\phi(X)$的特征矩阵求出，但在计算$z$时，由于每个分量需要求和$\sum\limits_{k=1}^m\alpha_{k,j}$，计算量非常大。






