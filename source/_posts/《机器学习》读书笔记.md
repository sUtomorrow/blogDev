---
title: 《机器学习》读书笔记
date: 2020-04-17 17:27:30
tags: [机器学习，读书笔记，杂项]
mathjax: true
---

# 线性模型

## 基于均方误差最小化来进行求解的方法称为最小二乘法

## 用最小二乘法来优化线性回归
线性回归的目标是学习函数$f(X) = Xw$，使得$f(X) \approx Y$，其中$X=\begin{bmatrix}x_1 & 1\\ x_2 & 1\\ \vdots & \vdots \\ x_m & 1 \end{bmatrix}$，$Y=\begin{bmatrix}y_1 \\ y_2 \\ \vdots \\ y_m\end{bmatrix}$，$w \in R^{(n+1) \times m}$，$n$是数据特征维数。

如果用均方误差作为损失函数$L = (Xw-Y)^T (Xw-Y)$，那么问题可以描述为

$$
\begin{aligned}
&\arg \min_{w} (Xw-Y)^T (Xw-Y)\\
\end{aligned}
$$
直接求导：
$$
\frac{\partial L}{\partial w} = \frac{\partial (w^T X^T X w - Y^TXw -XwY - Y^TY)}{\partial w} = 2X^T(Xw-Y)\\
$$
令：
$$
\begin{aligned}
&\frac{\partial L}{\partial w} = 0\\
&\Rightarrow w = (X^TX)^{-1}X^TY
\end{aligned}
$$
即得到使用均方误差的线性回归问题的解。

## 对数几率回归
对数几率回归即用线性回归去拟合对数几率$ln\frac{y}{1-y} = w^T x + b$

对数几率回归也等价于$y=\frac{1}{1+e^{-z}}, z = w^T x + b$

若将$y$视为后验概率，则$ln \frac{P(y=1|x)}{P(y=0|x)} = z, z=w^Tx + b$

显然有$P(y=1|x) = \frac{e^z}{1 + e^z}, P(y=0|x)=\frac{1}{1 + e^z}$

使用极大似然法求解对数几率回归：$\mathop{\arg\max}\limits_{w, b}\prod\limits_{i=1}^m P(y=y_i| x_i) \Rightarrow \mathop{\arg\max}\limits_{w, b}\sum\limits_{i=1}^m ln(P(y=y_i|x_i))$

令$\beta = \begin{bmatrix}w ^ T& b\end{bmatrix} ^ T \in R^{n+1}, Y=\begin{bmatrix}y_1&y_2&\dots&y_m\end{bmatrix} ^ T, X=\begin{bmatrix} x_1&x_2&\dots&x_m\\1&1&\dots&1\end{bmatrix},x_i \in R^n, X \in R^{(n+1) \times m}$，其中$m$是数据量。

使用极大似然法求解对数几率回归可以重写为：
$$
\begin{aligned}
&\mathop{\arg\max}\limits_{\beta} l(Z)\\
&Z = X^T \beta\\
&l(Z) = Y^Tln\frac{e^Z}{\mathbf{1} + e^Z} + (\mathbf{1}-Y)^Tln\frac{\mathbf{1}}{\mathbf{1}+e^Z}\\
&=Y^TZ - ln(\mathbf{1}+e^Z)
\end{aligned}
$$

使用牛顿法，第$t$次更新为$\beta^{t+1} \leftarrow \beta ^ t - (\triangledown_2l)^{-1}\frac{\partial l}{\partial \beta}$

$$
\begin{aligned}
dl &= Y^TdZ - \mathbf{1}^T\frac{e^Z}{\mathbf{1}+e^Z} \odot dZ\\
&=Y^TdZ -\mathbf{1}^T \hat{P}_1 \odot dZ, \hat{P} = \begin{bmatrix} P(y=1|x_1) & P(y=1|x_2)& \dots & P(y=1|x_m)\end{bmatrix}^T\\
&=Y^TX^Td\beta - \mathbf{1}^T \hat{P}_1 \odot (X^Td\beta)\\
&=Y^TX^Td\beta - (\mathbf{1} \odot \hat{P}_1)^TX^Td\beta\\
&=(Y^T-\hat{P}_1^T)X^Td\beta
\end{aligned}
$$

所以$\frac{\partial l}{\partial \beta} = X(Y-\hat{P}_1)$

$$
\begin{aligned}
    d(\frac{\partial l}{\partial \beta}) &= d(X(Y-\hat{P}_1))\\
    &=Xd\hat{P}_1\\
    &=Xd\frac{e^Z}{\mathbf{1}+e^Z}\\
    &=X(\frac{1}{1+e^Z}\odot\frac{e^Z}{1+e^Z} \odot dZ)\\
    &=X(\hat{P}_0 \odot \hat{P}_1 \odot (X^Td\beta))\\
    &=X diag(\hat{P}_0) diag(\hat{P}_1) X^Td\beta,\ diag(\hat{P}_0) = \begin{bmatrix}
        P(y=0|x_1)&\cdots&0\\
        \vdots&\ddots&\vdots\\
        0&\cdots&P(y=0|x_m)
    \end{bmatrix}
\end{aligned}
$$

所以$\frac{\partial^2 l}{\partial \beta \partial \beta^T} = \frac{\partial^2 l}{\partial \beta^T \partial \beta} = Xdiag(\hat{P}_1) diag(\hat{P}_0)X^T$

即如果用牛顿法来求解极大似然对数几率回归，第$t$次更新为
$$
\beta^{t+1} \leftarrow \beta ^ t - (Xdiag(\hat{P}_1) diag(\hat{P}_0)X^T)^{-1} X(Y-\hat{P}_1)
$$

## 线性判别分析
线性判别分析也叫“Fisher判别分析”，其思想非常朴素：试图找到一个低维空间（直线），可以表示为$y=w^Tx$(这个表达式中的$y$表示$x$投影到这个空间（直线）后和原点的距离)，使得当样本投影到该直线时不同类别样本的距离尽可能远，而相同类别的数据尽可能接近，预测时按照样本的投影位置对其进行分类。

以两类数据$x_1, x_2$为例，设$\mu_1,\mu_2,\Sigma_1,\Sigma_2$分别表示两类数据的均值和方差，则投影之后的均值和方差为$w\mu_1,w\mu_2,w^T\Sigma_1w,w^T\Sigma_2w$，因为直线是一维空间，所以这些均为实数，投影之后的类内距离可以用方差来衡量，这里使用$w^T\Sigma_1w + w^T\Sigma_2w$来度量投影之后的类内距离，而类间距离可以写成$||w\mu_2 - w\mu_1||_2^2$，同时考虑两种距离，给出希望最大化的目标函数如下。
$$
\begin{aligned}
J &= \frac{||w^T\mu_2 - w^T\mu_1||_2^2}{w^T\Sigma_1w + w^T\Sigma_2w}\\
&= \frac{w^T(\mu_2 - \mu_1)(\mu_2 - \mu_1)^Tw}{w^T(\Sigma_1 + \Sigma_2)w}
\end{aligned}
$$
定义类内散度矩阵$S_w = \Sigma_1 + \Sigma_2$，类间散度矩阵$S_b = (\mu_2 - \mu_1)(\mu_2 - \mu_1)^T$，上面的优化目标可以简写为如下。
$$
\begin{aligned}
    J = \frac{w^TS_bw}{w^TS_ww}
\end{aligned}
$$
这个优化目标又称为$S_b$和$S_w$的广义瑞利商，注意到分子分母中都有$w$的二次项，因此和$w$大小无关，只和w方向有关，所以优化问题可以写成下式。
$$
\begin{aligned}
\min_w& \quad-w^TS_bw\\
s.t.& \quad w^TS_ww = 1
\end{aligned}
$$
用拉格朗日乘子法进行优化，求解$S_bw = \lambda S_ww$，因$S_bw$方向和$\mu_2 - \mu_1$相同，因此令$S_bw = \lambda(\mu_2 - \mu_1)$，代入求解，可以得到$w = S_w^{-1}(\mu_2 - \mu_1)$。

将数据进行降维，使得类内散度最小，类间散度最大，本质上是有监督的降维。

## 多分类
可以将多分类问题拆解为二分类问题，拆解策略有三种：一对一（OvO）、一对其余（OvR）、多对多（MvM）

在MvM中，最常用的是纠错输出码（Error Correcting Output Codes，ECOC）,将$C_1C_2C_3...C_n$$n$个类别，每个样本属于其中的一种，训练m个二分类器$f_1, f_2, ..., f_m$，每个分类器将一些类作为正类，另一些类作为负类，这样对于某个类别的样本，理想情况是$m$个分类器对其进行预测的输出组成的0,1串，构成一种长度为$m$的固定的类别组合串，$n$个类就有$n$种组合，但在预测时，对一个样本预测得到的输出串，可能不在$n$个类的$n$种组合中，这时，计算预测输出串和每个类别组合串的距离（海明距离或者欧式距离），将样本判定为距离最小的那个类别组合串对应的类别。

## 类别不平衡
解决办法主要有三种：
- 再缩放（再平衡），根据样本数量移动判定阈值或者缩放预测概率。
- 欠采样，将样本量过多的类别进行采样，减少该类别的样本数量，再拿去训练，但是这个方法容易丢失数据中的信息，最好是分成多个模型，每个模型使用该类别的一部分数据。
- 过采样，将样本量过少的类别样本进行重复，然后训练，但是这个方法容易严重过拟合，一个办法是用两个该类别样本进行插值，生成新的该类别样本。

# 决策树

## 信息熵
样本集合$D$中第$k$类样本所占比例为$p_k$，则信息熵定义为$Ent(D) = -\sum\limits_{k=1}^C p_k\log_2p_k$，其中$C$为类别个数。

## 信息熵增益
假设离散属性$a$有$v$个取值：$a_1, a_2, ..., a_v$，可以将当前数据集合分成$V$个子集：$D_1, D_2, ..., D_V$，那么信息熵增益定义为$Gain(D,a)=Ent(D)-\sum\limits_{v=1}^V\frac{|D_v|}{|D|}Ent(D_v)$

决策树构造过程，即每次选择一个信息熵增益最大的属性$a$，将数据划分为$V$个子集，然后在每个子集中进行同样的操作（不能选择已经使用过的属性），若无属性可用或者当前子集类别相同，则当前子集标记为类别数量最多的类别，停止继续划分。

## 增益率
信息熵增益的定义导致其对数量较多的$D_v$更加敏感，因此又提出了增益率的概念：$Gain_ratio(D,a)=\frac{Gain(D,a)}{IV(a)}$，其中，$IV(a)=-\sum\limits_{v=1}^V \frac{|D_v|}{|D|} \log_2\frac{|D_v|}{|D|}$，称为属性$a$的固有值。

## 基尼指数
基尼值定义为$Gini(D) = \sum\limits_{k=1}^C\sum\limits_{k' \ne k}p_k p_{k'} = 1-\sum\limits_{k=1}^Cp_k^2$，其反映了在$D$中随机抽取两个样本，属于同一类别的概率。
和信息熵增益类似，定义基尼指数为$Gini_index(D, a) = \sum\limits_{v=1}^V\frac{|D_v|}{|D|}Gini(D_v)$，构造决策树时的最优划分属性选择也可以使用能够使基尼指数最小的属性。

# 决策树的剪枝

## 预剪枝
在生成决策树的过程中，利用验证集验证当前划分是否能提升泛化效果，如果不能，则停止继续划分，直接将当前样本集作为决策树叶节点，并指定数量最多的类别做为叶节点类别。预剪枝训练时间短，但是由于其是贪心策略，容易引起欠拟合。

## 后剪枝
在决策树生成完成之后，自底向上的对非叶节点进行考察，观察将该节点直接指定为叶节点是否能提升验证集泛化效果，如果能则将此节点的子树一起合并为一个叶节点。后剪枝的训练时间比预剪枝长很多，但是其欠拟合风险小，而且泛化性能往往优于预剪枝。

# 包含连续值的决策树
如果连续属性$a$在$D$中出现$n$个取值，则将其从小到大排序为$\begin{bmatrix}a_1, a_2, ... a_n\end{bmatrix}$，这样产生$n-1$个离散值$T_a = \{\frac{a_i + a_{i+1}}{2}|1 \le i \le n-1\}$
则$Gain(D,a)= \max\limits_{t \in T_a}Gain(D, a, t)$，其中$Gain(D, a, t)$表示将$a$属性使用$t$划分为两部分，这样，连续值的决策树构造和离散值类似了，不过连续值属性在构造决策树的过程中可以使用多次。