---
title: 学习笔记-2019-07
date: 2019-07-04 20:16:54
tags: [学习笔记，杂项]
mathjax: true
---
# 2019-07-04
## 几种回归的概念
线性回归很简单，就是$y=W^Tx + b$，而广义线性回归则可以用一个单调可微的函数$g(\centerdot)$写成$y=g^{-1}(W^Tx + b)$或者$g(y) = W^Tx + b$,对数回归$ln(y) = W^Tx + b$就是广义线性回归的一个例子。

如果要使用回归来解决分类问题，理想的情况是使用单位阶跃函数将回归值映射为类别标签，但单位阶跃函数不连续，因此使用对数几率函数来完成这个映射，对数几率函数即$\frac{1}{1 + e^{-z}}$，是sigmoid函数的一种。

这里顺便介绍下几率的概念：若一个事件发生的概率为$y$，则这个事件不发生的概率为$1-y$，两者的比值$\frac{y}{1-y}$被称为几率，反应的是事件发生的相对可能性，对几率取对数得到对数几率$log\frac{y}{1-y}$即为logit，逻辑回归(logistic regression)也可称为对数几率回归(logit regressoion)，就是用线性回归去逼近对数几率，如下：
$$
\begin{aligned}
    y &= \frac{1}{1 + e^{-z}}\\
    代入线&性回归得到的z：\\
    y &= \frac{1}{1 + e^{-(W^Tx + b)}}\\
    log\frac{y}{1-y} &= W^Tx + b
\end{aligned}
$$

## 线性判别分析(Linear Discriminant Analysis, LDA)
线性判别分析也叫“Fisher判别分析”，其思想非常朴素：试图找到一条直线$y=w^Tx$，可以使得当样本投影到该直线时不同类别样本的距离尽可能远，预测时按照样本的投影位置对其进行分类。

