---
title: 学习笔记-2019-07
date: 2019-07-04 20:16:54
tags: [学习笔记，杂项]
mathjax: true
---
# 2019-07-04
## 几种回归的概念
线性回归很简单，就是$y=W^Tx + b$，而广义线性回归则可以用一个单调可微的函数$g(\centerdot)$写成$y=g^{-1}(W^Tx + b)$或者$g(y) = W^Tx + b$,对数回归$ln(y) = W^Tx + b$就是广义线性回归的一个例子。

如果要使用回归来解决分类问题，理想的情况是使用单位阶跃函数将回归值映射为类别标签，但单位阶跃函数不连续，因此使用对数几率函数来完成这个映射，对数几率函数即$\frac{1}{1 + e^{-z}}$，是sigmoid函数的一种。

这里顺便介绍下几率的概念：若一个事件发生的概率为$y$，则这个事件不发生的概率为$1-y$，两者的比值$\frac{y}{1-y}$被称为几率，反应的是事件发生的相对可能性，对几率取对数得到对数几率$log\frac{y}{1-y}$即为logit，逻辑回归(logistic regression)也可称为对数几率回归(logit regressoion)，就是用线性回归去逼近对数几率，如下：
$$
\begin{aligned}
    y &= \frac{1}{1 + e^{-z}}\\
    代入线&性回归得到的z：\\
    y &= \frac{1}{1 + e^{-(W^Tx + b)}}\\
    log\frac{y}{1-y} &= W^Tx + b
\end{aligned}
$$

## 线性判别分析(Linear Discriminant Analysis, LDA)
线性判别分析也叫“Fisher判别分析”，其思想非常朴素：试图找到一条过原点的直线，可以表示为$y=w^Tx$(这个表达式中的$y$表示$x$投影到这条直线后和原点的距离)，使得当样本投影到该直线时不同类别样本的距离尽可能远，而相同类别的数据尽可能接近，预测时按照样本的投影位置对其进行分类。

以两类数据$x_1, x_2$为例，设$\mu_1,\mu_2,\Sigma_1,\Sigma_2$分别表示两类数据的均值和方差，则投影之后的均值和方差为$w\mu_1,w\mu_2,w^T\Sigma_1w,w^T\Sigma_2w$，因为直线是一维空间，所以这些均为实数，投影之后的类内距离可以用方差来衡量，这里使用$w^T\Sigma_1w + w^T\Sigma_2w$来度量投影之后的类内距离，而类间距离可以写成$||w\mu_2 - w\mu_1||_2^2$，同时考虑两种距离，给出希望最大化的目标函数如下。
$$
\begin{aligned}
J &= \frac{||w^T\mu_2 - w^T\mu_1||_2^2}{w^T\Sigma_1w + w^T\Sigma_2w}\\
&= \frac{w^T(\mu_2 - \mu_1)(\mu_2 - \mu_1)^Tw}{w^T(\Sigma_1 + \Sigma_2)w}
\end{aligned}
$$
定义类内散度矩阵$S_w = \Sigma_1 + \Sigma_2$，类间散度矩阵$S_b = (\mu_2 - \mu_1)(\mu_2 - \mu_1)^T$，上面的优化目标可以简写为如下。
$$
\begin{aligned}
    J = \frac{w^TS_bw}{w^TS_ww}
\end{aligned}
$$
这个优化目标又称为$S_b$和$S_w$的广义瑞利商，注意到分子分母中都有$w$的二次项，因此和$w$大小无关，只和w方向有关，所以优化问题可以写成下式。
$$
\begin{aligned}
\min_w& \quad-w^TS_bw\\
s.t.& \quad w^TS_ww = 1
\end{aligned}
$$
用拉格朗日乘子法进行优化，求解$S_bw = \lambda S_ww$，因$S_bw$方向和$\mu_2 - \mu_1$相同，因此令$S_bw = \lambda(\mu_2 - \mu_1)$，代入求解，可以得到$w = S_w^{-1}(\mu_2 - \mu_1)$。
