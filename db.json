{"meta":{"version":1,"warehouse":"4.0.0"},"models":{"Asset":[{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/hook-duoshuo.js","path":"js/src/hook-duoshuo.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","path":"lib/algolia-instant-search/instantsearch.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","path":"lib/algolia-instant-search/instantsearch.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","path":"lib/canvas-nest/canvas-nest.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","path":"lib/canvas-ribbon/canvas-ribbon.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/LICENSE","path":"lib/fastclick/LICENSE","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/README.md","path":"lib/fastclick/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/bower.json","path":"lib/fastclick/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","path":"lib/jquery_lazyload/CONTRIBUTING.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","path":"lib/jquery_lazyload/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","path":"lib/jquery_lazyload/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","path":"lib/jquery_lazyload/jquery.lazyload.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","path":"lib/jquery_lazyload/jquery.scrollstop.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/font-embedded.css","path":"lib/needsharebutton/font-embedded.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.css","path":"lib/needsharebutton/needsharebutton.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.js","path":"lib/needsharebutton/needsharebutton.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","path":"lib/pace/pace-theme-barber-shop.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","path":"lib/pace/pace-theme-big-counter.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","path":"lib/pace/pace-theme-bounce.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","path":"lib/pace/pace-theme-center-atom.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","path":"lib/pace/pace-theme-center-circle.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","path":"lib/pace/pace-theme-center-radar.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","path":"lib/pace/pace-theme-center-simple.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","path":"lib/pace/pace-theme-corner-indicator.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","path":"lib/pace/pace-theme-fill-left.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","path":"lib/pace/pace-theme-flash.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","path":"lib/pace/pace-theme-loading-bar.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","path":"lib/pace/pace-theme-mac-osx.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","path":"lib/pace/pace-theme-minimal.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace.min.js","path":"lib/pace/pace.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","path":"lib/three/canvas_lines.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","path":"lib/three/canvas_sphere.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/three-waves.min.js","path":"lib/three/three-waves.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/three.min.js","path":"lib/three/three.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/bower.json","path":"lib/velocity/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.js","path":"lib/Han/dist/han.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.css","path":"lib/Han/dist/han.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.css","path":"lib/Han/dist/han.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.js","path":"lib/Han/dist/han.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","path":"lib/fancybox/source/blank.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","path":"lib/fancybox/source/fancybox_loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","path":"lib/fancybox/source/fancybox_loading@2x.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","path":"lib/fancybox/source/fancybox_overlay.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","path":"lib/fancybox/source/fancybox_sprite.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","path":"lib/fancybox/source/fancybox_sprite@2x.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","path":"lib/fancybox/source/jquery.fancybox.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","path":"lib/fancybox/source/jquery.fancybox.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","path":"lib/fancybox/source/jquery.fancybox.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","path":"lib/fastclick/lib/fastclick.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","path":"lib/fastclick/lib/fastclick.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","path":"lib/font-awesome/fonts/FontAwesome.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","path":"lib/font-awesome/fonts/fontawesome-webfont.svg","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","path":"lib/font-awesome/fonts/fontawesome-webfont.ttf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","path":"lib/Han/dist/font/han-space.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","path":"lib/Han/dist/font/han-space.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","path":"lib/Han/dist/font/han.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","path":"lib/Han/dist/font/han.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","path":"lib/Han/dist/font/han.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","path":"lib/fancybox/source/helpers/fancybox_buttons.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","path":"lib/fancybox/source/helpers/jquery.fancybox-media.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":0,"renderable":1}],"Cache":[{"_id":"source/_posts/环境搭建/hexo安装/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1610673998034},{"_id":"source/_posts/学习笔记/BatchNormalization学习笔记.md","hash":"fffbbc6512730c06534ae2253820445607aaa172","modified":1598855960721},{"_id":"source/_posts/学习笔记/Boosting总结.md","hash":"283e7fb7e68b02c887af37eb59062fc1079b1a6e","modified":1598855960724},{"_id":"source/_posts/学习笔记/CNN感受野.md","hash":"6945333b020e1fdd37387b6286c964947d368a66","modified":1598934305219},{"_id":"source/_posts/学习笔记/CNN权重初始化.md","hash":"c9817a19d6fd9c56892eff81c7575cebe0bcc5c8","modified":1598855960737},{"_id":"source/_posts/学习笔记/CNN的反向传播.md","hash":"394821e93cf0c8c5840dee5376e8399969f098e6","modified":1598855960737},{"_id":"source/_posts/学习笔记/CNN相关知识.md","hash":"b3395343024298b7f0e40e52ec2162ea6226ebdd","modified":1598855960738},{"_id":"source/_posts/学习笔记/EM算法学习笔记.md","hash":"90459453aeb1c63360b2ba099c3249ab7961b384","modified":1598855960741},{"_id":"source/_posts/学习笔记/GAN概述.md","hash":"f3cecc50c3297ab40d6a31c6a55d7a11f55382fe","modified":1606728189581},{"_id":"source/_posts/学习笔记/ResNet总结.md","hash":"99f1345d93c088f4c057fd5a0f1c0c6bda7e3b29","modified":1598855960746},{"_id":"source/_posts/学习笔记/SVM学习笔记.md","hash":"46552df2b2393702343f4dc7696a394c032e61c9","modified":1598855960750},{"_id":"source/_posts/学习笔记/attention机制总结.md","hash":"9d82dc5cfb23f7ba6338033f0b467b39313d668e","modified":1598855960751},{"_id":"source/_posts/学习笔记/refineDet模型结构要点.md","hash":"fa8c5442ab2e41b09f7e5eee189f2d025b01341f","modified":1598855960760},{"_id":"source/_posts/学习笔记/《DeepLearning》读书笔记.md","hash":"7d696d2e15cb483f004b62ed64abfa3a57316fa5","modified":1598855960761},{"_id":"source/_posts/学习笔记/《机器学习》读书笔记.md","hash":"3af49504be16698044f0b0ffb7600db5f76cd404","modified":1598855960761},{"_id":"source/_posts/学习笔记/决策树总结.md","hash":"3ff4cfce4f064c641f382fd4e4bf96c560d28b22","modified":1598855960762},{"_id":"source/_posts/学习笔记/凸优化学习笔记.md","hash":"6c4ec145d7045fbad031c78baa9da4a45589302a","modified":1598855960763},{"_id":"source/_posts/学习笔记/各种优化算法总结.md","hash":"dfa92c37aaad033bde2c6c4d83bd4fa60e781542","modified":1598864594844},{"_id":"source/_posts/学习笔记/回归方法总结.md","hash":"a0a86040ac141274f5e9a5c2054a31e1450eb8da","modified":1598855960766},{"_id":"source/_posts/学习笔记/学习笔记-2019-01.md","hash":"f51b825a3176926a1d37f1c0080cbf1f296e63b2","modified":1562986302778},{"_id":"source/_posts/学习笔记/学习笔记-2019-02.md","hash":"0e6e2030851899585d4bb850db2fbb113516da48","modified":1598855960768},{"_id":"source/_posts/学习笔记/学习笔记-2019-03.md","hash":"0759f2e3c803b4b82e22766378e75c4a279ea27e","modified":1552459757183},{"_id":"source/_posts/学习笔记/学习笔记-2019-04.md","hash":"26f18ce78d07076dbc35cb8f4f42f36c3bd0913f","modified":1556411003417},{"_id":"source/_posts/学习笔记/学习笔记-2019-06.md","hash":"370032c7d43da308adf0f3ff525424cf81615f4a","modified":1569206726395},{"_id":"source/_posts/学习笔记/学习笔记-2019-07.md","hash":"e3405c7caa3e84b4934b48e03a34698f9ae72a6b","modified":1598855960768},{"_id":"source/_posts/学习笔记/常见概率分布.md","hash":"475aae30a677ba4785847340e62b2d4a010f8549","modified":1598855960769},{"_id":"source/_posts/学习笔记/推荐系统学习笔记.md","hash":"4430bcba6af929d85712434b04f62ff72a17ffc3","modified":1598855960769},{"_id":"source/_posts/学习笔记/标签噪声的处理.md","hash":"bdeb7ba194a8a9de19a99573933fe1de0fccf380","modified":1610679504402},{"_id":"source/_posts/学习笔记/正则化方法.md","hash":"cf50f2644e5f152c4a7a53613aad5a4b938a1d0c","modified":1598855960770},{"_id":"source/_posts/学习笔记/目标检测.md","hash":"3272c60d3159c08ffb38afd44e9a136a594da72c","modified":1599101933108},{"_id":"source/_posts/学习笔记/算法总结.md","hash":"d9ba27e7607b8d0ca782f434a2791cf1b3ce54d5","modified":1598855960784},{"_id":"source/_posts/学习笔记/线性代数杂项.md","hash":"d707788f04e61cab3739a5533008d23ba5974729","modified":1598855960784},{"_id":"source/_posts/学习笔记/编译安装emacs记录.md","hash":"310729b74e6d5cdceb28ca26cb143935576655b1","modified":1557298894620},{"_id":"source/_posts/学习笔记/非标量求导术.md","hash":"095208e03015ff495ac5bc86846a27fbee143f4d","modified":1598855960791},{"_id":"source/_posts/环境搭建/hexo安装.md","hash":"7c0ef19dde69599a801467fc14d1bba7310dbb97","modified":1610680649903},{"_id":"source/_posts/环境搭建/在win10上搭建python3和tensorflow-gpu环境.md","hash":"2c725a0877085a60b4d02092497c76c521df78a8","modified":1534474637781},{"_id":"source/_posts/论文阅读/论文翻译之《Perceptual-Losses-for-Real-Time-Style-Transferand-Super-Resolution》.md","hash":"a17c3ebebb734ac6ca92ab0b7a2aa0f1b655c585","modified":1547469534736},{"_id":"source/_posts/论文阅读/论文阅读《ACNet——Strengthening-the-Kernel-Skeletons-for-Powerful-CNN-via-AsymmetricConvolution-Blocks》.md","hash":"0cc86d7c23defe8a424e89071de2dac82d6b57be","modified":1577949944739},{"_id":"source/_posts/论文阅读/论文阅读《CondenseNet-An-Efficient-DenseNet-using-Learned-Group-Convolutions》.md","hash":"37765b73bf300ce38e22d839f50927f299d8de28","modified":1598855960785},{"_id":"source/_posts/论文阅读/论文阅读《CornerNet-Detecting-Objects-as-Paired-Keypoints》.md","hash":"84a08c2497332b821f76545d49e55336728ac863","modified":1598855960786},{"_id":"source/_posts/论文阅读/论文阅读《Deep-Bilateral-Learning-for-Real-Time-Image-Enhancement》.md","hash":"a33504647d48ff79afe5448b09a66af1a6cf071b","modified":1598855960787},{"_id":"source/_posts/论文阅读/论文阅读《Deformable-Convolutional-Networks》.md","hash":"617d6dbede8bae2b77e62d9c8811b3d2b2ccc811","modified":1567821425426},{"_id":"source/_posts/论文阅读/论文阅读《Expectation-Maximization-Attention-Networks-for-Semantic-Segmentation》.md","hash":"e93139aafe753f2a20377c9560aea572a7160f55","modified":1578707614088},{"_id":"source/_posts/论文阅读/论文阅读《Improving-neural-networks-by-preventingco-adaptation-of-feature-detectors》.md","hash":"4ed5e0a8f72d9262a243de51e36b830c9b8aa04a","modified":1564543669209},{"_id":"source/_posts/论文阅读/论文阅读《Learning-a-Discriminative-Feature-Network-for-Semantic-Segmentation》.md","hash":"1f2a6933d4f60c85a6fd277cb6e23a5e7937e78d","modified":1564486751271},{"_id":"source/_posts/论文阅读/论文阅读《Multi-Task-Learning-Using-Uncertainty-to-Weigh-Lossesfor-Scene-Geometry-and-Semantics》.md","hash":"f61a8984d6a11ad4fbc9d196e59d46d7f42ff96f","modified":1564487168376},{"_id":"source/_posts/论文阅读/论文阅读《Reducing-the-Dimensionality-of-Data-with-Neural-Networks》.md","hash":"eec63b4c583ee0639d4888006891fbe302b54ec4","modified":1565224973467},{"_id":"source/_posts/学习笔记/CNN感受野/感受野权重示意图.png","hash":"ba73ddbe36845aef181a0747ad56e529e9415a01","modified":1598855960735},{"_id":"source/_posts/学习笔记/GAN概述/GAN模型结构.png","hash":"c4e1ff5fc57d0f5c2ead71d40874acc3740ce0f7","modified":1598855960743},{"_id":"source/_posts/学习笔记/attention机制总结/Resnet_CBAM.png","hash":"1008e1e8ca297d3913928af0fd29b82a5ad471b7","modified":1598855960757},{"_id":"source/_posts/学习笔记/attention机制总结/Squeeze_and_Excitation.png","hash":"e49d6c81a6a076c5b3dbc53fd77b35747103ffe0","modified":1598855960759},{"_id":"source/_posts/学习笔记/attention机制总结/SEInceptionNet_SEResNet.png","hash":"428d407bbc9abce57b6f01d039b05d04c5e05fea","modified":1598855960758},{"_id":"source/_posts/学习笔记/ResNet总结/Residual_block.png","hash":"96fd8d38f8a1e0c51f1407dc10cc35d06da9a90a","modified":1598855960747},{"_id":"source/_posts/学习笔记/ResNet总结/Residual_block_v2.png","hash":"bf872dd3bfd0b510d5acb13c1acf60893401bf4e","modified":1598855960748},{"_id":"source/_posts/学习笔记/ResNet总结/exp_result.png","hash":"073fcdbdfcad14b600464075511da2cec255dbc3","modified":1598855960749},{"_id":"source/_posts/学习笔记/各种优化算法总结/狭长形状的损失函数下Momentum的运行示意图.png","hash":"465304cb67e7ca63e664daccab4a1b006e242728","modified":1598855960765},{"_id":"source/_posts/学习笔记/各种优化算法总结/狭长形状的损失函数下SGD的运行示意图.png","hash":"ec81ab5d59aa4d78bc98d0cb85f761d8ccc820a8","modified":1598855960766},{"_id":"source/_posts/学习笔记/学习笔记-2019-01/空洞卷积感受野示意图.png","hash":"d8194c57a43c68ad560a3f0a0c682ec4b4492207","modified":1550450688137},{"_id":"source/_posts/学习笔记/学习笔记-2019-02/l1正则化效果示意.png","hash":"09cb965d577256c48efff43766d0f3aa8a259256","modified":1551148699911},{"_id":"source/_posts/学习笔记/学习笔记-2019-02/支持向量机超平面示意.png","hash":"c1def292c6558ae83583536fa48ea2934d5d34c1","modified":1550450688137},{"_id":"source/_posts/学习笔记/标签噪声的处理/sModel.png","hash":"1bcc1a68b25b0d63cf6ee347aa1b404cf913c090","modified":1606465317670},{"_id":"source/_posts/学习笔记/标签噪声的处理/噪声标签和真实标签的关系.png","hash":"4fd0d02055fff2a0fec70e6aadd1e741ce0f2e98","modified":1606461883850},{"_id":"source/_posts/学习笔记/标签噪声的处理/标签修正框架.png","hash":"f288266778913d26bde2c266576c9a193e484ffc","modified":1606979767199},{"_id":"source/_posts/学习笔记/正则化方法/l1正则化效果示意.png","hash":"09cb965d577256c48efff43766d0f3aa8a259256","modified":1598855960771},{"_id":"source/_posts/学习笔记/目标检测/FPN.png","hash":"78bf46d0ff58f0e1a1032d34e946730c1607e17c","modified":1598855960774},{"_id":"source/_posts/学习笔记/目标检测/YOLOv1.png","hash":"334e2f9bf08a3d2dd48f9f8bf937682efa93b55c","modified":1598855960777},{"_id":"source/_posts/学习笔记/目标检测/YOLOv2_exp.png","hash":"4314768de5a3f9ebd91cd099a37f385e9c435ce7","modified":1598855960778},{"_id":"source/_posts/学习笔记/目标检测/YOLOv2_predict.png","hash":"80ce5335434911e6b119acbc70aef064578ed769","modified":1598855960778},{"_id":"source/_posts/环境搭建/在win10上搭建python3和tensorflow-gpu环境/Anaconda_install.png","hash":"f9d3912014594b32eae3d9039ef8030ffd5a404a","modified":1520673356275},{"_id":"source/_posts/环境搭建/在win10上搭建python3和tensorflow-gpu环境/cuda_dir.png","hash":"790da4d60ddec3998fdea3b8f11989d0955e1bf1","modified":1520819012927},{"_id":"source/_posts/论文阅读/论文阅读《ACNet——Strengthening-the-Kernel-Skeletons-for-Powerful-CNN-via-AsymmetricConvolution-Blocks》/ACB_evaluating.png","hash":"6e29b9f1d3cacdf68d5e348074e27ec404626e6f","modified":1577776678171},{"_id":"source/_posts/论文阅读/论文阅读《ACNet——Strengthening-the-Kernel-Skeletons-for-Powerful-CNN-via-AsymmetricConvolution-Blocks》/ACB_training.png","hash":"08edb687d5b343e72165e99782097da127eec718","modified":1577776493157},{"_id":"source/_posts/论文阅读/论文阅读《ACNet——Strengthening-the-Kernel-Skeletons-for-Powerful-CNN-via-AsymmetricConvolution-Blocks》/BN_fusion.png","hash":"4dc667fbebd595f4558d5315b59c2c2cb32a8b4f","modified":1577777278096},{"_id":"source/_posts/论文阅读/论文阅读《CondenseNet-An-Efficient-DenseNet-using-Learned-Group-Convolutions》/分组个数实验.png","hash":"e9c970e6892336d2597a72d7b94b82bf02db8699","modified":1559696727893},{"_id":"source/_posts/论文阅读/论文阅读《CondenseNet-An-Efficient-DenseNet-using-Learned-Group-Convolutions》/特征凝聚因子实验.png","hash":"ffd81ed17f29fda2d4ef5daf11c96e6223e49a24","modified":1559696608434},{"_id":"source/_posts/论文阅读/论文阅读《CondenseNet-An-Efficient-DenseNet-using-Learned-Group-Convolutions》/特征凝聚因子效率对比.png","hash":"f0c5a24a68f5a75a5c2eef3cd00e56a7aae45576","modified":1559697037224},{"_id":"source/_posts/论文阅读/论文阅读《CondenseNet-An-Efficient-DenseNet-using-Learned-Group-Convolutions》/训练loss和学习速率.png","hash":"4c3e14a188ea145a0e18e8795bd68d798c8e144b","modified":1559640170672},{"_id":"source/_posts/学习笔记/CNN感受野/CNN操作示意.png","hash":"9b06afd620766d69eba27c0d1cd7456d1ceeb4d8","modified":1598855960726},{"_id":"source/_posts/学习笔记/CNN感受野/有效感受野示意.png","hash":"3a5ad8aaf1ad0a9a08890d37711cb7223df9e475","modified":1598855960736},{"_id":"source/_posts/学习笔记/CNN相关知识/Conv.gif","hash":"e34969efb724d964627545e03268cdf2d7adcf9a","modified":1598855960740},{"_id":"source/_posts/学习笔记/GAN概述/AdaIN模型结构示意图.png","hash":"81f29ddac2eb1e9f4bd90c8749788abc9c0a5fbe","modified":1598855960743},{"_id":"source/_posts/学习笔记/attention机制总结/CBAM_detail.png","hash":"0d7aefc93be19f60a67d70cc24127693e0693fba","modified":1598855960755},{"_id":"source/_posts/学习笔记/attention机制总结/Non_local_NN.png","hash":"230de462095a1d0d19143edec44b1cea865fda59","modified":1598855960756},{"_id":"source/_posts/学习笔记/各种优化算法总结/Momentum和NAG的运行差别.png","hash":"af0be5976a1877ece739bebe6f09ccb33d2b647e","modified":1598855960765},{"_id":"source/_posts/学习笔记/学习笔记-2019-01/DenseNet模型结构.png","hash":"e99713cd518f0f9bb3d29f5d9e28bf24af4696ac","modified":1550450688059},{"_id":"source/_posts/学习笔记/学习笔记-2019-01/ROI_Pooling输出.png","hash":"5fd95485bb5f8a8d6991d81ae8c5649b748fd017","modified":1550450688075},{"_id":"source/_posts/学习笔记/学习笔记-2019-04/center_ness.png","hash":"7dccedb8edf8bf2241457a2e990916a43537f3b6","modified":1556411003433},{"_id":"source/_posts/学习笔记/目标检测/Cascade_R-CNN.png","hash":"f4ca2d6bcf3251e745fc8ced54794cc7afdbe614","modified":1598855960773},{"_id":"source/_posts/学习笔记/目标检测/FastRCNN.png","hash":"232be05e76cdd140bcbd9092078ccfca0ebcb8b2","modified":1598855960775},{"_id":"source/_posts/学习笔记/目标检测/SPP.png","hash":"c03aed16410792bcda6ebf10faed21f905eb0fcc","modified":1598855960776},{"_id":"source/_posts/学习笔记/目标检测/YOLOv3_exp.png","hash":"14006c34f84c0bf03d1044e60f649cee59b8c0f2","modified":1598855960779},{"_id":"source/_posts/环境搭建/在win10上搭建python3和tensorflow-gpu环境/cuda_download.png","hash":"2d0735274d17e597cd75c8f08163affe049b803a","modified":1520819012943},{"_id":"source/_posts/环境搭建/在win10上搭建python3和tensorflow-gpu环境/install_tensorflow-gpu.png","hash":"5660620d929c27d2e808cb91d24ed4cc81f73d11","modified":1520819012958},{"_id":"source/_posts/环境搭建/在win10上搭建python3和tensorflow-gpu环境/test.png","hash":"7ef534e2c3ba1a738f0fdaa0ff8c19c8aa5df076","modified":1520819012958},{"_id":"source/_posts/论文阅读/论文翻译之《Perceptual-Losses-for-Real-Time-Style-Transferand-Super-Resolution》/fig2.png","hash":"fdeeea40d847f05fc9853fd2bd842084be5bc8ca","modified":1534403053470},{"_id":"source/_posts/论文阅读/论文阅读《CornerNet-Detecting-Objects-as-Paired-Keypoints》/CornerPooling.png","hash":"b8ed6102e01fda8bb4656b2dd346e305e59c07ce","modified":1565187454558},{"_id":"source/_posts/环境搭建/在win10上搭建python3和tensorflow-gpu环境/cuda_version.png","hash":"6db8455d35b8e9dcb90c87d64b78d98bf5b1ef6c","modified":1520819012958},{"_id":"source/_posts/论文阅读/论文阅读《Improving-neural-networks-by-preventingco-adaptation-of-feature-detectors》/feature_map_activate_show.png","hash":"4621fb4e0bc394523abca6fec30bdaccf1609298","modified":1564543604190},{"_id":"source/_posts/学习笔记/学习笔记-2019-02/约束条件和目标函数的等值线.png","hash":"166ca388805e06241356d9db66f74d269b9f6e9b","modified":1550561044653},{"_id":"source/_posts/论文阅读/论文阅读《CondenseNet-An-Efficient-DenseNet-using-Learned-Group-Convolutions》/GroupConv_Learn_Test.png","hash":"663892b6ec4c7fabcfe2423a8db21d64b833ea88","modified":1559632325005},{"_id":"source/_posts/学习笔记/GAN概述/基于联合双边学习的图像风格转换模型结构示意图.png","hash":"4018a850fa01238278f953cf73d7a0c988373c10","modified":1598855960745},{"_id":"source/_posts/学习笔记/学习笔记-2019-01/ROI_Pooling输入.png","hash":"dce10b3ca7aa43be50a2a33588e8ec2c22530c9e","modified":1550450688075},{"_id":"source/_posts/论文阅读/论文阅读《Expectation-Maximization-Attention-Networks-for-Semantic-Segmentation》/EMA_Unit.png","hash":"684c720818729c12af4f9868742038f8e3a4340e","modified":1578466930225},{"_id":"source/_posts/论文阅读/论文阅读《Improving-neural-networks-by-preventingco-adaptation-of-feature-detectors》/feature_map_show.png","hash":"10ba84b6b9971492be86b82d8c8dfc7268df4f19","modified":1564542734171},{"_id":"source/_posts/论文阅读/论文阅读《Multi-Task-Learning-Using-Uncertainty-to-Weigh-Lossesfor-Scene-Geometry-and-Semantics》/paper_result.png","hash":"922b598aab56a5928784581a5d54f4361e467632","modified":1564486945156},{"_id":"source/_posts/学习笔记/BatchNormalization学习笔记/normalization.png","hash":"dbb45ccf5e39b66fc0bdc3e9de805bd365781984","modified":1598855960724},{"_id":"source/_posts/学习笔记/CNN感受野/VGG模型配置.png","hash":"dc2d84d0407175749e2c2d413fa85ed7af0fd833","modified":1598855960728},{"_id":"source/_posts/学习笔记/attention机制总结/Attention计算方式.png","hash":"7fca9b772316e00c24f8dbd1474bb4d53073d8e3","modified":1598855960753},{"_id":"source/_posts/学习笔记/学习笔记-2019-01/区域建议网络给出的位置.png","hash":"dcd2e0115913d000f90123c56e664d45ee3d4873","modified":1550450688091},{"_id":"source/_posts/学习笔记/学习笔记-2019-01/按照设定的输出大小进行划分.png","hash":"23fe0e46339df4b9ef33c35d24926737a50861f4","modified":1550450688122},{"_id":"source/_posts/论文阅读/论文阅读《Learning-a-Discriminative-Feature-Network-for-Semantic-Segmentation》/total_model.png","hash":"cfdac0109bdf6422d44fba38e123abcd195273bf","modified":1564145484602},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1610611236906},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1610611236858},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1610611236858},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1610611236895},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1610611236895},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1610611236896},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1610611236904},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1610611236905},{"_id":"themes/next/.bowerrc","hash":"334da94ca6f024d60d012cc26ea655681e724ad8","modified":1610611236839},{"_id":"themes/next/.editorconfig","hash":"211d2c92bfdddb3e81ea946f4ca7a539f150f4da","modified":1610611236840},{"_id":"themes/next/.gitattributes","hash":"8454b9313cb1a97b63fb87e2d29daee497ce6249","modified":1610611236840},{"_id":"themes/next/.gitignore","hash":"ee0b13c268cc8695d3883a5da84930af02d4ed08","modified":1610611236841},{"_id":"themes/next/.hound.yml","hash":"289dcf5bfe92dbd680d54d6e0668f41c9c9c0c78","modified":1610611236841},{"_id":"themes/next/.javascript_ignore","hash":"cd250ad74ca22bd2c054476456a73d9687f05f87","modified":1610611236842},{"_id":"themes/next/.jshintrc","hash":"b7d23f2ce8d99fa073f22f9960605f318acd7710","modified":1610611236842},{"_id":"themes/next/.stylintrc","hash":"3b7f9785e9ad0dab764e1c535b40df02f4ff5fd6","modified":1610611236842},{"_id":"themes/next/.travis.yml","hash":"6674fbdfe0d0c03b8a04527ffb8ab66a94253acd","modified":1610611236842},{"_id":"themes/next/LICENSE","hash":"ec44503d7e617144909e54533754f0147845f0c5","modified":1610611236843},{"_id":"themes/next/README.cn.md","hash":"b878b73f3fcdef47849453c94420871903d487b3","modified":1610611236843},{"_id":"themes/next/README.md","hash":"efcdc4b0ca791c3fc64afa28c8721e137f2d11ea","modified":1610611236843},{"_id":"themes/next/_config.yml","hash":"6451bb0bae48777a36b61e1c6e4f9066066b2de4","modified":1610680379993},{"_id":"themes/next/bower.json","hash":"486ebd72068848c97def75f36b71cbec9bb359c5","modified":1610611236844},{"_id":"themes/next/gulpfile.coffee","hash":"412defab3d93d404b7c26aaa0279e2e586e97454","modified":1610611236844},{"_id":"themes/next/package.json","hash":"3963ad558a24c78a3fd4ef23cf5f73f421854627","modified":1610611236870},{"_id":"themes/next/languages/de.yml","hash":"fd02d9c2035798d5dc7c1a96b4c3e24b05b31a47","modified":1610611236845},{"_id":"themes/next/languages/default.yml","hash":"b3bcd8934327448a43d9bfada5dd11b1b8c1402e","modified":1610611236845},{"_id":"themes/next/languages/en.yml","hash":"2f4b4776ca1a08cc266a19afb0d1350a3926f42c","modified":1610611236845},{"_id":"themes/next/languages/fr-FR.yml","hash":"efeeb55d5c4add54ad59a612fc0630ee1300388c","modified":1610611236845},{"_id":"themes/next/languages/id.yml","hash":"dccae33e2a5b3c9f11c0e05ec4a7201af1b25745","modified":1610611236846},{"_id":"themes/next/languages/it.yml","hash":"a215d016146b1bd92cef046042081cbe0c7f976f","modified":1610611236846},{"_id":"themes/next/languages/ja.yml","hash":"37f954e47a3bc669620ca559e3edb3b0072a4be5","modified":1610611236846},{"_id":"themes/next/languages/ko.yml","hash":"dc8f3e8c64eb7c4bb2385025b3006b8efec8b31d","modified":1610611236847},{"_id":"themes/next/languages/nl-NL.yml","hash":"213e7a002b82fb265f69dabafbbc382cfd460030","modified":1610611236847},{"_id":"themes/next/languages/pt-BR.yml","hash":"568d494a1f37726a5375b11452a45c71c3e2852d","modified":1610611236847},{"_id":"themes/next/languages/pt.yml","hash":"2efcd240c66ab1a122f061505ca0fb1e8819877b","modified":1610611236847},{"_id":"themes/next/languages/ru.yml","hash":"e33ee44e80f82e329900fc41eb0bb6823397a4d6","modified":1610611236848},{"_id":"themes/next/languages/vi.yml","hash":"a9b89ebd3e5933033d1386c7c56b66c44aca299a","modified":1610611236848},{"_id":"themes/next/languages/zh-Hans.yml","hash":"66b9b42f143c3cb2f782a94abd4c4cbd5fd7f55f","modified":1610675722891},{"_id":"themes/next/languages/zh-hk.yml","hash":"fe0d45807d015082049f05b54714988c244888da","modified":1610611236848},{"_id":"themes/next/languages/zh-tw.yml","hash":"432463b481e105073accda16c3e590e54c8e7b74","modified":1610611236849},{"_id":"themes/next/layout/_layout.swig","hash":"2164570bb05db11ee4bcfbbb5d183a759afe9d07","modified":1610611236850},{"_id":"themes/next/layout/archive.swig","hash":"9a2c14874a75c7085d2bada5e39201d3fc4fd2b4","modified":1610611236869},{"_id":"themes/next/layout/category.swig","hash":"3cbb3f72429647411f9e85f2544bdf0e3ad2e6b2","modified":1610611236869},{"_id":"themes/next/layout/index.swig","hash":"555a357ecf17128db4e29346c92bb6298e66547a","modified":1610611236869},{"_id":"themes/next/layout/page.swig","hash":"e8fcaa641d46930237675d2ad4b56964d9e262e9","modified":1610611236869},{"_id":"themes/next/layout/post.swig","hash":"7a6ce102ca82c3a80f776e555dddae1a9981e1ed","modified":1610611236870},{"_id":"themes/next/layout/schedule.swig","hash":"87ad6055df01fa2e63e51887d34a2d8f0fbd2f5a","modified":1610611236870},{"_id":"themes/next/layout/tag.swig","hash":"34e1c016cbdf94a31f9c5d494854ff46b2a182e9","modified":1610611236870},{"_id":"themes/next/scripts/merge-configs.js","hash":"38d86aab4fc12fb741ae52099be475196b9db972","modified":1610611236871},{"_id":"themes/next/scripts/merge.js","hash":"39b84b937b2a9608b94e5872349a47200e1800ff","modified":1610611236871},{"_id":"themes/next/test/.jshintrc","hash":"c9fca43ae0d99718e45a6f5ce736a18ba5fc8fb6","modified":1610611236961},{"_id":"themes/next/test/helpers.js","hash":"f25e7f3265eb5a6e1ccbb5e5012fa9bebf134105","modified":1610611236962},{"_id":"themes/next/test/intern.js","hash":"db90b1063356727d72be0d77054fdc32fa882a66","modified":1610611236962},{"_id":"themes/next/layout/_custom/header.swig","hash":"ba8ab5a0280b953aa97435ff8946cbcbb2755a27","modified":1610611236849},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"ba8ab5a0280b953aa97435ff8946cbcbb2755a27","modified":1610611236849},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"8c56dd26157cbc580ae41d97ac34b90ab48ced3f","modified":1610611236850},{"_id":"themes/next/layout/_macro/post-copyright.swig","hash":"f83befdc740beb8dc88805efd7fbb0fef9ed19be","modified":1610611236850},{"_id":"themes/next/layout/_macro/reward.swig","hash":"357d86ec9586705bfbb2c40a8c7d247a407db21a","modified":1610611236851},{"_id":"themes/next/layout/_macro/post.swig","hash":"4ba938822d56c597490f0731893eaa2443942e0f","modified":1610611236851},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"9c7343fd470e0943ebd75f227a083a980816290b","modified":1610611236851},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"e2e4eae391476da994045ed4c7faf5e05aca2cd7","modified":1610611236852},{"_id":"themes/next/layout/_partials/comments.swig","hash":"4adc65a602d1276615da3b887dcbf2ac68e7382b","modified":1610611236852},{"_id":"themes/next/layout/_partials/footer.swig","hash":"26e93336dc57a39590ba8dc80564a1d2ad5ff93b","modified":1610611236852},{"_id":"themes/next/layout/_partials/head.swig","hash":"f14a39dad1ddd98e6d3ceb25dda092ba80d391b5","modified":1610611236852},{"_id":"themes/next/layout/_partials/header.swig","hash":"c54b32263bc8d75918688fb21f795103b3f57f03","modified":1610611236853},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"77c61e0baea3544df361b7338c3cd13dc84dde22","modified":1610611236854},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"1634fb887842698e01ff6e632597fe03c75d2d01","modified":1610611236854},{"_id":"themes/next/layout/_partials/search.swig","hash":"b4ebe4a52a3b51efe549dd1cdee846103664f5eb","modified":1610611236854},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"c0f5a0955f69ca4ed9ee64a2d5f8aa75064935ad","modified":1610611236857},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"931808ad9b8d8390c0dcf9bdeb0954eeb9185d68","modified":1610611236857},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"9be624634703be496a5d2535228bc568a8373af9","modified":1610611236858},{"_id":"themes/next/layout/_third-party/duoshuo-hot-articles.swig","hash":"ba75672183d94f1de7c8bd0eeee497a58c70e889","modified":1610611236865},{"_id":"themes/next/layout/_third-party/exturl.swig","hash":"8301c9600bb3e47f7fb98b0e0332ef3c51bb1688","modified":1610611236865},{"_id":"themes/next/layout/_third-party/mathjax.swig","hash":"a0bd3388587fd943baae0d84ca779a707fbcad89","modified":1610611236865},{"_id":"themes/next/layout/_third-party/needsharebutton.swig","hash":"fa882641da3bd83d9a58a8a97f9d4c62a9ee7b5c","modified":1610611236866},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"554ec568e9d2c71e4a624a8de3cb5929050811d6","modified":1610611236866},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"db15d7e1552aa2d2386a6b8a33b3b3a40bf9e43d","modified":1610611236866},{"_id":"themes/next/layout/_third-party/scroll-cookie.swig","hash":"9a188938d46931d5f3882a140aa1c48b3a893f0c","modified":1610611236866},{"_id":"themes/next/scripts/tags/button.js","hash":"eddbb612c15ac27faf11c59c019ce188f33dec2c","modified":1610611236872},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"99b66949f18398689b904907af23c013be1b978f","modified":1610611236872},{"_id":"themes/next/scripts/tags/exturl.js","hash":"5022c0ba9f1d13192677cf1fd66005c57c3d0f53","modified":1610611236872},{"_id":"themes/next/scripts/tags/full-image.js","hash":"c9f833158c66bd72f627a0559cf96550e867aa72","modified":1610611236872},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"ac681b0d0d8d39ba3817336c0270c6787c2b6b70","modified":1610611236873},{"_id":"themes/next/scripts/tags/label.js","hash":"6f00952d70aadece844ce7fd27adc52816cc7374","modified":1610611236873},{"_id":"themes/next/scripts/tags/lazy-image.js","hash":"bcba2ff25cd7850ce6da322d8bd85a8dd00b5ceb","modified":1610611236873},{"_id":"themes/next/scripts/tags/note.js","hash":"f7eae135f35cdab23728e9d0d88b76e00715faa0","modified":1610611236874},{"_id":"themes/next/scripts/tags/tabs.js","hash":"aa7fc94a5ec27737458d9fe1a75c0db7593352fd","modified":1610611236874},{"_id":"themes/next/source/css/main.styl","hash":"a91dbb7ef799f0a171b5e726c801139efe545176","modified":1610679834986},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"45eeea0b5fba833e21e38ea10ed5ab385ceb4f01","modified":1610611236906},{"_id":"themes/next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1610611236906},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1610611236907},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"bc3588c9b2d7c68830524783120ff6cf957cf668","modified":1610611236907},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"6f55543d1fb9cbc436c101d24f802dec7b41efc3","modified":1610611236907},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"6f076713fb9bf934aa2c1046bdf2cf2e37bc1eab","modified":1610611236908},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"42cd73da328077ccc92f859bb8f3cf621b3484f8","modified":1610611236908},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"70c1535f43e54e5ff35ca81419e77e4c0c301398","modified":1610611236908},{"_id":"themes/next/source/images/cc-by.svg","hash":"e92a33c32d1dac8ed94849b2b4e6456e887efe70","modified":1610611236909},{"_id":"themes/next/source/images/cc-zero.svg","hash":"9bfb52b2f63527a7049247bf00d44e6dc1170e7d","modified":1610611236909},{"_id":"themes/next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1610611236909},{"_id":"themes/next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1610611236909},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1610611236910},{"_id":"themes/next/source/images/logo.svg","hash":"169f56fd82941591dad3abd734a50ec7259be950","modified":1610611236910},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1610611236910},{"_id":"themes/next/source/images/quote-l.svg","hash":"cd108d6f44351cadf8e6742565217f88818a0458","modified":1610611236910},{"_id":"themes/next/source/images/quote-r.svg","hash":"2a2a250b32a87c69dcc1b1976c74b747bedbfb41","modified":1610611236911},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1610611236911},{"_id":"themes/next/layout/_partials/head/custom-head.swig","hash":"a223919d2e1bf17ca4d6abb2c86f2efca9883dc1","modified":1610611236853},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"f5e487b0d213ca0bd94aa30bc23b240d65081627","modified":1610611236853},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"b2f0d247b213e4cf8de47af6a304d98070cc7256","modified":1610611236855},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"a8c7f9ca7c605d039a1f3bf4e4d3183700a3dd62","modified":1610611236855},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"b25002a83cbd2ca0c4a5df87ad5bff26477c0457","modified":1610611236855},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"9e3d133ac5bcc6cb51702c83b2611a49811abad1","modified":1610611236856},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"d9e2d9282f9be6e04eae105964abb81e512bffed","modified":1610611236856},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"d4fbffd7fa8f2090eb32a871872665d90a885fac","modified":1610611236856},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"0a9cdd6958395fcdffc80ab60f0c6301b63664a5","modified":1610611236856},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"9b84ab576982b2c3bb0291da49143bc77fba3cc6","modified":1610611236857},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"a9a3995b9615adfb8d6b127c78c6771627bee19a","modified":1610611236858},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a9a3995b9615adfb8d6b127c78c6771627bee19a","modified":1610611236858},{"_id":"themes/next/layout/_third-party/analytics/analytics-with-widget.swig","hash":"ff947f3561b229bc528cb1837d4ca19612219411","modified":1610611236859},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"71397a5823e8ec8aad3b68aace13150623b3e19d","modified":1610611236859},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"753d262911c27baf663fcaf199267133528656af","modified":1610611236859},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"7b11eac3a0685fa1ab2ab6ecff60afc4f15f0d16","modified":1610611236860},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"a10b7f19d7b5725527514622899df413a34a89db","modified":1610611236860},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"7d94845f96197d9d84a405fa5d4ede75fb81b225","modified":1610611236860},{"_id":"themes/next/layout/_third-party/analytics/firestore.swig","hash":"ccc443b22bd4f8c7ac4145664686c756395b90e0","modified":1610611236861},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"b1e13df83fb2b1d5d513b30b7aa6158b0837daab","modified":1610611236861},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"45f3f629c2aacc381095750e1c8649041a71a84b","modified":1610611236861},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"e6d10ee4fb70b3ae1cd37e9e36e000306734aa2e","modified":1610611236861},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"8a399df90dadba5ad4e781445b58f4765aeb701e","modified":1610611236861},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"5a8027328f060f965b3014060bebec1d7cf149c1","modified":1610611236862},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"f9a1647a8f1866deeb94052d1f87a5df99cb1e70","modified":1610611236862},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"4c501ea0b9c494181eb3c607c5526a5754e7fbd8","modified":1610611236862},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"b83a51bbe0f1e2ded9819070840b0ea145f003a6","modified":1610611236863},{"_id":"themes/next/layout/_third-party/comments/duoshuo.swig","hash":"1600f340e0225361580c44890568dc07dbcf2c89","modified":1610611236863},{"_id":"themes/next/layout/_third-party/comments/gitment.swig","hash":"4dcc3213c033994d342d02b800b6229295433d30","modified":1610611236863},{"_id":"themes/next/layout/_third-party/comments/hypercomments.swig","hash":"af7f3e43cbdc4f88c13f101f0f341af96ace3383","modified":1610611236863},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"493bd5999a1061b981922be92d8277a0f9152447","modified":1610611236863},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"9246162d4bc7e949ce1d12d135cbbaf5dc3024ec","modified":1610611236864},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"4050553d44ba1396174161c9a6bb0f89fa779eca","modified":1610611236864},{"_id":"themes/next/layout/_third-party/comments/youyan.swig","hash":"7e65ff8fe586cd655b0e9d1ad2912663ff9bd36c","modified":1610611236864},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"34599633658f3b0ffb487728b7766e1c7b551f5a","modified":1610611236867},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"93479642fd076a1257fecc25fcf5d20ccdefe509","modified":1610611236868},{"_id":"themes/next/layout/_third-party/search/tinysou.swig","hash":"fe95dd3d166634c466e19aa756e65ad6e8254d3e","modified":1610611236868},{"_id":"themes/next/layout/_third-party/seo/baidu-push.swig","hash":"d8c98938719284fa06492c114d99a1904652a555","modified":1610611236868},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"3403fdd8efde1a0afd11ae8a5a97673f5903087f","modified":1610611236895},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"07f7da320689f828f6e36a6123807964a45157a0","modified":1610611236895},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"7896c3ee107e1a8b9108b6019f1c070600a1e8cc","modified":1610611236896},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"0e55cbd93852dc3f8ccb44df74d35d9918f847e0","modified":1610611236896},{"_id":"themes/next/source/css/_my/katex.css","hash":"2076e1bd994b50d4dac3a944e5eafc468ec75b32","modified":1610678605615},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"58e7dd5947817d9fc30770712fc39b2f52230d1e","modified":1610611236904},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"a25408534f8fe6e321db4bbf9dd03335d648fe17","modified":1610611236904},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"4069f918ccc312da86db6c51205fc6c6eaabb116","modified":1610611236905},{"_id":"themes/next/source/css/_variables/base.styl","hash":"b1f6ea881a4938a54603d68282b0f8efb4d7915d","modified":1610611236905},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"cb431b54ba9c692165a1f5a12e4c564a560f8058","modified":1610611236912},{"_id":"themes/next/source/js/src/affix.js","hash":"1b509c3b5b290a6f4607f0f06461a0c33acb69b1","modified":1610611236911},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"0289031200c3d4c2bdd801ee10fff13bb2c353e4","modified":1610611236912},{"_id":"themes/next/source/js/src/exturl.js","hash":"a2a0f0de07e46211f74942a468f42ee270aa555c","modified":1610611236912},{"_id":"themes/next/source/js/src/hook-duoshuo.js","hash":"b35a7dc47b634197b93487cea8671a40a9fdffce","modified":1610611236912},{"_id":"themes/next/source/js/src/js.cookie.js","hash":"1512c751d219577d338ac0780fb2bbd9075d5298","modified":1610611236913},{"_id":"themes/next/source/js/src/motion.js","hash":"885176ed51d468f662fbf0fc09611f45c7e5a3b1","modified":1610611236913},{"_id":"themes/next/source/js/src/post-details.js","hash":"93a18271b4123dd8f94f09d1439b47c3c19a8712","modified":1610611236913},{"_id":"themes/next/source/js/src/scroll-cookie.js","hash":"02cf91514e41200bc9df5d8bdbeb58575ec06074","modified":1610611236914},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"b7657be25fc52ec67c75ab5481bdcb483573338b","modified":1610611236914},{"_id":"themes/next/source/js/src/utils.js","hash":"b3e9eca64aba59403334f3fa821f100d98d40337","modified":1610611236915},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1610611236920},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1610611236922},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","hash":"b02737510e9b89aeed6b54f89f602a9c24b06ff2","modified":1610611236923},{"_id":"themes/next/source/lib/fancybox/.bower.json","hash":"9be892a4e14e0da18ff9cb962c9ef71f163b1b22","modified":1610611236923},{"_id":"themes/next/source/lib/fancybox/.gitattributes","hash":"672d3b5767e0eacd83bb41b188c913f2cf754793","modified":1610611236924},{"_id":"themes/next/source/lib/fastclick/.bower.json","hash":"bf3eef9d647cd7c9b62feda3bc708c6cdd7c0877","modified":1610611236929},{"_id":"themes/next/source/lib/fastclick/LICENSE","hash":"6f474ea75c42442da7bbcf2e9143ce98258efd8d","modified":1610611236929},{"_id":"themes/next/source/lib/fastclick/README.md","hash":"68a9b9d53126405b0fa5f3324f1fb96dbcc547aa","modified":1610611236930},{"_id":"themes/next/source/lib/fastclick/bower.json","hash":"a9b3ee1e4db71a0e4ea6d5bed292d176dd68b261","modified":1610611236930},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"b4aefc910578d76b267e86dfffdd5121c8db9aec","modified":1610611236931},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"03ddbf76c1dd1afb93eed0b670d2eee747472ef1","modified":1610611236932},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"c31ff06a740955e44edd4403902e653ccabfd4db","modified":1610611236932},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"ee33b2798b1e714b904d663436c6b3521011d1fa","modified":1610611236932},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"71e7183634dc1b9449f590f15ebd7201add22ca7","modified":1610611236932},{"_id":"themes/next/source/lib/jquery/.bower.json","hash":"865d6c1328ab209a4376b9d2b7a7824369565f28","modified":1610611236945},{"_id":"themes/next/source/lib/jquery_lazyload/.bower.json","hash":"90fa628f156d8045357ff11eaf32e61abacf10e8","modified":1610611236946},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","hash":"4ded6fee668544778e97e38c2b211fc56c848e77","modified":1610611236946},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","hash":"b930297cb98b8e1dbd5abe9bc1ed9d5935d18ce8","modified":1610611236947},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","hash":"e0acf1db27b0cc16128a59c46db1db406b5c4c58","modified":1610611236947},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","hash":"f4a570908f6c89c6edfb1c74959e733eaadea4f2","modified":1610611236947},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","hash":"bf773ad48a0b9aa77681a89d7569eefc0f7b7b18","modified":1610611236948},{"_id":"themes/next/source/lib/needsharebutton/font-embedded.css","hash":"14264a210bf94232d58d7599ea2ba93bfa4fb458","modified":1610611236948},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.css","hash":"e33aa8fa48b6639d8d8b937d13261597dd473b3a","modified":1610611236949},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.js","hash":"2ce5f3bf15c523b9bfc97720d8884bb22602a454","modified":1610611236949},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1610611236949},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1610611236950},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1610611236950},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1610611236950},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1610611236950},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1610611236951},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1610611236951},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1610611236951},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1610611236951},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1610611236952},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1610611236952},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1610611236952},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1610611236952},{"_id":"themes/next/source/lib/pace/pace.min.js","hash":"8aaa675f577d5501f5f22d5ccb07c2b76310b690","modified":1610611236952},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","hash":"2d9a9f38c493fdf7c0b833bb9184b6a1645c11b2","modified":1610611236953},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","hash":"46a50b91c98b639c9a2b9265c5a1e66a5c656881","modified":1610611236953},{"_id":"themes/next/source/lib/three/three-waves.min.js","hash":"8148492dd49aa876d32bb7d5b728d3f5bf6f5074","modified":1610611236954},{"_id":"themes/next/source/lib/velocity/.bower.json","hash":"63da5e80ebb61bb66a2794d5936315ca44231f0c","modified":1610611236958},{"_id":"themes/next/source/lib/velocity/bower.json","hash":"92d92860418c4216aa59eb4cb4a556290a7ad9c3","modified":1610611236958},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"bf172816a9c57f9040e3d19c24e181a142daf92b","modified":1610611236960},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"dbbfb50f6502f6b81dcc9fee7b31f1e812da3464","modified":1610611236961},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"dde584994ac13dc601836e86f4cf490e418d9723","modified":1610611236961},{"_id":"themes/next/layout/_third-party/search/algolia-search/assets.swig","hash":"218cc936ba3518a3591b2c9eda46bc701edf7710","modified":1610611236867},{"_id":"themes/next/layout/_third-party/search/algolia-search/dom.swig","hash":"2530de0f3125a912756f6c0e9090cd012134a4c5","modified":1610611236867},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"8f86f694c0749a18ab3ad6f6df75466ca137a4bc","modified":1610611236875},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"237d185ac62ec9877e300947fa0109c44fb8db19","modified":1610611236875},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"8b32928686c327151e13d3ab100157f9a03cd59f","modified":1610611236875},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"ff4489cd582f518bba6909a301ac1292a38b4e96","modified":1610611236875},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"7ad4081466b397e2a6204141bb7768b7c01bd93c","modified":1610611236876},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"4f2801fc4cf3f31bf2069f41db8c6ce0e3da9e39","modified":1610611236880},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"6eb4bcc3056bd279d000607e8b4dad50d368ca69","modified":1610611236887},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"12662536c7a07fff548abe94171f34b768dd610f","modified":1610611236893},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"24ee4b356ff55fc6e58f26a929fa07750002cf29","modified":1610611236893},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"1da5c800d025345f212a3bf1be035060f4e5e6ed","modified":1610611236893},{"_id":"themes/next/source/css/_common/scaffolding/mobile.styl","hash":"91ca75492cd51f2553f4d294ed2f48239fcd55eb","modified":1610611236894},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"3f40e8a9fe8e7bd5cfc4cf4cbbbcb9539462e973","modified":1610611236894},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"a17e2b871a335f290afb392a08f94fd35f59c715","modified":1610611236894},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"ea9069645696f86c5df64208490876fe150c8cae","modified":1610611236894},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"25d5e45a355ee2093f3b8b8eeac125ebf3905026","modified":1610611236897},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"60fa84aa7731760f05f52dd7d8f79b5f74ac478d","modified":1610611236897},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"d0bfd1bef988c76f7d7dd72d88af6f0908a8b0db","modified":1610611236897},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"b1025c421406d2c24cc92a02ae28c1915b01e240","modified":1610611236898},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"26666c1f472bf5f3fb9bc62081cca22b4de15ccb","modified":1610611236899},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"9c99034f8e00d47e978b3959f51eb4a9ded0fcc8","modified":1610611236899},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"09c965022c13b84ed8a661fee8ac2a6d550495ae","modified":1610611236899},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"9b913b73d31d21f057f97115ffab93cfa578b884","modified":1610611236900},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"31127dcbf4c7b4ada53ffbf1638b5fe325b7cbc0","modified":1610611236901},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"e695e58f714129ca292c2e54cd62c251aca7f7fe","modified":1610611236901},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"748dbfbf9c08e719ddc775958003c64b00d39dab","modified":1610611236901},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"09c965022c13b84ed8a661fee8ac2a6d550495ae","modified":1610611236902},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"5dbc0d0c897e46760e5dbee416530d485c747bba","modified":1610611236902},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"bce344d3a665b4c55230d2a91eac2ad16d6f32fd","modified":1610611236903},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"416988dca389e6e2fdfa51fa7f4ee07eb53f82fb","modified":1610611236903},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"4642e30010af8b2b037f5b43146b10a934941958","modified":1610611236903},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"1f6e2ce674735269599acc6d77b3ea18d31967fc","modified":1610611236903},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"ad2dcedf393ed1f3f5afd2508d24969c916d02fc","modified":1610611236903},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"86197902dfd3bededba10ba62b8f9f22e0420bde","modified":1610611236904},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"f1d0b5d7af32c423eaa8bb93ab6a0b45655645dc","modified":1610611236914},{"_id":"themes/next/source/lib/Han/dist/han.css","hash":"6c26cdb36687d4f0a11dabf5290a909c3506be5c","modified":1610611236918},{"_id":"themes/next/source/lib/Han/dist/han.min.css","hash":"6d586bfcfb7ae48f1b12f76eec82d3ad31947501","modified":1610611236919},{"_id":"themes/next/source/lib/Han/dist/han.min.js","hash":"16b03db23a52623348f37c04544f2792032c1fb6","modified":1610611236919},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1610611236924},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1610611236924},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1610611236925},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1610611236925},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1610611236925},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1610611236926},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","hash":"82f33ad0842aa9c154d029e0dada2497d4eb1d57","modified":1610611236928},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","hash":"d71602cbca33b9ecdb7ab291b7f86a49530f3601","modified":1610611236928},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","hash":"ae6318aeb62ad4ce7a7e9a4cdacd93ffb004f0fb","modified":1610611236929},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","hash":"1d6aeda0480d0e4cb6198edf7719d601d4ae2ccc","modified":1610611236930},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1610611236931},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"3655f1fdf1e584c4d8e8d39026093ca306a5a341","modified":1610611236933},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"1573904b82807abbb32c97a3632c6c6808eaac50","modified":1610611236933},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"88af80502c44cd52ca81ffe7dc7276b7eccb06cf","modified":1610611236934},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"41ea797c68dbcff2f6fb3aba1d1043a22e7cc0f6","modified":1610611236957},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"a817b6c158cbc5bab3582713de9fe18a18a80552","modified":1610611236958},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"9f73c4696f0907aa451a855444f88fc0698fa472","modified":1610611236876},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"53cde051e0337f4bf42fb8d6d7a79fa3fa6d4ef2","modified":1610611236877},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d63e0cacc53dd375fcc113465a4328c59ff5f2c1","modified":1610611236877},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"1a0d059799a298fe17c49a44298d32cebde93785","modified":1610611236877},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"0656e753f182c9f47fef7304c847b7587a85ef0d","modified":1610611236877},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"1727702eac5d326b5c81a667944a245016668231","modified":1610611236878},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"167986d0f649516671ddf7193eebba7b421cd115","modified":1610611236878},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"50450d9fdc8a2b2be8cfca51e3e1a01ffd636c0b","modified":1610611236878},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"7fe4d4d656e86276c17cb4e48a560cb6a4def703","modified":1610611236879},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"b6f3a06a94a6ee5470c956663164d58eda818a64","modified":1610611236879},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"7fb593f90d74a99c21840679933b9ef6fdc16a61","modified":1610611236879},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"f9760ecf186954cee3ba4a149be334e9ba296b89","modified":1610611236879},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"4e3838d7ac81d9ad133960f0f7ed58a44a015285","modified":1610611236880},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"8cf318644acc8b4978537c263290363e21c7f5af","modified":1610611236880},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"62fbbd32cf5a99ae550c45c763a2c4813a138d01","modified":1610611236880},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"875cbe88d5c7f6248990e2beb97c9828920e7e24","modified":1610611236881},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"caf263d1928496688c0e1419801eafd7e6919ce5","modified":1610611236881},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"a6c6eb8adba0a090ad1f4b9124e866887f20d10d","modified":1610611236881},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"a200c0a1c5a895ac9dc41e0641a5dfcd766be99b","modified":1610611236881},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"cd9e214e502697f2f2db84eb721bac57a49b0fce","modified":1610611236882},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"d0d7a5c90d62b685520d2b47fea8ba6019ff5402","modified":1610611236882},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"27deb3d3a243d30022055dac7dad851024099a8b","modified":1610611236882},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"ca88ea6999a61fb905eb6e72eba5f92d4ee31e6e","modified":1610611236883},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"b2495ae5e04dcca610aacadc47881d9e716cd440","modified":1610611236883},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"5a982d8ef3b3623ea5f59e63728990f5623c1b57","modified":1610611236883},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"ccb34c52be8adba5996c6b94f9e723bd07d34c16","modified":1610611236883},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"01567edaea6978628aa5521a122a85434c418bfd","modified":1610611236883},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"7968343e41f8b94b318c36289dff1196c3eb1791","modified":1610611236884},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"89d6c3b697efc63de42afd2e89194b1be14152af","modified":1610611236884},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"39f04c4c7237a4e10acd3002331992b79945d241","modified":1610611236884},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"761eba9811b050b25d548cc0854de4824b41eb08","modified":1610611236885},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"8dd9a1c6f4f6baa00c2cf01837e7617120cf9660","modified":1610611236885},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"11c22f0fb3f6beb13e5a425ec064a4ff974c13b7","modified":1610611236885},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"61f8cea3c01acd600e90e1bc2a07def405503748","modified":1610611236885},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"1153bb71edf253765145559674390e16dd67c633","modified":1610611236886},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"c8fe49a4bc014c24dead05b782a7082411a4abc5","modified":1610611236886},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"a1521d48bb06d8d703753f52a198baa197af7da2","modified":1610611236887},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"5ef6343835f484a2c0770bd1eb9cc443609e4c39","modified":1610611236887},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"e71652d3216e289c8548b1ea2357822c1476a425","modified":1610611236887},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"2fe76476432b31993338cb45cdb3b29a518b6379","modified":1610611236888},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"f825da191816eef69ea8efb498a7f756d5ebb498","modified":1610611236888},{"_id":"themes/next/source/css/_common/components/tags/exturl.styl","hash":"a3bdd71237afc112b2aa255f278cab6baeb25351","modified":1610611236888},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"2ad1a2a9bbf6742d1b0762c4c623b68113d1e0fe","modified":1610611236888},{"_id":"themes/next/source/css/_common/components/tags/label.styl","hash":"2ab1322fe52ab5aafd49e68f5bd890e8380ee927","modified":1610611236888},{"_id":"themes/next/source/css/_common/components/tags/note-modern.styl","hash":"b7076e58d647265ee0ad2b461fe8ce72c9373bc5","modified":1610611236889},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"9a409b798decdefdaf7a23f0b11004a8c27e82f3","modified":1610611236889},{"_id":"themes/next/source/css/_common/components/tags/tabs.styl","hash":"154a87a32d2fead480d5e909c37f6c476671c5e6","modified":1610611236889},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"b80604868e4f5cf20fccafd7ee415c20c804f700","modified":1610611236890},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"bba4f3bdb7517cd85376df3e1209b570c0548c69","modified":1610611236890},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"5dbeed535d63a50265d96b396a5440f9bb31e4ba","modified":1610611236890},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"a6e7d698702c2e383dde3fde2abde27951679084","modified":1610611236891},{"_id":"themes/next/source/css/_common/components/third-party/duoshuo.styl","hash":"717cc7f82be9cc151e23a7678601ff2fd3a7fa1d","modified":1610611236891},{"_id":"themes/next/source/css/_common/components/third-party/gitment.styl","hash":"874278147115601d2abf15987f5f7a84ada1ac6b","modified":1610611236891},{"_id":"themes/next/source/css/_common/components/third-party/han.styl","hash":"10599e16414a8b7a76c4e79e6617b5fe3d4d1adf","modified":1610611236891},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"15975ba7456b96916b1dbac448a1a0d2c38b8f3d","modified":1610611236892},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"16087276945fa038f199692e3eabb1c52b8ea633","modified":1610611236892},{"_id":"themes/next/source/css/_common/components/third-party/needsharebutton.styl","hash":"28825ae15fa20ae3942cdaa7bcc1f3523ce59acc","modified":1610611236892},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"9c8196394a89dfa40b87bf0019e80144365a9c93","modified":1610611236892},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"a07aa12cc36ac5c819670c2a3c17d07ed7a08986","modified":1610611236900},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"1f09be9bb38411f0629b58c3b23873589a6dbcaa","modified":1610611236900},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"1f09be9bb38411f0629b58c3b23873589a6dbcaa","modified":1610611236902},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1610611236916},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1610611236916},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1610611236916},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1610611236917},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","hash":"623af3ed5423371ac136a4fe0e8cc7bb7396037a","modified":1610611236917},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1610611236926},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"6394c48092085788a8c0ef72670b0652006231a1","modified":1610611236926},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"ee948b4489aedeb548a77c9e45d8c7c5732fd62d","modified":1610611236927},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"51139a4c79573d372a347ef01a493222a1eaf10a","modified":1610611236927},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"b88b589f5f1aa1b3d87cc7eef34c281ff749b1ae","modified":1610611236927},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"d22b1629cb23a6181bebb70d0cf653ffe4b835c8","modified":1610611236928},{"_id":"source/_posts/论文阅读/论文阅读《Reducing-the-Dimensionality-of-Data-with-Neural-Networks》/RBM_and_multilayer_model.png","hash":"5cfe27daddd77f9c1decc2fb3c1668cfdd107976","modified":1564663338872},{"_id":"themes/next/source/lib/jquery/index.js","hash":"17a740d68a1c330876c198b6a4d9319f379f3af2","modified":1610611236946},{"_id":"themes/next/source/lib/Han/dist/han.js","hash":"4ac683b2bc8531c84d98f51b86957be0e6f830f3","modified":1610611236918},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1610611236944},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1610611236945},{"_id":"source/_posts/论文阅读/论文翻译之《Perceptual-Losses-for-Real-Time-Style-Transferand-Super-Resolution》/fig1.png","hash":"7c4afaf6a16863812d6fd85e0e5257e56b8b4d5c","modified":1533019715134},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1610611236936},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1610611236938},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1610611236943},{"_id":"source/_posts/学习笔记/目标检测/YOLOv3结构示意.png","hash":"b01d3a36bb75958e5b33c9a45ef5a7b7d734d11b","modified":1598855960783},{"_id":"source/_posts/论文阅读/论文阅读《Deep-Bilateral-Learning-for-Real-Time-Image-Enhancement》/模型整体结构.png","hash":"9cefc68f464160951057945b5a1b93bed5b73ecf","modified":1598855960791},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"4237c6e9d59da349639de20e559e87c2c0218cfd","modified":1610611236960},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","hash":"90a1b22129efc172e2dfcceeeb76bff58bc3192f","modified":1610611236922},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"b5483b11f8ba213e733b5b8af9927a04fec996f6","modified":1610611236941},{"_id":"themes/next/source/lib/three/three.min.js","hash":"26273b1cb4914850a89529b48091dc584f2c57b8","modified":1610611236957},{"_id":"source/about/index.md","hash":"5c53a986ba07bac508c07851961994787c9caf0f","modified":1610680600376},{"_id":"source/categories/index.md","hash":"792487291c7ae554645aaba0e845340420cbecb5","modified":1610680603973},{"_id":"source/tags/index.md","hash":"d32913cfdeaf744ed26b06999d944a3226a3ce3b","modified":1610680601433}],"Category":[{"name":"工程实践","_id":"ckjxpc3rr000o44mq44xa1188"},{"name":"环境搭建","_id":"ckjxpc3s8001v44mqc3776oyj"}],"Data":[],"Page":[{"title":"categories","date":"2021-01-15T03:16:43.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2021-01-15 11:16:43\ntype: \"categories\"\n---\n","updated":"2021-01-15T03:23:47.079Z","path":"categories/index.html","_id":"ckjxpokgh0000kwmqfey2836k","comments":1,"layout":"page","content":"\r\n","site":{"data":{}},"excerpt":"","more":"\r\n"},{"title":"about","date":"2021-01-15T03:16:40.000Z","_content":"","source":"about/index.md","raw":"---\ntitle: about\ndate: 2021-01-15 11:16:40\n---\n","updated":"2021-01-15T03:16:40.376Z","path":"about/index.html","comments":1,"layout":"page","_id":"ckjxpokgk0001kwmqb5zq7or0","content":"\r\n","site":{"data":{}},"excerpt":"","more":"\r\n"},{"title":"tags","date":"2021-01-15T03:16:41.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2021-01-15 11:16:41\ntype: \"tags\"\n---\n","updated":"2021-01-15T03:23:54.568Z","path":"tags/index.html","_id":"ckjxpokgn0002kwmqdz94fdrv","comments":1,"layout":"page","content":"\r\n","site":{"data":{}},"excerpt":"","more":"\r\n"}],"Post":[{"title":"BatchNormalization学习笔记","date":"2020-05-04T05:51:26.000Z","mathjax":true,"_content":"\n# BatchNormalization的作用机制\n## 推理过程\n对于深度神经网络中的一层，可以表示为$o = g(x),\\ x=wl + b$，其中$w,b$是该层的参数，$l$是该层的输入，$g$表示激活函数，$o$表示该层输出。\n\n在提出BN的原始论文《Batch Normalization: Accelerating Deep Network Training byReducing Internal Covariate Shift》中，BN层是加在每一层的激活函数之前的，即对上面的$x$进行操作。\n\nBN层的操作有两步：\n- 首先对数据进行归一化（normalize）：设$\\mu$，$\\sigma^2$分别是$x$的均值和方差，归一化之后的$x$变为$\\hat{x} = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}$，其中$\\epsilon$是个防止分母为零的平滑项，\n- 然后对归一化的数据进行scale和shift（transform）：$\\gamma \\hat{x} + \\beta$，最终该层加入BN之后，表达式变为$o = g(\\gamma \\hat{x} + \\beta)$\n\n这里先不管$\\mu,\\ \\sigma^2,\\ \\epsilon,\\ \\beta$怎么得到，上面的操作看起来非常简单，不过这里有一点细节：一般情况下，这里的$x,\\ x_i,\\ \\mu,\\ \\sigma^2,\\ \\epsilon,\\ \\gamma,\\ \\beta$全都是向量，因为一个样本由多个特征构成，BatchNormalization其实是对每个特征进行normalize和transform，因此上面涉及到的计算，其实全都是逐元素运算，如果是对于图像数据，在CNN中的BatchNormalization其实是将特征图上的每个点当做一个样本的，例如一个$N\\times H\\times W\\times C$大小的特征图（$N$是batch size），那么其中$1\\times 1 \\times 1 \\times C$的一个点就被当成一个样本，其包含$C$个特征，所有样本共用$C$个均值和方差。\n\n## 训练过程\n对于BatchNormalizatioin层，训练过程其实就是确定$\\mu,\\ \\sigma^2,\\ \\gamma,\\ \\beta$的过程。\n\n首先，对于$\\mu,\\ \\sigma^2$，在训练过程中，对于每一个batch的数据，我们可以得到$\\mu_B = \\frac{1}{m}\\sum\\limits_{i=1}^m x_i$，$\\sigma^2_B = \\frac{1}{m}\\sum\\limits_{i=1}^m(x_i -\\mu)^2$作为当前batch数据的均值和方差，其中$m$是一个batch中的样本数，下标$i$表示某个样本，训练是迭代进行的，为了得到对所有数据的均值和方差，需要使用一些方法根据$\\mu_B,\\ \\sigma^2_B$来估计整体数据的均值和方差。\n\n在主流的深度学习框架（tensorflow、pytorch）中，一般使用滑动平均来进行估计：\n将变量$\\mu,\\ \\sigma^2$初始化为$\\mu_0,\\ \\sigma^2_0$，因为每次迭代处理一个batch的数据，均可以得到$\\mu_B,\\ \\sigma^2_B$，第$k$次迭代时，更新$\\mu_k = \\alpha\\mu_{k-1} + (1-\\alpha)\\mu_B,\\ \\sigma^2_k = \\alpha\\sigma^2_{k-1} + (1-\\alpha)\\sigma^2_B$，其中$0 < \\alpha < 1$是滑动平均的滑动速率，训练完成之后，假设一共迭代了$K$次，则令$\\mu = \\mu_K,\\ \\sigma^2 = \\sigma^2_K$，即可用于推理过程。\n\n对于$\\gamma,\\ \\beta$两个参数，在训练过程中，将其和$w,\\ b$一样，使用优化器根据梯度进行更新即可。\n\n\n## BatchNormalization的推理加速\n将BatchNormalization的两个步骤结合起来，可以得到：\n\n$$\n\\begin{aligned}\n    o &= g(\\hat{z})\\\\\n    &= g(\\gamma \\hat{x} + \\beta)\\\\\n    &= g(\\gamma \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta)\\\\\n    &= g(\\gamma \\frac{wl + b - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta)\\\\\n    &= g(\\frac{\\gamma wl}{\\sqrt{\\sigma^2 + \\epsilon}} + \\frac{\\gamma b -\\gamma \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}+ \\beta)\\\\\n\\end{aligned}\n$$\n\n可以发现，在模型训练完成之后，如果仅需要使用模型进行推理，则可以将$\\frac{\\gamma w}{\\sqrt{\\sigma^2 + \\epsilon}}$保存为该层的$w$参数，将$\\frac{\\gamma b -\\gamma \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}+ \\beta$保存为该层的$b$参数。这样在推理过程中完全不增加计算量。\n\n有一种说法是如果使用了BatchNormalization，则该层的偏置参数$b$可以省略掉，这也是根据上式得来的，因为BatchNormalization中包含了一个可学习的$\\beta$项，完全可以替代$b$参数的效果，而且不影响模型的表达能力。\n\n# BatchNormalization的理解\n原论文中认为：BatchNormalization可以让深度神经网络使用更大的学习速率训练以达到加速收敛的目的。\n\n论文中作者解释说是因为深度神经网络在训练过程中，由于前一层参数变化，导致每一层的输入分布不断发生变化，这种情况在论文中被称为“internal convariate shift”，BatchNormalization的作用是缓解这种“internal convariate shift”：通过强行将激活之前的特征进行减平均除方差的操作，使得特征均值为0，方差为1，让激活之后的输出，即下一层的输入，维持在一个相对稳定的分布中。\n\n但是在该论文中，作者又说如果简单的将某一层的激活函数的输入进行normalize，会改变该层的表示能力（原文：Note  that  simply  normalizing  each  input  of  a  layer  may change  what  the  layer  can  represent.   For  instance,  nor-malizing the inputs of a sigmoid would constrain them tothe linear regime of the nonlinearity.），因此需要在normalize之后，再对其进行transform。\n\n这里的transform之后，如果不是接近于0均值，1方差的，那之前的normalize就白做了，原论文中也明确说了transform有能力将之前的normalize的效果完全还原，所以原论文得出的结论是这样一来模型可以选择是否进行normalize，相当于增强了模型的表达能力。\n\n我对这里的理解是：**BatchNormalization先将特征变换到0均值，1方差，再transform到$\\beta$均值，$\\gamma$标准差，这样一定程度上可以让模型来显式地选择特征的均值和方差，可以让数据的分布向更加适合随机初始化的权重的位置靠拢（否则就需要单方面调整随机初始化的权重来适应数据的分布，这样显然需要更长时间的训练），因此BatchNormalization有缩短收敛时间的效果，也可以说是减小了对参数初始化的依赖，另外，BN层的先normalize再transform的操作，如果通过$\\beta$和$\\gamma$将特征的均值和方差限定在一定范围，一定程度上是对模型表达能力的一种限制，有一定的类似正则化的作用，一定情况下或许可以稍微提高模型的泛化能力。**\n\n从梯度方面来看，BN是通过某种对梯度的调整，从而对整个学习过程产生影响，我分析了论文中得出的求导结果，并将$\\frac{\\partial l}{\\partial x_i}$继续化简，如下：\n$$\n\\begin{aligned}\n    \\frac{\\partial l}{\\partial \\hat{x}} &= \\frac{\\partial l}{\\partial \\hat{z}} \\gamma\\\\\n    \\frac{\\partial l}{\\partial \\sigma^2_B} &= \\sum\\limits_{i=1}^m \\frac{\\partial l}{\\partial \\hat{x}_i}(x_i - \\mu_B)\\cdot-\\frac{1}{2}(\\sigma^2_B +\\epsilon)^{-\\frac{3}{2}}\\\\\n    &= \\sum\\limits_{i=1}^m \\frac{\\partial l}{\\partial \\hat{z}_i} \\gamma (x_i - \\mu_B)\\cdot-\\frac{1}{2}(\\sigma^2_B +\\epsilon)^{-\\frac{3}{2}}\\\\\n    \\frac{\\partial l}{\\partial \\mu_B} &= \\sum_{i=1}^m\\frac{\\partial l}{\\partial \\hat{x}_i} \\cdot -\\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}}\\\\\n    &=\\sum\\limits_{i=1}^m \\frac{\\partial l}{\\partial \\hat{z}_i}\\gamma \\cdot -\\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}}\\\\\n    \\frac{\\partial l}{\\partial x_i} &= \\frac{\\partial l}{\\partial \\hat{x}_i} \\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}} + \\frac{\\partial l}{\\partial \\sigma^2_B} \\frac{2(x_i - \\mu_B)}{m} + \\frac{\\partial l}{\\partial \\mu_B} \\frac{1}{m}\\\\\n    &= \\frac{\\partial l}{\\partial \\hat{z}_i} \\gamma \\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}} + \\sum\\limits_{j=1}^m (\\frac{\\partial l}{\\partial \\hat{z}_j} \\gamma (x_j - \\mu_B)\\cdot-\\frac{1}{2}(\\sigma^2_B +\\epsilon)^{-\\frac{3}{2}}) \\frac{2(x_i - \\mu_B)}{m} + \\sum\\limits_{k=1}^m (\\frac{\\partial l}{\\partial \\hat{z}_k}\\gamma \\cdot -\\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}}) \\frac{1}{m}\\\\\n    &= \\frac{\\partial l}{\\partial \\hat{z}_i} \\gamma \\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}} + \\sum\\limits_{j=1}^m \\{\\frac{\\partial l}{\\partial \\hat{z}_j} \\gamma [(x_j - \\mu_B)\\cdot-\\frac{1}{2}(\\sigma^2_B +\\epsilon)^{-\\frac{3}{2}}\\frac{2(x_i - \\mu_B)}{m} - \\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}} \\frac{1}{m}]\\}\\\\\n    &= \\frac{\\partial l}{\\partial \\hat{z}_i} \\gamma \\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}} - \\sum\\limits_{j=1}^m \\{\\frac{\\partial l}{\\partial \\hat{z}_j} \\frac{\\gamma}{m} [(x_j - \\mu_B)\\cdot(\\sigma^2_B +\\epsilon)^{-\\frac{3}{2}}(x_i - \\mu_B) + \\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}}]\\}\\\\\n    &= \\frac{\\partial l}{\\partial \\hat{z}_i} \\gamma \\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}} - \\sum\\limits_{j=1}^m \\{\\frac{\\partial l}{\\partial \\hat{z}_j} \\frac{\\gamma}{m} [\\frac{(x_j - \\mu_B)(x_i - \\mu_B)}{(\\sigma^2_B +\\epsilon)^{\\frac{3}{2}}} + \\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}}]\\}\\\\\n    &= \\frac{\\partial l}{\\partial \\hat{z}_i} \\gamma \\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}} - \\sum\\limits_{j=1}^m \\{\\frac{\\partial l}{\\partial \\hat{z}_j} \\frac{\\gamma}{m} [\\frac{(x_j - \\mu_B)(x_i - \\mu_B) + \\sigma^2_B +\\epsilon}{(\\sigma^2_B +\\epsilon)^{\\frac{3}{2}}}]\\}\\\\\n\\end{aligned}\n$$\n\n**如果没有BatchNormalization，则$\\hat{z} = \\hat{x} = x = wl + b$，可以看出，添加了BatchNormalization之后，计算$\\frac{\\partial l}{\\partial x_i}$的过程变得复杂了很多，仅考虑$\\frac{\\partial l}{\\partial \\hat{z}_i} \\gamma \\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}}$这一项的话，可以看出，如果一个batch中，如果特征的标准差越大，则传递回去的梯度越小，如果模型学习到的标准差$\\gamma$很大，则梯度也相应变大，这里相当于有个自动调整学习速率的功能因此增加了BN层的模型，能够适应更高的学习速率。但是后面一项就不好分析了，我暂时没有什么思路。如果有自己的想法欢迎留言讨论。**\n\nBatchNormalization的缺点也很明显：如果batch size比较小，那么想要通过$\\sigma^2_B$和$\\mu_B$来估计所有特征的方差和均值非常困难。\n\n# BatchNormalization的变种\n除了BatchNormalization，还有一些Normalization方式，其对比如下图所示。\n\n![各种normalization对比](normalization.png)\n\n## LayerNormalization\n\nLN对每个样本计算一个标量均值和方差，计算过程不受batch size的影响，常用在RNN中，但是如果同一个样本的特征区别较大，则不适合使用统一的均值和方差。\n\n## InstanceNormalization\n\n对每个样本计算C个均值和方差，不受batch size和通道数的影响，常用于风格化迁移，如果通道之间有相关性，则不适合使用IN。\n\n## GroupNormalization\n\n作为BatchNormalization的变种之一，GroupNormalization主要解决的问题是BatchNormalization对batch大小的依赖性。\n\n在CNN中，对于$N\\times H\\times W\\times C$大小的特征图，BatchNormalization将其中每一个大小为$1\\times 1\\times 1\\times C$看做一个样本，而GroupNormalization首先将$N\\times H\\times W\\times C$大小的特征图拆分成$N\\times H\\times W\\times \\frac{C}{G}\\times G$，然后在$H\\times W\\times \\frac{C}{G}$范围内求方差和均值，得到$N \\times G$个均值和方差，可以理解为样本个数为$H\\times W\\times \\frac{C}{G}$，每个样本的维度为$N \\times G$，这样做的好处是样本个数不依赖batch size，原论文中作者还解释说使用GroupNormalization将特征分组处理，更加符合特征之间的依赖性，对模型性能有提升。\n\n训练时GroupNormalization的方差和均值都是$N\\times 1\\times 1\\times 1\\times G$大小，在测试时，Batch size基本上不会和训练时一样，不能和BN一样保存均值和方差参数，因此GN在模型保存时，只保存$\\gamma$和$\\beta$的值，均值和方差在预测过程中是每个样本计算一次的。\n\n","source":"_posts/学习笔记/BatchNormalization学习笔记.md","raw":"---\ntitle: BatchNormalization学习笔记\ndate: 2020-05-04 13:51:26\ntags: [深度学习, 论文笔记]\nmathjax: true\n---\n\n# BatchNormalization的作用机制\n## 推理过程\n对于深度神经网络中的一层，可以表示为$o = g(x),\\ x=wl + b$，其中$w,b$是该层的参数，$l$是该层的输入，$g$表示激活函数，$o$表示该层输出。\n\n在提出BN的原始论文《Batch Normalization: Accelerating Deep Network Training byReducing Internal Covariate Shift》中，BN层是加在每一层的激活函数之前的，即对上面的$x$进行操作。\n\nBN层的操作有两步：\n- 首先对数据进行归一化（normalize）：设$\\mu$，$\\sigma^2$分别是$x$的均值和方差，归一化之后的$x$变为$\\hat{x} = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}$，其中$\\epsilon$是个防止分母为零的平滑项，\n- 然后对归一化的数据进行scale和shift（transform）：$\\gamma \\hat{x} + \\beta$，最终该层加入BN之后，表达式变为$o = g(\\gamma \\hat{x} + \\beta)$\n\n这里先不管$\\mu,\\ \\sigma^2,\\ \\epsilon,\\ \\beta$怎么得到，上面的操作看起来非常简单，不过这里有一点细节：一般情况下，这里的$x,\\ x_i,\\ \\mu,\\ \\sigma^2,\\ \\epsilon,\\ \\gamma,\\ \\beta$全都是向量，因为一个样本由多个特征构成，BatchNormalization其实是对每个特征进行normalize和transform，因此上面涉及到的计算，其实全都是逐元素运算，如果是对于图像数据，在CNN中的BatchNormalization其实是将特征图上的每个点当做一个样本的，例如一个$N\\times H\\times W\\times C$大小的特征图（$N$是batch size），那么其中$1\\times 1 \\times 1 \\times C$的一个点就被当成一个样本，其包含$C$个特征，所有样本共用$C$个均值和方差。\n\n## 训练过程\n对于BatchNormalizatioin层，训练过程其实就是确定$\\mu,\\ \\sigma^2,\\ \\gamma,\\ \\beta$的过程。\n\n首先，对于$\\mu,\\ \\sigma^2$，在训练过程中，对于每一个batch的数据，我们可以得到$\\mu_B = \\frac{1}{m}\\sum\\limits_{i=1}^m x_i$，$\\sigma^2_B = \\frac{1}{m}\\sum\\limits_{i=1}^m(x_i -\\mu)^2$作为当前batch数据的均值和方差，其中$m$是一个batch中的样本数，下标$i$表示某个样本，训练是迭代进行的，为了得到对所有数据的均值和方差，需要使用一些方法根据$\\mu_B,\\ \\sigma^2_B$来估计整体数据的均值和方差。\n\n在主流的深度学习框架（tensorflow、pytorch）中，一般使用滑动平均来进行估计：\n将变量$\\mu,\\ \\sigma^2$初始化为$\\mu_0,\\ \\sigma^2_0$，因为每次迭代处理一个batch的数据，均可以得到$\\mu_B,\\ \\sigma^2_B$，第$k$次迭代时，更新$\\mu_k = \\alpha\\mu_{k-1} + (1-\\alpha)\\mu_B,\\ \\sigma^2_k = \\alpha\\sigma^2_{k-1} + (1-\\alpha)\\sigma^2_B$，其中$0 < \\alpha < 1$是滑动平均的滑动速率，训练完成之后，假设一共迭代了$K$次，则令$\\mu = \\mu_K,\\ \\sigma^2 = \\sigma^2_K$，即可用于推理过程。\n\n对于$\\gamma,\\ \\beta$两个参数，在训练过程中，将其和$w,\\ b$一样，使用优化器根据梯度进行更新即可。\n\n\n## BatchNormalization的推理加速\n将BatchNormalization的两个步骤结合起来，可以得到：\n\n$$\n\\begin{aligned}\n    o &= g(\\hat{z})\\\\\n    &= g(\\gamma \\hat{x} + \\beta)\\\\\n    &= g(\\gamma \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta)\\\\\n    &= g(\\gamma \\frac{wl + b - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta)\\\\\n    &= g(\\frac{\\gamma wl}{\\sqrt{\\sigma^2 + \\epsilon}} + \\frac{\\gamma b -\\gamma \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}+ \\beta)\\\\\n\\end{aligned}\n$$\n\n可以发现，在模型训练完成之后，如果仅需要使用模型进行推理，则可以将$\\frac{\\gamma w}{\\sqrt{\\sigma^2 + \\epsilon}}$保存为该层的$w$参数，将$\\frac{\\gamma b -\\gamma \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}+ \\beta$保存为该层的$b$参数。这样在推理过程中完全不增加计算量。\n\n有一种说法是如果使用了BatchNormalization，则该层的偏置参数$b$可以省略掉，这也是根据上式得来的，因为BatchNormalization中包含了一个可学习的$\\beta$项，完全可以替代$b$参数的效果，而且不影响模型的表达能力。\n\n# BatchNormalization的理解\n原论文中认为：BatchNormalization可以让深度神经网络使用更大的学习速率训练以达到加速收敛的目的。\n\n论文中作者解释说是因为深度神经网络在训练过程中，由于前一层参数变化，导致每一层的输入分布不断发生变化，这种情况在论文中被称为“internal convariate shift”，BatchNormalization的作用是缓解这种“internal convariate shift”：通过强行将激活之前的特征进行减平均除方差的操作，使得特征均值为0，方差为1，让激活之后的输出，即下一层的输入，维持在一个相对稳定的分布中。\n\n但是在该论文中，作者又说如果简单的将某一层的激活函数的输入进行normalize，会改变该层的表示能力（原文：Note  that  simply  normalizing  each  input  of  a  layer  may change  what  the  layer  can  represent.   For  instance,  nor-malizing the inputs of a sigmoid would constrain them tothe linear regime of the nonlinearity.），因此需要在normalize之后，再对其进行transform。\n\n这里的transform之后，如果不是接近于0均值，1方差的，那之前的normalize就白做了，原论文中也明确说了transform有能力将之前的normalize的效果完全还原，所以原论文得出的结论是这样一来模型可以选择是否进行normalize，相当于增强了模型的表达能力。\n\n我对这里的理解是：**BatchNormalization先将特征变换到0均值，1方差，再transform到$\\beta$均值，$\\gamma$标准差，这样一定程度上可以让模型来显式地选择特征的均值和方差，可以让数据的分布向更加适合随机初始化的权重的位置靠拢（否则就需要单方面调整随机初始化的权重来适应数据的分布，这样显然需要更长时间的训练），因此BatchNormalization有缩短收敛时间的效果，也可以说是减小了对参数初始化的依赖，另外，BN层的先normalize再transform的操作，如果通过$\\beta$和$\\gamma$将特征的均值和方差限定在一定范围，一定程度上是对模型表达能力的一种限制，有一定的类似正则化的作用，一定情况下或许可以稍微提高模型的泛化能力。**\n\n从梯度方面来看，BN是通过某种对梯度的调整，从而对整个学习过程产生影响，我分析了论文中得出的求导结果，并将$\\frac{\\partial l}{\\partial x_i}$继续化简，如下：\n$$\n\\begin{aligned}\n    \\frac{\\partial l}{\\partial \\hat{x}} &= \\frac{\\partial l}{\\partial \\hat{z}} \\gamma\\\\\n    \\frac{\\partial l}{\\partial \\sigma^2_B} &= \\sum\\limits_{i=1}^m \\frac{\\partial l}{\\partial \\hat{x}_i}(x_i - \\mu_B)\\cdot-\\frac{1}{2}(\\sigma^2_B +\\epsilon)^{-\\frac{3}{2}}\\\\\n    &= \\sum\\limits_{i=1}^m \\frac{\\partial l}{\\partial \\hat{z}_i} \\gamma (x_i - \\mu_B)\\cdot-\\frac{1}{2}(\\sigma^2_B +\\epsilon)^{-\\frac{3}{2}}\\\\\n    \\frac{\\partial l}{\\partial \\mu_B} &= \\sum_{i=1}^m\\frac{\\partial l}{\\partial \\hat{x}_i} \\cdot -\\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}}\\\\\n    &=\\sum\\limits_{i=1}^m \\frac{\\partial l}{\\partial \\hat{z}_i}\\gamma \\cdot -\\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}}\\\\\n    \\frac{\\partial l}{\\partial x_i} &= \\frac{\\partial l}{\\partial \\hat{x}_i} \\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}} + \\frac{\\partial l}{\\partial \\sigma^2_B} \\frac{2(x_i - \\mu_B)}{m} + \\frac{\\partial l}{\\partial \\mu_B} \\frac{1}{m}\\\\\n    &= \\frac{\\partial l}{\\partial \\hat{z}_i} \\gamma \\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}} + \\sum\\limits_{j=1}^m (\\frac{\\partial l}{\\partial \\hat{z}_j} \\gamma (x_j - \\mu_B)\\cdot-\\frac{1}{2}(\\sigma^2_B +\\epsilon)^{-\\frac{3}{2}}) \\frac{2(x_i - \\mu_B)}{m} + \\sum\\limits_{k=1}^m (\\frac{\\partial l}{\\partial \\hat{z}_k}\\gamma \\cdot -\\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}}) \\frac{1}{m}\\\\\n    &= \\frac{\\partial l}{\\partial \\hat{z}_i} \\gamma \\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}} + \\sum\\limits_{j=1}^m \\{\\frac{\\partial l}{\\partial \\hat{z}_j} \\gamma [(x_j - \\mu_B)\\cdot-\\frac{1}{2}(\\sigma^2_B +\\epsilon)^{-\\frac{3}{2}}\\frac{2(x_i - \\mu_B)}{m} - \\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}} \\frac{1}{m}]\\}\\\\\n    &= \\frac{\\partial l}{\\partial \\hat{z}_i} \\gamma \\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}} - \\sum\\limits_{j=1}^m \\{\\frac{\\partial l}{\\partial \\hat{z}_j} \\frac{\\gamma}{m} [(x_j - \\mu_B)\\cdot(\\sigma^2_B +\\epsilon)^{-\\frac{3}{2}}(x_i - \\mu_B) + \\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}}]\\}\\\\\n    &= \\frac{\\partial l}{\\partial \\hat{z}_i} \\gamma \\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}} - \\sum\\limits_{j=1}^m \\{\\frac{\\partial l}{\\partial \\hat{z}_j} \\frac{\\gamma}{m} [\\frac{(x_j - \\mu_B)(x_i - \\mu_B)}{(\\sigma^2_B +\\epsilon)^{\\frac{3}{2}}} + \\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}}]\\}\\\\\n    &= \\frac{\\partial l}{\\partial \\hat{z}_i} \\gamma \\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}} - \\sum\\limits_{j=1}^m \\{\\frac{\\partial l}{\\partial \\hat{z}_j} \\frac{\\gamma}{m} [\\frac{(x_j - \\mu_B)(x_i - \\mu_B) + \\sigma^2_B +\\epsilon}{(\\sigma^2_B +\\epsilon)^{\\frac{3}{2}}}]\\}\\\\\n\\end{aligned}\n$$\n\n**如果没有BatchNormalization，则$\\hat{z} = \\hat{x} = x = wl + b$，可以看出，添加了BatchNormalization之后，计算$\\frac{\\partial l}{\\partial x_i}$的过程变得复杂了很多，仅考虑$\\frac{\\partial l}{\\partial \\hat{z}_i} \\gamma \\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}}$这一项的话，可以看出，如果一个batch中，如果特征的标准差越大，则传递回去的梯度越小，如果模型学习到的标准差$\\gamma$很大，则梯度也相应变大，这里相当于有个自动调整学习速率的功能因此增加了BN层的模型，能够适应更高的学习速率。但是后面一项就不好分析了，我暂时没有什么思路。如果有自己的想法欢迎留言讨论。**\n\nBatchNormalization的缺点也很明显：如果batch size比较小，那么想要通过$\\sigma^2_B$和$\\mu_B$来估计所有特征的方差和均值非常困难。\n\n# BatchNormalization的变种\n除了BatchNormalization，还有一些Normalization方式，其对比如下图所示。\n\n![各种normalization对比](normalization.png)\n\n## LayerNormalization\n\nLN对每个样本计算一个标量均值和方差，计算过程不受batch size的影响，常用在RNN中，但是如果同一个样本的特征区别较大，则不适合使用统一的均值和方差。\n\n## InstanceNormalization\n\n对每个样本计算C个均值和方差，不受batch size和通道数的影响，常用于风格化迁移，如果通道之间有相关性，则不适合使用IN。\n\n## GroupNormalization\n\n作为BatchNormalization的变种之一，GroupNormalization主要解决的问题是BatchNormalization对batch大小的依赖性。\n\n在CNN中，对于$N\\times H\\times W\\times C$大小的特征图，BatchNormalization将其中每一个大小为$1\\times 1\\times 1\\times C$看做一个样本，而GroupNormalization首先将$N\\times H\\times W\\times C$大小的特征图拆分成$N\\times H\\times W\\times \\frac{C}{G}\\times G$，然后在$H\\times W\\times \\frac{C}{G}$范围内求方差和均值，得到$N \\times G$个均值和方差，可以理解为样本个数为$H\\times W\\times \\frac{C}{G}$，每个样本的维度为$N \\times G$，这样做的好处是样本个数不依赖batch size，原论文中作者还解释说使用GroupNormalization将特征分组处理，更加符合特征之间的依赖性，对模型性能有提升。\n\n训练时GroupNormalization的方差和均值都是$N\\times 1\\times 1\\times 1\\times G$大小，在测试时，Batch size基本上不会和训练时一样，不能和BN一样保存均值和方差参数，因此GN在模型保存时，只保存$\\gamma$和$\\beta$的值，均值和方差在预测过程中是每个样本计算一次的。\n\n","slug":"学习笔记/BatchNormalization学习笔记","published":1,"updated":"2020-08-31T06:39:20.721Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckjxpc3ra000044mqfmzib5d6","content":"<h1 id=\"batchnormalization的作用机制\">BatchNormalization的作用机制</h1>\r\n<h2 id=\"推理过程\">推理过程</h2>\r\n<p>对于深度神经网络中的一层，可以表示为<span class=\"math inline\">\\(o = g(x),\\ x=wl + b\\)</span>，其中<span class=\"math inline\">\\(w,b\\)</span>是该层的参数，<span class=\"math inline\">\\(l\\)</span>是该层的输入，<span class=\"math inline\">\\(g\\)</span>表示激活函数，<span class=\"math inline\">\\(o\\)</span>表示该层输出。</p>\r\n<p>在提出BN的原始论文《Batch Normalization: Accelerating Deep Network Training byReducing Internal Covariate Shift》中，BN层是加在每一层的激活函数之前的，即对上面的<span class=\"math inline\">\\(x\\)</span>进行操作。</p>\r\n<p>BN层的操作有两步： - 首先对数据进行归一化（normalize）：设<span class=\"math inline\">\\(\\mu\\)</span>，<span class=\"math inline\">\\(\\sigma^2\\)</span>分别是<span class=\"math inline\">\\(x\\)</span>的均值和方差，归一化之后的<span class=\"math inline\">\\(x\\)</span>变为<span class=\"math inline\">\\(\\hat{x} = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\\)</span>，其中<span class=\"math inline\">\\(\\epsilon\\)</span>是个防止分母为零的平滑项， - 然后对归一化的数据进行scale和shift（transform）：<span class=\"math inline\">\\(\\gamma \\hat{x} + \\beta\\)</span>，最终该层加入BN之后，表达式变为<span class=\"math inline\">\\(o = g(\\gamma \\hat{x} + \\beta)\\)</span></p>\r\n<p>这里先不管<span class=\"math inline\">\\(\\mu,\\ \\sigma^2,\\ \\epsilon,\\ \\beta\\)</span>怎么得到，上面的操作看起来非常简单，不过这里有一点细节：一般情况下，这里的<span class=\"math inline\">\\(x,\\ x_i,\\ \\mu,\\ \\sigma^2,\\ \\epsilon,\\ \\gamma,\\ \\beta\\)</span>全都是向量，因为一个样本由多个特征构成，BatchNormalization其实是对每个特征进行normalize和transform，因此上面涉及到的计算，其实全都是逐元素运算，如果是对于图像数据，在CNN中的BatchNormalization其实是将特征图上的每个点当做一个样本的，例如一个<span class=\"math inline\">\\(N\\times H\\times W\\times C\\)</span>大小的特征图（<span class=\"math inline\">\\(N\\)</span>是batch size），那么其中<span class=\"math inline\">\\(1\\times 1 \\times 1 \\times C\\)</span>的一个点就被当成一个样本，其包含<span class=\"math inline\">\\(C\\)</span>个特征，所有样本共用<span class=\"math inline\">\\(C\\)</span>个均值和方差。</p>\r\n<h2 id=\"训练过程\">训练过程</h2>\r\n<p>对于BatchNormalizatioin层，训练过程其实就是确定<span class=\"math inline\">\\(\\mu,\\ \\sigma^2,\\ \\gamma,\\ \\beta\\)</span>的过程。</p>\r\n<p>首先，对于<span class=\"math inline\">\\(\\mu,\\ \\sigma^2\\)</span>，在训练过程中，对于每一个batch的数据，我们可以得到<span class=\"math inline\">\\(\\mu_B = \\frac{1}{m}\\sum\\limits_{i=1}^m x_i\\)</span>，<span class=\"math inline\">\\(\\sigma^2_B = \\frac{1}{m}\\sum\\limits_{i=1}^m(x_i -\\mu)^2\\)</span>作为当前batch数据的均值和方差，其中<span class=\"math inline\">\\(m\\)</span>是一个batch中的样本数，下标<span class=\"math inline\">\\(i\\)</span>表示某个样本，训练是迭代进行的，为了得到对所有数据的均值和方差，需要使用一些方法根据<span class=\"math inline\">\\(\\mu_B,\\ \\sigma^2_B\\)</span>来估计整体数据的均值和方差。</p>\r\n<p>在主流的深度学习框架（tensorflow、pytorch）中，一般使用滑动平均来进行估计： 将变量<span class=\"math inline\">\\(\\mu,\\ \\sigma^2\\)</span>初始化为<span class=\"math inline\">\\(\\mu_0,\\ \\sigma^2_0\\)</span>，因为每次迭代处理一个batch的数据，均可以得到<span class=\"math inline\">\\(\\mu_B,\\ \\sigma^2_B\\)</span>，第<span class=\"math inline\">\\(k\\)</span>次迭代时，更新<span class=\"math inline\">\\(\\mu_k = \\alpha\\mu_{k-1} + (1-\\alpha)\\mu_B,\\ \\sigma^2_k = \\alpha\\sigma^2_{k-1} + (1-\\alpha)\\sigma^2_B\\)</span>，其中<span class=\"math inline\">\\(0 &lt; \\alpha &lt; 1\\)</span>是滑动平均的滑动速率，训练完成之后，假设一共迭代了<span class=\"math inline\">\\(K\\)</span>次，则令<span class=\"math inline\">\\(\\mu = \\mu_K,\\ \\sigma^2 = \\sigma^2_K\\)</span>，即可用于推理过程。</p>\r\n<p>对于<span class=\"math inline\">\\(\\gamma,\\ \\beta\\)</span>两个参数，在训练过程中，将其和<span class=\"math inline\">\\(w,\\ b\\)</span>一样，使用优化器根据梯度进行更新即可。</p>\r\n<h2 id=\"batchnormalization的推理加速\">BatchNormalization的推理加速</h2>\r\n<p>将BatchNormalization的两个步骤结合起来，可以得到：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    o &amp;= g(\\hat{z})\\\\\r\n    &amp;= g(\\gamma \\hat{x} + \\beta)\\\\\r\n    &amp;= g(\\gamma \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta)\\\\\r\n    &amp;= g(\\gamma \\frac{wl + b - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta)\\\\\r\n    &amp;= g(\\frac{\\gamma wl}{\\sqrt{\\sigma^2 + \\epsilon}} + \\frac{\\gamma b -\\gamma \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}+ \\beta)\\\\\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>可以发现，在模型训练完成之后，如果仅需要使用模型进行推理，则可以将<span class=\"math inline\">\\(\\frac{\\gamma w}{\\sqrt{\\sigma^2 + \\epsilon}}\\)</span>保存为该层的<span class=\"math inline\">\\(w\\)</span>参数，将<span class=\"math inline\">\\(\\frac{\\gamma b -\\gamma \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}+ \\beta\\)</span>保存为该层的<span class=\"math inline\">\\(b\\)</span>参数。这样在推理过程中完全不增加计算量。</p>\r\n<p>有一种说法是如果使用了BatchNormalization，则该层的偏置参数<span class=\"math inline\">\\(b\\)</span>可以省略掉，这也是根据上式得来的，因为BatchNormalization中包含了一个可学习的<span class=\"math inline\">\\(\\beta\\)</span>项，完全可以替代<span class=\"math inline\">\\(b\\)</span>参数的效果，而且不影响模型的表达能力。</p>\r\n<h1 id=\"batchnormalization的理解\">BatchNormalization的理解</h1>\r\n<p>原论文中认为：BatchNormalization可以让深度神经网络使用更大的学习速率训练以达到加速收敛的目的。</p>\r\n<p>论文中作者解释说是因为深度神经网络在训练过程中，由于前一层参数变化，导致每一层的输入分布不断发生变化，这种情况在论文中被称为“internal convariate shift”，BatchNormalization的作用是缓解这种“internal convariate shift”：通过强行将激活之前的特征进行减平均除方差的操作，使得特征均值为0，方差为1，让激活之后的输出，即下一层的输入，维持在一个相对稳定的分布中。</p>\r\n<p>但是在该论文中，作者又说如果简单的将某一层的激活函数的输入进行normalize，会改变该层的表示能力（原文：Note that simply normalizing each input of a layer may change what the layer can represent. For instance, nor-malizing the inputs of a sigmoid would constrain them tothe linear regime of the nonlinearity.），因此需要在normalize之后，再对其进行transform。</p>\r\n<p>这里的transform之后，如果不是接近于0均值，1方差的，那之前的normalize就白做了，原论文中也明确说了transform有能力将之前的normalize的效果完全还原，所以原论文得出的结论是这样一来模型可以选择是否进行normalize，相当于增强了模型的表达能力。</p>\r\n<p>我对这里的理解是：<strong>BatchNormalization先将特征变换到0均值，1方差，再transform到<span class=\"math inline\">\\(\\beta\\)</span>均值，<span class=\"math inline\">\\(\\gamma\\)</span>标准差，这样一定程度上可以让模型来显式地选择特征的均值和方差，可以让数据的分布向更加适合随机初始化的权重的位置靠拢（否则就需要单方面调整随机初始化的权重来适应数据的分布，这样显然需要更长时间的训练），因此BatchNormalization有缩短收敛时间的效果，也可以说是减小了对参数初始化的依赖，另外，BN层的先normalize再transform的操作，如果通过<span class=\"math inline\">\\(\\beta\\)</span>和<span class=\"math inline\">\\(\\gamma\\)</span>将特征的均值和方差限定在一定范围，一定程度上是对模型表达能力的一种限制，有一定的类似正则化的作用，一定情况下或许可以稍微提高模型的泛化能力。</strong></p>\r\n<p>从梯度方面来看，BN是通过某种对梯度的调整，从而对整个学习过程产生影响，我分析了论文中得出的求导结果，并将<span class=\"math inline\">\\(\\frac{\\partial l}{\\partial x_i}\\)</span>继续化简，如下： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\frac{\\partial l}{\\partial \\hat{x}} &amp;= \\frac{\\partial l}{\\partial \\hat{z}} \\gamma\\\\\r\n    \\frac{\\partial l}{\\partial \\sigma^2_B} &amp;= \\sum\\limits_{i=1}^m \\frac{\\partial l}{\\partial \\hat{x}_i}(x_i - \\mu_B)\\cdot-\\frac{1}{2}(\\sigma^2_B +\\epsilon)^{-\\frac{3}{2}}\\\\\r\n    &amp;= \\sum\\limits_{i=1}^m \\frac{\\partial l}{\\partial \\hat{z}_i} \\gamma (x_i - \\mu_B)\\cdot-\\frac{1}{2}(\\sigma^2_B +\\epsilon)^{-\\frac{3}{2}}\\\\\r\n    \\frac{\\partial l}{\\partial \\mu_B} &amp;= \\sum_{i=1}^m\\frac{\\partial l}{\\partial \\hat{x}_i} \\cdot -\\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}}\\\\\r\n    &amp;=\\sum\\limits_{i=1}^m \\frac{\\partial l}{\\partial \\hat{z}_i}\\gamma \\cdot -\\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}}\\\\\r\n    \\frac{\\partial l}{\\partial x_i} &amp;= \\frac{\\partial l}{\\partial \\hat{x}_i} \\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}} + \\frac{\\partial l}{\\partial \\sigma^2_B} \\frac{2(x_i - \\mu_B)}{m} + \\frac{\\partial l}{\\partial \\mu_B} \\frac{1}{m}\\\\\r\n    &amp;= \\frac{\\partial l}{\\partial \\hat{z}_i} \\gamma \\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}} + \\sum\\limits_{j=1}^m (\\frac{\\partial l}{\\partial \\hat{z}_j} \\gamma (x_j - \\mu_B)\\cdot-\\frac{1}{2}(\\sigma^2_B +\\epsilon)^{-\\frac{3}{2}}) \\frac{2(x_i - \\mu_B)}{m} + \\sum\\limits_{k=1}^m (\\frac{\\partial l}{\\partial \\hat{z}_k}\\gamma \\cdot -\\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}}) \\frac{1}{m}\\\\\r\n    &amp;= \\frac{\\partial l}{\\partial \\hat{z}_i} \\gamma \\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}} + \\sum\\limits_{j=1}^m \\{\\frac{\\partial l}{\\partial \\hat{z}_j} \\gamma [(x_j - \\mu_B)\\cdot-\\frac{1}{2}(\\sigma^2_B +\\epsilon)^{-\\frac{3}{2}}\\frac{2(x_i - \\mu_B)}{m} - \\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}} \\frac{1}{m}]\\}\\\\\r\n    &amp;= \\frac{\\partial l}{\\partial \\hat{z}_i} \\gamma \\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}} - \\sum\\limits_{j=1}^m \\{\\frac{\\partial l}{\\partial \\hat{z}_j} \\frac{\\gamma}{m} [(x_j - \\mu_B)\\cdot(\\sigma^2_B +\\epsilon)^{-\\frac{3}{2}}(x_i - \\mu_B) + \\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}}]\\}\\\\\r\n    &amp;= \\frac{\\partial l}{\\partial \\hat{z}_i} \\gamma \\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}} - \\sum\\limits_{j=1}^m \\{\\frac{\\partial l}{\\partial \\hat{z}_j} \\frac{\\gamma}{m} [\\frac{(x_j - \\mu_B)(x_i - \\mu_B)}{(\\sigma^2_B +\\epsilon)^{\\frac{3}{2}}} + \\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}}]\\}\\\\\r\n    &amp;= \\frac{\\partial l}{\\partial \\hat{z}_i} \\gamma \\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}} - \\sum\\limits_{j=1}^m \\{\\frac{\\partial l}{\\partial \\hat{z}_j} \\frac{\\gamma}{m} [\\frac{(x_j - \\mu_B)(x_i - \\mu_B) + \\sigma^2_B +\\epsilon}{(\\sigma^2_B +\\epsilon)^{\\frac{3}{2}}}]\\}\\\\\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p><strong>如果没有BatchNormalization，则<span class=\"math inline\">\\(\\hat{z} = \\hat{x} = x = wl + b\\)</span>，可以看出，添加了BatchNormalization之后，计算<span class=\"math inline\">\\(\\frac{\\partial l}{\\partial x_i}\\)</span>的过程变得复杂了很多，仅考虑<span class=\"math inline\">\\(\\frac{\\partial l}{\\partial \\hat{z}_i} \\gamma \\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}}\\)</span>这一项的话，可以看出，如果一个batch中，如果特征的标准差越大，则传递回去的梯度越小，如果模型学习到的标准差<span class=\"math inline\">\\(\\gamma\\)</span>很大，则梯度也相应变大，这里相当于有个自动调整学习速率的功能因此增加了BN层的模型，能够适应更高的学习速率。但是后面一项就不好分析了，我暂时没有什么思路。如果有自己的想法欢迎留言讨论。</strong></p>\r\n<p>BatchNormalization的缺点也很明显：如果batch size比较小，那么想要通过<span class=\"math inline\">\\(\\sigma^2_B\\)</span>和<span class=\"math inline\">\\(\\mu_B\\)</span>来估计所有特征的方差和均值非常困难。</p>\r\n<h1 id=\"batchnormalization的变种\">BatchNormalization的变种</h1>\r\n<p>除了BatchNormalization，还有一些Normalization方式，其对比如下图所示。</p>\r\n<figure>\r\n<img src=\"normalization.png\" alt=\"各种normalization对比\" /><figcaption aria-hidden=\"true\">各种normalization对比</figcaption>\r\n</figure>\r\n<h2 id=\"layernormalization\">LayerNormalization</h2>\r\n<p>LN对每个样本计算一个标量均值和方差，计算过程不受batch size的影响，常用在RNN中，但是如果同一个样本的特征区别较大，则不适合使用统一的均值和方差。</p>\r\n<h2 id=\"instancenormalization\">InstanceNormalization</h2>\r\n<p>对每个样本计算C个均值和方差，不受batch size和通道数的影响，常用于风格化迁移，如果通道之间有相关性，则不适合使用IN。</p>\r\n<h2 id=\"groupnormalization\">GroupNormalization</h2>\r\n<p>作为BatchNormalization的变种之一，GroupNormalization主要解决的问题是BatchNormalization对batch大小的依赖性。</p>\r\n<p>在CNN中，对于<span class=\"math inline\">\\(N\\times H\\times W\\times C\\)</span>大小的特征图，BatchNormalization将其中每一个大小为<span class=\"math inline\">\\(1\\times 1\\times 1\\times C\\)</span>看做一个样本，而GroupNormalization首先将<span class=\"math inline\">\\(N\\times H\\times W\\times C\\)</span>大小的特征图拆分成<span class=\"math inline\">\\(N\\times H\\times W\\times \\frac{C}{G}\\times G\\)</span>，然后在<span class=\"math inline\">\\(H\\times W\\times \\frac{C}{G}\\)</span>范围内求方差和均值，得到<span class=\"math inline\">\\(N \\times G\\)</span>个均值和方差，可以理解为样本个数为<span class=\"math inline\">\\(H\\times W\\times \\frac{C}{G}\\)</span>，每个样本的维度为<span class=\"math inline\">\\(N \\times G\\)</span>，这样做的好处是样本个数不依赖batch size，原论文中作者还解释说使用GroupNormalization将特征分组处理，更加符合特征之间的依赖性，对模型性能有提升。</p>\r\n<p>训练时GroupNormalization的方差和均值都是<span class=\"math inline\">\\(N\\times 1\\times 1\\times 1\\times G\\)</span>大小，在测试时，Batch size基本上不会和训练时一样，不能和BN一样保存均值和方差参数，因此GN在模型保存时，只保存<span class=\"math inline\">\\(\\gamma\\)</span>和<span class=\"math inline\">\\(\\beta\\)</span>的值，均值和方差在预测过程中是每个样本计算一次的。</p>\r\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"batchnormalization的作用机制\">BatchNormalization的作用机制</h1>\r\n<h2 id=\"推理过程\">推理过程</h2>\r\n<p>对于深度神经网络中的一层，可以表示为<span class=\"math inline\">\\(o = g(x),\\ x=wl + b\\)</span>，其中<span class=\"math inline\">\\(w,b\\)</span>是该层的参数，<span class=\"math inline\">\\(l\\)</span>是该层的输入，<span class=\"math inline\">\\(g\\)</span>表示激活函数，<span class=\"math inline\">\\(o\\)</span>表示该层输出。</p>\r\n<p>在提出BN的原始论文《Batch Normalization: Accelerating Deep Network Training byReducing Internal Covariate Shift》中，BN层是加在每一层的激活函数之前的，即对上面的<span class=\"math inline\">\\(x\\)</span>进行操作。</p>\r\n<p>BN层的操作有两步： - 首先对数据进行归一化（normalize）：设<span class=\"math inline\">\\(\\mu\\)</span>，<span class=\"math inline\">\\(\\sigma^2\\)</span>分别是<span class=\"math inline\">\\(x\\)</span>的均值和方差，归一化之后的<span class=\"math inline\">\\(x\\)</span>变为<span class=\"math inline\">\\(\\hat{x} = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\\)</span>，其中<span class=\"math inline\">\\(\\epsilon\\)</span>是个防止分母为零的平滑项， - 然后对归一化的数据进行scale和shift（transform）：<span class=\"math inline\">\\(\\gamma \\hat{x} + \\beta\\)</span>，最终该层加入BN之后，表达式变为<span class=\"math inline\">\\(o = g(\\gamma \\hat{x} + \\beta)\\)</span></p>\r\n<p>这里先不管<span class=\"math inline\">\\(\\mu,\\ \\sigma^2,\\ \\epsilon,\\ \\beta\\)</span>怎么得到，上面的操作看起来非常简单，不过这里有一点细节：一般情况下，这里的<span class=\"math inline\">\\(x,\\ x_i,\\ \\mu,\\ \\sigma^2,\\ \\epsilon,\\ \\gamma,\\ \\beta\\)</span>全都是向量，因为一个样本由多个特征构成，BatchNormalization其实是对每个特征进行normalize和transform，因此上面涉及到的计算，其实全都是逐元素运算，如果是对于图像数据，在CNN中的BatchNormalization其实是将特征图上的每个点当做一个样本的，例如一个<span class=\"math inline\">\\(N\\times H\\times W\\times C\\)</span>大小的特征图（<span class=\"math inline\">\\(N\\)</span>是batch size），那么其中<span class=\"math inline\">\\(1\\times 1 \\times 1 \\times C\\)</span>的一个点就被当成一个样本，其包含<span class=\"math inline\">\\(C\\)</span>个特征，所有样本共用<span class=\"math inline\">\\(C\\)</span>个均值和方差。</p>\r\n<h2 id=\"训练过程\">训练过程</h2>\r\n<p>对于BatchNormalizatioin层，训练过程其实就是确定<span class=\"math inline\">\\(\\mu,\\ \\sigma^2,\\ \\gamma,\\ \\beta\\)</span>的过程。</p>\r\n<p>首先，对于<span class=\"math inline\">\\(\\mu,\\ \\sigma^2\\)</span>，在训练过程中，对于每一个batch的数据，我们可以得到<span class=\"math inline\">\\(\\mu_B = \\frac{1}{m}\\sum\\limits_{i=1}^m x_i\\)</span>，<span class=\"math inline\">\\(\\sigma^2_B = \\frac{1}{m}\\sum\\limits_{i=1}^m(x_i -\\mu)^2\\)</span>作为当前batch数据的均值和方差，其中<span class=\"math inline\">\\(m\\)</span>是一个batch中的样本数，下标<span class=\"math inline\">\\(i\\)</span>表示某个样本，训练是迭代进行的，为了得到对所有数据的均值和方差，需要使用一些方法根据<span class=\"math inline\">\\(\\mu_B,\\ \\sigma^2_B\\)</span>来估计整体数据的均值和方差。</p>\r\n<p>在主流的深度学习框架（tensorflow、pytorch）中，一般使用滑动平均来进行估计： 将变量<span class=\"math inline\">\\(\\mu,\\ \\sigma^2\\)</span>初始化为<span class=\"math inline\">\\(\\mu_0,\\ \\sigma^2_0\\)</span>，因为每次迭代处理一个batch的数据，均可以得到<span class=\"math inline\">\\(\\mu_B,\\ \\sigma^2_B\\)</span>，第<span class=\"math inline\">\\(k\\)</span>次迭代时，更新<span class=\"math inline\">\\(\\mu_k = \\alpha\\mu_{k-1} + (1-\\alpha)\\mu_B,\\ \\sigma^2_k = \\alpha\\sigma^2_{k-1} + (1-\\alpha)\\sigma^2_B\\)</span>，其中<span class=\"math inline\">\\(0 &lt; \\alpha &lt; 1\\)</span>是滑动平均的滑动速率，训练完成之后，假设一共迭代了<span class=\"math inline\">\\(K\\)</span>次，则令<span class=\"math inline\">\\(\\mu = \\mu_K,\\ \\sigma^2 = \\sigma^2_K\\)</span>，即可用于推理过程。</p>\r\n<p>对于<span class=\"math inline\">\\(\\gamma,\\ \\beta\\)</span>两个参数，在训练过程中，将其和<span class=\"math inline\">\\(w,\\ b\\)</span>一样，使用优化器根据梯度进行更新即可。</p>\r\n<h2 id=\"batchnormalization的推理加速\">BatchNormalization的推理加速</h2>\r\n<p>将BatchNormalization的两个步骤结合起来，可以得到：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    o &amp;= g(\\hat{z})\\\\\r\n    &amp;= g(\\gamma \\hat{x} + \\beta)\\\\\r\n    &amp;= g(\\gamma \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta)\\\\\r\n    &amp;= g(\\gamma \\frac{wl + b - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta)\\\\\r\n    &amp;= g(\\frac{\\gamma wl}{\\sqrt{\\sigma^2 + \\epsilon}} + \\frac{\\gamma b -\\gamma \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}+ \\beta)\\\\\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>可以发现，在模型训练完成之后，如果仅需要使用模型进行推理，则可以将<span class=\"math inline\">\\(\\frac{\\gamma w}{\\sqrt{\\sigma^2 + \\epsilon}}\\)</span>保存为该层的<span class=\"math inline\">\\(w\\)</span>参数，将<span class=\"math inline\">\\(\\frac{\\gamma b -\\gamma \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}+ \\beta\\)</span>保存为该层的<span class=\"math inline\">\\(b\\)</span>参数。这样在推理过程中完全不增加计算量。</p>\r\n<p>有一种说法是如果使用了BatchNormalization，则该层的偏置参数<span class=\"math inline\">\\(b\\)</span>可以省略掉，这也是根据上式得来的，因为BatchNormalization中包含了一个可学习的<span class=\"math inline\">\\(\\beta\\)</span>项，完全可以替代<span class=\"math inline\">\\(b\\)</span>参数的效果，而且不影响模型的表达能力。</p>\r\n<h1 id=\"batchnormalization的理解\">BatchNormalization的理解</h1>\r\n<p>原论文中认为：BatchNormalization可以让深度神经网络使用更大的学习速率训练以达到加速收敛的目的。</p>\r\n<p>论文中作者解释说是因为深度神经网络在训练过程中，由于前一层参数变化，导致每一层的输入分布不断发生变化，这种情况在论文中被称为“internal convariate shift”，BatchNormalization的作用是缓解这种“internal convariate shift”：通过强行将激活之前的特征进行减平均除方差的操作，使得特征均值为0，方差为1，让激活之后的输出，即下一层的输入，维持在一个相对稳定的分布中。</p>\r\n<p>但是在该论文中，作者又说如果简单的将某一层的激活函数的输入进行normalize，会改变该层的表示能力（原文：Note that simply normalizing each input of a layer may change what the layer can represent. For instance, nor-malizing the inputs of a sigmoid would constrain them tothe linear regime of the nonlinearity.），因此需要在normalize之后，再对其进行transform。</p>\r\n<p>这里的transform之后，如果不是接近于0均值，1方差的，那之前的normalize就白做了，原论文中也明确说了transform有能力将之前的normalize的效果完全还原，所以原论文得出的结论是这样一来模型可以选择是否进行normalize，相当于增强了模型的表达能力。</p>\r\n<p>我对这里的理解是：<strong>BatchNormalization先将特征变换到0均值，1方差，再transform到<span class=\"math inline\">\\(\\beta\\)</span>均值，<span class=\"math inline\">\\(\\gamma\\)</span>标准差，这样一定程度上可以让模型来显式地选择特征的均值和方差，可以让数据的分布向更加适合随机初始化的权重的位置靠拢（否则就需要单方面调整随机初始化的权重来适应数据的分布，这样显然需要更长时间的训练），因此BatchNormalization有缩短收敛时间的效果，也可以说是减小了对参数初始化的依赖，另外，BN层的先normalize再transform的操作，如果通过<span class=\"math inline\">\\(\\beta\\)</span>和<span class=\"math inline\">\\(\\gamma\\)</span>将特征的均值和方差限定在一定范围，一定程度上是对模型表达能力的一种限制，有一定的类似正则化的作用，一定情况下或许可以稍微提高模型的泛化能力。</strong></p>\r\n<p>从梯度方面来看，BN是通过某种对梯度的调整，从而对整个学习过程产生影响，我分析了论文中得出的求导结果，并将<span class=\"math inline\">\\(\\frac{\\partial l}{\\partial x_i}\\)</span>继续化简，如下： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\frac{\\partial l}{\\partial \\hat{x}} &amp;= \\frac{\\partial l}{\\partial \\hat{z}} \\gamma\\\\\r\n    \\frac{\\partial l}{\\partial \\sigma^2_B} &amp;= \\sum\\limits_{i=1}^m \\frac{\\partial l}{\\partial \\hat{x}_i}(x_i - \\mu_B)\\cdot-\\frac{1}{2}(\\sigma^2_B +\\epsilon)^{-\\frac{3}{2}}\\\\\r\n    &amp;= \\sum\\limits_{i=1}^m \\frac{\\partial l}{\\partial \\hat{z}_i} \\gamma (x_i - \\mu_B)\\cdot-\\frac{1}{2}(\\sigma^2_B +\\epsilon)^{-\\frac{3}{2}}\\\\\r\n    \\frac{\\partial l}{\\partial \\mu_B} &amp;= \\sum_{i=1}^m\\frac{\\partial l}{\\partial \\hat{x}_i} \\cdot -\\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}}\\\\\r\n    &amp;=\\sum\\limits_{i=1}^m \\frac{\\partial l}{\\partial \\hat{z}_i}\\gamma \\cdot -\\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}}\\\\\r\n    \\frac{\\partial l}{\\partial x_i} &amp;= \\frac{\\partial l}{\\partial \\hat{x}_i} \\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}} + \\frac{\\partial l}{\\partial \\sigma^2_B} \\frac{2(x_i - \\mu_B)}{m} + \\frac{\\partial l}{\\partial \\mu_B} \\frac{1}{m}\\\\\r\n    &amp;= \\frac{\\partial l}{\\partial \\hat{z}_i} \\gamma \\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}} + \\sum\\limits_{j=1}^m (\\frac{\\partial l}{\\partial \\hat{z}_j} \\gamma (x_j - \\mu_B)\\cdot-\\frac{1}{2}(\\sigma^2_B +\\epsilon)^{-\\frac{3}{2}}) \\frac{2(x_i - \\mu_B)}{m} + \\sum\\limits_{k=1}^m (\\frac{\\partial l}{\\partial \\hat{z}_k}\\gamma \\cdot -\\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}}) \\frac{1}{m}\\\\\r\n    &amp;= \\frac{\\partial l}{\\partial \\hat{z}_i} \\gamma \\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}} + \\sum\\limits_{j=1}^m \\{\\frac{\\partial l}{\\partial \\hat{z}_j} \\gamma [(x_j - \\mu_B)\\cdot-\\frac{1}{2}(\\sigma^2_B +\\epsilon)^{-\\frac{3}{2}}\\frac{2(x_i - \\mu_B)}{m} - \\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}} \\frac{1}{m}]\\}\\\\\r\n    &amp;= \\frac{\\partial l}{\\partial \\hat{z}_i} \\gamma \\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}} - \\sum\\limits_{j=1}^m \\{\\frac{\\partial l}{\\partial \\hat{z}_j} \\frac{\\gamma}{m} [(x_j - \\mu_B)\\cdot(\\sigma^2_B +\\epsilon)^{-\\frac{3}{2}}(x_i - \\mu_B) + \\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}}]\\}\\\\\r\n    &amp;= \\frac{\\partial l}{\\partial \\hat{z}_i} \\gamma \\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}} - \\sum\\limits_{j=1}^m \\{\\frac{\\partial l}{\\partial \\hat{z}_j} \\frac{\\gamma}{m} [\\frac{(x_j - \\mu_B)(x_i - \\mu_B)}{(\\sigma^2_B +\\epsilon)^{\\frac{3}{2}}} + \\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}}]\\}\\\\\r\n    &amp;= \\frac{\\partial l}{\\partial \\hat{z}_i} \\gamma \\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}} - \\sum\\limits_{j=1}^m \\{\\frac{\\partial l}{\\partial \\hat{z}_j} \\frac{\\gamma}{m} [\\frac{(x_j - \\mu_B)(x_i - \\mu_B) + \\sigma^2_B +\\epsilon}{(\\sigma^2_B +\\epsilon)^{\\frac{3}{2}}}]\\}\\\\\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p><strong>如果没有BatchNormalization，则<span class=\"math inline\">\\(\\hat{z} = \\hat{x} = x = wl + b\\)</span>，可以看出，添加了BatchNormalization之后，计算<span class=\"math inline\">\\(\\frac{\\partial l}{\\partial x_i}\\)</span>的过程变得复杂了很多，仅考虑<span class=\"math inline\">\\(\\frac{\\partial l}{\\partial \\hat{z}_i} \\gamma \\frac{1}{\\sqrt{\\sigma^2_B + \\epsilon}}\\)</span>这一项的话，可以看出，如果一个batch中，如果特征的标准差越大，则传递回去的梯度越小，如果模型学习到的标准差<span class=\"math inline\">\\(\\gamma\\)</span>很大，则梯度也相应变大，这里相当于有个自动调整学习速率的功能因此增加了BN层的模型，能够适应更高的学习速率。但是后面一项就不好分析了，我暂时没有什么思路。如果有自己的想法欢迎留言讨论。</strong></p>\r\n<p>BatchNormalization的缺点也很明显：如果batch size比较小，那么想要通过<span class=\"math inline\">\\(\\sigma^2_B\\)</span>和<span class=\"math inline\">\\(\\mu_B\\)</span>来估计所有特征的方差和均值非常困难。</p>\r\n<h1 id=\"batchnormalization的变种\">BatchNormalization的变种</h1>\r\n<p>除了BatchNormalization，还有一些Normalization方式，其对比如下图所示。</p>\r\n<figure>\r\n<img src=\"normalization.png\" alt=\"各种normalization对比\" /><figcaption aria-hidden=\"true\">各种normalization对比</figcaption>\r\n</figure>\r\n<h2 id=\"layernormalization\">LayerNormalization</h2>\r\n<p>LN对每个样本计算一个标量均值和方差，计算过程不受batch size的影响，常用在RNN中，但是如果同一个样本的特征区别较大，则不适合使用统一的均值和方差。</p>\r\n<h2 id=\"instancenormalization\">InstanceNormalization</h2>\r\n<p>对每个样本计算C个均值和方差，不受batch size和通道数的影响，常用于风格化迁移，如果通道之间有相关性，则不适合使用IN。</p>\r\n<h2 id=\"groupnormalization\">GroupNormalization</h2>\r\n<p>作为BatchNormalization的变种之一，GroupNormalization主要解决的问题是BatchNormalization对batch大小的依赖性。</p>\r\n<p>在CNN中，对于<span class=\"math inline\">\\(N\\times H\\times W\\times C\\)</span>大小的特征图，BatchNormalization将其中每一个大小为<span class=\"math inline\">\\(1\\times 1\\times 1\\times C\\)</span>看做一个样本，而GroupNormalization首先将<span class=\"math inline\">\\(N\\times H\\times W\\times C\\)</span>大小的特征图拆分成<span class=\"math inline\">\\(N\\times H\\times W\\times \\frac{C}{G}\\times G\\)</span>，然后在<span class=\"math inline\">\\(H\\times W\\times \\frac{C}{G}\\)</span>范围内求方差和均值，得到<span class=\"math inline\">\\(N \\times G\\)</span>个均值和方差，可以理解为样本个数为<span class=\"math inline\">\\(H\\times W\\times \\frac{C}{G}\\)</span>，每个样本的维度为<span class=\"math inline\">\\(N \\times G\\)</span>，这样做的好处是样本个数不依赖batch size，原论文中作者还解释说使用GroupNormalization将特征分组处理，更加符合特征之间的依赖性，对模型性能有提升。</p>\r\n<p>训练时GroupNormalization的方差和均值都是<span class=\"math inline\">\\(N\\times 1\\times 1\\times 1\\times G\\)</span>大小，在测试时，Batch size基本上不会和训练时一样，不能和BN一样保存均值和方差参数，因此GN在模型保存时，只保存<span class=\"math inline\">\\(\\gamma\\)</span>和<span class=\"math inline\">\\(\\beta\\)</span>的值，均值和方差在预测过程中是每个样本计算一次的。</p>\r\n"},{"title":"Boosting总结","date":"2020-05-28T08:56:22.000Z","mathjax":true,"_content":"\n# Boosting\nBoosting是集成学习的一类算法的总称，这类算法的流程可以概括为：首先从初始训练集中训练出一个基学习器，然后根据基学习器的表现对训练样本分布进行调整，使得之前做错的样本在后面受到更多的关注，然后基于调整后的样本分布来训练下一个基学习器，如此逐步增加基学习器，直到基学习器的数目达到了指定值T。\n\n## AdaBoost\nAdaBoost是Boosting方法中的一个代表，主要应用在二分类任务上，AdaBoost可以理解为将基学习进行线性组合得到最终的学习器，其中$\\alpha_t$是一个正数，表示第$t$个基学习器的权重，如下所示。\n\n$$\nH(x) = \\sum\\limits_{t=1}^T \\alpha_t h_t(x)\n$$\n\nAdaBoost使用指数损失函数作为优化目标，如下所示，其中$f(x)$表示真实函数，$f(x) \\in \\{1, -1\\}$，优化这个指数损失函数其实相当于优化0/1损失函数（证明略），因为指数损失函数的平滑性质更利于优化过程，因此这里用指数损失函数，其中$E$表示期望，$D$表示原始数据的分布（每个数据出现的概率是$\\frac{1}{N}$，$N$为数据个数）。\n\n$$\nl(H|D) = E_{x \\sim D}[e^{-f(x)H(x)}]\n$$\n\n因此AdaBoost的目标可以表示为：\n$$\n\\mathop{\\arg\\min}\\limits_{\\alpha, h} E_{x \\sim D}[e^{-f(x)(\\sum\\limits_{t=1}^T \\alpha_t h_t(x))}]\n$$\n\n但是这个问题的直接求解非常困难，因此这里使用一种**前向分步算法**来解决这个问题，其思路是：每增加一个基学习器，就让损失函数减小一些，这样来逐步逼近最优解。\n\n假设现在已经得到$m$个基学习器组成的集成学习器：$H_m(x)$，现在下一个学习器（$\\alpha_{m+1}, h_{m+1}$）的目标可以表示为：\n$$\n\\begin{aligned}\n    &\\mathop{\\arg\\min}\\limits_{\\alpha_{m+1}, h_{m+1}} E_{x \\sim D}[e^{-f(x)(H_m(x) + \\alpha_{m+1} h_{m+1}(x))}]\\\\\n    &=\\mathop{\\arg\\min}\\limits_{\\alpha_{m+1}, h_{m+1}} E_{x \\sim D}[e^{-f(x)(H_m(x)} \\times e^{-f(x) \\alpha_{m+1} h_{m+1}(x))}]\n\\end{aligned}\n$$\n\n$H_m(x)$和$\\alpha_{m+1}, h_{m+1}$无关，因此可以将$e^{-f(x)(H_m(x)}$看做数据的一种权重，即使用$e^{-f(x)(H_m(x)}$这一项来改变数据的分布，假设这样改变数据权重之后，得到的数据分布为$D_m$，那么可以进一步将目标化简为：\n\n$$\n\\mathop{\\arg\\min}\\limits_{\\alpha_{m+1}, h_{m+1}} E_{x \\sim D_m}\\ e^{-f(x) \\alpha_{m+1} h_{m+1}(x)}\n$$\n\n因为$f(x)$和$h_m{x}$的取值要么是1，要么是$-1$，又因为$\\alpha_m$为正数，因此这里如果要最小化目标，可以先不考虑$\\alpha_m$，而是先最小化：\n\n$$\n\\mathop{\\arg\\min}\\limits_{h_{m+1}} E_{x \\sim D_m}\\ e^{-f(x)h_{m+1}(x)}\n$$\n\n因此第$m+1$个基学习器的目标就是在数据分布$D_m$上优化指数损失函数：\n\n$$\n\\begin{aligned}\nh^\\star_{m+1} &= \\mathop{\\arg\\min}\\limits_{h_{m+1}} E_{x \\sim D_m}\\ e^{-f(x)h_{m+1}(x)}\\\\\n&=\\mathop{\\arg\\min}\\limits_{h_{m+1}} \\sum\\limits_{i=1}^N w_{mi}I\\{f(x_i) \\ne h_{m+1}(x_i)\\}\n\\end{aligned}\n$$\n\n用$w_{mi} = \\frac{e^{-f(x_i)(H_m(x_i)}}{\\sum\\limits_{i=1}^N e^{-f(x_i)(H_m(x_i)}}$来表示数据$x_i$的权重，这里多出来了一个用于权重归一化的分母，但是不影响$\\alpha_{m+1}，h_{m+1}$的训练，只是后面推导会方便些。\n\n第$m+1$个基学习器$h^\\star_{m+1}$得到之后，再来确定其权重$\\alpha_{m+1}$，其目标是：\n\n$$\n\\begin{aligned}\n    &\\mathop{\\arg\\min}\\limits_{\\alpha_{m+1}} E_{x \\sim D_m}\\ e^{-f(x) \\alpha_{m+1} h^\\star_{m+1}(x)}\\\\\n    &=\\mathop{\\arg\\min}\\limits_{\\alpha_{m+1}} \\sum\\limits_{i=1}^N w_{mi} e^{-f(x) \\alpha_{m+1} h^\\star_{m+1}(x)}\n\\end{aligned}\n$$\n\n求损失函数$l(\\alpha_{m+1}) = \\sum\\limits_{i=1}^N w_{mi} e^{-f(x) \\alpha_{m+1} h^\\star_{m+1}(x)}$对于$\\alpha_{m+1}$的导数，可以得到：\n\n$$\n\\begin{aligned}\n    \\frac{\\partial l(\\alpha_{m+1})}{\\partial \\alpha_{m+1}} &= \\sum\\limits_{f(x_i) \\ne h_{m+1}(x_i)} w_{mi}e^{\\alpha_{m+1}} - \\sum\\limits_{f(x_i) = h_{m+1}(x_i)} w_{mi}e^{-\\alpha_{m+1}}\n\\end{aligned}\n$$\n\n令导数$\\frac{\\partial l(\\alpha_{m+1})}{\\partial \\alpha_{m+1}} = 0$可得：\n$$\n\\begin{aligned}\n    \\sum\\limits_{f(x_i) \\ne h_{m+1}(x_i)} w_{mi}e^{\\alpha_{m+1}} &= \\sum\\limits_{f(x_i) = h_{m+1}(x_i)} w_{mi}e^{-\\alpha_{m+1}}\\\\\n    e^{2\\alpha_{m+1}} &= \\frac{\\sum\\limits_{f(x_i) = h_{m+1}(x_i)} w_{mi}}{\\sum\\limits_{f(x_i) \\ne h_{m+1}(x_i)} w_{mi}}\n\\end{aligned}\n$$\n\n容易发现，因为$w_{mi}$是经过了归一化的，因此$\\sum\\limits_{i=1}^N w_{mi} = 1$，那么可以得到$\\sum\\limits_{f(x_i) = h_{m+1}(x_i)} w_{mi} = 1 - \\sum\\limits_{f(x_i) \\ne h_{m+1}(x_i)} w_{mi}$，这里令$\\sum\\limits_{f(x_i) \\ne h_{m+1}(x_i)} w_{mi} = e_{m+1}$表示$h_{m+1}$的损失，那么$\\sum\\limits_{f(x_i) = h_{m+1}(x_i)} w_{mi}$可以表示为$1 - e_{m+1}$，代入上面的导数为0的推导，可以得到：\n\n$$\n\\begin{aligned}\n    e^{2\\alpha_{m+1}} &= \\frac{1 - e_{m+1}}{e_{m+1}}\\\\\n    \\Rightarrow \\alpha_{m+1} &= \\frac{1}{2}\\ln{\\frac{1 - e_{m+1}}{e_{m+1}}}\n\\end{aligned}\n$$\n\n因此AdaBoost的流程就很明确了：\n\n1. 在数据分布$D_{m-1}$上训练基学习器$h_m(x)$\n2. 计算$e_m = \\sum\\limits_{f(x_i) \\ne h_m(x_i)} w_{mi}$\n3. 确定当前基学习器的权重$\\alpha_m = \\frac{1}{2}\\ln{\\frac{1 - e_m}{e_m}}$\n4. 得到新的学习器$H_m(x) = \\sum\\limits_{i=1}^m \\alpha_i h_i(x)$\n5. 如果学习器个数达到指定个数，则退出，否则使用新的学习器调整权重参数$w_{mi} = \\frac{e^{-f(x_i)(H_m(x_i)}}{\\sum\\limits_{i=1}^N e^{-f(x_i)(H_m(x_i)}}$得到分布$D_m$，跳转到第一步，进行第$m+1$个基学习器的训练。\n\n## 提升树（Boosting Decision Tree, BDT）\n残差树是针对回归问题的一种boosting方法。其基学习器是基于CART算法的回归树（关于决策树的相关内容可以见我的另外一篇文章{% post_link 决策树总结 决策树总结 %}），模型依旧为加法模型、损失函数为平方函数、学习算法为前向分步算法。\n\n第$m$个树模型基学习器可以表示为$T(x; \\theta_m)$，得到前$m$个基学习器之后，残差树的预测函数可以看做：$H_m(x) = \\sum\\limits_{i = 1}^mT(x; \\theta_i)$，那么下一个树模型$T(x; \\theta_{m+1})$的目标损失函数可以写作：\n$$\n\\begin{aligned}\nL(y, T(x; \\theta_{m+1})) &= (y - H_m(x) - T(x; \\theta_{m+1}))^2\\\\\n&= (r - T(x; \\theta_{m+1}))^2\n\\end{aligned}\n$$\n这里的$r$表示上一次的残差，这也是残差树名字的由来，例如现在需要拟合的值为20，第一次残差树拟合的值为18，那么第二次拟合的目标值为上一次的残差：20-18=2。\n\n## 梯度提升决策树（Gradient Boosting Decision Tree, GBDT）\n在提升树的训练过程中，每次增加的决策树是以残差作为目标，并使用CART方法构造决策树，而在GBDT中，添加新的决策树是以损失函数在当前模型预测值下的负梯度作为目标，同样使用CART方法构造决策树。如果损失函数使用的是均方误差，那么新的决策树的优化目标其实和提升树类似。\n\n其实严格来将，GBDT并不是去拟合梯度，而是在进行决策树构建时，尽量将梯度相近的样本划分到同一个叶节点。\n\n例如对于损失函数$\\sum\\limits_{i=1}^N l(y_i, \\hat{y}_i)$，其中$N$为样本个数，$y_i$表示第$i$个样本的目标值，$\\hat{y}_i$表示对第$i$个样本的预测值，现在GBDT已经构造出了第$t$颗决策树，目前的预测函数表示为$F_t(x) = \\sum\\limits_{i=1}^t f_i(x)$，这里每个$f_i(x)$表示一个决策树，现在的损失函数表示为$\\sum\\limits_{i=1}^N l(y_i, F_t(x_i))$，现在要构造第$t+1$颗树，那么其目标很简单，就是进一步使得损失函数$\\sum\\limits_{i=1}^N l(y_i, F_t(x_i) + f_{t+1}(x_i))$最小，这里将损失函数进行一次泰勒展开，可以将损失函数近似为：\n\n$$\n\\begin{aligned}\n    \\sum\\limits_{i=1}^N [l(y_i, F_t(x_i)) + \\frac{\\partial l(y_i, F_t(x_i))}{\\partial F_t(x_i)}f_{t+1}(x_i)]\n\\end{aligned}\n$$\n\n其中$l(y_i, F_t(x_i))$的值和$f_{t+1}$无关，因此$f_{t+1}$只关心下面的表达式是否能够进一步将损失函数减小：\n\n$$\n\\sum\\limits_{i=1}^N \\frac{\\partial l(y_i, F_t(x_i))}{\\partial F_t(x_i)}f_{t+1}(x_i)\n$$\n\n但是上面这个函数没有最小值，而且损失函数的一次展开是有误差的，$f_{t+1}$的预测值越大，误差越大，不能按照最小化问题来处理，自然的一个想法是使得$f_{t+1}(x_i) = -\\frac{\\partial l(y_i, F_t(x_i))}{\\partial F_t(x_i)}$，因此GBDT这里将其简化为拟合负梯度。\n\n### XGBoost\nXGBoost是GBDT的一种改进版本。\n\n在已知第$t$颗决策树时，下一个决策树的损失函数为：$\\sum\\limits_{i=1}^N l(y_i, F_t(x_i) + f_{t+1}(x_i)) + \\Omega(f_{t+1})$。\n\n这里损失函数中新增了正则化项$\\Omega(f_{t+1})$，可以表示为：$\\Omega(f_{t+1}) = \\gamma T + \\frac{1}{2}\\lambda\\sum\\limits_{j=1}^Tw^2_j$，其中$T$表示第$t+1$颗树中的叶节点个数，而$w_j$表示第$j$个叶节点的输出值。\n\n在XGBoost中，对损失函数进行二次泰勒展开近似：\n\n$$\n\\begin{aligned}\n    &\\sum\\limits_{i=1}^N l(y_i, F_t(x_i) + f_{t+1}(x_i)) + \\Omega(f_{t+1})\\\\\n    &= \\sum\\limits_{i=1}^N l(y_i, F_t(x_i) + f_{t+1}(x_i)) + \\gamma T + \\frac{1}{2}\\lambda\\sum\\limits_{j=1}^Tw^2_j\\\\\n    &\\simeq \\sum\\limits_{i=1}^N [l(y_i, F_t(x_i)) + \\frac{\\partial l(y_i, F_t(x_i))}{\\partial F_t(x_i)}f_{t+1}(x_i) + \\frac{1}{2}\\frac{\\partial^2 l(y_i, F_t(x_i))}{\\partial^2 F_t(x_i)}f^2_{t+1}(x_i)] + \\gamma T + \\frac{1}{2}\\lambda\\sum\\limits_{j=1}^Tw^2_j\\\\\n    &=constant + \\sum\\limits_{i=1}^N [\\frac{\\partial l(y_i, F_t(x_i))}{\\partial F_t(x_i)}f_{t+1}(x_i) + \\frac{1}{2}\\frac{\\partial^2 l(y_i, F_t(x_i))}{\\partial^2 F_t(x_i)}f^2_{t+1}(x_i)] + \\gamma T + \\frac{1}{2}\\lambda\\sum\\limits_{j=1}^Tw^2_j\\\\\n    &=constant + \\sum\\limits_{j=1}^T [\\sum\\limits_{i\\in I_j}\\frac{\\partial l(y_i, F_t(x_i))}{\\partial F_t(x_i)}w_j + \\sum\\limits_{i\\in I_j}\\frac{1}{2}\\frac{\\partial^2 l(y_i, F_t(x_i))}{\\partial^2 F_t(x_i)}w_j^2] + \\gamma T + \\frac{1}{2}\\lambda\\sum\\limits_{j=1}^Tw^2_j\\\\\n    &=constant + \\sum\\limits_{j=1}^T[\\sum\\limits_{i\\in I_j} g_i w_j + \\frac{1}{2}(\\sum\\limits_{i\\in I_j}h_i + \\lambda)w^2_j] + \\gamma T\\\\\n    &=constant + \\sum\\limits_{j=1}^T[G_j w_j + \\frac{1}{2}(H_j + \\lambda)w^2_j] + \\gamma T\n\\end{aligned}\n$$\n这里的化简过程中，$I_j$表示属于第$j$个叶节点的样本id的集合，$g_i$表示第$i$个样本在$H_t(x_i)$这个预测结果下的一阶导数：$\\frac{\\partial l(y_i, F_t(x_i))}{\\partial F_t(x_i)}$，$h_i$表示第$i$个样本在$F_t(x_i)$这个预测结果下的二阶导数：$\\frac{\\partial^2 l(y_i, F_t(x_i))}{\\partial^2 F_t(x_i)}$，$G_j$表示属于第$j$个叶节点的样本的导数之和，$H_j$表示属于第$j$个叶节点的样本的二阶导数之和。\n\n得到上面的化简结果之后，首先对$w_j$做个处理，令上式对$w_j$的导数为0，可以得到$w^\\star_j = -\\frac{G_j}{H_j + \\lambda}$就是第$j$个叶节点的预测值的最优解，因此上式可以进一步写成：\n\n$$\nconstant - \\sum\\limits_{j=1}^T \\frac{1}{2}\\frac{G^2_j}{H_j + \\lambda} + \\gamma T\n$$\n\nXGBoost在构造新的决策树时，和CART类似，将当前节点分裂成两个子节点，但是选择特征以及特征的最优划分，不是使用Gini_index，为了最小化上面的目标，XGBoost在划分新的叶节点时，判断是否将叶节点$j$划分为新的左节点和右节点时，主要考察下式：\n$$\n\\begin{aligned}\n&constant - \\frac{1}{2}\\frac{G^2_j}{H_j + \\lambda} + \\gamma T - (constant - \\frac{1}{2}\\frac{G^2_L}{H_L + \\lambda} - \\frac{1}{2}\\frac{G^2_R}{H_R + \\lambda} + \\gamma(T+1))\\\\\n&= \\frac{1}{2}( \\frac{G^2_L}{H_L + \\lambda} + \\frac{G^2_R}{H_R + \\lambda} - \\frac{G^2_j}{H_j + \\lambda}) - \\gamma    \n\\end{aligned}\n$$\n\n这里$G_L， H_L$分别表示划分出的左叶节点的导数和二阶导数和，$G_R， H_R$分别表示划分出的右叶节点的导数和二阶导数和。上式的结果表示将叶节点$j$划分为新的左节点和右节点时的增益情况。如果上式为正，则表示新划分节点有助于降低损失函数值，所以这里创建新节点时，遍历所有特征以及特征的划分位置，找到使得上式为正且最大的划分来创建新节点，如果没有能够使得上式为正的划分，那么停止继续添加叶节点。\n\n按照上面的方法，逐个构造新的决策树以降低损失函数值，就是XGBoost的主要工作原理。\n","source":"_posts/学习笔记/Boosting总结.md","raw":"---\ntitle: Boosting总结\ndate: 2020-05-28 16:56:22\ntags: [机器学习]\nmathjax: true\n---\n\n# Boosting\nBoosting是集成学习的一类算法的总称，这类算法的流程可以概括为：首先从初始训练集中训练出一个基学习器，然后根据基学习器的表现对训练样本分布进行调整，使得之前做错的样本在后面受到更多的关注，然后基于调整后的样本分布来训练下一个基学习器，如此逐步增加基学习器，直到基学习器的数目达到了指定值T。\n\n## AdaBoost\nAdaBoost是Boosting方法中的一个代表，主要应用在二分类任务上，AdaBoost可以理解为将基学习进行线性组合得到最终的学习器，其中$\\alpha_t$是一个正数，表示第$t$个基学习器的权重，如下所示。\n\n$$\nH(x) = \\sum\\limits_{t=1}^T \\alpha_t h_t(x)\n$$\n\nAdaBoost使用指数损失函数作为优化目标，如下所示，其中$f(x)$表示真实函数，$f(x) \\in \\{1, -1\\}$，优化这个指数损失函数其实相当于优化0/1损失函数（证明略），因为指数损失函数的平滑性质更利于优化过程，因此这里用指数损失函数，其中$E$表示期望，$D$表示原始数据的分布（每个数据出现的概率是$\\frac{1}{N}$，$N$为数据个数）。\n\n$$\nl(H|D) = E_{x \\sim D}[e^{-f(x)H(x)}]\n$$\n\n因此AdaBoost的目标可以表示为：\n$$\n\\mathop{\\arg\\min}\\limits_{\\alpha, h} E_{x \\sim D}[e^{-f(x)(\\sum\\limits_{t=1}^T \\alpha_t h_t(x))}]\n$$\n\n但是这个问题的直接求解非常困难，因此这里使用一种**前向分步算法**来解决这个问题，其思路是：每增加一个基学习器，就让损失函数减小一些，这样来逐步逼近最优解。\n\n假设现在已经得到$m$个基学习器组成的集成学习器：$H_m(x)$，现在下一个学习器（$\\alpha_{m+1}, h_{m+1}$）的目标可以表示为：\n$$\n\\begin{aligned}\n    &\\mathop{\\arg\\min}\\limits_{\\alpha_{m+1}, h_{m+1}} E_{x \\sim D}[e^{-f(x)(H_m(x) + \\alpha_{m+1} h_{m+1}(x))}]\\\\\n    &=\\mathop{\\arg\\min}\\limits_{\\alpha_{m+1}, h_{m+1}} E_{x \\sim D}[e^{-f(x)(H_m(x)} \\times e^{-f(x) \\alpha_{m+1} h_{m+1}(x))}]\n\\end{aligned}\n$$\n\n$H_m(x)$和$\\alpha_{m+1}, h_{m+1}$无关，因此可以将$e^{-f(x)(H_m(x)}$看做数据的一种权重，即使用$e^{-f(x)(H_m(x)}$这一项来改变数据的分布，假设这样改变数据权重之后，得到的数据分布为$D_m$，那么可以进一步将目标化简为：\n\n$$\n\\mathop{\\arg\\min}\\limits_{\\alpha_{m+1}, h_{m+1}} E_{x \\sim D_m}\\ e^{-f(x) \\alpha_{m+1} h_{m+1}(x)}\n$$\n\n因为$f(x)$和$h_m{x}$的取值要么是1，要么是$-1$，又因为$\\alpha_m$为正数，因此这里如果要最小化目标，可以先不考虑$\\alpha_m$，而是先最小化：\n\n$$\n\\mathop{\\arg\\min}\\limits_{h_{m+1}} E_{x \\sim D_m}\\ e^{-f(x)h_{m+1}(x)}\n$$\n\n因此第$m+1$个基学习器的目标就是在数据分布$D_m$上优化指数损失函数：\n\n$$\n\\begin{aligned}\nh^\\star_{m+1} &= \\mathop{\\arg\\min}\\limits_{h_{m+1}} E_{x \\sim D_m}\\ e^{-f(x)h_{m+1}(x)}\\\\\n&=\\mathop{\\arg\\min}\\limits_{h_{m+1}} \\sum\\limits_{i=1}^N w_{mi}I\\{f(x_i) \\ne h_{m+1}(x_i)\\}\n\\end{aligned}\n$$\n\n用$w_{mi} = \\frac{e^{-f(x_i)(H_m(x_i)}}{\\sum\\limits_{i=1}^N e^{-f(x_i)(H_m(x_i)}}$来表示数据$x_i$的权重，这里多出来了一个用于权重归一化的分母，但是不影响$\\alpha_{m+1}，h_{m+1}$的训练，只是后面推导会方便些。\n\n第$m+1$个基学习器$h^\\star_{m+1}$得到之后，再来确定其权重$\\alpha_{m+1}$，其目标是：\n\n$$\n\\begin{aligned}\n    &\\mathop{\\arg\\min}\\limits_{\\alpha_{m+1}} E_{x \\sim D_m}\\ e^{-f(x) \\alpha_{m+1} h^\\star_{m+1}(x)}\\\\\n    &=\\mathop{\\arg\\min}\\limits_{\\alpha_{m+1}} \\sum\\limits_{i=1}^N w_{mi} e^{-f(x) \\alpha_{m+1} h^\\star_{m+1}(x)}\n\\end{aligned}\n$$\n\n求损失函数$l(\\alpha_{m+1}) = \\sum\\limits_{i=1}^N w_{mi} e^{-f(x) \\alpha_{m+1} h^\\star_{m+1}(x)}$对于$\\alpha_{m+1}$的导数，可以得到：\n\n$$\n\\begin{aligned}\n    \\frac{\\partial l(\\alpha_{m+1})}{\\partial \\alpha_{m+1}} &= \\sum\\limits_{f(x_i) \\ne h_{m+1}(x_i)} w_{mi}e^{\\alpha_{m+1}} - \\sum\\limits_{f(x_i) = h_{m+1}(x_i)} w_{mi}e^{-\\alpha_{m+1}}\n\\end{aligned}\n$$\n\n令导数$\\frac{\\partial l(\\alpha_{m+1})}{\\partial \\alpha_{m+1}} = 0$可得：\n$$\n\\begin{aligned}\n    \\sum\\limits_{f(x_i) \\ne h_{m+1}(x_i)} w_{mi}e^{\\alpha_{m+1}} &= \\sum\\limits_{f(x_i) = h_{m+1}(x_i)} w_{mi}e^{-\\alpha_{m+1}}\\\\\n    e^{2\\alpha_{m+1}} &= \\frac{\\sum\\limits_{f(x_i) = h_{m+1}(x_i)} w_{mi}}{\\sum\\limits_{f(x_i) \\ne h_{m+1}(x_i)} w_{mi}}\n\\end{aligned}\n$$\n\n容易发现，因为$w_{mi}$是经过了归一化的，因此$\\sum\\limits_{i=1}^N w_{mi} = 1$，那么可以得到$\\sum\\limits_{f(x_i) = h_{m+1}(x_i)} w_{mi} = 1 - \\sum\\limits_{f(x_i) \\ne h_{m+1}(x_i)} w_{mi}$，这里令$\\sum\\limits_{f(x_i) \\ne h_{m+1}(x_i)} w_{mi} = e_{m+1}$表示$h_{m+1}$的损失，那么$\\sum\\limits_{f(x_i) = h_{m+1}(x_i)} w_{mi}$可以表示为$1 - e_{m+1}$，代入上面的导数为0的推导，可以得到：\n\n$$\n\\begin{aligned}\n    e^{2\\alpha_{m+1}} &= \\frac{1 - e_{m+1}}{e_{m+1}}\\\\\n    \\Rightarrow \\alpha_{m+1} &= \\frac{1}{2}\\ln{\\frac{1 - e_{m+1}}{e_{m+1}}}\n\\end{aligned}\n$$\n\n因此AdaBoost的流程就很明确了：\n\n1. 在数据分布$D_{m-1}$上训练基学习器$h_m(x)$\n2. 计算$e_m = \\sum\\limits_{f(x_i) \\ne h_m(x_i)} w_{mi}$\n3. 确定当前基学习器的权重$\\alpha_m = \\frac{1}{2}\\ln{\\frac{1 - e_m}{e_m}}$\n4. 得到新的学习器$H_m(x) = \\sum\\limits_{i=1}^m \\alpha_i h_i(x)$\n5. 如果学习器个数达到指定个数，则退出，否则使用新的学习器调整权重参数$w_{mi} = \\frac{e^{-f(x_i)(H_m(x_i)}}{\\sum\\limits_{i=1}^N e^{-f(x_i)(H_m(x_i)}}$得到分布$D_m$，跳转到第一步，进行第$m+1$个基学习器的训练。\n\n## 提升树（Boosting Decision Tree, BDT）\n残差树是针对回归问题的一种boosting方法。其基学习器是基于CART算法的回归树（关于决策树的相关内容可以见我的另外一篇文章{% post_link 决策树总结 决策树总结 %}），模型依旧为加法模型、损失函数为平方函数、学习算法为前向分步算法。\n\n第$m$个树模型基学习器可以表示为$T(x; \\theta_m)$，得到前$m$个基学习器之后，残差树的预测函数可以看做：$H_m(x) = \\sum\\limits_{i = 1}^mT(x; \\theta_i)$，那么下一个树模型$T(x; \\theta_{m+1})$的目标损失函数可以写作：\n$$\n\\begin{aligned}\nL(y, T(x; \\theta_{m+1})) &= (y - H_m(x) - T(x; \\theta_{m+1}))^2\\\\\n&= (r - T(x; \\theta_{m+1}))^2\n\\end{aligned}\n$$\n这里的$r$表示上一次的残差，这也是残差树名字的由来，例如现在需要拟合的值为20，第一次残差树拟合的值为18，那么第二次拟合的目标值为上一次的残差：20-18=2。\n\n## 梯度提升决策树（Gradient Boosting Decision Tree, GBDT）\n在提升树的训练过程中，每次增加的决策树是以残差作为目标，并使用CART方法构造决策树，而在GBDT中，添加新的决策树是以损失函数在当前模型预测值下的负梯度作为目标，同样使用CART方法构造决策树。如果损失函数使用的是均方误差，那么新的决策树的优化目标其实和提升树类似。\n\n其实严格来将，GBDT并不是去拟合梯度，而是在进行决策树构建时，尽量将梯度相近的样本划分到同一个叶节点。\n\n例如对于损失函数$\\sum\\limits_{i=1}^N l(y_i, \\hat{y}_i)$，其中$N$为样本个数，$y_i$表示第$i$个样本的目标值，$\\hat{y}_i$表示对第$i$个样本的预测值，现在GBDT已经构造出了第$t$颗决策树，目前的预测函数表示为$F_t(x) = \\sum\\limits_{i=1}^t f_i(x)$，这里每个$f_i(x)$表示一个决策树，现在的损失函数表示为$\\sum\\limits_{i=1}^N l(y_i, F_t(x_i))$，现在要构造第$t+1$颗树，那么其目标很简单，就是进一步使得损失函数$\\sum\\limits_{i=1}^N l(y_i, F_t(x_i) + f_{t+1}(x_i))$最小，这里将损失函数进行一次泰勒展开，可以将损失函数近似为：\n\n$$\n\\begin{aligned}\n    \\sum\\limits_{i=1}^N [l(y_i, F_t(x_i)) + \\frac{\\partial l(y_i, F_t(x_i))}{\\partial F_t(x_i)}f_{t+1}(x_i)]\n\\end{aligned}\n$$\n\n其中$l(y_i, F_t(x_i))$的值和$f_{t+1}$无关，因此$f_{t+1}$只关心下面的表达式是否能够进一步将损失函数减小：\n\n$$\n\\sum\\limits_{i=1}^N \\frac{\\partial l(y_i, F_t(x_i))}{\\partial F_t(x_i)}f_{t+1}(x_i)\n$$\n\n但是上面这个函数没有最小值，而且损失函数的一次展开是有误差的，$f_{t+1}$的预测值越大，误差越大，不能按照最小化问题来处理，自然的一个想法是使得$f_{t+1}(x_i) = -\\frac{\\partial l(y_i, F_t(x_i))}{\\partial F_t(x_i)}$，因此GBDT这里将其简化为拟合负梯度。\n\n### XGBoost\nXGBoost是GBDT的一种改进版本。\n\n在已知第$t$颗决策树时，下一个决策树的损失函数为：$\\sum\\limits_{i=1}^N l(y_i, F_t(x_i) + f_{t+1}(x_i)) + \\Omega(f_{t+1})$。\n\n这里损失函数中新增了正则化项$\\Omega(f_{t+1})$，可以表示为：$\\Omega(f_{t+1}) = \\gamma T + \\frac{1}{2}\\lambda\\sum\\limits_{j=1}^Tw^2_j$，其中$T$表示第$t+1$颗树中的叶节点个数，而$w_j$表示第$j$个叶节点的输出值。\n\n在XGBoost中，对损失函数进行二次泰勒展开近似：\n\n$$\n\\begin{aligned}\n    &\\sum\\limits_{i=1}^N l(y_i, F_t(x_i) + f_{t+1}(x_i)) + \\Omega(f_{t+1})\\\\\n    &= \\sum\\limits_{i=1}^N l(y_i, F_t(x_i) + f_{t+1}(x_i)) + \\gamma T + \\frac{1}{2}\\lambda\\sum\\limits_{j=1}^Tw^2_j\\\\\n    &\\simeq \\sum\\limits_{i=1}^N [l(y_i, F_t(x_i)) + \\frac{\\partial l(y_i, F_t(x_i))}{\\partial F_t(x_i)}f_{t+1}(x_i) + \\frac{1}{2}\\frac{\\partial^2 l(y_i, F_t(x_i))}{\\partial^2 F_t(x_i)}f^2_{t+1}(x_i)] + \\gamma T + \\frac{1}{2}\\lambda\\sum\\limits_{j=1}^Tw^2_j\\\\\n    &=constant + \\sum\\limits_{i=1}^N [\\frac{\\partial l(y_i, F_t(x_i))}{\\partial F_t(x_i)}f_{t+1}(x_i) + \\frac{1}{2}\\frac{\\partial^2 l(y_i, F_t(x_i))}{\\partial^2 F_t(x_i)}f^2_{t+1}(x_i)] + \\gamma T + \\frac{1}{2}\\lambda\\sum\\limits_{j=1}^Tw^2_j\\\\\n    &=constant + \\sum\\limits_{j=1}^T [\\sum\\limits_{i\\in I_j}\\frac{\\partial l(y_i, F_t(x_i))}{\\partial F_t(x_i)}w_j + \\sum\\limits_{i\\in I_j}\\frac{1}{2}\\frac{\\partial^2 l(y_i, F_t(x_i))}{\\partial^2 F_t(x_i)}w_j^2] + \\gamma T + \\frac{1}{2}\\lambda\\sum\\limits_{j=1}^Tw^2_j\\\\\n    &=constant + \\sum\\limits_{j=1}^T[\\sum\\limits_{i\\in I_j} g_i w_j + \\frac{1}{2}(\\sum\\limits_{i\\in I_j}h_i + \\lambda)w^2_j] + \\gamma T\\\\\n    &=constant + \\sum\\limits_{j=1}^T[G_j w_j + \\frac{1}{2}(H_j + \\lambda)w^2_j] + \\gamma T\n\\end{aligned}\n$$\n这里的化简过程中，$I_j$表示属于第$j$个叶节点的样本id的集合，$g_i$表示第$i$个样本在$H_t(x_i)$这个预测结果下的一阶导数：$\\frac{\\partial l(y_i, F_t(x_i))}{\\partial F_t(x_i)}$，$h_i$表示第$i$个样本在$F_t(x_i)$这个预测结果下的二阶导数：$\\frac{\\partial^2 l(y_i, F_t(x_i))}{\\partial^2 F_t(x_i)}$，$G_j$表示属于第$j$个叶节点的样本的导数之和，$H_j$表示属于第$j$个叶节点的样本的二阶导数之和。\n\n得到上面的化简结果之后，首先对$w_j$做个处理，令上式对$w_j$的导数为0，可以得到$w^\\star_j = -\\frac{G_j}{H_j + \\lambda}$就是第$j$个叶节点的预测值的最优解，因此上式可以进一步写成：\n\n$$\nconstant - \\sum\\limits_{j=1}^T \\frac{1}{2}\\frac{G^2_j}{H_j + \\lambda} + \\gamma T\n$$\n\nXGBoost在构造新的决策树时，和CART类似，将当前节点分裂成两个子节点，但是选择特征以及特征的最优划分，不是使用Gini_index，为了最小化上面的目标，XGBoost在划分新的叶节点时，判断是否将叶节点$j$划分为新的左节点和右节点时，主要考察下式：\n$$\n\\begin{aligned}\n&constant - \\frac{1}{2}\\frac{G^2_j}{H_j + \\lambda} + \\gamma T - (constant - \\frac{1}{2}\\frac{G^2_L}{H_L + \\lambda} - \\frac{1}{2}\\frac{G^2_R}{H_R + \\lambda} + \\gamma(T+1))\\\\\n&= \\frac{1}{2}( \\frac{G^2_L}{H_L + \\lambda} + \\frac{G^2_R}{H_R + \\lambda} - \\frac{G^2_j}{H_j + \\lambda}) - \\gamma    \n\\end{aligned}\n$$\n\n这里$G_L， H_L$分别表示划分出的左叶节点的导数和二阶导数和，$G_R， H_R$分别表示划分出的右叶节点的导数和二阶导数和。上式的结果表示将叶节点$j$划分为新的左节点和右节点时的增益情况。如果上式为正，则表示新划分节点有助于降低损失函数值，所以这里创建新节点时，遍历所有特征以及特征的划分位置，找到使得上式为正且最大的划分来创建新节点，如果没有能够使得上式为正的划分，那么停止继续添加叶节点。\n\n按照上面的方法，逐个构造新的决策树以降低损失函数值，就是XGBoost的主要工作原理。\n","slug":"学习笔记/Boosting总结","published":1,"updated":"2020-08-31T06:39:20.724Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckjxpc3re000144mq5b8hgmw4","content":"<h1 id=\"boosting\">Boosting</h1>\r\n<p>Boosting是集成学习的一类算法的总称，这类算法的流程可以概括为：首先从初始训练集中训练出一个基学习器，然后根据基学习器的表现对训练样本分布进行调整，使得之前做错的样本在后面受到更多的关注，然后基于调整后的样本分布来训练下一个基学习器，如此逐步增加基学习器，直到基学习器的数目达到了指定值T。</p>\r\n<h2 id=\"adaboost\">AdaBoost</h2>\r\n<p>AdaBoost是Boosting方法中的一个代表，主要应用在二分类任务上，AdaBoost可以理解为将基学习进行线性组合得到最终的学习器，其中<span class=\"math inline\">\\(\\alpha_t\\)</span>是一个正数，表示第<span class=\"math inline\">\\(t\\)</span>个基学习器的权重，如下所示。</p>\r\n<p><span class=\"math display\">\\[\r\nH(x) = \\sum\\limits_{t=1}^T \\alpha_t h_t(x)\r\n\\]</span></p>\r\n<p>AdaBoost使用指数损失函数作为优化目标，如下所示，其中<span class=\"math inline\">\\(f(x)\\)</span>表示真实函数，<span class=\"math inline\">\\(f(x) \\in \\{1, -1\\}\\)</span>，优化这个指数损失函数其实相当于优化0/1损失函数（证明略），因为指数损失函数的平滑性质更利于优化过程，因此这里用指数损失函数，其中<span class=\"math inline\">\\(E\\)</span>表示期望，<span class=\"math inline\">\\(D\\)</span>表示原始数据的分布（每个数据出现的概率是<span class=\"math inline\">\\(\\frac{1}{N}\\)</span>，<span class=\"math inline\">\\(N\\)</span>为数据个数）。</p>\r\n<p><span class=\"math display\">\\[\r\nl(H|D) = E_{x \\sim D}[e^{-f(x)H(x)}]\r\n\\]</span></p>\r\n<p>因此AdaBoost的目标可以表示为： <span class=\"math display\">\\[\r\n\\mathop{\\arg\\min}\\limits_{\\alpha, h} E_{x \\sim D}[e^{-f(x)(\\sum\\limits_{t=1}^T \\alpha_t h_t(x))}]\r\n\\]</span></p>\r\n<p>但是这个问题的直接求解非常困难，因此这里使用一种<strong>前向分步算法</strong>来解决这个问题，其思路是：每增加一个基学习器，就让损失函数减小一些，这样来逐步逼近最优解。</p>\r\n<p>假设现在已经得到<span class=\"math inline\">\\(m\\)</span>个基学习器组成的集成学习器：<span class=\"math inline\">\\(H_m(x)\\)</span>，现在下一个学习器（<span class=\"math inline\">\\(\\alpha_{m+1}, h_{m+1}\\)</span>）的目标可以表示为： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    &amp;\\mathop{\\arg\\min}\\limits_{\\alpha_{m+1}, h_{m+1}} E_{x \\sim D}[e^{-f(x)(H_m(x) + \\alpha_{m+1} h_{m+1}(x))}]\\\\\r\n    &amp;=\\mathop{\\arg\\min}\\limits_{\\alpha_{m+1}, h_{m+1}} E_{x \\sim D}[e^{-f(x)(H_m(x)} \\times e^{-f(x) \\alpha_{m+1} h_{m+1}(x))}]\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p><span class=\"math inline\">\\(H_m(x)\\)</span>和<span class=\"math inline\">\\(\\alpha_{m+1}, h_{m+1}\\)</span>无关，因此可以将<span class=\"math inline\">\\(e^{-f(x)(H_m(x)}\\)</span>看做数据的一种权重，即使用<span class=\"math inline\">\\(e^{-f(x)(H_m(x)}\\)</span>这一项来改变数据的分布，假设这样改变数据权重之后，得到的数据分布为<span class=\"math inline\">\\(D_m\\)</span>，那么可以进一步将目标化简为：</p>\r\n<p><span class=\"math display\">\\[\r\n\\mathop{\\arg\\min}\\limits_{\\alpha_{m+1}, h_{m+1}} E_{x \\sim D_m}\\ e^{-f(x) \\alpha_{m+1} h_{m+1}(x)}\r\n\\]</span></p>\r\n<p>因为<span class=\"math inline\">\\(f(x)\\)</span>和<span class=\"math inline\">\\(h_m{x}\\)</span>的取值要么是1，要么是<span class=\"math inline\">\\(-1\\)</span>，又因为<span class=\"math inline\">\\(\\alpha_m\\)</span>为正数，因此这里如果要最小化目标，可以先不考虑<span class=\"math inline\">\\(\\alpha_m\\)</span>，而是先最小化：</p>\r\n<p><span class=\"math display\">\\[\r\n\\mathop{\\arg\\min}\\limits_{h_{m+1}} E_{x \\sim D_m}\\ e^{-f(x)h_{m+1}(x)}\r\n\\]</span></p>\r\n<p>因此第<span class=\"math inline\">\\(m+1\\)</span>个基学习器的目标就是在数据分布<span class=\"math inline\">\\(D_m\\)</span>上优化指数损失函数：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\nh^\\star_{m+1} &amp;= \\mathop{\\arg\\min}\\limits_{h_{m+1}} E_{x \\sim D_m}\\ e^{-f(x)h_{m+1}(x)}\\\\\r\n&amp;=\\mathop{\\arg\\min}\\limits_{h_{m+1}} \\sum\\limits_{i=1}^N w_{mi}I\\{f(x_i) \\ne h_{m+1}(x_i)\\}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>用<span class=\"math inline\">\\(w_{mi} = \\frac{e^{-f(x_i)(H_m(x_i)}}{\\sum\\limits_{i=1}^N e^{-f(x_i)(H_m(x_i)}}\\)</span>来表示数据<span class=\"math inline\">\\(x_i\\)</span>的权重，这里多出来了一个用于权重归一化的分母，但是不影响<span class=\"math inline\">\\(\\alpha_{m+1}，h_{m+1}\\)</span>的训练，只是后面推导会方便些。</p>\r\n<p>第<span class=\"math inline\">\\(m+1\\)</span>个基学习器<span class=\"math inline\">\\(h^\\star_{m+1}\\)</span>得到之后，再来确定其权重<span class=\"math inline\">\\(\\alpha_{m+1}\\)</span>，其目标是：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    &amp;\\mathop{\\arg\\min}\\limits_{\\alpha_{m+1}} E_{x \\sim D_m}\\ e^{-f(x) \\alpha_{m+1} h^\\star_{m+1}(x)}\\\\\r\n    &amp;=\\mathop{\\arg\\min}\\limits_{\\alpha_{m+1}} \\sum\\limits_{i=1}^N w_{mi} e^{-f(x) \\alpha_{m+1} h^\\star_{m+1}(x)}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>求损失函数<span class=\"math inline\">\\(l(\\alpha_{m+1}) = \\sum\\limits_{i=1}^N w_{mi} e^{-f(x) \\alpha_{m+1} h^\\star_{m+1}(x)}\\)</span>对于<span class=\"math inline\">\\(\\alpha_{m+1}\\)</span>的导数，可以得到：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\frac{\\partial l(\\alpha_{m+1})}{\\partial \\alpha_{m+1}} &amp;= \\sum\\limits_{f(x_i) \\ne h_{m+1}(x_i)} w_{mi}e^{\\alpha_{m+1}} - \\sum\\limits_{f(x_i) = h_{m+1}(x_i)} w_{mi}e^{-\\alpha_{m+1}}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>令导数<span class=\"math inline\">\\(\\frac{\\partial l(\\alpha_{m+1})}{\\partial \\alpha_{m+1}} = 0\\)</span>可得： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\sum\\limits_{f(x_i) \\ne h_{m+1}(x_i)} w_{mi}e^{\\alpha_{m+1}} &amp;= \\sum\\limits_{f(x_i) = h_{m+1}(x_i)} w_{mi}e^{-\\alpha_{m+1}}\\\\\r\n    e^{2\\alpha_{m+1}} &amp;= \\frac{\\sum\\limits_{f(x_i) = h_{m+1}(x_i)} w_{mi}}{\\sum\\limits_{f(x_i) \\ne h_{m+1}(x_i)} w_{mi}}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>容易发现，因为<span class=\"math inline\">\\(w_{mi}\\)</span>是经过了归一化的，因此<span class=\"math inline\">\\(\\sum\\limits_{i=1}^N w_{mi} = 1\\)</span>，那么可以得到<span class=\"math inline\">\\(\\sum\\limits_{f(x_i) = h_{m+1}(x_i)} w_{mi} = 1 - \\sum\\limits_{f(x_i) \\ne h_{m+1}(x_i)} w_{mi}\\)</span>，这里令<span class=\"math inline\">\\(\\sum\\limits_{f(x_i) \\ne h_{m+1}(x_i)} w_{mi} = e_{m+1}\\)</span>表示<span class=\"math inline\">\\(h_{m+1}\\)</span>的损失，那么<span class=\"math inline\">\\(\\sum\\limits_{f(x_i) = h_{m+1}(x_i)} w_{mi}\\)</span>可以表示为<span class=\"math inline\">\\(1 - e_{m+1}\\)</span>，代入上面的导数为0的推导，可以得到：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    e^{2\\alpha_{m+1}} &amp;= \\frac{1 - e_{m+1}}{e_{m+1}}\\\\\r\n    \\Rightarrow \\alpha_{m+1} &amp;= \\frac{1}{2}\\ln{\\frac{1 - e_{m+1}}{e_{m+1}}}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>因此AdaBoost的流程就很明确了：</p>\r\n<ol type=\"1\">\r\n<li>在数据分布<span class=\"math inline\">\\(D_{m-1}\\)</span>上训练基学习器<span class=\"math inline\">\\(h_m(x)\\)</span></li>\r\n<li>计算<span class=\"math inline\">\\(e_m = \\sum\\limits_{f(x_i) \\ne h_m(x_i)} w_{mi}\\)</span></li>\r\n<li>确定当前基学习器的权重<span class=\"math inline\">\\(\\alpha_m = \\frac{1}{2}\\ln{\\frac{1 - e_m}{e_m}}\\)</span></li>\r\n<li>得到新的学习器<span class=\"math inline\">\\(H_m(x) = \\sum\\limits_{i=1}^m \\alpha_i h_i(x)\\)</span></li>\r\n<li>如果学习器个数达到指定个数，则退出，否则使用新的学习器调整权重参数<span class=\"math inline\">\\(w_{mi} = \\frac{e^{-f(x_i)(H_m(x_i)}}{\\sum\\limits_{i=1}^N e^{-f(x_i)(H_m(x_i)}}\\)</span>得到分布<span class=\"math inline\">\\(D_m\\)</span>，跳转到第一步，进行第<span class=\"math inline\">\\(m+1\\)</span>个基学习器的训练。</li>\r\n</ol>\r\n<h2 id=\"提升树boosting-decision-tree-bdt\">提升树（Boosting Decision Tree, BDT）</h2>\r\n<p>残差树是针对回归问题的一种boosting方法。其基学习器是基于CART算法的回归树（关于决策树的相关内容可以见我的另外一篇文章<a href=\"#\">Post not found: 决策树总结 决策树总结</a>），模型依旧为加法模型、损失函数为平方函数、学习算法为前向分步算法。</p>\r\n<p>第<span class=\"math inline\">\\(m\\)</span>个树模型基学习器可以表示为<span class=\"math inline\">\\(T(x; \\theta_m)\\)</span>，得到前<span class=\"math inline\">\\(m\\)</span>个基学习器之后，残差树的预测函数可以看做：<span class=\"math inline\">\\(H_m(x) = \\sum\\limits_{i = 1}^mT(x; \\theta_i)\\)</span>，那么下一个树模型<span class=\"math inline\">\\(T(x; \\theta_{m+1})\\)</span>的目标损失函数可以写作： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\nL(y, T(x; \\theta_{m+1})) &amp;= (y - H_m(x) - T(x; \\theta_{m+1}))^2\\\\\r\n&amp;= (r - T(x; \\theta_{m+1}))^2\r\n\\end{aligned}\r\n\\]</span> 这里的<span class=\"math inline\">\\(r\\)</span>表示上一次的残差，这也是残差树名字的由来，例如现在需要拟合的值为20，第一次残差树拟合的值为18，那么第二次拟合的目标值为上一次的残差：20-18=2。</p>\r\n<h2 id=\"梯度提升决策树gradient-boosting-decision-tree-gbdt\">梯度提升决策树（Gradient Boosting Decision Tree, GBDT）</h2>\r\n<p>在提升树的训练过程中，每次增加的决策树是以残差作为目标，并使用CART方法构造决策树，而在GBDT中，添加新的决策树是以损失函数在当前模型预测值下的负梯度作为目标，同样使用CART方法构造决策树。如果损失函数使用的是均方误差，那么新的决策树的优化目标其实和提升树类似。</p>\r\n<p>其实严格来将，GBDT并不是去拟合梯度，而是在进行决策树构建时，尽量将梯度相近的样本划分到同一个叶节点。</p>\r\n<p>例如对于损失函数<span class=\"math inline\">\\(\\sum\\limits_{i=1}^N l(y_i, \\hat{y}_i)\\)</span>，其中<span class=\"math inline\">\\(N\\)</span>为样本个数，<span class=\"math inline\">\\(y_i\\)</span>表示第<span class=\"math inline\">\\(i\\)</span>个样本的目标值，<span class=\"math inline\">\\(\\hat{y}_i\\)</span>表示对第<span class=\"math inline\">\\(i\\)</span>个样本的预测值，现在GBDT已经构造出了第<span class=\"math inline\">\\(t\\)</span>颗决策树，目前的预测函数表示为<span class=\"math inline\">\\(F_t(x) = \\sum\\limits_{i=1}^t f_i(x)\\)</span>，这里每个<span class=\"math inline\">\\(f_i(x)\\)</span>表示一个决策树，现在的损失函数表示为<span class=\"math inline\">\\(\\sum\\limits_{i=1}^N l(y_i, F_t(x_i))\\)</span>，现在要构造第<span class=\"math inline\">\\(t+1\\)</span>颗树，那么其目标很简单，就是进一步使得损失函数<span class=\"math inline\">\\(\\sum\\limits_{i=1}^N l(y_i, F_t(x_i) + f_{t+1}(x_i))\\)</span>最小，这里将损失函数进行一次泰勒展开，可以将损失函数近似为：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\sum\\limits_{i=1}^N [l(y_i, F_t(x_i)) + \\frac{\\partial l(y_i, F_t(x_i))}{\\partial F_t(x_i)}f_{t+1}(x_i)]\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>其中<span class=\"math inline\">\\(l(y_i, F_t(x_i))\\)</span>的值和<span class=\"math inline\">\\(f_{t+1}\\)</span>无关，因此<span class=\"math inline\">\\(f_{t+1}\\)</span>只关心下面的表达式是否能够进一步将损失函数减小：</p>\r\n<p><span class=\"math display\">\\[\r\n\\sum\\limits_{i=1}^N \\frac{\\partial l(y_i, F_t(x_i))}{\\partial F_t(x_i)}f_{t+1}(x_i)\r\n\\]</span></p>\r\n<p>但是上面这个函数没有最小值，而且损失函数的一次展开是有误差的，<span class=\"math inline\">\\(f_{t+1}\\)</span>的预测值越大，误差越大，不能按照最小化问题来处理，自然的一个想法是使得<span class=\"math inline\">\\(f_{t+1}(x_i) = -\\frac{\\partial l(y_i, F_t(x_i))}{\\partial F_t(x_i)}\\)</span>，因此GBDT这里将其简化为拟合负梯度。</p>\r\n<h3 id=\"xgboost\">XGBoost</h3>\r\n<p>XGBoost是GBDT的一种改进版本。</p>\r\n<p>在已知第<span class=\"math inline\">\\(t\\)</span>颗决策树时，下一个决策树的损失函数为：<span class=\"math inline\">\\(\\sum\\limits_{i=1}^N l(y_i, F_t(x_i) + f_{t+1}(x_i)) + \\Omega(f_{t+1})\\)</span>。</p>\r\n<p>这里损失函数中新增了正则化项<span class=\"math inline\">\\(\\Omega(f_{t+1})\\)</span>，可以表示为：<span class=\"math inline\">\\(\\Omega(f_{t+1}) = \\gamma T + \\frac{1}{2}\\lambda\\sum\\limits_{j=1}^Tw^2_j\\)</span>，其中<span class=\"math inline\">\\(T\\)</span>表示第<span class=\"math inline\">\\(t+1\\)</span>颗树中的叶节点个数，而<span class=\"math inline\">\\(w_j\\)</span>表示第<span class=\"math inline\">\\(j\\)</span>个叶节点的输出值。</p>\r\n<p>在XGBoost中，对损失函数进行二次泰勒展开近似：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    &amp;\\sum\\limits_{i=1}^N l(y_i, F_t(x_i) + f_{t+1}(x_i)) + \\Omega(f_{t+1})\\\\\r\n    &amp;= \\sum\\limits_{i=1}^N l(y_i, F_t(x_i) + f_{t+1}(x_i)) + \\gamma T + \\frac{1}{2}\\lambda\\sum\\limits_{j=1}^Tw^2_j\\\\\r\n    &amp;\\simeq \\sum\\limits_{i=1}^N [l(y_i, F_t(x_i)) + \\frac{\\partial l(y_i, F_t(x_i))}{\\partial F_t(x_i)}f_{t+1}(x_i) + \\frac{1}{2}\\frac{\\partial^2 l(y_i, F_t(x_i))}{\\partial^2 F_t(x_i)}f^2_{t+1}(x_i)] + \\gamma T + \\frac{1}{2}\\lambda\\sum\\limits_{j=1}^Tw^2_j\\\\\r\n    &amp;=constant + \\sum\\limits_{i=1}^N [\\frac{\\partial l(y_i, F_t(x_i))}{\\partial F_t(x_i)}f_{t+1}(x_i) + \\frac{1}{2}\\frac{\\partial^2 l(y_i, F_t(x_i))}{\\partial^2 F_t(x_i)}f^2_{t+1}(x_i)] + \\gamma T + \\frac{1}{2}\\lambda\\sum\\limits_{j=1}^Tw^2_j\\\\\r\n    &amp;=constant + \\sum\\limits_{j=1}^T [\\sum\\limits_{i\\in I_j}\\frac{\\partial l(y_i, F_t(x_i))}{\\partial F_t(x_i)}w_j + \\sum\\limits_{i\\in I_j}\\frac{1}{2}\\frac{\\partial^2 l(y_i, F_t(x_i))}{\\partial^2 F_t(x_i)}w_j^2] + \\gamma T + \\frac{1}{2}\\lambda\\sum\\limits_{j=1}^Tw^2_j\\\\\r\n    &amp;=constant + \\sum\\limits_{j=1}^T[\\sum\\limits_{i\\in I_j} g_i w_j + \\frac{1}{2}(\\sum\\limits_{i\\in I_j}h_i + \\lambda)w^2_j] + \\gamma T\\\\\r\n    &amp;=constant + \\sum\\limits_{j=1}^T[G_j w_j + \\frac{1}{2}(H_j + \\lambda)w^2_j] + \\gamma T\r\n\\end{aligned}\r\n\\]</span> 这里的化简过程中，<span class=\"math inline\">\\(I_j\\)</span>表示属于第<span class=\"math inline\">\\(j\\)</span>个叶节点的样本id的集合，<span class=\"math inline\">\\(g_i\\)</span>表示第<span class=\"math inline\">\\(i\\)</span>个样本在<span class=\"math inline\">\\(H_t(x_i)\\)</span>这个预测结果下的一阶导数：<span class=\"math inline\">\\(\\frac{\\partial l(y_i, F_t(x_i))}{\\partial F_t(x_i)}\\)</span>，<span class=\"math inline\">\\(h_i\\)</span>表示第<span class=\"math inline\">\\(i\\)</span>个样本在<span class=\"math inline\">\\(F_t(x_i)\\)</span>这个预测结果下的二阶导数：<span class=\"math inline\">\\(\\frac{\\partial^2 l(y_i, F_t(x_i))}{\\partial^2 F_t(x_i)}\\)</span>，<span class=\"math inline\">\\(G_j\\)</span>表示属于第<span class=\"math inline\">\\(j\\)</span>个叶节点的样本的导数之和，<span class=\"math inline\">\\(H_j\\)</span>表示属于第<span class=\"math inline\">\\(j\\)</span>个叶节点的样本的二阶导数之和。</p>\r\n<p>得到上面的化简结果之后，首先对<span class=\"math inline\">\\(w_j\\)</span>做个处理，令上式对<span class=\"math inline\">\\(w_j\\)</span>的导数为0，可以得到<span class=\"math inline\">\\(w^\\star_j = -\\frac{G_j}{H_j + \\lambda}\\)</span>就是第<span class=\"math inline\">\\(j\\)</span>个叶节点的预测值的最优解，因此上式可以进一步写成：</p>\r\n<p><span class=\"math display\">\\[\r\nconstant - \\sum\\limits_{j=1}^T \\frac{1}{2}\\frac{G^2_j}{H_j + \\lambda} + \\gamma T\r\n\\]</span></p>\r\n<p>XGBoost在构造新的决策树时，和CART类似，将当前节点分裂成两个子节点，但是选择特征以及特征的最优划分，不是使用Gini_index，为了最小化上面的目标，XGBoost在划分新的叶节点时，判断是否将叶节点<span class=\"math inline\">\\(j\\)</span>划分为新的左节点和右节点时，主要考察下式： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n&amp;constant - \\frac{1}{2}\\frac{G^2_j}{H_j + \\lambda} + \\gamma T - (constant - \\frac{1}{2}\\frac{G^2_L}{H_L + \\lambda} - \\frac{1}{2}\\frac{G^2_R}{H_R + \\lambda} + \\gamma(T+1))\\\\\r\n&amp;= \\frac{1}{2}( \\frac{G^2_L}{H_L + \\lambda} + \\frac{G^2_R}{H_R + \\lambda} - \\frac{G^2_j}{H_j + \\lambda}) - \\gamma    \r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>这里<span class=\"math inline\">\\(G_L， H_L\\)</span>分别表示划分出的左叶节点的导数和二阶导数和，<span class=\"math inline\">\\(G_R， H_R\\)</span>分别表示划分出的右叶节点的导数和二阶导数和。上式的结果表示将叶节点<span class=\"math inline\">\\(j\\)</span>划分为新的左节点和右节点时的增益情况。如果上式为正，则表示新划分节点有助于降低损失函数值，所以这里创建新节点时，遍历所有特征以及特征的划分位置，找到使得上式为正且最大的划分来创建新节点，如果没有能够使得上式为正的划分，那么停止继续添加叶节点。</p>\r\n<p>按照上面的方法，逐个构造新的决策树以降低损失函数值，就是XGBoost的主要工作原理。</p>\r\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"boosting\">Boosting</h1>\r\n<p>Boosting是集成学习的一类算法的总称，这类算法的流程可以概括为：首先从初始训练集中训练出一个基学习器，然后根据基学习器的表现对训练样本分布进行调整，使得之前做错的样本在后面受到更多的关注，然后基于调整后的样本分布来训练下一个基学习器，如此逐步增加基学习器，直到基学习器的数目达到了指定值T。</p>\r\n<h2 id=\"adaboost\">AdaBoost</h2>\r\n<p>AdaBoost是Boosting方法中的一个代表，主要应用在二分类任务上，AdaBoost可以理解为将基学习进行线性组合得到最终的学习器，其中<span class=\"math inline\">\\(\\alpha_t\\)</span>是一个正数，表示第<span class=\"math inline\">\\(t\\)</span>个基学习器的权重，如下所示。</p>\r\n<p><span class=\"math display\">\\[\r\nH(x) = \\sum\\limits_{t=1}^T \\alpha_t h_t(x)\r\n\\]</span></p>\r\n<p>AdaBoost使用指数损失函数作为优化目标，如下所示，其中<span class=\"math inline\">\\(f(x)\\)</span>表示真实函数，<span class=\"math inline\">\\(f(x) \\in \\{1, -1\\}\\)</span>，优化这个指数损失函数其实相当于优化0/1损失函数（证明略），因为指数损失函数的平滑性质更利于优化过程，因此这里用指数损失函数，其中<span class=\"math inline\">\\(E\\)</span>表示期望，<span class=\"math inline\">\\(D\\)</span>表示原始数据的分布（每个数据出现的概率是<span class=\"math inline\">\\(\\frac{1}{N}\\)</span>，<span class=\"math inline\">\\(N\\)</span>为数据个数）。</p>\r\n<p><span class=\"math display\">\\[\r\nl(H|D) = E_{x \\sim D}[e^{-f(x)H(x)}]\r\n\\]</span></p>\r\n<p>因此AdaBoost的目标可以表示为： <span class=\"math display\">\\[\r\n\\mathop{\\arg\\min}\\limits_{\\alpha, h} E_{x \\sim D}[e^{-f(x)(\\sum\\limits_{t=1}^T \\alpha_t h_t(x))}]\r\n\\]</span></p>\r\n<p>但是这个问题的直接求解非常困难，因此这里使用一种<strong>前向分步算法</strong>来解决这个问题，其思路是：每增加一个基学习器，就让损失函数减小一些，这样来逐步逼近最优解。</p>\r\n<p>假设现在已经得到<span class=\"math inline\">\\(m\\)</span>个基学习器组成的集成学习器：<span class=\"math inline\">\\(H_m(x)\\)</span>，现在下一个学习器（<span class=\"math inline\">\\(\\alpha_{m+1}, h_{m+1}\\)</span>）的目标可以表示为： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    &amp;\\mathop{\\arg\\min}\\limits_{\\alpha_{m+1}, h_{m+1}} E_{x \\sim D}[e^{-f(x)(H_m(x) + \\alpha_{m+1} h_{m+1}(x))}]\\\\\r\n    &amp;=\\mathop{\\arg\\min}\\limits_{\\alpha_{m+1}, h_{m+1}} E_{x \\sim D}[e^{-f(x)(H_m(x)} \\times e^{-f(x) \\alpha_{m+1} h_{m+1}(x))}]\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p><span class=\"math inline\">\\(H_m(x)\\)</span>和<span class=\"math inline\">\\(\\alpha_{m+1}, h_{m+1}\\)</span>无关，因此可以将<span class=\"math inline\">\\(e^{-f(x)(H_m(x)}\\)</span>看做数据的一种权重，即使用<span class=\"math inline\">\\(e^{-f(x)(H_m(x)}\\)</span>这一项来改变数据的分布，假设这样改变数据权重之后，得到的数据分布为<span class=\"math inline\">\\(D_m\\)</span>，那么可以进一步将目标化简为：</p>\r\n<p><span class=\"math display\">\\[\r\n\\mathop{\\arg\\min}\\limits_{\\alpha_{m+1}, h_{m+1}} E_{x \\sim D_m}\\ e^{-f(x) \\alpha_{m+1} h_{m+1}(x)}\r\n\\]</span></p>\r\n<p>因为<span class=\"math inline\">\\(f(x)\\)</span>和<span class=\"math inline\">\\(h_m{x}\\)</span>的取值要么是1，要么是<span class=\"math inline\">\\(-1\\)</span>，又因为<span class=\"math inline\">\\(\\alpha_m\\)</span>为正数，因此这里如果要最小化目标，可以先不考虑<span class=\"math inline\">\\(\\alpha_m\\)</span>，而是先最小化：</p>\r\n<p><span class=\"math display\">\\[\r\n\\mathop{\\arg\\min}\\limits_{h_{m+1}} E_{x \\sim D_m}\\ e^{-f(x)h_{m+1}(x)}\r\n\\]</span></p>\r\n<p>因此第<span class=\"math inline\">\\(m+1\\)</span>个基学习器的目标就是在数据分布<span class=\"math inline\">\\(D_m\\)</span>上优化指数损失函数：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\nh^\\star_{m+1} &amp;= \\mathop{\\arg\\min}\\limits_{h_{m+1}} E_{x \\sim D_m}\\ e^{-f(x)h_{m+1}(x)}\\\\\r\n&amp;=\\mathop{\\arg\\min}\\limits_{h_{m+1}} \\sum\\limits_{i=1}^N w_{mi}I\\{f(x_i) \\ne h_{m+1}(x_i)\\}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>用<span class=\"math inline\">\\(w_{mi} = \\frac{e^{-f(x_i)(H_m(x_i)}}{\\sum\\limits_{i=1}^N e^{-f(x_i)(H_m(x_i)}}\\)</span>来表示数据<span class=\"math inline\">\\(x_i\\)</span>的权重，这里多出来了一个用于权重归一化的分母，但是不影响<span class=\"math inline\">\\(\\alpha_{m+1}，h_{m+1}\\)</span>的训练，只是后面推导会方便些。</p>\r\n<p>第<span class=\"math inline\">\\(m+1\\)</span>个基学习器<span class=\"math inline\">\\(h^\\star_{m+1}\\)</span>得到之后，再来确定其权重<span class=\"math inline\">\\(\\alpha_{m+1}\\)</span>，其目标是：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    &amp;\\mathop{\\arg\\min}\\limits_{\\alpha_{m+1}} E_{x \\sim D_m}\\ e^{-f(x) \\alpha_{m+1} h^\\star_{m+1}(x)}\\\\\r\n    &amp;=\\mathop{\\arg\\min}\\limits_{\\alpha_{m+1}} \\sum\\limits_{i=1}^N w_{mi} e^{-f(x) \\alpha_{m+1} h^\\star_{m+1}(x)}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>求损失函数<span class=\"math inline\">\\(l(\\alpha_{m+1}) = \\sum\\limits_{i=1}^N w_{mi} e^{-f(x) \\alpha_{m+1} h^\\star_{m+1}(x)}\\)</span>对于<span class=\"math inline\">\\(\\alpha_{m+1}\\)</span>的导数，可以得到：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\frac{\\partial l(\\alpha_{m+1})}{\\partial \\alpha_{m+1}} &amp;= \\sum\\limits_{f(x_i) \\ne h_{m+1}(x_i)} w_{mi}e^{\\alpha_{m+1}} - \\sum\\limits_{f(x_i) = h_{m+1}(x_i)} w_{mi}e^{-\\alpha_{m+1}}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>令导数<span class=\"math inline\">\\(\\frac{\\partial l(\\alpha_{m+1})}{\\partial \\alpha_{m+1}} = 0\\)</span>可得： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\sum\\limits_{f(x_i) \\ne h_{m+1}(x_i)} w_{mi}e^{\\alpha_{m+1}} &amp;= \\sum\\limits_{f(x_i) = h_{m+1}(x_i)} w_{mi}e^{-\\alpha_{m+1}}\\\\\r\n    e^{2\\alpha_{m+1}} &amp;= \\frac{\\sum\\limits_{f(x_i) = h_{m+1}(x_i)} w_{mi}}{\\sum\\limits_{f(x_i) \\ne h_{m+1}(x_i)} w_{mi}}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>容易发现，因为<span class=\"math inline\">\\(w_{mi}\\)</span>是经过了归一化的，因此<span class=\"math inline\">\\(\\sum\\limits_{i=1}^N w_{mi} = 1\\)</span>，那么可以得到<span class=\"math inline\">\\(\\sum\\limits_{f(x_i) = h_{m+1}(x_i)} w_{mi} = 1 - \\sum\\limits_{f(x_i) \\ne h_{m+1}(x_i)} w_{mi}\\)</span>，这里令<span class=\"math inline\">\\(\\sum\\limits_{f(x_i) \\ne h_{m+1}(x_i)} w_{mi} = e_{m+1}\\)</span>表示<span class=\"math inline\">\\(h_{m+1}\\)</span>的损失，那么<span class=\"math inline\">\\(\\sum\\limits_{f(x_i) = h_{m+1}(x_i)} w_{mi}\\)</span>可以表示为<span class=\"math inline\">\\(1 - e_{m+1}\\)</span>，代入上面的导数为0的推导，可以得到：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    e^{2\\alpha_{m+1}} &amp;= \\frac{1 - e_{m+1}}{e_{m+1}}\\\\\r\n    \\Rightarrow \\alpha_{m+1} &amp;= \\frac{1}{2}\\ln{\\frac{1 - e_{m+1}}{e_{m+1}}}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>因此AdaBoost的流程就很明确了：</p>\r\n<ol type=\"1\">\r\n<li>在数据分布<span class=\"math inline\">\\(D_{m-1}\\)</span>上训练基学习器<span class=\"math inline\">\\(h_m(x)\\)</span></li>\r\n<li>计算<span class=\"math inline\">\\(e_m = \\sum\\limits_{f(x_i) \\ne h_m(x_i)} w_{mi}\\)</span></li>\r\n<li>确定当前基学习器的权重<span class=\"math inline\">\\(\\alpha_m = \\frac{1}{2}\\ln{\\frac{1 - e_m}{e_m}}\\)</span></li>\r\n<li>得到新的学习器<span class=\"math inline\">\\(H_m(x) = \\sum\\limits_{i=1}^m \\alpha_i h_i(x)\\)</span></li>\r\n<li>如果学习器个数达到指定个数，则退出，否则使用新的学习器调整权重参数<span class=\"math inline\">\\(w_{mi} = \\frac{e^{-f(x_i)(H_m(x_i)}}{\\sum\\limits_{i=1}^N e^{-f(x_i)(H_m(x_i)}}\\)</span>得到分布<span class=\"math inline\">\\(D_m\\)</span>，跳转到第一步，进行第<span class=\"math inline\">\\(m+1\\)</span>个基学习器的训练。</li>\r\n</ol>\r\n<h2 id=\"提升树boosting-decision-tree-bdt\">提升树（Boosting Decision Tree, BDT）</h2>\r\n<p>残差树是针对回归问题的一种boosting方法。其基学习器是基于CART算法的回归树（关于决策树的相关内容可以见我的另外一篇文章<a href=\"#\">Post not found: 决策树总结 决策树总结</a>），模型依旧为加法模型、损失函数为平方函数、学习算法为前向分步算法。</p>\r\n<p>第<span class=\"math inline\">\\(m\\)</span>个树模型基学习器可以表示为<span class=\"math inline\">\\(T(x; \\theta_m)\\)</span>，得到前<span class=\"math inline\">\\(m\\)</span>个基学习器之后，残差树的预测函数可以看做：<span class=\"math inline\">\\(H_m(x) = \\sum\\limits_{i = 1}^mT(x; \\theta_i)\\)</span>，那么下一个树模型<span class=\"math inline\">\\(T(x; \\theta_{m+1})\\)</span>的目标损失函数可以写作： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\nL(y, T(x; \\theta_{m+1})) &amp;= (y - H_m(x) - T(x; \\theta_{m+1}))^2\\\\\r\n&amp;= (r - T(x; \\theta_{m+1}))^2\r\n\\end{aligned}\r\n\\]</span> 这里的<span class=\"math inline\">\\(r\\)</span>表示上一次的残差，这也是残差树名字的由来，例如现在需要拟合的值为20，第一次残差树拟合的值为18，那么第二次拟合的目标值为上一次的残差：20-18=2。</p>\r\n<h2 id=\"梯度提升决策树gradient-boosting-decision-tree-gbdt\">梯度提升决策树（Gradient Boosting Decision Tree, GBDT）</h2>\r\n<p>在提升树的训练过程中，每次增加的决策树是以残差作为目标，并使用CART方法构造决策树，而在GBDT中，添加新的决策树是以损失函数在当前模型预测值下的负梯度作为目标，同样使用CART方法构造决策树。如果损失函数使用的是均方误差，那么新的决策树的优化目标其实和提升树类似。</p>\r\n<p>其实严格来将，GBDT并不是去拟合梯度，而是在进行决策树构建时，尽量将梯度相近的样本划分到同一个叶节点。</p>\r\n<p>例如对于损失函数<span class=\"math inline\">\\(\\sum\\limits_{i=1}^N l(y_i, \\hat{y}_i)\\)</span>，其中<span class=\"math inline\">\\(N\\)</span>为样本个数，<span class=\"math inline\">\\(y_i\\)</span>表示第<span class=\"math inline\">\\(i\\)</span>个样本的目标值，<span class=\"math inline\">\\(\\hat{y}_i\\)</span>表示对第<span class=\"math inline\">\\(i\\)</span>个样本的预测值，现在GBDT已经构造出了第<span class=\"math inline\">\\(t\\)</span>颗决策树，目前的预测函数表示为<span class=\"math inline\">\\(F_t(x) = \\sum\\limits_{i=1}^t f_i(x)\\)</span>，这里每个<span class=\"math inline\">\\(f_i(x)\\)</span>表示一个决策树，现在的损失函数表示为<span class=\"math inline\">\\(\\sum\\limits_{i=1}^N l(y_i, F_t(x_i))\\)</span>，现在要构造第<span class=\"math inline\">\\(t+1\\)</span>颗树，那么其目标很简单，就是进一步使得损失函数<span class=\"math inline\">\\(\\sum\\limits_{i=1}^N l(y_i, F_t(x_i) + f_{t+1}(x_i))\\)</span>最小，这里将损失函数进行一次泰勒展开，可以将损失函数近似为：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\sum\\limits_{i=1}^N [l(y_i, F_t(x_i)) + \\frac{\\partial l(y_i, F_t(x_i))}{\\partial F_t(x_i)}f_{t+1}(x_i)]\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>其中<span class=\"math inline\">\\(l(y_i, F_t(x_i))\\)</span>的值和<span class=\"math inline\">\\(f_{t+1}\\)</span>无关，因此<span class=\"math inline\">\\(f_{t+1}\\)</span>只关心下面的表达式是否能够进一步将损失函数减小：</p>\r\n<p><span class=\"math display\">\\[\r\n\\sum\\limits_{i=1}^N \\frac{\\partial l(y_i, F_t(x_i))}{\\partial F_t(x_i)}f_{t+1}(x_i)\r\n\\]</span></p>\r\n<p>但是上面这个函数没有最小值，而且损失函数的一次展开是有误差的，<span class=\"math inline\">\\(f_{t+1}\\)</span>的预测值越大，误差越大，不能按照最小化问题来处理，自然的一个想法是使得<span class=\"math inline\">\\(f_{t+1}(x_i) = -\\frac{\\partial l(y_i, F_t(x_i))}{\\partial F_t(x_i)}\\)</span>，因此GBDT这里将其简化为拟合负梯度。</p>\r\n<h3 id=\"xgboost\">XGBoost</h3>\r\n<p>XGBoost是GBDT的一种改进版本。</p>\r\n<p>在已知第<span class=\"math inline\">\\(t\\)</span>颗决策树时，下一个决策树的损失函数为：<span class=\"math inline\">\\(\\sum\\limits_{i=1}^N l(y_i, F_t(x_i) + f_{t+1}(x_i)) + \\Omega(f_{t+1})\\)</span>。</p>\r\n<p>这里损失函数中新增了正则化项<span class=\"math inline\">\\(\\Omega(f_{t+1})\\)</span>，可以表示为：<span class=\"math inline\">\\(\\Omega(f_{t+1}) = \\gamma T + \\frac{1}{2}\\lambda\\sum\\limits_{j=1}^Tw^2_j\\)</span>，其中<span class=\"math inline\">\\(T\\)</span>表示第<span class=\"math inline\">\\(t+1\\)</span>颗树中的叶节点个数，而<span class=\"math inline\">\\(w_j\\)</span>表示第<span class=\"math inline\">\\(j\\)</span>个叶节点的输出值。</p>\r\n<p>在XGBoost中，对损失函数进行二次泰勒展开近似：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    &amp;\\sum\\limits_{i=1}^N l(y_i, F_t(x_i) + f_{t+1}(x_i)) + \\Omega(f_{t+1})\\\\\r\n    &amp;= \\sum\\limits_{i=1}^N l(y_i, F_t(x_i) + f_{t+1}(x_i)) + \\gamma T + \\frac{1}{2}\\lambda\\sum\\limits_{j=1}^Tw^2_j\\\\\r\n    &amp;\\simeq \\sum\\limits_{i=1}^N [l(y_i, F_t(x_i)) + \\frac{\\partial l(y_i, F_t(x_i))}{\\partial F_t(x_i)}f_{t+1}(x_i) + \\frac{1}{2}\\frac{\\partial^2 l(y_i, F_t(x_i))}{\\partial^2 F_t(x_i)}f^2_{t+1}(x_i)] + \\gamma T + \\frac{1}{2}\\lambda\\sum\\limits_{j=1}^Tw^2_j\\\\\r\n    &amp;=constant + \\sum\\limits_{i=1}^N [\\frac{\\partial l(y_i, F_t(x_i))}{\\partial F_t(x_i)}f_{t+1}(x_i) + \\frac{1}{2}\\frac{\\partial^2 l(y_i, F_t(x_i))}{\\partial^2 F_t(x_i)}f^2_{t+1}(x_i)] + \\gamma T + \\frac{1}{2}\\lambda\\sum\\limits_{j=1}^Tw^2_j\\\\\r\n    &amp;=constant + \\sum\\limits_{j=1}^T [\\sum\\limits_{i\\in I_j}\\frac{\\partial l(y_i, F_t(x_i))}{\\partial F_t(x_i)}w_j + \\sum\\limits_{i\\in I_j}\\frac{1}{2}\\frac{\\partial^2 l(y_i, F_t(x_i))}{\\partial^2 F_t(x_i)}w_j^2] + \\gamma T + \\frac{1}{2}\\lambda\\sum\\limits_{j=1}^Tw^2_j\\\\\r\n    &amp;=constant + \\sum\\limits_{j=1}^T[\\sum\\limits_{i\\in I_j} g_i w_j + \\frac{1}{2}(\\sum\\limits_{i\\in I_j}h_i + \\lambda)w^2_j] + \\gamma T\\\\\r\n    &amp;=constant + \\sum\\limits_{j=1}^T[G_j w_j + \\frac{1}{2}(H_j + \\lambda)w^2_j] + \\gamma T\r\n\\end{aligned}\r\n\\]</span> 这里的化简过程中，<span class=\"math inline\">\\(I_j\\)</span>表示属于第<span class=\"math inline\">\\(j\\)</span>个叶节点的样本id的集合，<span class=\"math inline\">\\(g_i\\)</span>表示第<span class=\"math inline\">\\(i\\)</span>个样本在<span class=\"math inline\">\\(H_t(x_i)\\)</span>这个预测结果下的一阶导数：<span class=\"math inline\">\\(\\frac{\\partial l(y_i, F_t(x_i))}{\\partial F_t(x_i)}\\)</span>，<span class=\"math inline\">\\(h_i\\)</span>表示第<span class=\"math inline\">\\(i\\)</span>个样本在<span class=\"math inline\">\\(F_t(x_i)\\)</span>这个预测结果下的二阶导数：<span class=\"math inline\">\\(\\frac{\\partial^2 l(y_i, F_t(x_i))}{\\partial^2 F_t(x_i)}\\)</span>，<span class=\"math inline\">\\(G_j\\)</span>表示属于第<span class=\"math inline\">\\(j\\)</span>个叶节点的样本的导数之和，<span class=\"math inline\">\\(H_j\\)</span>表示属于第<span class=\"math inline\">\\(j\\)</span>个叶节点的样本的二阶导数之和。</p>\r\n<p>得到上面的化简结果之后，首先对<span class=\"math inline\">\\(w_j\\)</span>做个处理，令上式对<span class=\"math inline\">\\(w_j\\)</span>的导数为0，可以得到<span class=\"math inline\">\\(w^\\star_j = -\\frac{G_j}{H_j + \\lambda}\\)</span>就是第<span class=\"math inline\">\\(j\\)</span>个叶节点的预测值的最优解，因此上式可以进一步写成：</p>\r\n<p><span class=\"math display\">\\[\r\nconstant - \\sum\\limits_{j=1}^T \\frac{1}{2}\\frac{G^2_j}{H_j + \\lambda} + \\gamma T\r\n\\]</span></p>\r\n<p>XGBoost在构造新的决策树时，和CART类似，将当前节点分裂成两个子节点，但是选择特征以及特征的最优划分，不是使用Gini_index，为了最小化上面的目标，XGBoost在划分新的叶节点时，判断是否将叶节点<span class=\"math inline\">\\(j\\)</span>划分为新的左节点和右节点时，主要考察下式： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n&amp;constant - \\frac{1}{2}\\frac{G^2_j}{H_j + \\lambda} + \\gamma T - (constant - \\frac{1}{2}\\frac{G^2_L}{H_L + \\lambda} - \\frac{1}{2}\\frac{G^2_R}{H_R + \\lambda} + \\gamma(T+1))\\\\\r\n&amp;= \\frac{1}{2}( \\frac{G^2_L}{H_L + \\lambda} + \\frac{G^2_R}{H_R + \\lambda} - \\frac{G^2_j}{H_j + \\lambda}) - \\gamma    \r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>这里<span class=\"math inline\">\\(G_L， H_L\\)</span>分别表示划分出的左叶节点的导数和二阶导数和，<span class=\"math inline\">\\(G_R， H_R\\)</span>分别表示划分出的右叶节点的导数和二阶导数和。上式的结果表示将叶节点<span class=\"math inline\">\\(j\\)</span>划分为新的左节点和右节点时的增益情况。如果上式为正，则表示新划分节点有助于降低损失函数值，所以这里创建新节点时，遍历所有特征以及特征的划分位置，找到使得上式为正且最大的划分来创建新节点，如果没有能够使得上式为正的划分，那么停止继续添加叶节点。</p>\r\n<p>按照上面的方法，逐个构造新的决策树以降低损失函数值，就是XGBoost的主要工作原理。</p>\r\n"},{"title":"CNN感受野","date":"2020-05-09T05:45:50.000Z","mathjax":true,"_content":"\n# 感受野（Receptive Filed, RF）\n由于卷积层是一种局部连接操作，对于卷积层的输入和输出，在输出的特征图上，一个像素的值，只取决于输入特征图上的一部分区域，如图所示：\n\n![CNN操作示意](CNN操作示意.png)\n\n## 感受野的大小\n从原始输入图像开始，如果第一层卷积的卷积核是$3\\times 3$大小，那么第一层卷积层的输出特征图上，左上角第一个像素点连接了原始输入图像左上角的$3\\times3$大小的范围（这个范围被称作这个像素对于输入图像的感受野），即这个特征图上，每个像素对于输入图像的的感受野大小是$3\\times 3$，如果第二层卷积也是$3\\times 3$大小的卷积核，那么第二层输出的特征图上的一个像素对于第一层输出的特征图的感受野是$3\\times 3$，但是对应到原图上的感受野大小和范围就要依赖于第一层卷积层的卷积步长参数了，如果第一层卷积的卷积步长为1，那么可以得出第二层输出的特征图上，一个像素对应于原始输入图像上$5\\times 5$的区域，即这层特征图上每个像素对于输入图像的感受野大小是$5\\times 5$。\n\n对于特征图上的一个像素，其感受野之外的区域不影响该像素的值，因此对于该像素，不能从其值上获取其感受野之外的信息。\n\n## 感受野的步长\n从原始输入图像开始，如果第一层卷积的卷积核是$3\\times 3$大小，卷积步长是1，那么在第一层卷积层的输出特征图上坐标为$(0, 0)$的第一个像素的感受野是输入图像的左上角从$(0,0)$坐标开始的$3\\times 3$的区域，而第一层卷积层的输出特征图上坐标为$(0, 1)$的像素则对应输入图像的左上角从$(0,1)$坐标开始的$3\\times 3$的区域，这两个区域之间的步长为1，也称为第一层卷积层的输出特征图对于输入图像的感受野步长为1。\n\n## 感受野的padding\n感受野的padding和卷积层的padding是类似的，指的就是感受野分布在对应特征图上时，超出特征图的范围大小，这个是由于卷积层引入padding导致的。\n\n## 感受野的理解\n感受野可以理解为将信息经过的层合并成一个大核卷积层（虽然一个大卷积层不能完全实现多层堆叠的计算，例如其中包含了激活、池化等层，但是在感受野大小、步长、padding方面是等效的），其中卷积核大小就是感受野大小，卷积步长就是感受野步长。\n\n# 感受野大小和步长的计算\n一个适合口算的方式是从后往前计算，假设现在要计算第$i+m$层的输出特征图对于第$i$层特征图的感受野大小和步长，那么首先看第$i+m$层的输出特征图对于第$i+m-1$层的输出特征图的感受野大小和步长。\n\n这里只考虑卷积核大小和步长在长宽上相同的情况，否则的话，需要按照长宽分别计算，记第$n$层输出特征图对于第$m$层输出特征图的感受野大小为$F^n_m$，感受野步长为$S^n_m$，感受野padding记为$P^n_m$，如果第$i$层是卷积层，则记其卷积核大小为$k_i$，记其卷积步长为$s_i$，记padding大小为$p_i$，如果第$i$层是池化层，则同样记其池化核大小为$k_i$，记其池化步长为$s_i$，记padding大小为$p_i$，那么不论对池化层还是卷积层，很显然有：\n$$\n\\begin{aligned}\nF^n_n &= 1\\\\\nS^n_n &= 1\\\\\nP^n_n &= 0\\\\\nF^n_m &= (F^n_{m+1} - 1) \\times s_{m+1} + k_{m+1} = F^n_{m+1} \\times s_{m+1} - s_{m+1} + k_{m+1}\\\\\nS^n_m &= S^n_{m+1} \\times s_{m+1}\\\\\nP^n_m &= P^n_{m+1} * s_{m+1} + p_{m+1}\n\\end{aligned}\n$$\n\nVGG模型的配置如图所示：\n![VGG模型配置](VGG模型配置.png)\n\n这里以VGG-16为例，其中的卷积层全是卷积核大小为3，卷积步长为1，padding为1，其中的池化层全是池化核大小为2，池化步长为2，padding为0，记第四个maxPooling之前的特征图即Conv4-3为第13层的输出，记输入的$224\\times 224$大小的图片为第0层的输出。\n\n这里尝试计算$F^{13}_0$, $S^{13}_0$, $P^{13}_0$:\n$$\n\\begin{aligned}\nF^{13}_0 &= F^{13}_1 \\times s_1 - s_1 + k_1\\\\\n&= F^{13}_1 \\times 1 - 1 + 3\\\\\n&= (F^{13}_2 \\times s_2 - s_2 + k_2) \\times 1 - 1 + 3\\\\\n&= (F^{13}_2 \\times 1  - 1 + 3) \\times 1 - 1 + 3\\\\\n&\\cdots\\\\\n&= 1 \\times 1 - 1 + 3) \\times 1 - 1 + 3) \\times 1 - 1 + 3) \\times 2 - 2 + 2) \\times 1 - 1 + 3) \\times 1 - 1 + 3) \\times 1 - 1 + 3) \\times 2 - 2 + 2) \\times 1 - 1 + 3) \\times 1 - 1 + 3) \\times 2 - 2 + 2) \\times 1 - 1 + 3) \\times 1 - 1 + 3\\\\\n&= 1 + 2 + 2 + 2)\\times 2 + 2 + 2 + 2)\\times 2 + 2 + 2)\\times 2 + 2 + 2\n&= 92\\\\\n\nS^{13}_0 &= S^{13}_1 \\times s_1\\\\\n&= S^{13}_2 \\times s_2 \\times s_1\\\\\n&= \\cdots\\\\\n&= 1 \\times 1\\times 1\\times 1\\times 2\\times 1\\times 1\\times 1\\times 2\\times 1\\times 1\\times 2\\times 1\\times 1\\\\\n&= 8\\\\\n\nP^{13}_0 &= P^{13}_1 * s_1 + p_1\\\\\n&=P^{13}_2 * s_2 + p_2) * s_1 + p_1\\\\\n&= \\cdots\\\\\n&=0 * 1 + 1) * 1 + 1) * 1 + 1) * 2 + 0) * 1 + 1) * 1 + 1) * 1 + 1) * 2 + 0) * 1 + 1) * 1 + 1) * 2 + 0) * 1 + 1) * 1 + 1\\\\\n&= 42\n\\end{aligned}\n$$\n即Conv4-3的输出特征图上，每个像素的感受野在原始输入图片上的大小是92，padding 42，步长为8的分布，因为Conv4-3的输出特征图大小是$28\\times 28$，所以一共有$28\\times 28$个感受野，按照这个排列，其一行所占的像素可以计算为$92 + (28 - 1) \\times 8 = 308$正好等于$224 + 2 \\times 42$。\n\n对于ResNet、Inception这种包含多个路径模型来说，其某一层的感受野，应该取其中感受野最大的路径来计算。\n\n空洞卷积（Dialated Convolution）有助于增加感受野，可以将其卷积核看做一个填充了0值的大卷积核。\n\n# 有效感受野（Efficient Receptive Filed）\n一个特征图上某个像素的感受野虽然可以反应哪些位置可以影响该像素的值，但是在其感受野中，不同区域的影响程度是不一样的，论文《Understanding the Effective Receptive Field in Deep Convolutional Neural Networks》中讨论了这个问题，论文中指出，如果卷积核全都初始化为相同的值，那么感受野中的原图像素对于特征像素的影响大小成正态分布，这取决于中间的卷积核权重，并在模型训练前后会发生变化，如图所示，其中越亮的区域代表有越大的影响。\n\n![感受野权重示意图](感受野权重示意图.png)\n\n基于这个现象，论文中提出了有效感受野的概念，即真正能影响特征图像素值的范围只占理论感受野的一小部分。\n\n从上图中可以看出，即使在训练完成之后，有效感受野也不能铺满整个理论感受野范围，因此在设计模型时，还需要让感受野超出目标大小，否则很难提取到完整的目标信息。\n\n根据论文中给出的示意图，不同的初始化方式对有效感受野有一些影响，随着层数加深，理论感受野增大，有效感受野占理论感受野的比重越来越小。\n\n![有效感受野示意](有效感受野示意.png)\n\n为了使得感受野中的影响分布更加均匀，论文中提出加大卷积核边缘权重，减少卷积核中心权重的初始化方式。\n\n\n\n","source":"_posts/学习笔记/CNN感受野.md","raw":"---\ntitle: CNN感受野\ndate: 2020-05-09 13:45:50\ntags: [深度学习]\nmathjax: true\n---\n\n# 感受野（Receptive Filed, RF）\n由于卷积层是一种局部连接操作，对于卷积层的输入和输出，在输出的特征图上，一个像素的值，只取决于输入特征图上的一部分区域，如图所示：\n\n![CNN操作示意](CNN操作示意.png)\n\n## 感受野的大小\n从原始输入图像开始，如果第一层卷积的卷积核是$3\\times 3$大小，那么第一层卷积层的输出特征图上，左上角第一个像素点连接了原始输入图像左上角的$3\\times3$大小的范围（这个范围被称作这个像素对于输入图像的感受野），即这个特征图上，每个像素对于输入图像的的感受野大小是$3\\times 3$，如果第二层卷积也是$3\\times 3$大小的卷积核，那么第二层输出的特征图上的一个像素对于第一层输出的特征图的感受野是$3\\times 3$，但是对应到原图上的感受野大小和范围就要依赖于第一层卷积层的卷积步长参数了，如果第一层卷积的卷积步长为1，那么可以得出第二层输出的特征图上，一个像素对应于原始输入图像上$5\\times 5$的区域，即这层特征图上每个像素对于输入图像的感受野大小是$5\\times 5$。\n\n对于特征图上的一个像素，其感受野之外的区域不影响该像素的值，因此对于该像素，不能从其值上获取其感受野之外的信息。\n\n## 感受野的步长\n从原始输入图像开始，如果第一层卷积的卷积核是$3\\times 3$大小，卷积步长是1，那么在第一层卷积层的输出特征图上坐标为$(0, 0)$的第一个像素的感受野是输入图像的左上角从$(0,0)$坐标开始的$3\\times 3$的区域，而第一层卷积层的输出特征图上坐标为$(0, 1)$的像素则对应输入图像的左上角从$(0,1)$坐标开始的$3\\times 3$的区域，这两个区域之间的步长为1，也称为第一层卷积层的输出特征图对于输入图像的感受野步长为1。\n\n## 感受野的padding\n感受野的padding和卷积层的padding是类似的，指的就是感受野分布在对应特征图上时，超出特征图的范围大小，这个是由于卷积层引入padding导致的。\n\n## 感受野的理解\n感受野可以理解为将信息经过的层合并成一个大核卷积层（虽然一个大卷积层不能完全实现多层堆叠的计算，例如其中包含了激活、池化等层，但是在感受野大小、步长、padding方面是等效的），其中卷积核大小就是感受野大小，卷积步长就是感受野步长。\n\n# 感受野大小和步长的计算\n一个适合口算的方式是从后往前计算，假设现在要计算第$i+m$层的输出特征图对于第$i$层特征图的感受野大小和步长，那么首先看第$i+m$层的输出特征图对于第$i+m-1$层的输出特征图的感受野大小和步长。\n\n这里只考虑卷积核大小和步长在长宽上相同的情况，否则的话，需要按照长宽分别计算，记第$n$层输出特征图对于第$m$层输出特征图的感受野大小为$F^n_m$，感受野步长为$S^n_m$，感受野padding记为$P^n_m$，如果第$i$层是卷积层，则记其卷积核大小为$k_i$，记其卷积步长为$s_i$，记padding大小为$p_i$，如果第$i$层是池化层，则同样记其池化核大小为$k_i$，记其池化步长为$s_i$，记padding大小为$p_i$，那么不论对池化层还是卷积层，很显然有：\n$$\n\\begin{aligned}\nF^n_n &= 1\\\\\nS^n_n &= 1\\\\\nP^n_n &= 0\\\\\nF^n_m &= (F^n_{m+1} - 1) \\times s_{m+1} + k_{m+1} = F^n_{m+1} \\times s_{m+1} - s_{m+1} + k_{m+1}\\\\\nS^n_m &= S^n_{m+1} \\times s_{m+1}\\\\\nP^n_m &= P^n_{m+1} * s_{m+1} + p_{m+1}\n\\end{aligned}\n$$\n\nVGG模型的配置如图所示：\n![VGG模型配置](VGG模型配置.png)\n\n这里以VGG-16为例，其中的卷积层全是卷积核大小为3，卷积步长为1，padding为1，其中的池化层全是池化核大小为2，池化步长为2，padding为0，记第四个maxPooling之前的特征图即Conv4-3为第13层的输出，记输入的$224\\times 224$大小的图片为第0层的输出。\n\n这里尝试计算$F^{13}_0$, $S^{13}_0$, $P^{13}_0$:\n$$\n\\begin{aligned}\nF^{13}_0 &= F^{13}_1 \\times s_1 - s_1 + k_1\\\\\n&= F^{13}_1 \\times 1 - 1 + 3\\\\\n&= (F^{13}_2 \\times s_2 - s_2 + k_2) \\times 1 - 1 + 3\\\\\n&= (F^{13}_2 \\times 1  - 1 + 3) \\times 1 - 1 + 3\\\\\n&\\cdots\\\\\n&= 1 \\times 1 - 1 + 3) \\times 1 - 1 + 3) \\times 1 - 1 + 3) \\times 2 - 2 + 2) \\times 1 - 1 + 3) \\times 1 - 1 + 3) \\times 1 - 1 + 3) \\times 2 - 2 + 2) \\times 1 - 1 + 3) \\times 1 - 1 + 3) \\times 2 - 2 + 2) \\times 1 - 1 + 3) \\times 1 - 1 + 3\\\\\n&= 1 + 2 + 2 + 2)\\times 2 + 2 + 2 + 2)\\times 2 + 2 + 2)\\times 2 + 2 + 2\n&= 92\\\\\n\nS^{13}_0 &= S^{13}_1 \\times s_1\\\\\n&= S^{13}_2 \\times s_2 \\times s_1\\\\\n&= \\cdots\\\\\n&= 1 \\times 1\\times 1\\times 1\\times 2\\times 1\\times 1\\times 1\\times 2\\times 1\\times 1\\times 2\\times 1\\times 1\\\\\n&= 8\\\\\n\nP^{13}_0 &= P^{13}_1 * s_1 + p_1\\\\\n&=P^{13}_2 * s_2 + p_2) * s_1 + p_1\\\\\n&= \\cdots\\\\\n&=0 * 1 + 1) * 1 + 1) * 1 + 1) * 2 + 0) * 1 + 1) * 1 + 1) * 1 + 1) * 2 + 0) * 1 + 1) * 1 + 1) * 2 + 0) * 1 + 1) * 1 + 1\\\\\n&= 42\n\\end{aligned}\n$$\n即Conv4-3的输出特征图上，每个像素的感受野在原始输入图片上的大小是92，padding 42，步长为8的分布，因为Conv4-3的输出特征图大小是$28\\times 28$，所以一共有$28\\times 28$个感受野，按照这个排列，其一行所占的像素可以计算为$92 + (28 - 1) \\times 8 = 308$正好等于$224 + 2 \\times 42$。\n\n对于ResNet、Inception这种包含多个路径模型来说，其某一层的感受野，应该取其中感受野最大的路径来计算。\n\n空洞卷积（Dialated Convolution）有助于增加感受野，可以将其卷积核看做一个填充了0值的大卷积核。\n\n# 有效感受野（Efficient Receptive Filed）\n一个特征图上某个像素的感受野虽然可以反应哪些位置可以影响该像素的值，但是在其感受野中，不同区域的影响程度是不一样的，论文《Understanding the Effective Receptive Field in Deep Convolutional Neural Networks》中讨论了这个问题，论文中指出，如果卷积核全都初始化为相同的值，那么感受野中的原图像素对于特征像素的影响大小成正态分布，这取决于中间的卷积核权重，并在模型训练前后会发生变化，如图所示，其中越亮的区域代表有越大的影响。\n\n![感受野权重示意图](感受野权重示意图.png)\n\n基于这个现象，论文中提出了有效感受野的概念，即真正能影响特征图像素值的范围只占理论感受野的一小部分。\n\n从上图中可以看出，即使在训练完成之后，有效感受野也不能铺满整个理论感受野范围，因此在设计模型时，还需要让感受野超出目标大小，否则很难提取到完整的目标信息。\n\n根据论文中给出的示意图，不同的初始化方式对有效感受野有一些影响，随着层数加深，理论感受野增大，有效感受野占理论感受野的比重越来越小。\n\n![有效感受野示意](有效感受野示意.png)\n\n为了使得感受野中的影响分布更加均匀，论文中提出加大卷积核边缘权重，减少卷积核中心权重的初始化方式。\n\n\n\n","slug":"学习笔记/CNN感受野","published":1,"updated":"2020-09-01T04:25:05.219Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckjxpc3rh000344mq3gxf88ft","content":"<h1 id=\"感受野receptive-filed-rf\">感受野（Receptive Filed, RF）</h1>\r\n<p>由于卷积层是一种局部连接操作，对于卷积层的输入和输出，在输出的特征图上，一个像素的值，只取决于输入特征图上的一部分区域，如图所示：</p>\r\n<figure>\r\n<img src=\"CNN操作示意.png\" alt=\"CNN操作示意\" /><figcaption aria-hidden=\"true\">CNN操作示意</figcaption>\r\n</figure>\r\n<h2 id=\"感受野的大小\">感受野的大小</h2>\r\n<p>从原始输入图像开始，如果第一层卷积的卷积核是<span class=\"math inline\">\\(3\\times 3\\)</span>大小，那么第一层卷积层的输出特征图上，左上角第一个像素点连接了原始输入图像左上角的<span class=\"math inline\">\\(3\\times3\\)</span>大小的范围（这个范围被称作这个像素对于输入图像的感受野），即这个特征图上，每个像素对于输入图像的的感受野大小是<span class=\"math inline\">\\(3\\times 3\\)</span>，如果第二层卷积也是<span class=\"math inline\">\\(3\\times 3\\)</span>大小的卷积核，那么第二层输出的特征图上的一个像素对于第一层输出的特征图的感受野是<span class=\"math inline\">\\(3\\times 3\\)</span>，但是对应到原图上的感受野大小和范围就要依赖于第一层卷积层的卷积步长参数了，如果第一层卷积的卷积步长为1，那么可以得出第二层输出的特征图上，一个像素对应于原始输入图像上<span class=\"math inline\">\\(5\\times 5\\)</span>的区域，即这层特征图上每个像素对于输入图像的感受野大小是<span class=\"math inline\">\\(5\\times 5\\)</span>。</p>\r\n<p>对于特征图上的一个像素，其感受野之外的区域不影响该像素的值，因此对于该像素，不能从其值上获取其感受野之外的信息。</p>\r\n<h2 id=\"感受野的步长\">感受野的步长</h2>\r\n<p>从原始输入图像开始，如果第一层卷积的卷积核是<span class=\"math inline\">\\(3\\times 3\\)</span>大小，卷积步长是1，那么在第一层卷积层的输出特征图上坐标为<span class=\"math inline\">\\((0, 0)\\)</span>的第一个像素的感受野是输入图像的左上角从<span class=\"math inline\">\\((0,0)\\)</span>坐标开始的<span class=\"math inline\">\\(3\\times 3\\)</span>的区域，而第一层卷积层的输出特征图上坐标为<span class=\"math inline\">\\((0, 1)\\)</span>的像素则对应输入图像的左上角从<span class=\"math inline\">\\((0,1)\\)</span>坐标开始的<span class=\"math inline\">\\(3\\times 3\\)</span>的区域，这两个区域之间的步长为1，也称为第一层卷积层的输出特征图对于输入图像的感受野步长为1。</p>\r\n<h2 id=\"感受野的padding\">感受野的padding</h2>\r\n<p>感受野的padding和卷积层的padding是类似的，指的就是感受野分布在对应特征图上时，超出特征图的范围大小，这个是由于卷积层引入padding导致的。</p>\r\n<h2 id=\"感受野的理解\">感受野的理解</h2>\r\n<p>感受野可以理解为将信息经过的层合并成一个大核卷积层（虽然一个大卷积层不能完全实现多层堆叠的计算，例如其中包含了激活、池化等层，但是在感受野大小、步长、padding方面是等效的），其中卷积核大小就是感受野大小，卷积步长就是感受野步长。</p>\r\n<h1 id=\"感受野大小和步长的计算\">感受野大小和步长的计算</h1>\r\n<p>一个适合口算的方式是从后往前计算，假设现在要计算第<span class=\"math inline\">\\(i+m\\)</span>层的输出特征图对于第<span class=\"math inline\">\\(i\\)</span>层特征图的感受野大小和步长，那么首先看第<span class=\"math inline\">\\(i+m\\)</span>层的输出特征图对于第<span class=\"math inline\">\\(i+m-1\\)</span>层的输出特征图的感受野大小和步长。</p>\r\n<p>这里只考虑卷积核大小和步长在长宽上相同的情况，否则的话，需要按照长宽分别计算，记第<span class=\"math inline\">\\(n\\)</span>层输出特征图对于第<span class=\"math inline\">\\(m\\)</span>层输出特征图的感受野大小为<span class=\"math inline\">\\(F^n_m\\)</span>，感受野步长为<span class=\"math inline\">\\(S^n_m\\)</span>，感受野padding记为<span class=\"math inline\">\\(P^n_m\\)</span>，如果第<span class=\"math inline\">\\(i\\)</span>层是卷积层，则记其卷积核大小为<span class=\"math inline\">\\(k_i\\)</span>，记其卷积步长为<span class=\"math inline\">\\(s_i\\)</span>，记padding大小为<span class=\"math inline\">\\(p_i\\)</span>，如果第<span class=\"math inline\">\\(i\\)</span>层是池化层，则同样记其池化核大小为<span class=\"math inline\">\\(k_i\\)</span>，记其池化步长为<span class=\"math inline\">\\(s_i\\)</span>，记padding大小为<span class=\"math inline\">\\(p_i\\)</span>，那么不论对池化层还是卷积层，很显然有： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\nF^n_n &amp;= 1\\\\\r\nS^n_n &amp;= 1\\\\\r\nP^n_n &amp;= 0\\\\\r\nF^n_m &amp;= (F^n_{m+1} - 1) \\times s_{m+1} + k_{m+1} = F^n_{m+1} \\times s_{m+1} - s_{m+1} + k_{m+1}\\\\\r\nS^n_m &amp;= S^n_{m+1} \\times s_{m+1}\\\\\r\nP^n_m &amp;= P^n_{m+1} * s_{m+1} + p_{m+1}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>VGG模型的配置如图所示： <img src=\"VGG模型配置.png\" alt=\"VGG模型配置\" /></p>\r\n<p>这里以VGG-16为例，其中的卷积层全是卷积核大小为3，卷积步长为1，padding为1，其中的池化层全是池化核大小为2，池化步长为2，padding为0，记第四个maxPooling之前的特征图即Conv4-3为第13层的输出，记输入的<span class=\"math inline\">\\(224\\times 224\\)</span>大小的图片为第0层的输出。</p>\r\n这里尝试计算<span class=\"math inline\">\\(F^{13}_0\\)</span>, <span class=\"math inline\">\\(S^{13}_0\\)</span>, <span class=\"math inline\">\\(P^{13}_0\\)</span>: $$\r\n<span class=\"math display\">\\[\\begin{aligned}\r\nF^{13}_0 &amp;= F^{13}_1 \\times s_1 - s_1 + k_1\\\\\r\n&amp;= F^{13}_1 \\times 1 - 1 + 3\\\\\r\n&amp;= (F^{13}_2 \\times s_2 - s_2 + k_2) \\times 1 - 1 + 3\\\\\r\n&amp;= (F^{13}_2 \\times 1  - 1 + 3) \\times 1 - 1 + 3\\\\\r\n&amp;\\cdots\\\\\r\n&amp;= 1 \\times 1 - 1 + 3) \\times 1 - 1 + 3) \\times 1 - 1 + 3) \\times 2 - 2 + 2) \\times 1 - 1 + 3) \\times 1 - 1 + 3) \\times 1 - 1 + 3) \\times 2 - 2 + 2) \\times 1 - 1 + 3) \\times 1 - 1 + 3) \\times 2 - 2 + 2) \\times 1 - 1 + 3) \\times 1 - 1 + 3\\\\\r\n&amp;= 1 + 2 + 2 + 2)\\times 2 + 2 + 2 + 2)\\times 2 + 2 + 2)\\times 2 + 2 + 2\r\n&amp;= 92\\\\\r\n\r\nS^{13}_0 &amp;= S^{13}_1 \\times s_1\\\\\r\n&amp;= S^{13}_2 \\times s_2 \\times s_1\\\\\r\n&amp;= \\cdots\\\\\r\n&amp;= 1 \\times 1\\times 1\\times 1\\times 2\\times 1\\times 1\\times 1\\times 2\\times 1\\times 1\\times 2\\times 1\\times 1\\\\\r\n&amp;= 8\\\\\r\n\r\nP^{13}_0 &amp;= P^{13}_1 * s_1 + p_1\\\\\r\n&amp;=P^{13}_2 * s_2 + p_2) * s_1 + p_1\\\\\r\n&amp;= \\cdots\\\\\r\n&amp;=0 * 1 + 1) * 1 + 1) * 1 + 1) * 2 + 0) * 1 + 1) * 1 + 1) * 1 + 1) * 2 + 0) * 1 + 1) * 1 + 1) * 2 + 0) * 1 + 1) * 1 + 1\\\\\r\n&amp;= 42\r\n\\end{aligned}\\]</span>\r\n<p>$$ 即Conv4-3的输出特征图上，每个像素的感受野在原始输入图片上的大小是92，padding 42，步长为8的分布，因为Conv4-3的输出特征图大小是<span class=\"math inline\">\\(28\\times 28\\)</span>，所以一共有<span class=\"math inline\">\\(28\\times 28\\)</span>个感受野，按照这个排列，其一行所占的像素可以计算为<span class=\"math inline\">\\(92 + (28 - 1) \\times 8 = 308\\)</span>正好等于<span class=\"math inline\">\\(224 + 2 \\times 42\\)</span>。</p>\r\n<p>对于ResNet、Inception这种包含多个路径模型来说，其某一层的感受野，应该取其中感受野最大的路径来计算。</p>\r\n<p>空洞卷积（Dialated Convolution）有助于增加感受野，可以将其卷积核看做一个填充了0值的大卷积核。</p>\r\n<h1 id=\"有效感受野efficient-receptive-filed\">有效感受野（Efficient Receptive Filed）</h1>\r\n<p>一个特征图上某个像素的感受野虽然可以反应哪些位置可以影响该像素的值，但是在其感受野中，不同区域的影响程度是不一样的，论文《Understanding the Effective Receptive Field in Deep Convolutional Neural Networks》中讨论了这个问题，论文中指出，如果卷积核全都初始化为相同的值，那么感受野中的原图像素对于特征像素的影响大小成正态分布，这取决于中间的卷积核权重，并在模型训练前后会发生变化，如图所示，其中越亮的区域代表有越大的影响。</p>\r\n<figure>\r\n<img src=\"感受野权重示意图.png\" alt=\"感受野权重示意图\" /><figcaption aria-hidden=\"true\">感受野权重示意图</figcaption>\r\n</figure>\r\n<p>基于这个现象，论文中提出了有效感受野的概念，即真正能影响特征图像素值的范围只占理论感受野的一小部分。</p>\r\n<p>从上图中可以看出，即使在训练完成之后，有效感受野也不能铺满整个理论感受野范围，因此在设计模型时，还需要让感受野超出目标大小，否则很难提取到完整的目标信息。</p>\r\n<p>根据论文中给出的示意图，不同的初始化方式对有效感受野有一些影响，随着层数加深，理论感受野增大，有效感受野占理论感受野的比重越来越小。</p>\r\n<figure>\r\n<img src=\"有效感受野示意.png\" alt=\"有效感受野示意\" /><figcaption aria-hidden=\"true\">有效感受野示意</figcaption>\r\n</figure>\r\n<p>为了使得感受野中的影响分布更加均匀，论文中提出加大卷积核边缘权重，减少卷积核中心权重的初始化方式。</p>\r\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"感受野receptive-filed-rf\">感受野（Receptive Filed, RF）</h1>\r\n<p>由于卷积层是一种局部连接操作，对于卷积层的输入和输出，在输出的特征图上，一个像素的值，只取决于输入特征图上的一部分区域，如图所示：</p>\r\n<figure>\r\n<img src=\"CNN操作示意.png\" alt=\"CNN操作示意\" /><figcaption aria-hidden=\"true\">CNN操作示意</figcaption>\r\n</figure>\r\n<h2 id=\"感受野的大小\">感受野的大小</h2>\r\n<p>从原始输入图像开始，如果第一层卷积的卷积核是<span class=\"math inline\">\\(3\\times 3\\)</span>大小，那么第一层卷积层的输出特征图上，左上角第一个像素点连接了原始输入图像左上角的<span class=\"math inline\">\\(3\\times3\\)</span>大小的范围（这个范围被称作这个像素对于输入图像的感受野），即这个特征图上，每个像素对于输入图像的的感受野大小是<span class=\"math inline\">\\(3\\times 3\\)</span>，如果第二层卷积也是<span class=\"math inline\">\\(3\\times 3\\)</span>大小的卷积核，那么第二层输出的特征图上的一个像素对于第一层输出的特征图的感受野是<span class=\"math inline\">\\(3\\times 3\\)</span>，但是对应到原图上的感受野大小和范围就要依赖于第一层卷积层的卷积步长参数了，如果第一层卷积的卷积步长为1，那么可以得出第二层输出的特征图上，一个像素对应于原始输入图像上<span class=\"math inline\">\\(5\\times 5\\)</span>的区域，即这层特征图上每个像素对于输入图像的感受野大小是<span class=\"math inline\">\\(5\\times 5\\)</span>。</p>\r\n<p>对于特征图上的一个像素，其感受野之外的区域不影响该像素的值，因此对于该像素，不能从其值上获取其感受野之外的信息。</p>\r\n<h2 id=\"感受野的步长\">感受野的步长</h2>\r\n<p>从原始输入图像开始，如果第一层卷积的卷积核是<span class=\"math inline\">\\(3\\times 3\\)</span>大小，卷积步长是1，那么在第一层卷积层的输出特征图上坐标为<span class=\"math inline\">\\((0, 0)\\)</span>的第一个像素的感受野是输入图像的左上角从<span class=\"math inline\">\\((0,0)\\)</span>坐标开始的<span class=\"math inline\">\\(3\\times 3\\)</span>的区域，而第一层卷积层的输出特征图上坐标为<span class=\"math inline\">\\((0, 1)\\)</span>的像素则对应输入图像的左上角从<span class=\"math inline\">\\((0,1)\\)</span>坐标开始的<span class=\"math inline\">\\(3\\times 3\\)</span>的区域，这两个区域之间的步长为1，也称为第一层卷积层的输出特征图对于输入图像的感受野步长为1。</p>\r\n<h2 id=\"感受野的padding\">感受野的padding</h2>\r\n<p>感受野的padding和卷积层的padding是类似的，指的就是感受野分布在对应特征图上时，超出特征图的范围大小，这个是由于卷积层引入padding导致的。</p>\r\n<h2 id=\"感受野的理解\">感受野的理解</h2>\r\n<p>感受野可以理解为将信息经过的层合并成一个大核卷积层（虽然一个大卷积层不能完全实现多层堆叠的计算，例如其中包含了激活、池化等层，但是在感受野大小、步长、padding方面是等效的），其中卷积核大小就是感受野大小，卷积步长就是感受野步长。</p>\r\n<h1 id=\"感受野大小和步长的计算\">感受野大小和步长的计算</h1>\r\n<p>一个适合口算的方式是从后往前计算，假设现在要计算第<span class=\"math inline\">\\(i+m\\)</span>层的输出特征图对于第<span class=\"math inline\">\\(i\\)</span>层特征图的感受野大小和步长，那么首先看第<span class=\"math inline\">\\(i+m\\)</span>层的输出特征图对于第<span class=\"math inline\">\\(i+m-1\\)</span>层的输出特征图的感受野大小和步长。</p>\r\n<p>这里只考虑卷积核大小和步长在长宽上相同的情况，否则的话，需要按照长宽分别计算，记第<span class=\"math inline\">\\(n\\)</span>层输出特征图对于第<span class=\"math inline\">\\(m\\)</span>层输出特征图的感受野大小为<span class=\"math inline\">\\(F^n_m\\)</span>，感受野步长为<span class=\"math inline\">\\(S^n_m\\)</span>，感受野padding记为<span class=\"math inline\">\\(P^n_m\\)</span>，如果第<span class=\"math inline\">\\(i\\)</span>层是卷积层，则记其卷积核大小为<span class=\"math inline\">\\(k_i\\)</span>，记其卷积步长为<span class=\"math inline\">\\(s_i\\)</span>，记padding大小为<span class=\"math inline\">\\(p_i\\)</span>，如果第<span class=\"math inline\">\\(i\\)</span>层是池化层，则同样记其池化核大小为<span class=\"math inline\">\\(k_i\\)</span>，记其池化步长为<span class=\"math inline\">\\(s_i\\)</span>，记padding大小为<span class=\"math inline\">\\(p_i\\)</span>，那么不论对池化层还是卷积层，很显然有： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\nF^n_n &amp;= 1\\\\\r\nS^n_n &amp;= 1\\\\\r\nP^n_n &amp;= 0\\\\\r\nF^n_m &amp;= (F^n_{m+1} - 1) \\times s_{m+1} + k_{m+1} = F^n_{m+1} \\times s_{m+1} - s_{m+1} + k_{m+1}\\\\\r\nS^n_m &amp;= S^n_{m+1} \\times s_{m+1}\\\\\r\nP^n_m &amp;= P^n_{m+1} * s_{m+1} + p_{m+1}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>VGG模型的配置如图所示： <img src=\"VGG模型配置.png\" alt=\"VGG模型配置\" /></p>\r\n<p>这里以VGG-16为例，其中的卷积层全是卷积核大小为3，卷积步长为1，padding为1，其中的池化层全是池化核大小为2，池化步长为2，padding为0，记第四个maxPooling之前的特征图即Conv4-3为第13层的输出，记输入的<span class=\"math inline\">\\(224\\times 224\\)</span>大小的图片为第0层的输出。</p>\r\n这里尝试计算<span class=\"math inline\">\\(F^{13}_0\\)</span>, <span class=\"math inline\">\\(S^{13}_0\\)</span>, <span class=\"math inline\">\\(P^{13}_0\\)</span>: $$\r\n<span class=\"math display\">\\[\\begin{aligned}\r\nF^{13}_0 &amp;= F^{13}_1 \\times s_1 - s_1 + k_1\\\\\r\n&amp;= F^{13}_1 \\times 1 - 1 + 3\\\\\r\n&amp;= (F^{13}_2 \\times s_2 - s_2 + k_2) \\times 1 - 1 + 3\\\\\r\n&amp;= (F^{13}_2 \\times 1  - 1 + 3) \\times 1 - 1 + 3\\\\\r\n&amp;\\cdots\\\\\r\n&amp;= 1 \\times 1 - 1 + 3) \\times 1 - 1 + 3) \\times 1 - 1 + 3) \\times 2 - 2 + 2) \\times 1 - 1 + 3) \\times 1 - 1 + 3) \\times 1 - 1 + 3) \\times 2 - 2 + 2) \\times 1 - 1 + 3) \\times 1 - 1 + 3) \\times 2 - 2 + 2) \\times 1 - 1 + 3) \\times 1 - 1 + 3\\\\\r\n&amp;= 1 + 2 + 2 + 2)\\times 2 + 2 + 2 + 2)\\times 2 + 2 + 2)\\times 2 + 2 + 2\r\n&amp;= 92\\\\\r\n\r\nS^{13}_0 &amp;= S^{13}_1 \\times s_1\\\\\r\n&amp;= S^{13}_2 \\times s_2 \\times s_1\\\\\r\n&amp;= \\cdots\\\\\r\n&amp;= 1 \\times 1\\times 1\\times 1\\times 2\\times 1\\times 1\\times 1\\times 2\\times 1\\times 1\\times 2\\times 1\\times 1\\\\\r\n&amp;= 8\\\\\r\n\r\nP^{13}_0 &amp;= P^{13}_1 * s_1 + p_1\\\\\r\n&amp;=P^{13}_2 * s_2 + p_2) * s_1 + p_1\\\\\r\n&amp;= \\cdots\\\\\r\n&amp;=0 * 1 + 1) * 1 + 1) * 1 + 1) * 2 + 0) * 1 + 1) * 1 + 1) * 1 + 1) * 2 + 0) * 1 + 1) * 1 + 1) * 2 + 0) * 1 + 1) * 1 + 1\\\\\r\n&amp;= 42\r\n\\end{aligned}\\]</span>\r\n<p>$$ 即Conv4-3的输出特征图上，每个像素的感受野在原始输入图片上的大小是92，padding 42，步长为8的分布，因为Conv4-3的输出特征图大小是<span class=\"math inline\">\\(28\\times 28\\)</span>，所以一共有<span class=\"math inline\">\\(28\\times 28\\)</span>个感受野，按照这个排列，其一行所占的像素可以计算为<span class=\"math inline\">\\(92 + (28 - 1) \\times 8 = 308\\)</span>正好等于<span class=\"math inline\">\\(224 + 2 \\times 42\\)</span>。</p>\r\n<p>对于ResNet、Inception这种包含多个路径模型来说，其某一层的感受野，应该取其中感受野最大的路径来计算。</p>\r\n<p>空洞卷积（Dialated Convolution）有助于增加感受野，可以将其卷积核看做一个填充了0值的大卷积核。</p>\r\n<h1 id=\"有效感受野efficient-receptive-filed\">有效感受野（Efficient Receptive Filed）</h1>\r\n<p>一个特征图上某个像素的感受野虽然可以反应哪些位置可以影响该像素的值，但是在其感受野中，不同区域的影响程度是不一样的，论文《Understanding the Effective Receptive Field in Deep Convolutional Neural Networks》中讨论了这个问题，论文中指出，如果卷积核全都初始化为相同的值，那么感受野中的原图像素对于特征像素的影响大小成正态分布，这取决于中间的卷积核权重，并在模型训练前后会发生变化，如图所示，其中越亮的区域代表有越大的影响。</p>\r\n<figure>\r\n<img src=\"感受野权重示意图.png\" alt=\"感受野权重示意图\" /><figcaption aria-hidden=\"true\">感受野权重示意图</figcaption>\r\n</figure>\r\n<p>基于这个现象，论文中提出了有效感受野的概念，即真正能影响特征图像素值的范围只占理论感受野的一小部分。</p>\r\n<p>从上图中可以看出，即使在训练完成之后，有效感受野也不能铺满整个理论感受野范围，因此在设计模型时，还需要让感受野超出目标大小，否则很难提取到完整的目标信息。</p>\r\n<p>根据论文中给出的示意图，不同的初始化方式对有效感受野有一些影响，随着层数加深，理论感受野增大，有效感受野占理论感受野的比重越来越小。</p>\r\n<figure>\r\n<img src=\"有效感受野示意.png\" alt=\"有效感受野示意\" /><figcaption aria-hidden=\"true\">有效感受野示意</figcaption>\r\n</figure>\r\n<p>为了使得感受野中的影响分布更加均匀，论文中提出加大卷积核边缘权重，减少卷积核中心权重的初始化方式。</p>\r\n"},{"title":"CNN权重初始化","date":"2020-05-11T08:00:57.000Z","mathjax":true,"_content":"\n# 权重初始化\n\n## 权重初始化的意义\nCNN在训练开始之前，首先需要进行权重的初始化，如果初始化搞得好，例如直接初始化到最优点，那么训练步骤就可以省略了，当然这个概率基本为零，但是初始化如果做得好，让避免梯度爆炸或者消失，使梯度更加均匀，跳开一些平坦区域或者局部最优点也是可能的，这就是为什么神经网络如果不固定参数的初始化，每次训练得到的结果可能差别很大的原因。\n\n初始化这里有个要注意的是：所有权重不能同时初始化为0。因为CNN中的反向传播如下所示，具体推导和含义可以查看我的另一篇文章{% post_link CNN的反向传播 CNN的反向传播 %}。如果全都初始化为0，那么所有的$\\delta_i$全是0，从下面的式子可以看出，$\\frac{\\partial E}{\\partial w_i}$和$\\frac{\\partial E}{\\partial b_i}$恒为零，也就导致会导致学习过程中权重完全不变，那么学习过程也就失去了意义。\n$$\n\\begin{aligned}\n    \\delta_l = \\frac{\\partial E}{\\partial z_l} &= diag(\\sigma_l'(z_l))E'\\\\\n    \\delta_{i-1} &= \\begin{cases}\n    diag(\\sigma_i'(z_{i-1}))w_i^T\\delta_i &如果第i层是全连接层\\\\\n    padpad(\\delta_i) \\star rot_{180}trans(w_i) \\odot\\sigma'(z_{i-1}) &如果第i层是卷积层\\\\\n    M\\odot upsample(\\delta_i) &如果第i层是池化层\n    \\end{cases}\\\\\n    \\frac{\\partial E}{\\partial w_i} &= \\begin{cases}\n    \\delta_il_{i-1}^T &如果第i层是全连接层\\\\\n    padpad(\\delta_i) \\star rot_{180}trans(l_{i-1}) &如果第i层是卷积层\\\\\n    \\end{cases}\\\\\n    \\frac{\\partial E}{\\partial b_i} &= \\delta_i,\\ i=1,2,...,l\n\\end{aligned}\n$$\n\n另外，初始化的选择还和激活函数有关，如果使用ReLU激活函数，那么如果初始化权重让ReLU函数的输入全是小于0的，那得到的输出也是全0，并且不能进行有效的更新（ReLU死节点），如果使用Sigmoid激活函数，那么初始化权重如果使得Sigmoid函数的输入值偏离原点太远，也会导致梯度非常小（Sigmoid函数饱和）。\n\n一般在初始化时，选择随机初始化方式，目前主流的初始化方式有：随机分布的初始化、Xavier、MSRA等。\n\n## 随机分布的初始化\n一般选择Gaussian分布或者均匀分布，这个没啥技术含量，主要是让均值和方差在合适的位置就好（参考前面对于不同激活函数的初始化要求的分析）。\n\n## Xavier初始化\nXavier初始化来源于论文《Understanding the difficulty of training deep feedforward neural networks》，其中以tanh为激活函数，对权重初始化的方差提出了一些要求。\n\n首先从正向传播过程来看：\n\n如果第$i$层输入的方差是$\\sigma^{i-1}_x$，权重初始化的方差$\\sigma^i_w$，如果是卷积层，那么对于输出特征图的一个像素$z^{i+1}_j = \\sum\\limits_{k=1}^n x^{i-1}_k \\times w^i_k$，其中$n = channel_{in} \\times k_w \\times k_h$，即输入通道数乘以卷积核面积。因为$x_i$，$w_i$相互独立（方差的和等于和的方差，方差的乘积等于乘积的方差），因此方差$\\sigma^i_z = n^i \\times \\sigma^i_w \\times \\sigma^{i-1}_x$，因为tanh激活函数在原点附近区域可以近似为$f(x) = x$这样一个函数，所以论文中假设$x^i = z^i$，所以根据上面的分析，$\\sigma^i_x = \\sigma^i_z = n^i \\times \\sigma^i_w \\times \\sigma^{i-1}_x$，进一步可以得到：\n\n$$\n\\begin{aligned}\n    \\sigma^i_x = \\sigma^0_x\\prod\\limits_{k=1}^i(n^k \\times \\sigma^k_w)\n\\end{aligned}\n$$\n\n其中$\\sigma^0_x$为模型的输入。从这个结果看，如果每一层的$\\sigma^k_w$过大，则会引起深层的方差爆炸，如果过小，又会引起深层的方差消失。如果想要保证每一层的输入和输出的方差基本一致，需要$\\sigma^k_w = \\frac{1}{n^k}$。\n\n其次从反向传播过程来看:\n\n根据最前面列出的CNN反向传播过程，如果假设激活函数大致等价于$f(x) = x$，那么激活函数的导数$\\sigma'(z_{i-1}) = 1$，因此同样有类似的结论$Var(\\frac{\\partial E}{\\partial w_i}) = Var(\\frac{\\partial E}{\\partial w_j}) \\prod\\limits_{k=i}^{j-1}(m^{k+1} \\sigma^k_w)$，其中$m^{k+1}$表示第$k+1$层的每个输入值连接的权重个数，如果第$k+1$层是卷积层，则$m = channel_{out} \\times k_w \\times k_h$。\n\n同样，如果要保证反向传播时梯度方差基本不变，则需要满足$\\sigma^k_w = \\frac{1}{m^{k+1}}$\n\n因此根据以上前向传播和反向传播过程的分析，论文均衡了两个分析的结果，提出了Xavier初始化：\n$$\n\\sigma^k_w = \\frac{2}{m^{k+1} + n^k}\n$$\n\n如果使用均匀分布初始化，因为要满足权重分布在0附近的假设（否则上面的假设激活函数等价于$f(x) = x$不成立），我们选择$[-a, a]$范围的均匀分布，其方差$\\frac{a^2}{3}$需要满足：\n\n$$\n\\frac{a^2}{3} = \\sigma^k_w = \\frac{2}{m^{k+1} + n^k}\n$$\n\n可以得出$a = \\sqrt{\\frac{6}{m^{k+1} + n^k}}$，因此Xavier初始化建议使用$[-\\sqrt{\\frac{6}{m^{k+1} + n^k}}, \\sqrt{\\frac{6}{m^{k+1} + n^k}}]$范围的均匀分布，其中$m^{k+1}$表示第$k+1$层的每个输入值的连接个数，$n^k$表示第$k$层每个输出值的连接个数。\n\n## MSRA初始化（一些深度学习框架中称为he_normal）\n在论文 《Delving Deep into Rectifiers:Surpassing Human-Level Performance on ImageNet Classification》中提出了PReLU激活函数和MSRA初始化方法。\n\nMSRA初始化和Xavier初始化的动机类似，不过MSRA初始化是对于ReLU（或者PReLu）激活函数在原点附近进行分析，且使用Gaussian分布。\n\n因为是ReLU激活函数，和上面Xavier初始化的分析类似，从正向传播的角度可以得到$\\sigma^k_w = \\frac{2}{n^k}$，从反向传播的角度可以得到$\\sigma^k_w = \\frac{2}{m^{k+1}}$。（这里的分析使用了x为0均值的假设）\n\n如果是PReLU，则变成$\\sigma^k_w = \\frac{2}{(a^2 + 1)n^k}$和$\\sigma^k_w = \\frac{2}{(a^2 + 1)m^{k+1}}$，$a$为PReLU负区间的斜率。\n\n因此MSRA初始化方法建议使用均值为0，方差为$\\sigma^k_w = \\frac{2}{(a^2 + 1)n^k}$或者$\\sigma^k_w = \\frac{2}{(a^2 + 1)m^{k+1}}$的Gaussian分布初始化。这里没有像Xavier初始化那样使用两个推理结果的折中，论文中说两个初始化方式差不多，都能够使模型收敛。\n","source":"_posts/学习笔记/CNN权重初始化.md","raw":"---\ntitle: CNN权重初始化\ndate: 2020-05-11 16:00:57\ntags: [深度学习]\nmathjax: true\n---\n\n# 权重初始化\n\n## 权重初始化的意义\nCNN在训练开始之前，首先需要进行权重的初始化，如果初始化搞得好，例如直接初始化到最优点，那么训练步骤就可以省略了，当然这个概率基本为零，但是初始化如果做得好，让避免梯度爆炸或者消失，使梯度更加均匀，跳开一些平坦区域或者局部最优点也是可能的，这就是为什么神经网络如果不固定参数的初始化，每次训练得到的结果可能差别很大的原因。\n\n初始化这里有个要注意的是：所有权重不能同时初始化为0。因为CNN中的反向传播如下所示，具体推导和含义可以查看我的另一篇文章{% post_link CNN的反向传播 CNN的反向传播 %}。如果全都初始化为0，那么所有的$\\delta_i$全是0，从下面的式子可以看出，$\\frac{\\partial E}{\\partial w_i}$和$\\frac{\\partial E}{\\partial b_i}$恒为零，也就导致会导致学习过程中权重完全不变，那么学习过程也就失去了意义。\n$$\n\\begin{aligned}\n    \\delta_l = \\frac{\\partial E}{\\partial z_l} &= diag(\\sigma_l'(z_l))E'\\\\\n    \\delta_{i-1} &= \\begin{cases}\n    diag(\\sigma_i'(z_{i-1}))w_i^T\\delta_i &如果第i层是全连接层\\\\\n    padpad(\\delta_i) \\star rot_{180}trans(w_i) \\odot\\sigma'(z_{i-1}) &如果第i层是卷积层\\\\\n    M\\odot upsample(\\delta_i) &如果第i层是池化层\n    \\end{cases}\\\\\n    \\frac{\\partial E}{\\partial w_i} &= \\begin{cases}\n    \\delta_il_{i-1}^T &如果第i层是全连接层\\\\\n    padpad(\\delta_i) \\star rot_{180}trans(l_{i-1}) &如果第i层是卷积层\\\\\n    \\end{cases}\\\\\n    \\frac{\\partial E}{\\partial b_i} &= \\delta_i,\\ i=1,2,...,l\n\\end{aligned}\n$$\n\n另外，初始化的选择还和激活函数有关，如果使用ReLU激活函数，那么如果初始化权重让ReLU函数的输入全是小于0的，那得到的输出也是全0，并且不能进行有效的更新（ReLU死节点），如果使用Sigmoid激活函数，那么初始化权重如果使得Sigmoid函数的输入值偏离原点太远，也会导致梯度非常小（Sigmoid函数饱和）。\n\n一般在初始化时，选择随机初始化方式，目前主流的初始化方式有：随机分布的初始化、Xavier、MSRA等。\n\n## 随机分布的初始化\n一般选择Gaussian分布或者均匀分布，这个没啥技术含量，主要是让均值和方差在合适的位置就好（参考前面对于不同激活函数的初始化要求的分析）。\n\n## Xavier初始化\nXavier初始化来源于论文《Understanding the difficulty of training deep feedforward neural networks》，其中以tanh为激活函数，对权重初始化的方差提出了一些要求。\n\n首先从正向传播过程来看：\n\n如果第$i$层输入的方差是$\\sigma^{i-1}_x$，权重初始化的方差$\\sigma^i_w$，如果是卷积层，那么对于输出特征图的一个像素$z^{i+1}_j = \\sum\\limits_{k=1}^n x^{i-1}_k \\times w^i_k$，其中$n = channel_{in} \\times k_w \\times k_h$，即输入通道数乘以卷积核面积。因为$x_i$，$w_i$相互独立（方差的和等于和的方差，方差的乘积等于乘积的方差），因此方差$\\sigma^i_z = n^i \\times \\sigma^i_w \\times \\sigma^{i-1}_x$，因为tanh激活函数在原点附近区域可以近似为$f(x) = x$这样一个函数，所以论文中假设$x^i = z^i$，所以根据上面的分析，$\\sigma^i_x = \\sigma^i_z = n^i \\times \\sigma^i_w \\times \\sigma^{i-1}_x$，进一步可以得到：\n\n$$\n\\begin{aligned}\n    \\sigma^i_x = \\sigma^0_x\\prod\\limits_{k=1}^i(n^k \\times \\sigma^k_w)\n\\end{aligned}\n$$\n\n其中$\\sigma^0_x$为模型的输入。从这个结果看，如果每一层的$\\sigma^k_w$过大，则会引起深层的方差爆炸，如果过小，又会引起深层的方差消失。如果想要保证每一层的输入和输出的方差基本一致，需要$\\sigma^k_w = \\frac{1}{n^k}$。\n\n其次从反向传播过程来看:\n\n根据最前面列出的CNN反向传播过程，如果假设激活函数大致等价于$f(x) = x$，那么激活函数的导数$\\sigma'(z_{i-1}) = 1$，因此同样有类似的结论$Var(\\frac{\\partial E}{\\partial w_i}) = Var(\\frac{\\partial E}{\\partial w_j}) \\prod\\limits_{k=i}^{j-1}(m^{k+1} \\sigma^k_w)$，其中$m^{k+1}$表示第$k+1$层的每个输入值连接的权重个数，如果第$k+1$层是卷积层，则$m = channel_{out} \\times k_w \\times k_h$。\n\n同样，如果要保证反向传播时梯度方差基本不变，则需要满足$\\sigma^k_w = \\frac{1}{m^{k+1}}$\n\n因此根据以上前向传播和反向传播过程的分析，论文均衡了两个分析的结果，提出了Xavier初始化：\n$$\n\\sigma^k_w = \\frac{2}{m^{k+1} + n^k}\n$$\n\n如果使用均匀分布初始化，因为要满足权重分布在0附近的假设（否则上面的假设激活函数等价于$f(x) = x$不成立），我们选择$[-a, a]$范围的均匀分布，其方差$\\frac{a^2}{3}$需要满足：\n\n$$\n\\frac{a^2}{3} = \\sigma^k_w = \\frac{2}{m^{k+1} + n^k}\n$$\n\n可以得出$a = \\sqrt{\\frac{6}{m^{k+1} + n^k}}$，因此Xavier初始化建议使用$[-\\sqrt{\\frac{6}{m^{k+1} + n^k}}, \\sqrt{\\frac{6}{m^{k+1} + n^k}}]$范围的均匀分布，其中$m^{k+1}$表示第$k+1$层的每个输入值的连接个数，$n^k$表示第$k$层每个输出值的连接个数。\n\n## MSRA初始化（一些深度学习框架中称为he_normal）\n在论文 《Delving Deep into Rectifiers:Surpassing Human-Level Performance on ImageNet Classification》中提出了PReLU激活函数和MSRA初始化方法。\n\nMSRA初始化和Xavier初始化的动机类似，不过MSRA初始化是对于ReLU（或者PReLu）激活函数在原点附近进行分析，且使用Gaussian分布。\n\n因为是ReLU激活函数，和上面Xavier初始化的分析类似，从正向传播的角度可以得到$\\sigma^k_w = \\frac{2}{n^k}$，从反向传播的角度可以得到$\\sigma^k_w = \\frac{2}{m^{k+1}}$。（这里的分析使用了x为0均值的假设）\n\n如果是PReLU，则变成$\\sigma^k_w = \\frac{2}{(a^2 + 1)n^k}$和$\\sigma^k_w = \\frac{2}{(a^2 + 1)m^{k+1}}$，$a$为PReLU负区间的斜率。\n\n因此MSRA初始化方法建议使用均值为0，方差为$\\sigma^k_w = \\frac{2}{(a^2 + 1)n^k}$或者$\\sigma^k_w = \\frac{2}{(a^2 + 1)m^{k+1}}$的Gaussian分布初始化。这里没有像Xavier初始化那样使用两个推理结果的折中，论文中说两个初始化方式差不多，都能够使模型收敛。\n","slug":"学习笔记/CNN权重初始化","published":1,"updated":"2020-08-31T06:39:20.737Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckjxpc3rh000444mq5o941xoa","content":"<h1 id=\"权重初始化\">权重初始化</h1>\r\n<h2 id=\"权重初始化的意义\">权重初始化的意义</h2>\r\n<p>CNN在训练开始之前，首先需要进行权重的初始化，如果初始化搞得好，例如直接初始化到最优点，那么训练步骤就可以省略了，当然这个概率基本为零，但是初始化如果做得好，让避免梯度爆炸或者消失，使梯度更加均匀，跳开一些平坦区域或者局部最优点也是可能的，这就是为什么神经网络如果不固定参数的初始化，每次训练得到的结果可能差别很大的原因。</p>\r\n<p>初始化这里有个要注意的是：所有权重不能同时初始化为0。因为CNN中的反向传播如下所示，具体推导和含义可以查看我的另一篇文章<a href=\"#\">Post not found: CNN的反向传播 CNN的反向传播</a>。如果全都初始化为0，那么所有的<span class=\"math inline\">\\(\\delta_i\\)</span>全是0，从下面的式子可以看出，<span class=\"math inline\">\\(\\frac{\\partial E}{\\partial w_i}\\)</span>和<span class=\"math inline\">\\(\\frac{\\partial E}{\\partial b_i}\\)</span>恒为零，也就导致会导致学习过程中权重完全不变，那么学习过程也就失去了意义。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\delta_l = \\frac{\\partial E}{\\partial z_l} &amp;= diag(\\sigma_l&#39;(z_l))E&#39;\\\\\r\n    \\delta_{i-1} &amp;= \\begin{cases}\r\n    diag(\\sigma_i&#39;(z_{i-1}))w_i^T\\delta_i &amp;如果第i层是全连接层\\\\\r\n    padpad(\\delta_i) \\star rot_{180}trans(w_i) \\odot\\sigma&#39;(z_{i-1}) &amp;如果第i层是卷积层\\\\\r\n    M\\odot upsample(\\delta_i) &amp;如果第i层是池化层\r\n    \\end{cases}\\\\\r\n    \\frac{\\partial E}{\\partial w_i} &amp;= \\begin{cases}\r\n    \\delta_il_{i-1}^T &amp;如果第i层是全连接层\\\\\r\n    padpad(\\delta_i) \\star rot_{180}trans(l_{i-1}) &amp;如果第i层是卷积层\\\\\r\n    \\end{cases}\\\\\r\n    \\frac{\\partial E}{\\partial b_i} &amp;= \\delta_i,\\ i=1,2,...,l\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>另外，初始化的选择还和激活函数有关，如果使用ReLU激活函数，那么如果初始化权重让ReLU函数的输入全是小于0的，那得到的输出也是全0，并且不能进行有效的更新（ReLU死节点），如果使用Sigmoid激活函数，那么初始化权重如果使得Sigmoid函数的输入值偏离原点太远，也会导致梯度非常小（Sigmoid函数饱和）。</p>\r\n<p>一般在初始化时，选择随机初始化方式，目前主流的初始化方式有：随机分布的初始化、Xavier、MSRA等。</p>\r\n<h2 id=\"随机分布的初始化\">随机分布的初始化</h2>\r\n<p>一般选择Gaussian分布或者均匀分布，这个没啥技术含量，主要是让均值和方差在合适的位置就好（参考前面对于不同激活函数的初始化要求的分析）。</p>\r\n<h2 id=\"xavier初始化\">Xavier初始化</h2>\r\n<p>Xavier初始化来源于论文《Understanding the difficulty of training deep feedforward neural networks》，其中以tanh为激活函数，对权重初始化的方差提出了一些要求。</p>\r\n<p>首先从正向传播过程来看：</p>\r\n<p>如果第<span class=\"math inline\">\\(i\\)</span>层输入的方差是<span class=\"math inline\">\\(\\sigma^{i-1}_x\\)</span>，权重初始化的方差<span class=\"math inline\">\\(\\sigma^i_w\\)</span>，如果是卷积层，那么对于输出特征图的一个像素<span class=\"math inline\">\\(z^{i+1}_j = \\sum\\limits_{k=1}^n x^{i-1}_k \\times w^i_k\\)</span>，其中<span class=\"math inline\">\\(n = channel_{in} \\times k_w \\times k_h\\)</span>，即输入通道数乘以卷积核面积。因为<span class=\"math inline\">\\(x_i\\)</span>，<span class=\"math inline\">\\(w_i\\)</span>相互独立（方差的和等于和的方差，方差的乘积等于乘积的方差），因此方差<span class=\"math inline\">\\(\\sigma^i_z = n^i \\times \\sigma^i_w \\times \\sigma^{i-1}_x\\)</span>，因为tanh激活函数在原点附近区域可以近似为<span class=\"math inline\">\\(f(x) = x\\)</span>这样一个函数，所以论文中假设<span class=\"math inline\">\\(x^i = z^i\\)</span>，所以根据上面的分析，<span class=\"math inline\">\\(\\sigma^i_x = \\sigma^i_z = n^i \\times \\sigma^i_w \\times \\sigma^{i-1}_x\\)</span>，进一步可以得到：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\sigma^i_x = \\sigma^0_x\\prod\\limits_{k=1}^i(n^k \\times \\sigma^k_w)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>其中<span class=\"math inline\">\\(\\sigma^0_x\\)</span>为模型的输入。从这个结果看，如果每一层的<span class=\"math inline\">\\(\\sigma^k_w\\)</span>过大，则会引起深层的方差爆炸，如果过小，又会引起深层的方差消失。如果想要保证每一层的输入和输出的方差基本一致，需要<span class=\"math inline\">\\(\\sigma^k_w = \\frac{1}{n^k}\\)</span>。</p>\r\n<p>其次从反向传播过程来看:</p>\r\n<p>根据最前面列出的CNN反向传播过程，如果假设激活函数大致等价于<span class=\"math inline\">\\(f(x) = x\\)</span>，那么激活函数的导数<span class=\"math inline\">\\(\\sigma&#39;(z_{i-1}) = 1\\)</span>，因此同样有类似的结论<span class=\"math inline\">\\(Var(\\frac{\\partial E}{\\partial w_i}) = Var(\\frac{\\partial E}{\\partial w_j}) \\prod\\limits_{k=i}^{j-1}(m^{k+1} \\sigma^k_w)\\)</span>，其中<span class=\"math inline\">\\(m^{k+1}\\)</span>表示第<span class=\"math inline\">\\(k+1\\)</span>层的每个输入值连接的权重个数，如果第<span class=\"math inline\">\\(k+1\\)</span>层是卷积层，则<span class=\"math inline\">\\(m = channel_{out} \\times k_w \\times k_h\\)</span>。</p>\r\n<p>同样，如果要保证反向传播时梯度方差基本不变，则需要满足<span class=\"math inline\">\\(\\sigma^k_w = \\frac{1}{m^{k+1}}\\)</span></p>\r\n<p>因此根据以上前向传播和反向传播过程的分析，论文均衡了两个分析的结果，提出了Xavier初始化： <span class=\"math display\">\\[\r\n\\sigma^k_w = \\frac{2}{m^{k+1} + n^k}\r\n\\]</span></p>\r\n<p>如果使用均匀分布初始化，因为要满足权重分布在0附近的假设（否则上面的假设激活函数等价于<span class=\"math inline\">\\(f(x) = x\\)</span>不成立），我们选择<span class=\"math inline\">\\([-a, a]\\)</span>范围的均匀分布，其方差<span class=\"math inline\">\\(\\frac{a^2}{3}\\)</span>需要满足：</p>\r\n<p><span class=\"math display\">\\[\r\n\\frac{a^2}{3} = \\sigma^k_w = \\frac{2}{m^{k+1} + n^k}\r\n\\]</span></p>\r\n<p>可以得出<span class=\"math inline\">\\(a = \\sqrt{\\frac{6}{m^{k+1} + n^k}}\\)</span>，因此Xavier初始化建议使用<span class=\"math inline\">\\([-\\sqrt{\\frac{6}{m^{k+1} + n^k}}, \\sqrt{\\frac{6}{m^{k+1} + n^k}}]\\)</span>范围的均匀分布，其中<span class=\"math inline\">\\(m^{k+1}\\)</span>表示第<span class=\"math inline\">\\(k+1\\)</span>层的每个输入值的连接个数，<span class=\"math inline\">\\(n^k\\)</span>表示第<span class=\"math inline\">\\(k\\)</span>层每个输出值的连接个数。</p>\r\n<h2 id=\"msra初始化一些深度学习框架中称为he_normal\">MSRA初始化（一些深度学习框架中称为he_normal）</h2>\r\n<p>在论文 《Delving Deep into Rectifiers:Surpassing Human-Level Performance on ImageNet Classification》中提出了PReLU激活函数和MSRA初始化方法。</p>\r\n<p>MSRA初始化和Xavier初始化的动机类似，不过MSRA初始化是对于ReLU（或者PReLu）激活函数在原点附近进行分析，且使用Gaussian分布。</p>\r\n<p>因为是ReLU激活函数，和上面Xavier初始化的分析类似，从正向传播的角度可以得到<span class=\"math inline\">\\(\\sigma^k_w = \\frac{2}{n^k}\\)</span>，从反向传播的角度可以得到<span class=\"math inline\">\\(\\sigma^k_w = \\frac{2}{m^{k+1}}\\)</span>。（这里的分析使用了x为0均值的假设）</p>\r\n<p>如果是PReLU，则变成<span class=\"math inline\">\\(\\sigma^k_w = \\frac{2}{(a^2 + 1)n^k}\\)</span>和<span class=\"math inline\">\\(\\sigma^k_w = \\frac{2}{(a^2 + 1)m^{k+1}}\\)</span>，<span class=\"math inline\">\\(a\\)</span>为PReLU负区间的斜率。</p>\r\n<p>因此MSRA初始化方法建议使用均值为0，方差为<span class=\"math inline\">\\(\\sigma^k_w = \\frac{2}{(a^2 + 1)n^k}\\)</span>或者<span class=\"math inline\">\\(\\sigma^k_w = \\frac{2}{(a^2 + 1)m^{k+1}}\\)</span>的Gaussian分布初始化。这里没有像Xavier初始化那样使用两个推理结果的折中，论文中说两个初始化方式差不多，都能够使模型收敛。</p>\r\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"权重初始化\">权重初始化</h1>\r\n<h2 id=\"权重初始化的意义\">权重初始化的意义</h2>\r\n<p>CNN在训练开始之前，首先需要进行权重的初始化，如果初始化搞得好，例如直接初始化到最优点，那么训练步骤就可以省略了，当然这个概率基本为零，但是初始化如果做得好，让避免梯度爆炸或者消失，使梯度更加均匀，跳开一些平坦区域或者局部最优点也是可能的，这就是为什么神经网络如果不固定参数的初始化，每次训练得到的结果可能差别很大的原因。</p>\r\n<p>初始化这里有个要注意的是：所有权重不能同时初始化为0。因为CNN中的反向传播如下所示，具体推导和含义可以查看我的另一篇文章<a href=\"#\">Post not found: CNN的反向传播 CNN的反向传播</a>。如果全都初始化为0，那么所有的<span class=\"math inline\">\\(\\delta_i\\)</span>全是0，从下面的式子可以看出，<span class=\"math inline\">\\(\\frac{\\partial E}{\\partial w_i}\\)</span>和<span class=\"math inline\">\\(\\frac{\\partial E}{\\partial b_i}\\)</span>恒为零，也就导致会导致学习过程中权重完全不变，那么学习过程也就失去了意义。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\delta_l = \\frac{\\partial E}{\\partial z_l} &amp;= diag(\\sigma_l&#39;(z_l))E&#39;\\\\\r\n    \\delta_{i-1} &amp;= \\begin{cases}\r\n    diag(\\sigma_i&#39;(z_{i-1}))w_i^T\\delta_i &amp;如果第i层是全连接层\\\\\r\n    padpad(\\delta_i) \\star rot_{180}trans(w_i) \\odot\\sigma&#39;(z_{i-1}) &amp;如果第i层是卷积层\\\\\r\n    M\\odot upsample(\\delta_i) &amp;如果第i层是池化层\r\n    \\end{cases}\\\\\r\n    \\frac{\\partial E}{\\partial w_i} &amp;= \\begin{cases}\r\n    \\delta_il_{i-1}^T &amp;如果第i层是全连接层\\\\\r\n    padpad(\\delta_i) \\star rot_{180}trans(l_{i-1}) &amp;如果第i层是卷积层\\\\\r\n    \\end{cases}\\\\\r\n    \\frac{\\partial E}{\\partial b_i} &amp;= \\delta_i,\\ i=1,2,...,l\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>另外，初始化的选择还和激活函数有关，如果使用ReLU激活函数，那么如果初始化权重让ReLU函数的输入全是小于0的，那得到的输出也是全0，并且不能进行有效的更新（ReLU死节点），如果使用Sigmoid激活函数，那么初始化权重如果使得Sigmoid函数的输入值偏离原点太远，也会导致梯度非常小（Sigmoid函数饱和）。</p>\r\n<p>一般在初始化时，选择随机初始化方式，目前主流的初始化方式有：随机分布的初始化、Xavier、MSRA等。</p>\r\n<h2 id=\"随机分布的初始化\">随机分布的初始化</h2>\r\n<p>一般选择Gaussian分布或者均匀分布，这个没啥技术含量，主要是让均值和方差在合适的位置就好（参考前面对于不同激活函数的初始化要求的分析）。</p>\r\n<h2 id=\"xavier初始化\">Xavier初始化</h2>\r\n<p>Xavier初始化来源于论文《Understanding the difficulty of training deep feedforward neural networks》，其中以tanh为激活函数，对权重初始化的方差提出了一些要求。</p>\r\n<p>首先从正向传播过程来看：</p>\r\n<p>如果第<span class=\"math inline\">\\(i\\)</span>层输入的方差是<span class=\"math inline\">\\(\\sigma^{i-1}_x\\)</span>，权重初始化的方差<span class=\"math inline\">\\(\\sigma^i_w\\)</span>，如果是卷积层，那么对于输出特征图的一个像素<span class=\"math inline\">\\(z^{i+1}_j = \\sum\\limits_{k=1}^n x^{i-1}_k \\times w^i_k\\)</span>，其中<span class=\"math inline\">\\(n = channel_{in} \\times k_w \\times k_h\\)</span>，即输入通道数乘以卷积核面积。因为<span class=\"math inline\">\\(x_i\\)</span>，<span class=\"math inline\">\\(w_i\\)</span>相互独立（方差的和等于和的方差，方差的乘积等于乘积的方差），因此方差<span class=\"math inline\">\\(\\sigma^i_z = n^i \\times \\sigma^i_w \\times \\sigma^{i-1}_x\\)</span>，因为tanh激活函数在原点附近区域可以近似为<span class=\"math inline\">\\(f(x) = x\\)</span>这样一个函数，所以论文中假设<span class=\"math inline\">\\(x^i = z^i\\)</span>，所以根据上面的分析，<span class=\"math inline\">\\(\\sigma^i_x = \\sigma^i_z = n^i \\times \\sigma^i_w \\times \\sigma^{i-1}_x\\)</span>，进一步可以得到：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\sigma^i_x = \\sigma^0_x\\prod\\limits_{k=1}^i(n^k \\times \\sigma^k_w)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>其中<span class=\"math inline\">\\(\\sigma^0_x\\)</span>为模型的输入。从这个结果看，如果每一层的<span class=\"math inline\">\\(\\sigma^k_w\\)</span>过大，则会引起深层的方差爆炸，如果过小，又会引起深层的方差消失。如果想要保证每一层的输入和输出的方差基本一致，需要<span class=\"math inline\">\\(\\sigma^k_w = \\frac{1}{n^k}\\)</span>。</p>\r\n<p>其次从反向传播过程来看:</p>\r\n<p>根据最前面列出的CNN反向传播过程，如果假设激活函数大致等价于<span class=\"math inline\">\\(f(x) = x\\)</span>，那么激活函数的导数<span class=\"math inline\">\\(\\sigma&#39;(z_{i-1}) = 1\\)</span>，因此同样有类似的结论<span class=\"math inline\">\\(Var(\\frac{\\partial E}{\\partial w_i}) = Var(\\frac{\\partial E}{\\partial w_j}) \\prod\\limits_{k=i}^{j-1}(m^{k+1} \\sigma^k_w)\\)</span>，其中<span class=\"math inline\">\\(m^{k+1}\\)</span>表示第<span class=\"math inline\">\\(k+1\\)</span>层的每个输入值连接的权重个数，如果第<span class=\"math inline\">\\(k+1\\)</span>层是卷积层，则<span class=\"math inline\">\\(m = channel_{out} \\times k_w \\times k_h\\)</span>。</p>\r\n<p>同样，如果要保证反向传播时梯度方差基本不变，则需要满足<span class=\"math inline\">\\(\\sigma^k_w = \\frac{1}{m^{k+1}}\\)</span></p>\r\n<p>因此根据以上前向传播和反向传播过程的分析，论文均衡了两个分析的结果，提出了Xavier初始化： <span class=\"math display\">\\[\r\n\\sigma^k_w = \\frac{2}{m^{k+1} + n^k}\r\n\\]</span></p>\r\n<p>如果使用均匀分布初始化，因为要满足权重分布在0附近的假设（否则上面的假设激活函数等价于<span class=\"math inline\">\\(f(x) = x\\)</span>不成立），我们选择<span class=\"math inline\">\\([-a, a]\\)</span>范围的均匀分布，其方差<span class=\"math inline\">\\(\\frac{a^2}{3}\\)</span>需要满足：</p>\r\n<p><span class=\"math display\">\\[\r\n\\frac{a^2}{3} = \\sigma^k_w = \\frac{2}{m^{k+1} + n^k}\r\n\\]</span></p>\r\n<p>可以得出<span class=\"math inline\">\\(a = \\sqrt{\\frac{6}{m^{k+1} + n^k}}\\)</span>，因此Xavier初始化建议使用<span class=\"math inline\">\\([-\\sqrt{\\frac{6}{m^{k+1} + n^k}}, \\sqrt{\\frac{6}{m^{k+1} + n^k}}]\\)</span>范围的均匀分布，其中<span class=\"math inline\">\\(m^{k+1}\\)</span>表示第<span class=\"math inline\">\\(k+1\\)</span>层的每个输入值的连接个数，<span class=\"math inline\">\\(n^k\\)</span>表示第<span class=\"math inline\">\\(k\\)</span>层每个输出值的连接个数。</p>\r\n<h2 id=\"msra初始化一些深度学习框架中称为he_normal\">MSRA初始化（一些深度学习框架中称为he_normal）</h2>\r\n<p>在论文 《Delving Deep into Rectifiers:Surpassing Human-Level Performance on ImageNet Classification》中提出了PReLU激活函数和MSRA初始化方法。</p>\r\n<p>MSRA初始化和Xavier初始化的动机类似，不过MSRA初始化是对于ReLU（或者PReLu）激活函数在原点附近进行分析，且使用Gaussian分布。</p>\r\n<p>因为是ReLU激活函数，和上面Xavier初始化的分析类似，从正向传播的角度可以得到<span class=\"math inline\">\\(\\sigma^k_w = \\frac{2}{n^k}\\)</span>，从反向传播的角度可以得到<span class=\"math inline\">\\(\\sigma^k_w = \\frac{2}{m^{k+1}}\\)</span>。（这里的分析使用了x为0均值的假设）</p>\r\n<p>如果是PReLU，则变成<span class=\"math inline\">\\(\\sigma^k_w = \\frac{2}{(a^2 + 1)n^k}\\)</span>和<span class=\"math inline\">\\(\\sigma^k_w = \\frac{2}{(a^2 + 1)m^{k+1}}\\)</span>，<span class=\"math inline\">\\(a\\)</span>为PReLU负区间的斜率。</p>\r\n<p>因此MSRA初始化方法建议使用均值为0，方差为<span class=\"math inline\">\\(\\sigma^k_w = \\frac{2}{(a^2 + 1)n^k}\\)</span>或者<span class=\"math inline\">\\(\\sigma^k_w = \\frac{2}{(a^2 + 1)m^{k+1}}\\)</span>的Gaussian分布初始化。这里没有像Xavier初始化那样使用两个推理结果的折中，论文中说两个初始化方式差不多，都能够使模型收敛。</p>\r\n"},{"title":"CNN的反向传播","date":"2020-05-07T05:47:45.000Z","mathjax":true,"_content":"在卷积神经网络中，主要由三种层结构构成：卷积层、池化层、全连接层。\n\n# 全连接层的反向传播\n全连接层的反向传播比较简单，使用单纯的BP算法即可，这里先来复习一下全连接层的BP算法：\n\n对于$l$层神经网络,输入$x \\in R^n$，标签$y \\in R^c$，第$i$层权重表示为$w_i \\in R^{O_i \\times I_i}, I_1 = n，O_l = c$，第$i$层偏移表示为$b_i \\in R^{O_i}$，第$i$层激活函数表示为$\\sigma_i$，这一般是个逐元素函数，第$i$层输入即第$i-1$层的输出，表示为$l_{i-1}$，其中$l_0 = x, z_i = w_i l_{i-1} + b_i, l_i = \\sigma_i(z_i)$\n\nloss函数记为$E(l_l, y)$，BP算法每次更新$w_i = w_i - \\eta \\frac{\\partial E}{\\partial w_i}$， $b_i = b_i - \\eta \\frac{\\partial E}{\\partial b_i}$，即让参数像梯度最小的方向前进。\n\n首先定义$E$对$l_l$的偏导为$\\frac{\\partial E}{\\partial l_l} = E'$，这个值由loss函数决定。\n因此\n$$\n\\begin{aligned}\ndE &= E'^Tdl_l\\\\\n&=E'^T(\\sigma_l'(z_l) \\odot (dz_l))\\\\\n&=E'^Tdiag(\\sigma_l'(z_l))da_l\\\\\n\\Rightarrow \\frac{\\partial E}{\\partial z_l} &= diag(\\sigma_l'(z_l))E'\n\\end{aligned}\n$$\n\n这里把$\\frac{\\partial E}{\\partial z_l}$记作$\\delta_l$\n\n因为：\n$$\n\\begin{aligned}\n    da_i &= w_idl_{i-1}\\\\\n    &= w_i(\\sigma_i'(z_{i-1}) \\odot (dz_{i-1}))\\\\\n    &=w_idiag(\\sigma_{i-1}'(z_{i-1}))dz_{i-1}\\\\\n    \\Rightarrow \\frac{\\partial z_i}{\\partial z_{i-1}} &= diag(\\sigma_{i-1}'(z_{i-1}))w_i^T\\\\\n\\end{aligned}\n$$\n\n所以定义:\n$$\n\\begin{aligned}\n    \\delta_i &= \\frac{\\partial E}{\\partial z_i},\\ i=1,2,...,l-1\\\\\n    \\Rightarrow \\delta_{i-1} &= \\frac{\\partial z_i}{\\partial z_{i-1}}\\frac{\\partial E}{\\partial z_i},\\ i=2,...,l\\\\\n    &= diag(\\sigma_{i-1}'(z_{i-1}))w_i^T\\delta_i\\\\\n\\end{aligned}\n$$\n\n现在再来考虑$E$对$w_{l-k}$的导数：\n$$\n\\begin{aligned}\n    dE &= \\frac{\\partial E}{\\partial z_{l-k}}^Tdz_{l-k}\\\\\n    &= \\delta_{l-k}^T(dw_{l-k}l_{l-k-1} + db_{l-k})\\\\\n    &= tr(\\delta_{l-k}^Tdw_{l-k}l_{l-k-1} + \\delta_{l-k}^Tdb_{l-k})\\\\\n    &= tr(l_{l-k-1}\\delta_{l-k}^Tdw_{l-k} + \\delta_{l-k}^Tdb_{l-k})\\\\\n    \\Rightarrow \\frac{\\partial E}{\\partial w_{l-k}} &= \\delta_{l-k}l_{l-k-1}^T\\\\\n    \\Rightarrow \\frac{\\partial E}{\\partial b_{l-k}} &= \\delta_{l-k}\n\\end{aligned}\n$$\n这里的变换属于标量对矩阵求导$d f = tr((\\frac{\\partial f}{\\partial X}) ^ T dX)$，且用到了迹的一个性质：$tr(A B) = tr(B A)$，其中$A$和$B^T$大小相同\n\n全连接层的BP算法看起来很复杂，其实非常简单，只要使用以下几个等式即可求出任一层的权重和偏置的导数：\n$$\n\\begin{aligned}\n    \\delta_l = \\frac{\\partial E}{\\partial z_l} &= diag(\\sigma_l'(z_l))E'\\\\\n    \\delta_{i} = \\frac{\\partial E}{\\partial z_i} &= diag(\\sigma_i'(z_i))w_{i+1}^T\\delta_{i+1},\\ i=1,2,...,l-1\\\\\n    \\frac{\\partial E}{\\partial w_i} &= \\delta_il_{i-1}^T,\\ i=1,2,...,l\\\\\n    \\frac{\\partial E}{\\partial b_i} &= \\delta_i,\\ i=1,2,...,l\n\\end{aligned}\n$$\n\n# 卷积层的反向传播\n\n对于卷积层而言，和前面的定义类似，只不过其表达式变为：$l_0 = x, z_i =l_{i-1} \\star w_i + b_i, l_i = \\sigma_i(z_i)$，其中$\\star$表示卷积操作，$w_i \\in R^{H\\times W\\times C_{i}\\times C_{i-1}}$，$x\\in R^{H\\times W\\times C_0}$，$z_i,l_i \\in R^{H\\times W\\times C_i}$，$b_i \\in R^{C_i}$。\n\n按照之前全连接层的反向传播套路，自然也希望首先定义\n$$\n\\begin{aligned}\n    \\delta_l &= \\frac{\\partial E}{\\partial z_l} = diag(\\sigma_l'(z_l))E'\\\\\n    \\delta_{i} &= \\frac{\\partial E}{\\partial z_i}\n\\end{aligned}\n$$\n那么第一个问题是如何根据$\\delta_{i+1}$求出$\\delta_{i}$。\n\n因为：\n$$\n\\begin{aligned}\n    \\delta_{i} &= \\frac{\\partial E}{\\partial z_i}\\\\\n    &= \\frac{\\partial z_{i+1}}{\\partial z_i}\\frac{\\partial E}{\\partial z_{i+1}}\\\\\n    &= \\frac{\\partial z_{i+1}}{\\partial z_i}\\delta_{i+1}\n\\end{aligned}\n$$\n\n所以问题在于如何求$\\frac{\\partial z_{i+1}}{\\partial z_i}$，如果$i+1$层是个卷积层，那么$z_{i+1} = \\sigma_i(z_i) \\star w_{i+1} + b_{i+1}$，这里想使用一般的方法求解$\\frac{\\partial z_{i+1}}{\\partial z_i}$是很困难的，下面直接对$\\delta_{i}$给出一般化的表达。\n\n\n卷积操作的具体分析需要画图讨论，这里不再赘述，定义一个卷积操作，$O = I \\star W$，其中卷积核$W$的大小为$H_k\\times W_k\\times C_o\\times C_i$，输入特征图的大小为$H\\times W\\times C_i$，卷积操作的padding大小为$P_H,P_W$，stride大小为$S_H,S_W$，则输出特征图的大小为$\\lceil\\frac{H + 2P_H - H_k + 1}{S_H}\\rceil\\times \\lceil\\frac{W + 2P_W - H_k + 1}{S_W}\\rceil\\times C_o$，注意这里$0 \\le P_H \\le H_k - 1$，$0 \\le P_W \\le W_k - 1$，否则过多的padding没有意义。\n\n设L为损失函数，令$\\frac{\\partial E}{\\partial O}$记作$\\delta_O$，其大小和$O$相同。\n$$\n\\begin{aligned}\n    \\frac{\\partial E}{\\partial I} &= padpad(\\delta_O) \\star rot_{180}trans(W)\n\\end{aligned}\n$$\n其中$padpad(\\delta_O)$是将$\\delta_O$进行外部padding和内部padding得到的，其外部padding大小：$P'_H = H_k - P_H - 1$和$P'_W = W_k - P_W - 1$，内部padding大小为$S_H - 1$和$S_W - 1$。\n\n$rot_{180}trans(W)$则首先需要将$W$在$H_k\\times W_k$大小上，旋转180度，然后对其中每个像素（其实每个像素都是个$C_o\\times C_i$大小的矩阵）求转置得到，最终的形状是$H_k\\times W_k\\times C_i\\times C_o$，其中$rot_{180}trans(W)_{i,j,k,l} = W_{H_k-i+1,W_k-j+1,l,k},\\ i=1,2,...,H_k,\\ j=1,2,...,W_k,\\ k=1,2,...,C_o,\\ l=1,2,...,C_i$\n\n则有:\n$$\\delta_{i} = padpad(\\delta_{i+1}) \\star rot_{180}trans(w_{i+1}) \\odot\\sigma_{i}'(z_i)$$\n\n其中$\\odot$表示逐元素乘法，由于这里不是向量和向量乘积，因此不能像之前一样表示成$diag$矩阵乘以向量的形式。\n\n$\\delta_i,\\ i=1,2,...,l$能够求出来了，接下来的问题是如何根据$\\delta_i$求出$\\frac{\\partial E}{\\partial w_i}$。\n\n因为$a_i =l_{i-1} \\star w_i + b_i$，将$a_i$、$l_{i-1}$和$w_i$都旋转180度之后，可以看成$rot180(a_i) = rot180(w_i) \\star rot180(l_{i-1}) + b_i$，这里将$l_{i-1}$看成卷积核，卷积核大小$\\hat{W}_k = W, \\hat{H}_k = H$，Padding变成了$\\hat{P}_H = H - H_k + P_H$和$\\hat{P}_W = W - W_k + P_W$，stride不变，则按照上面的分析结果，对$rot_{180}(\\delta_i)$和$rot_{180}(l_{i-1})$做同样的变换：\n\n$$\n\\begin{aligned}\n    \\frac{\\partial E}{\\partial w_i} &= rot_{180}(padpad(rot_{180}(\\delta_i)) \\star rot_{180}trans(rot_{180}(l_{i-1})))\\\\\n    &= rot_{180}(padpad(rot_{180}(\\delta_i)) \\star trans(l_{i-1}))\\\\\n    &= padpad(\\delta_i) \\star rot_{180}trans(l_{i-1})\n\\end{aligned}\n$$\n\n\n其中$padpad(rot180(\\delta_i))$是将$rot180(\\delta_i)$进行外部padding和内部padding得到的，其外部padding大小：$P'_H = \\hat{H}_k - \\hat{P}_H - 1 = H - H + H_k - P_H - 1 = H_k - P_H - 1$和$P'_W = \\hat{W}_k - \\hat{P}_W - 1 = W_k - P_W - 1$，内部padding大小为$S_H - 1$和$S_W - 1$，可以看出来这里的padpad操作和之前的padpad操作是一样的，两个地方完美等价。\n\n因此如果知道了$\\delta_i$，那么：\n$$\n\\frac{\\partial E}{\\partial w_i} = padpad(\\delta_i) \\star rot_{180}trans(l_{i-1})\n$$\n\n因此如果第$i$层是个卷积层，那么这一层的反向传播核心公式如下：\n$$\n\\begin{aligned}\n    \\delta_{i-1} &= padpad(\\delta_i) \\star rot_{180}trans(w_i) \\odot\\sigma_{i-1}'(z_{i-1})\\\\\n    \\frac{\\partial E}{\\partial w_i} &= padpad(\\delta_i) \\star rot_{180}trans(l_{i-1})\\\\\n    \\frac{\\partial E}{\\partial b_i} &= \\delta_{i}\n\\end{aligned}\n$$\n\n# 池化层的反向传播\n\n池化层没有参数，如果第$i$层是池化层，其反向传播主要需要计算$\\delta_i$和$\\delta_{i-1}$的关系即可。\n\n池化层的$\\delta_i$和$\\delta_{i-1}$的关系取决于池化的类型，如果是最大池化，则需要构造一个非0即1的掩码矩阵，用于标记哪些位置被向前传播，如果是平均池化，则将权重1平均分配到池化核大小的窗口中，以此来构造掩码矩阵\n\n首先上采样$\\delta_i$，使其和$\\delta_{i-1}$的大小相同，然后根据池化类型，构造掩码矩阵$M$，则$\\delta_{i-1} = M\\odot upsample(\\delta_i)$\n\n# CNN反向传播的总结\n\n对于共$l$层的CNN，如果第$i$层是全连接层，则其权重表示为$w_i \\in R^{O_i \\times I_i},O_l = c$，第$i$层偏移表示为$b_i \\in R^{O_i}$，第$i$层激活函数表示为$\\sigma_i$，这一般是个逐元素函数，第$i$层输入即第$i-1$层的输出，表示为$l_{i-1}$，其中$z_i = w_i l_{i-1} + b_i, l_i = \\sigma_i(z_i)$\n\n如果第$i$层是卷积层，则其表达式变为：$z_i =l_{i-1} \\star w_i + b_i, l_i = \\sigma_i(z_i)$，其中$\\star$表示卷积操作，$w_i \\in R^{H\\times W\\times C_{i}\\times C_{i-1}}$，$x\\in R^{H\\times W\\times C_0}$，$z_i,l_i \\in R^{H\\times W\\times C_i}$，$b_i \\in R^{C_i}$。\n\nloss函数记为$E(l_l, y)$。\n\n这里默认最后一层即第$l$层为全连接层。\n$$\n\\begin{aligned}\n    \\delta_l = \\frac{\\partial E}{\\partial z_l} &= diag(\\sigma_l'(z_l))E'\\\\\n    \\delta_{i-1} &= \\begin{cases}\n    diag(\\sigma_{i-1}'(z_{i-1}))w_i^T\\delta_i &如果第i层是全连接层\\\\\n    padpad(\\delta_i) \\star rot_{180}trans(w_i) \\odot\\sigma_{i-1}'(z_{i-1}) &如果第i层是卷积层\\\\\n    M\\odot upsample(\\delta_i) &如果第i层是池化层\n    \\end{cases}\\\\\n    \\frac{\\partial E}{\\partial w_i} &= \\begin{cases}\n    \\delta_il_{i-1}^T &如果第i层是全连接层\\\\\n    padpad(\\delta_i) \\star rot_{180}trans(l_{i-1}) &如果第i层是卷积层\\\\\n    \\end{cases}\\\\\n    \\frac{\\partial E}{\\partial b_i} &= \\delta_i,\\ i=1,2,...,l\n\\end{aligned}\n$$\n\n其中$padpad$表示内部padding（列和行中每两个元素之间）：$(S_H - 1, S_W - 1)$和外部padding：$(H_k - P_H - 1, W_k - P_W - 1)$，$rot_{180}$表示在空间维度(H,W)上旋转180度，$trans$表示在通道维度上转置，$M$表示池化操作的掩码。\n\n","source":"_posts/学习笔记/CNN的反向传播.md","raw":"---\ntitle: CNN的反向传播\ndate: 2020-05-07 13:47:45\ntags: [深度学习]\nmathjax: true\n---\n在卷积神经网络中，主要由三种层结构构成：卷积层、池化层、全连接层。\n\n# 全连接层的反向传播\n全连接层的反向传播比较简单，使用单纯的BP算法即可，这里先来复习一下全连接层的BP算法：\n\n对于$l$层神经网络,输入$x \\in R^n$，标签$y \\in R^c$，第$i$层权重表示为$w_i \\in R^{O_i \\times I_i}, I_1 = n，O_l = c$，第$i$层偏移表示为$b_i \\in R^{O_i}$，第$i$层激活函数表示为$\\sigma_i$，这一般是个逐元素函数，第$i$层输入即第$i-1$层的输出，表示为$l_{i-1}$，其中$l_0 = x, z_i = w_i l_{i-1} + b_i, l_i = \\sigma_i(z_i)$\n\nloss函数记为$E(l_l, y)$，BP算法每次更新$w_i = w_i - \\eta \\frac{\\partial E}{\\partial w_i}$， $b_i = b_i - \\eta \\frac{\\partial E}{\\partial b_i}$，即让参数像梯度最小的方向前进。\n\n首先定义$E$对$l_l$的偏导为$\\frac{\\partial E}{\\partial l_l} = E'$，这个值由loss函数决定。\n因此\n$$\n\\begin{aligned}\ndE &= E'^Tdl_l\\\\\n&=E'^T(\\sigma_l'(z_l) \\odot (dz_l))\\\\\n&=E'^Tdiag(\\sigma_l'(z_l))da_l\\\\\n\\Rightarrow \\frac{\\partial E}{\\partial z_l} &= diag(\\sigma_l'(z_l))E'\n\\end{aligned}\n$$\n\n这里把$\\frac{\\partial E}{\\partial z_l}$记作$\\delta_l$\n\n因为：\n$$\n\\begin{aligned}\n    da_i &= w_idl_{i-1}\\\\\n    &= w_i(\\sigma_i'(z_{i-1}) \\odot (dz_{i-1}))\\\\\n    &=w_idiag(\\sigma_{i-1}'(z_{i-1}))dz_{i-1}\\\\\n    \\Rightarrow \\frac{\\partial z_i}{\\partial z_{i-1}} &= diag(\\sigma_{i-1}'(z_{i-1}))w_i^T\\\\\n\\end{aligned}\n$$\n\n所以定义:\n$$\n\\begin{aligned}\n    \\delta_i &= \\frac{\\partial E}{\\partial z_i},\\ i=1,2,...,l-1\\\\\n    \\Rightarrow \\delta_{i-1} &= \\frac{\\partial z_i}{\\partial z_{i-1}}\\frac{\\partial E}{\\partial z_i},\\ i=2,...,l\\\\\n    &= diag(\\sigma_{i-1}'(z_{i-1}))w_i^T\\delta_i\\\\\n\\end{aligned}\n$$\n\n现在再来考虑$E$对$w_{l-k}$的导数：\n$$\n\\begin{aligned}\n    dE &= \\frac{\\partial E}{\\partial z_{l-k}}^Tdz_{l-k}\\\\\n    &= \\delta_{l-k}^T(dw_{l-k}l_{l-k-1} + db_{l-k})\\\\\n    &= tr(\\delta_{l-k}^Tdw_{l-k}l_{l-k-1} + \\delta_{l-k}^Tdb_{l-k})\\\\\n    &= tr(l_{l-k-1}\\delta_{l-k}^Tdw_{l-k} + \\delta_{l-k}^Tdb_{l-k})\\\\\n    \\Rightarrow \\frac{\\partial E}{\\partial w_{l-k}} &= \\delta_{l-k}l_{l-k-1}^T\\\\\n    \\Rightarrow \\frac{\\partial E}{\\partial b_{l-k}} &= \\delta_{l-k}\n\\end{aligned}\n$$\n这里的变换属于标量对矩阵求导$d f = tr((\\frac{\\partial f}{\\partial X}) ^ T dX)$，且用到了迹的一个性质：$tr(A B) = tr(B A)$，其中$A$和$B^T$大小相同\n\n全连接层的BP算法看起来很复杂，其实非常简单，只要使用以下几个等式即可求出任一层的权重和偏置的导数：\n$$\n\\begin{aligned}\n    \\delta_l = \\frac{\\partial E}{\\partial z_l} &= diag(\\sigma_l'(z_l))E'\\\\\n    \\delta_{i} = \\frac{\\partial E}{\\partial z_i} &= diag(\\sigma_i'(z_i))w_{i+1}^T\\delta_{i+1},\\ i=1,2,...,l-1\\\\\n    \\frac{\\partial E}{\\partial w_i} &= \\delta_il_{i-1}^T,\\ i=1,2,...,l\\\\\n    \\frac{\\partial E}{\\partial b_i} &= \\delta_i,\\ i=1,2,...,l\n\\end{aligned}\n$$\n\n# 卷积层的反向传播\n\n对于卷积层而言，和前面的定义类似，只不过其表达式变为：$l_0 = x, z_i =l_{i-1} \\star w_i + b_i, l_i = \\sigma_i(z_i)$，其中$\\star$表示卷积操作，$w_i \\in R^{H\\times W\\times C_{i}\\times C_{i-1}}$，$x\\in R^{H\\times W\\times C_0}$，$z_i,l_i \\in R^{H\\times W\\times C_i}$，$b_i \\in R^{C_i}$。\n\n按照之前全连接层的反向传播套路，自然也希望首先定义\n$$\n\\begin{aligned}\n    \\delta_l &= \\frac{\\partial E}{\\partial z_l} = diag(\\sigma_l'(z_l))E'\\\\\n    \\delta_{i} &= \\frac{\\partial E}{\\partial z_i}\n\\end{aligned}\n$$\n那么第一个问题是如何根据$\\delta_{i+1}$求出$\\delta_{i}$。\n\n因为：\n$$\n\\begin{aligned}\n    \\delta_{i} &= \\frac{\\partial E}{\\partial z_i}\\\\\n    &= \\frac{\\partial z_{i+1}}{\\partial z_i}\\frac{\\partial E}{\\partial z_{i+1}}\\\\\n    &= \\frac{\\partial z_{i+1}}{\\partial z_i}\\delta_{i+1}\n\\end{aligned}\n$$\n\n所以问题在于如何求$\\frac{\\partial z_{i+1}}{\\partial z_i}$，如果$i+1$层是个卷积层，那么$z_{i+1} = \\sigma_i(z_i) \\star w_{i+1} + b_{i+1}$，这里想使用一般的方法求解$\\frac{\\partial z_{i+1}}{\\partial z_i}$是很困难的，下面直接对$\\delta_{i}$给出一般化的表达。\n\n\n卷积操作的具体分析需要画图讨论，这里不再赘述，定义一个卷积操作，$O = I \\star W$，其中卷积核$W$的大小为$H_k\\times W_k\\times C_o\\times C_i$，输入特征图的大小为$H\\times W\\times C_i$，卷积操作的padding大小为$P_H,P_W$，stride大小为$S_H,S_W$，则输出特征图的大小为$\\lceil\\frac{H + 2P_H - H_k + 1}{S_H}\\rceil\\times \\lceil\\frac{W + 2P_W - H_k + 1}{S_W}\\rceil\\times C_o$，注意这里$0 \\le P_H \\le H_k - 1$，$0 \\le P_W \\le W_k - 1$，否则过多的padding没有意义。\n\n设L为损失函数，令$\\frac{\\partial E}{\\partial O}$记作$\\delta_O$，其大小和$O$相同。\n$$\n\\begin{aligned}\n    \\frac{\\partial E}{\\partial I} &= padpad(\\delta_O) \\star rot_{180}trans(W)\n\\end{aligned}\n$$\n其中$padpad(\\delta_O)$是将$\\delta_O$进行外部padding和内部padding得到的，其外部padding大小：$P'_H = H_k - P_H - 1$和$P'_W = W_k - P_W - 1$，内部padding大小为$S_H - 1$和$S_W - 1$。\n\n$rot_{180}trans(W)$则首先需要将$W$在$H_k\\times W_k$大小上，旋转180度，然后对其中每个像素（其实每个像素都是个$C_o\\times C_i$大小的矩阵）求转置得到，最终的形状是$H_k\\times W_k\\times C_i\\times C_o$，其中$rot_{180}trans(W)_{i,j,k,l} = W_{H_k-i+1,W_k-j+1,l,k},\\ i=1,2,...,H_k,\\ j=1,2,...,W_k,\\ k=1,2,...,C_o,\\ l=1,2,...,C_i$\n\n则有:\n$$\\delta_{i} = padpad(\\delta_{i+1}) \\star rot_{180}trans(w_{i+1}) \\odot\\sigma_{i}'(z_i)$$\n\n其中$\\odot$表示逐元素乘法，由于这里不是向量和向量乘积，因此不能像之前一样表示成$diag$矩阵乘以向量的形式。\n\n$\\delta_i,\\ i=1,2,...,l$能够求出来了，接下来的问题是如何根据$\\delta_i$求出$\\frac{\\partial E}{\\partial w_i}$。\n\n因为$a_i =l_{i-1} \\star w_i + b_i$，将$a_i$、$l_{i-1}$和$w_i$都旋转180度之后，可以看成$rot180(a_i) = rot180(w_i) \\star rot180(l_{i-1}) + b_i$，这里将$l_{i-1}$看成卷积核，卷积核大小$\\hat{W}_k = W, \\hat{H}_k = H$，Padding变成了$\\hat{P}_H = H - H_k + P_H$和$\\hat{P}_W = W - W_k + P_W$，stride不变，则按照上面的分析结果，对$rot_{180}(\\delta_i)$和$rot_{180}(l_{i-1})$做同样的变换：\n\n$$\n\\begin{aligned}\n    \\frac{\\partial E}{\\partial w_i} &= rot_{180}(padpad(rot_{180}(\\delta_i)) \\star rot_{180}trans(rot_{180}(l_{i-1})))\\\\\n    &= rot_{180}(padpad(rot_{180}(\\delta_i)) \\star trans(l_{i-1}))\\\\\n    &= padpad(\\delta_i) \\star rot_{180}trans(l_{i-1})\n\\end{aligned}\n$$\n\n\n其中$padpad(rot180(\\delta_i))$是将$rot180(\\delta_i)$进行外部padding和内部padding得到的，其外部padding大小：$P'_H = \\hat{H}_k - \\hat{P}_H - 1 = H - H + H_k - P_H - 1 = H_k - P_H - 1$和$P'_W = \\hat{W}_k - \\hat{P}_W - 1 = W_k - P_W - 1$，内部padding大小为$S_H - 1$和$S_W - 1$，可以看出来这里的padpad操作和之前的padpad操作是一样的，两个地方完美等价。\n\n因此如果知道了$\\delta_i$，那么：\n$$\n\\frac{\\partial E}{\\partial w_i} = padpad(\\delta_i) \\star rot_{180}trans(l_{i-1})\n$$\n\n因此如果第$i$层是个卷积层，那么这一层的反向传播核心公式如下：\n$$\n\\begin{aligned}\n    \\delta_{i-1} &= padpad(\\delta_i) \\star rot_{180}trans(w_i) \\odot\\sigma_{i-1}'(z_{i-1})\\\\\n    \\frac{\\partial E}{\\partial w_i} &= padpad(\\delta_i) \\star rot_{180}trans(l_{i-1})\\\\\n    \\frac{\\partial E}{\\partial b_i} &= \\delta_{i}\n\\end{aligned}\n$$\n\n# 池化层的反向传播\n\n池化层没有参数，如果第$i$层是池化层，其反向传播主要需要计算$\\delta_i$和$\\delta_{i-1}$的关系即可。\n\n池化层的$\\delta_i$和$\\delta_{i-1}$的关系取决于池化的类型，如果是最大池化，则需要构造一个非0即1的掩码矩阵，用于标记哪些位置被向前传播，如果是平均池化，则将权重1平均分配到池化核大小的窗口中，以此来构造掩码矩阵\n\n首先上采样$\\delta_i$，使其和$\\delta_{i-1}$的大小相同，然后根据池化类型，构造掩码矩阵$M$，则$\\delta_{i-1} = M\\odot upsample(\\delta_i)$\n\n# CNN反向传播的总结\n\n对于共$l$层的CNN，如果第$i$层是全连接层，则其权重表示为$w_i \\in R^{O_i \\times I_i},O_l = c$，第$i$层偏移表示为$b_i \\in R^{O_i}$，第$i$层激活函数表示为$\\sigma_i$，这一般是个逐元素函数，第$i$层输入即第$i-1$层的输出，表示为$l_{i-1}$，其中$z_i = w_i l_{i-1} + b_i, l_i = \\sigma_i(z_i)$\n\n如果第$i$层是卷积层，则其表达式变为：$z_i =l_{i-1} \\star w_i + b_i, l_i = \\sigma_i(z_i)$，其中$\\star$表示卷积操作，$w_i \\in R^{H\\times W\\times C_{i}\\times C_{i-1}}$，$x\\in R^{H\\times W\\times C_0}$，$z_i,l_i \\in R^{H\\times W\\times C_i}$，$b_i \\in R^{C_i}$。\n\nloss函数记为$E(l_l, y)$。\n\n这里默认最后一层即第$l$层为全连接层。\n$$\n\\begin{aligned}\n    \\delta_l = \\frac{\\partial E}{\\partial z_l} &= diag(\\sigma_l'(z_l))E'\\\\\n    \\delta_{i-1} &= \\begin{cases}\n    diag(\\sigma_{i-1}'(z_{i-1}))w_i^T\\delta_i &如果第i层是全连接层\\\\\n    padpad(\\delta_i) \\star rot_{180}trans(w_i) \\odot\\sigma_{i-1}'(z_{i-1}) &如果第i层是卷积层\\\\\n    M\\odot upsample(\\delta_i) &如果第i层是池化层\n    \\end{cases}\\\\\n    \\frac{\\partial E}{\\partial w_i} &= \\begin{cases}\n    \\delta_il_{i-1}^T &如果第i层是全连接层\\\\\n    padpad(\\delta_i) \\star rot_{180}trans(l_{i-1}) &如果第i层是卷积层\\\\\n    \\end{cases}\\\\\n    \\frac{\\partial E}{\\partial b_i} &= \\delta_i,\\ i=1,2,...,l\n\\end{aligned}\n$$\n\n其中$padpad$表示内部padding（列和行中每两个元素之间）：$(S_H - 1, S_W - 1)$和外部padding：$(H_k - P_H - 1, W_k - P_W - 1)$，$rot_{180}$表示在空间维度(H,W)上旋转180度，$trans$表示在通道维度上转置，$M$表示池化操作的掩码。\n\n","slug":"学习笔记/CNN的反向传播","published":1,"updated":"2020-08-31T06:39:20.737Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckjxpc3ri000544mq65dob8pu","content":"<p>在卷积神经网络中，主要由三种层结构构成：卷积层、池化层、全连接层。</p>\r\n<h1 id=\"全连接层的反向传播\">全连接层的反向传播</h1>\r\n<p>全连接层的反向传播比较简单，使用单纯的BP算法即可，这里先来复习一下全连接层的BP算法：</p>\r\n<p>对于<span class=\"math inline\">\\(l\\)</span>层神经网络,输入<span class=\"math inline\">\\(x \\in R^n\\)</span>，标签<span class=\"math inline\">\\(y \\in R^c\\)</span>，第<span class=\"math inline\">\\(i\\)</span>层权重表示为<span class=\"math inline\">\\(w_i \\in R^{O_i \\times I_i}, I_1 = n，O_l = c\\)</span>，第<span class=\"math inline\">\\(i\\)</span>层偏移表示为<span class=\"math inline\">\\(b_i \\in R^{O_i}\\)</span>，第<span class=\"math inline\">\\(i\\)</span>层激活函数表示为<span class=\"math inline\">\\(\\sigma_i\\)</span>，这一般是个逐元素函数，第<span class=\"math inline\">\\(i\\)</span>层输入即第<span class=\"math inline\">\\(i-1\\)</span>层的输出，表示为<span class=\"math inline\">\\(l_{i-1}\\)</span>，其中<span class=\"math inline\">\\(l_0 = x, z_i = w_i l_{i-1} + b_i, l_i = \\sigma_i(z_i)\\)</span></p>\r\n<p>loss函数记为<span class=\"math inline\">\\(E(l_l, y)\\)</span>，BP算法每次更新<span class=\"math inline\">\\(w_i = w_i - \\eta \\frac{\\partial E}{\\partial w_i}\\)</span>， <span class=\"math inline\">\\(b_i = b_i - \\eta \\frac{\\partial E}{\\partial b_i}\\)</span>，即让参数像梯度最小的方向前进。</p>\r\n<p>首先定义<span class=\"math inline\">\\(E\\)</span>对<span class=\"math inline\">\\(l_l\\)</span>的偏导为<span class=\"math inline\">\\(\\frac{\\partial E}{\\partial l_l} = E&#39;\\)</span>，这个值由loss函数决定。 因此 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\ndE &amp;= E&#39;^Tdl_l\\\\\r\n&amp;=E&#39;^T(\\sigma_l&#39;(z_l) \\odot (dz_l))\\\\\r\n&amp;=E&#39;^Tdiag(\\sigma_l&#39;(z_l))da_l\\\\\r\n\\Rightarrow \\frac{\\partial E}{\\partial z_l} &amp;= diag(\\sigma_l&#39;(z_l))E&#39;\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>这里把<span class=\"math inline\">\\(\\frac{\\partial E}{\\partial z_l}\\)</span>记作<span class=\"math inline\">\\(\\delta_l\\)</span></p>\r\n<p>因为： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    da_i &amp;= w_idl_{i-1}\\\\\r\n    &amp;= w_i(\\sigma_i&#39;(z_{i-1}) \\odot (dz_{i-1}))\\\\\r\n    &amp;=w_idiag(\\sigma_{i-1}&#39;(z_{i-1}))dz_{i-1}\\\\\r\n    \\Rightarrow \\frac{\\partial z_i}{\\partial z_{i-1}} &amp;= diag(\\sigma_{i-1}&#39;(z_{i-1}))w_i^T\\\\\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>所以定义: <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\delta_i &amp;= \\frac{\\partial E}{\\partial z_i},\\ i=1,2,...,l-1\\\\\r\n    \\Rightarrow \\delta_{i-1} &amp;= \\frac{\\partial z_i}{\\partial z_{i-1}}\\frac{\\partial E}{\\partial z_i},\\ i=2,...,l\\\\\r\n    &amp;= diag(\\sigma_{i-1}&#39;(z_{i-1}))w_i^T\\delta_i\\\\\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>现在再来考虑<span class=\"math inline\">\\(E\\)</span>对<span class=\"math inline\">\\(w_{l-k}\\)</span>的导数： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    dE &amp;= \\frac{\\partial E}{\\partial z_{l-k}}^Tdz_{l-k}\\\\\r\n    &amp;= \\delta_{l-k}^T(dw_{l-k}l_{l-k-1} + db_{l-k})\\\\\r\n    &amp;= tr(\\delta_{l-k}^Tdw_{l-k}l_{l-k-1} + \\delta_{l-k}^Tdb_{l-k})\\\\\r\n    &amp;= tr(l_{l-k-1}\\delta_{l-k}^Tdw_{l-k} + \\delta_{l-k}^Tdb_{l-k})\\\\\r\n    \\Rightarrow \\frac{\\partial E}{\\partial w_{l-k}} &amp;= \\delta_{l-k}l_{l-k-1}^T\\\\\r\n    \\Rightarrow \\frac{\\partial E}{\\partial b_{l-k}} &amp;= \\delta_{l-k}\r\n\\end{aligned}\r\n\\]</span> 这里的变换属于标量对矩阵求导<span class=\"math inline\">\\(d f = tr((\\frac{\\partial f}{\\partial X}) ^ T dX)\\)</span>，且用到了迹的一个性质：<span class=\"math inline\">\\(tr(A B) = tr(B A)\\)</span>，其中<span class=\"math inline\">\\(A\\)</span>和<span class=\"math inline\">\\(B^T\\)</span>大小相同</p>\r\n<p>全连接层的BP算法看起来很复杂，其实非常简单，只要使用以下几个等式即可求出任一层的权重和偏置的导数： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\delta_l = \\frac{\\partial E}{\\partial z_l} &amp;= diag(\\sigma_l&#39;(z_l))E&#39;\\\\\r\n    \\delta_{i} = \\frac{\\partial E}{\\partial z_i} &amp;= diag(\\sigma_i&#39;(z_i))w_{i+1}^T\\delta_{i+1},\\ i=1,2,...,l-1\\\\\r\n    \\frac{\\partial E}{\\partial w_i} &amp;= \\delta_il_{i-1}^T,\\ i=1,2,...,l\\\\\r\n    \\frac{\\partial E}{\\partial b_i} &amp;= \\delta_i,\\ i=1,2,...,l\r\n\\end{aligned}\r\n\\]</span></p>\r\n<h1 id=\"卷积层的反向传播\">卷积层的反向传播</h1>\r\n<p>对于卷积层而言，和前面的定义类似，只不过其表达式变为：<span class=\"math inline\">\\(l_0 = x, z_i =l_{i-1} \\star w_i + b_i, l_i = \\sigma_i(z_i)\\)</span>，其中<span class=\"math inline\">\\(\\star\\)</span>表示卷积操作，<span class=\"math inline\">\\(w_i \\in R^{H\\times W\\times C_{i}\\times C_{i-1}}\\)</span>，<span class=\"math inline\">\\(x\\in R^{H\\times W\\times C_0}\\)</span>，<span class=\"math inline\">\\(z_i,l_i \\in R^{H\\times W\\times C_i}\\)</span>，<span class=\"math inline\">\\(b_i \\in R^{C_i}\\)</span>。</p>\r\n<p>按照之前全连接层的反向传播套路，自然也希望首先定义 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\delta_l &amp;= \\frac{\\partial E}{\\partial z_l} = diag(\\sigma_l&#39;(z_l))E&#39;\\\\\r\n    \\delta_{i} &amp;= \\frac{\\partial E}{\\partial z_i}\r\n\\end{aligned}\r\n\\]</span> 那么第一个问题是如何根据<span class=\"math inline\">\\(\\delta_{i+1}\\)</span>求出<span class=\"math inline\">\\(\\delta_{i}\\)</span>。</p>\r\n<p>因为： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\delta_{i} &amp;= \\frac{\\partial E}{\\partial z_i}\\\\\r\n    &amp;= \\frac{\\partial z_{i+1}}{\\partial z_i}\\frac{\\partial E}{\\partial z_{i+1}}\\\\\r\n    &amp;= \\frac{\\partial z_{i+1}}{\\partial z_i}\\delta_{i+1}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>所以问题在于如何求<span class=\"math inline\">\\(\\frac{\\partial z_{i+1}}{\\partial z_i}\\)</span>，如果<span class=\"math inline\">\\(i+1\\)</span>层是个卷积层，那么<span class=\"math inline\">\\(z_{i+1} = \\sigma_i(z_i) \\star w_{i+1} + b_{i+1}\\)</span>，这里想使用一般的方法求解<span class=\"math inline\">\\(\\frac{\\partial z_{i+1}}{\\partial z_i}\\)</span>是很困难的，下面直接对<span class=\"math inline\">\\(\\delta_{i}\\)</span>给出一般化的表达。</p>\r\n<p>卷积操作的具体分析需要画图讨论，这里不再赘述，定义一个卷积操作，<span class=\"math inline\">\\(O = I \\star W\\)</span>，其中卷积核<span class=\"math inline\">\\(W\\)</span>的大小为<span class=\"math inline\">\\(H_k\\times W_k\\times C_o\\times C_i\\)</span>，输入特征图的大小为<span class=\"math inline\">\\(H\\times W\\times C_i\\)</span>，卷积操作的padding大小为<span class=\"math inline\">\\(P_H,P_W\\)</span>，stride大小为<span class=\"math inline\">\\(S_H,S_W\\)</span>，则输出特征图的大小为<span class=\"math inline\">\\(\\lceil\\frac{H + 2P_H - H_k + 1}{S_H}\\rceil\\times \\lceil\\frac{W + 2P_W - H_k + 1}{S_W}\\rceil\\times C_o\\)</span>，注意这里<span class=\"math inline\">\\(0 \\le P_H \\le H_k - 1\\)</span>，<span class=\"math inline\">\\(0 \\le P_W \\le W_k - 1\\)</span>，否则过多的padding没有意义。</p>\r\n<p>设L为损失函数，令<span class=\"math inline\">\\(\\frac{\\partial E}{\\partial O}\\)</span>记作<span class=\"math inline\">\\(\\delta_O\\)</span>，其大小和<span class=\"math inline\">\\(O\\)</span>相同。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\frac{\\partial E}{\\partial I} &amp;= padpad(\\delta_O) \\star rot_{180}trans(W)\r\n\\end{aligned}\r\n\\]</span> 其中<span class=\"math inline\">\\(padpad(\\delta_O)\\)</span>是将<span class=\"math inline\">\\(\\delta_O\\)</span>进行外部padding和内部padding得到的，其外部padding大小：<span class=\"math inline\">\\(P&#39;_H = H_k - P_H - 1\\)</span>和<span class=\"math inline\">\\(P&#39;_W = W_k - P_W - 1\\)</span>，内部padding大小为<span class=\"math inline\">\\(S_H - 1\\)</span>和<span class=\"math inline\">\\(S_W - 1\\)</span>。</p>\r\n<p><span class=\"math inline\">\\(rot_{180}trans(W)\\)</span>则首先需要将<span class=\"math inline\">\\(W\\)</span>在<span class=\"math inline\">\\(H_k\\times W_k\\)</span>大小上，旋转180度，然后对其中每个像素（其实每个像素都是个<span class=\"math inline\">\\(C_o\\times C_i\\)</span>大小的矩阵）求转置得到，最终的形状是<span class=\"math inline\">\\(H_k\\times W_k\\times C_i\\times C_o\\)</span>，其中<span class=\"math inline\">\\(rot_{180}trans(W)_{i,j,k,l} = W_{H_k-i+1,W_k-j+1,l,k},\\ i=1,2,...,H_k,\\ j=1,2,...,W_k,\\ k=1,2,...,C_o,\\ l=1,2,...,C_i\\)</span></p>\r\n<p>则有: <span class=\"math display\">\\[\\delta_{i} = padpad(\\delta_{i+1}) \\star rot_{180}trans(w_{i+1}) \\odot\\sigma_{i}&#39;(z_i)\\]</span></p>\r\n<p>其中<span class=\"math inline\">\\(\\odot\\)</span>表示逐元素乘法，由于这里不是向量和向量乘积，因此不能像之前一样表示成<span class=\"math inline\">\\(diag\\)</span>矩阵乘以向量的形式。</p>\r\n<p><span class=\"math inline\">\\(\\delta_i,\\ i=1,2,...,l\\)</span>能够求出来了，接下来的问题是如何根据<span class=\"math inline\">\\(\\delta_i\\)</span>求出<span class=\"math inline\">\\(\\frac{\\partial E}{\\partial w_i}\\)</span>。</p>\r\n<p>因为<span class=\"math inline\">\\(a_i =l_{i-1} \\star w_i + b_i\\)</span>，将<span class=\"math inline\">\\(a_i\\)</span>、<span class=\"math inline\">\\(l_{i-1}\\)</span>和<span class=\"math inline\">\\(w_i\\)</span>都旋转180度之后，可以看成<span class=\"math inline\">\\(rot180(a_i) = rot180(w_i) \\star rot180(l_{i-1}) + b_i\\)</span>，这里将<span class=\"math inline\">\\(l_{i-1}\\)</span>看成卷积核，卷积核大小<span class=\"math inline\">\\(\\hat{W}_k = W, \\hat{H}_k = H\\)</span>，Padding变成了<span class=\"math inline\">\\(\\hat{P}_H = H - H_k + P_H\\)</span>和<span class=\"math inline\">\\(\\hat{P}_W = W - W_k + P_W\\)</span>，stride不变，则按照上面的分析结果，对<span class=\"math inline\">\\(rot_{180}(\\delta_i)\\)</span>和<span class=\"math inline\">\\(rot_{180}(l_{i-1})\\)</span>做同样的变换：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\frac{\\partial E}{\\partial w_i} &amp;= rot_{180}(padpad(rot_{180}(\\delta_i)) \\star rot_{180}trans(rot_{180}(l_{i-1})))\\\\\r\n    &amp;= rot_{180}(padpad(rot_{180}(\\delta_i)) \\star trans(l_{i-1}))\\\\\r\n    &amp;= padpad(\\delta_i) \\star rot_{180}trans(l_{i-1})\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>其中<span class=\"math inline\">\\(padpad(rot180(\\delta_i))\\)</span>是将<span class=\"math inline\">\\(rot180(\\delta_i)\\)</span>进行外部padding和内部padding得到的，其外部padding大小：<span class=\"math inline\">\\(P&#39;_H = \\hat{H}_k - \\hat{P}_H - 1 = H - H + H_k - P_H - 1 = H_k - P_H - 1\\)</span>和<span class=\"math inline\">\\(P&#39;_W = \\hat{W}_k - \\hat{P}_W - 1 = W_k - P_W - 1\\)</span>，内部padding大小为<span class=\"math inline\">\\(S_H - 1\\)</span>和<span class=\"math inline\">\\(S_W - 1\\)</span>，可以看出来这里的padpad操作和之前的padpad操作是一样的，两个地方完美等价。</p>\r\n<p>因此如果知道了<span class=\"math inline\">\\(\\delta_i\\)</span>，那么： <span class=\"math display\">\\[\r\n\\frac{\\partial E}{\\partial w_i} = padpad(\\delta_i) \\star rot_{180}trans(l_{i-1})\r\n\\]</span></p>\r\n<p>因此如果第<span class=\"math inline\">\\(i\\)</span>层是个卷积层，那么这一层的反向传播核心公式如下： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\delta_{i-1} &amp;= padpad(\\delta_i) \\star rot_{180}trans(w_i) \\odot\\sigma_{i-1}&#39;(z_{i-1})\\\\\r\n    \\frac{\\partial E}{\\partial w_i} &amp;= padpad(\\delta_i) \\star rot_{180}trans(l_{i-1})\\\\\r\n    \\frac{\\partial E}{\\partial b_i} &amp;= \\delta_{i}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<h1 id=\"池化层的反向传播\">池化层的反向传播</h1>\r\n<p>池化层没有参数，如果第<span class=\"math inline\">\\(i\\)</span>层是池化层，其反向传播主要需要计算<span class=\"math inline\">\\(\\delta_i\\)</span>和<span class=\"math inline\">\\(\\delta_{i-1}\\)</span>的关系即可。</p>\r\n<p>池化层的<span class=\"math inline\">\\(\\delta_i\\)</span>和<span class=\"math inline\">\\(\\delta_{i-1}\\)</span>的关系取决于池化的类型，如果是最大池化，则需要构造一个非0即1的掩码矩阵，用于标记哪些位置被向前传播，如果是平均池化，则将权重1平均分配到池化核大小的窗口中，以此来构造掩码矩阵</p>\r\n<p>首先上采样<span class=\"math inline\">\\(\\delta_i\\)</span>，使其和<span class=\"math inline\">\\(\\delta_{i-1}\\)</span>的大小相同，然后根据池化类型，构造掩码矩阵<span class=\"math inline\">\\(M\\)</span>，则<span class=\"math inline\">\\(\\delta_{i-1} = M\\odot upsample(\\delta_i)\\)</span></p>\r\n<h1 id=\"cnn反向传播的总结\">CNN反向传播的总结</h1>\r\n<p>对于共<span class=\"math inline\">\\(l\\)</span>层的CNN，如果第<span class=\"math inline\">\\(i\\)</span>层是全连接层，则其权重表示为<span class=\"math inline\">\\(w_i \\in R^{O_i \\times I_i},O_l = c\\)</span>，第<span class=\"math inline\">\\(i\\)</span>层偏移表示为<span class=\"math inline\">\\(b_i \\in R^{O_i}\\)</span>，第<span class=\"math inline\">\\(i\\)</span>层激活函数表示为<span class=\"math inline\">\\(\\sigma_i\\)</span>，这一般是个逐元素函数，第<span class=\"math inline\">\\(i\\)</span>层输入即第<span class=\"math inline\">\\(i-1\\)</span>层的输出，表示为<span class=\"math inline\">\\(l_{i-1}\\)</span>，其中<span class=\"math inline\">\\(z_i = w_i l_{i-1} + b_i, l_i = \\sigma_i(z_i)\\)</span></p>\r\n<p>如果第<span class=\"math inline\">\\(i\\)</span>层是卷积层，则其表达式变为：<span class=\"math inline\">\\(z_i =l_{i-1} \\star w_i + b_i, l_i = \\sigma_i(z_i)\\)</span>，其中<span class=\"math inline\">\\(\\star\\)</span>表示卷积操作，<span class=\"math inline\">\\(w_i \\in R^{H\\times W\\times C_{i}\\times C_{i-1}}\\)</span>，<span class=\"math inline\">\\(x\\in R^{H\\times W\\times C_0}\\)</span>，<span class=\"math inline\">\\(z_i,l_i \\in R^{H\\times W\\times C_i}\\)</span>，<span class=\"math inline\">\\(b_i \\in R^{C_i}\\)</span>。</p>\r\n<p>loss函数记为<span class=\"math inline\">\\(E(l_l, y)\\)</span>。</p>\r\n<p>这里默认最后一层即第<span class=\"math inline\">\\(l\\)</span>层为全连接层。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\delta_l = \\frac{\\partial E}{\\partial z_l} &amp;= diag(\\sigma_l&#39;(z_l))E&#39;\\\\\r\n    \\delta_{i-1} &amp;= \\begin{cases}\r\n    diag(\\sigma_{i-1}&#39;(z_{i-1}))w_i^T\\delta_i &amp;如果第i层是全连接层\\\\\r\n    padpad(\\delta_i) \\star rot_{180}trans(w_i) \\odot\\sigma_{i-1}&#39;(z_{i-1}) &amp;如果第i层是卷积层\\\\\r\n    M\\odot upsample(\\delta_i) &amp;如果第i层是池化层\r\n    \\end{cases}\\\\\r\n    \\frac{\\partial E}{\\partial w_i} &amp;= \\begin{cases}\r\n    \\delta_il_{i-1}^T &amp;如果第i层是全连接层\\\\\r\n    padpad(\\delta_i) \\star rot_{180}trans(l_{i-1}) &amp;如果第i层是卷积层\\\\\r\n    \\end{cases}\\\\\r\n    \\frac{\\partial E}{\\partial b_i} &amp;= \\delta_i,\\ i=1,2,...,l\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>其中<span class=\"math inline\">\\(padpad\\)</span>表示内部padding（列和行中每两个元素之间）：<span class=\"math inline\">\\((S_H - 1, S_W - 1)\\)</span>和外部padding：<span class=\"math inline\">\\((H_k - P_H - 1, W_k - P_W - 1)\\)</span>，<span class=\"math inline\">\\(rot_{180}\\)</span>表示在空间维度(H,W)上旋转180度，<span class=\"math inline\">\\(trans\\)</span>表示在通道维度上转置，<span class=\"math inline\">\\(M\\)</span>表示池化操作的掩码。</p>\r\n","site":{"data":{}},"excerpt":"","more":"<p>在卷积神经网络中，主要由三种层结构构成：卷积层、池化层、全连接层。</p>\r\n<h1 id=\"全连接层的反向传播\">全连接层的反向传播</h1>\r\n<p>全连接层的反向传播比较简单，使用单纯的BP算法即可，这里先来复习一下全连接层的BP算法：</p>\r\n<p>对于<span class=\"math inline\">\\(l\\)</span>层神经网络,输入<span class=\"math inline\">\\(x \\in R^n\\)</span>，标签<span class=\"math inline\">\\(y \\in R^c\\)</span>，第<span class=\"math inline\">\\(i\\)</span>层权重表示为<span class=\"math inline\">\\(w_i \\in R^{O_i \\times I_i}, I_1 = n，O_l = c\\)</span>，第<span class=\"math inline\">\\(i\\)</span>层偏移表示为<span class=\"math inline\">\\(b_i \\in R^{O_i}\\)</span>，第<span class=\"math inline\">\\(i\\)</span>层激活函数表示为<span class=\"math inline\">\\(\\sigma_i\\)</span>，这一般是个逐元素函数，第<span class=\"math inline\">\\(i\\)</span>层输入即第<span class=\"math inline\">\\(i-1\\)</span>层的输出，表示为<span class=\"math inline\">\\(l_{i-1}\\)</span>，其中<span class=\"math inline\">\\(l_0 = x, z_i = w_i l_{i-1} + b_i, l_i = \\sigma_i(z_i)\\)</span></p>\r\n<p>loss函数记为<span class=\"math inline\">\\(E(l_l, y)\\)</span>，BP算法每次更新<span class=\"math inline\">\\(w_i = w_i - \\eta \\frac{\\partial E}{\\partial w_i}\\)</span>， <span class=\"math inline\">\\(b_i = b_i - \\eta \\frac{\\partial E}{\\partial b_i}\\)</span>，即让参数像梯度最小的方向前进。</p>\r\n<p>首先定义<span class=\"math inline\">\\(E\\)</span>对<span class=\"math inline\">\\(l_l\\)</span>的偏导为<span class=\"math inline\">\\(\\frac{\\partial E}{\\partial l_l} = E&#39;\\)</span>，这个值由loss函数决定。 因此 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\ndE &amp;= E&#39;^Tdl_l\\\\\r\n&amp;=E&#39;^T(\\sigma_l&#39;(z_l) \\odot (dz_l))\\\\\r\n&amp;=E&#39;^Tdiag(\\sigma_l&#39;(z_l))da_l\\\\\r\n\\Rightarrow \\frac{\\partial E}{\\partial z_l} &amp;= diag(\\sigma_l&#39;(z_l))E&#39;\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>这里把<span class=\"math inline\">\\(\\frac{\\partial E}{\\partial z_l}\\)</span>记作<span class=\"math inline\">\\(\\delta_l\\)</span></p>\r\n<p>因为： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    da_i &amp;= w_idl_{i-1}\\\\\r\n    &amp;= w_i(\\sigma_i&#39;(z_{i-1}) \\odot (dz_{i-1}))\\\\\r\n    &amp;=w_idiag(\\sigma_{i-1}&#39;(z_{i-1}))dz_{i-1}\\\\\r\n    \\Rightarrow \\frac{\\partial z_i}{\\partial z_{i-1}} &amp;= diag(\\sigma_{i-1}&#39;(z_{i-1}))w_i^T\\\\\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>所以定义: <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\delta_i &amp;= \\frac{\\partial E}{\\partial z_i},\\ i=1,2,...,l-1\\\\\r\n    \\Rightarrow \\delta_{i-1} &amp;= \\frac{\\partial z_i}{\\partial z_{i-1}}\\frac{\\partial E}{\\partial z_i},\\ i=2,...,l\\\\\r\n    &amp;= diag(\\sigma_{i-1}&#39;(z_{i-1}))w_i^T\\delta_i\\\\\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>现在再来考虑<span class=\"math inline\">\\(E\\)</span>对<span class=\"math inline\">\\(w_{l-k}\\)</span>的导数： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    dE &amp;= \\frac{\\partial E}{\\partial z_{l-k}}^Tdz_{l-k}\\\\\r\n    &amp;= \\delta_{l-k}^T(dw_{l-k}l_{l-k-1} + db_{l-k})\\\\\r\n    &amp;= tr(\\delta_{l-k}^Tdw_{l-k}l_{l-k-1} + \\delta_{l-k}^Tdb_{l-k})\\\\\r\n    &amp;= tr(l_{l-k-1}\\delta_{l-k}^Tdw_{l-k} + \\delta_{l-k}^Tdb_{l-k})\\\\\r\n    \\Rightarrow \\frac{\\partial E}{\\partial w_{l-k}} &amp;= \\delta_{l-k}l_{l-k-1}^T\\\\\r\n    \\Rightarrow \\frac{\\partial E}{\\partial b_{l-k}} &amp;= \\delta_{l-k}\r\n\\end{aligned}\r\n\\]</span> 这里的变换属于标量对矩阵求导<span class=\"math inline\">\\(d f = tr((\\frac{\\partial f}{\\partial X}) ^ T dX)\\)</span>，且用到了迹的一个性质：<span class=\"math inline\">\\(tr(A B) = tr(B A)\\)</span>，其中<span class=\"math inline\">\\(A\\)</span>和<span class=\"math inline\">\\(B^T\\)</span>大小相同</p>\r\n<p>全连接层的BP算法看起来很复杂，其实非常简单，只要使用以下几个等式即可求出任一层的权重和偏置的导数： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\delta_l = \\frac{\\partial E}{\\partial z_l} &amp;= diag(\\sigma_l&#39;(z_l))E&#39;\\\\\r\n    \\delta_{i} = \\frac{\\partial E}{\\partial z_i} &amp;= diag(\\sigma_i&#39;(z_i))w_{i+1}^T\\delta_{i+1},\\ i=1,2,...,l-1\\\\\r\n    \\frac{\\partial E}{\\partial w_i} &amp;= \\delta_il_{i-1}^T,\\ i=1,2,...,l\\\\\r\n    \\frac{\\partial E}{\\partial b_i} &amp;= \\delta_i,\\ i=1,2,...,l\r\n\\end{aligned}\r\n\\]</span></p>\r\n<h1 id=\"卷积层的反向传播\">卷积层的反向传播</h1>\r\n<p>对于卷积层而言，和前面的定义类似，只不过其表达式变为：<span class=\"math inline\">\\(l_0 = x, z_i =l_{i-1} \\star w_i + b_i, l_i = \\sigma_i(z_i)\\)</span>，其中<span class=\"math inline\">\\(\\star\\)</span>表示卷积操作，<span class=\"math inline\">\\(w_i \\in R^{H\\times W\\times C_{i}\\times C_{i-1}}\\)</span>，<span class=\"math inline\">\\(x\\in R^{H\\times W\\times C_0}\\)</span>，<span class=\"math inline\">\\(z_i,l_i \\in R^{H\\times W\\times C_i}\\)</span>，<span class=\"math inline\">\\(b_i \\in R^{C_i}\\)</span>。</p>\r\n<p>按照之前全连接层的反向传播套路，自然也希望首先定义 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\delta_l &amp;= \\frac{\\partial E}{\\partial z_l} = diag(\\sigma_l&#39;(z_l))E&#39;\\\\\r\n    \\delta_{i} &amp;= \\frac{\\partial E}{\\partial z_i}\r\n\\end{aligned}\r\n\\]</span> 那么第一个问题是如何根据<span class=\"math inline\">\\(\\delta_{i+1}\\)</span>求出<span class=\"math inline\">\\(\\delta_{i}\\)</span>。</p>\r\n<p>因为： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\delta_{i} &amp;= \\frac{\\partial E}{\\partial z_i}\\\\\r\n    &amp;= \\frac{\\partial z_{i+1}}{\\partial z_i}\\frac{\\partial E}{\\partial z_{i+1}}\\\\\r\n    &amp;= \\frac{\\partial z_{i+1}}{\\partial z_i}\\delta_{i+1}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>所以问题在于如何求<span class=\"math inline\">\\(\\frac{\\partial z_{i+1}}{\\partial z_i}\\)</span>，如果<span class=\"math inline\">\\(i+1\\)</span>层是个卷积层，那么<span class=\"math inline\">\\(z_{i+1} = \\sigma_i(z_i) \\star w_{i+1} + b_{i+1}\\)</span>，这里想使用一般的方法求解<span class=\"math inline\">\\(\\frac{\\partial z_{i+1}}{\\partial z_i}\\)</span>是很困难的，下面直接对<span class=\"math inline\">\\(\\delta_{i}\\)</span>给出一般化的表达。</p>\r\n<p>卷积操作的具体分析需要画图讨论，这里不再赘述，定义一个卷积操作，<span class=\"math inline\">\\(O = I \\star W\\)</span>，其中卷积核<span class=\"math inline\">\\(W\\)</span>的大小为<span class=\"math inline\">\\(H_k\\times W_k\\times C_o\\times C_i\\)</span>，输入特征图的大小为<span class=\"math inline\">\\(H\\times W\\times C_i\\)</span>，卷积操作的padding大小为<span class=\"math inline\">\\(P_H,P_W\\)</span>，stride大小为<span class=\"math inline\">\\(S_H,S_W\\)</span>，则输出特征图的大小为<span class=\"math inline\">\\(\\lceil\\frac{H + 2P_H - H_k + 1}{S_H}\\rceil\\times \\lceil\\frac{W + 2P_W - H_k + 1}{S_W}\\rceil\\times C_o\\)</span>，注意这里<span class=\"math inline\">\\(0 \\le P_H \\le H_k - 1\\)</span>，<span class=\"math inline\">\\(0 \\le P_W \\le W_k - 1\\)</span>，否则过多的padding没有意义。</p>\r\n<p>设L为损失函数，令<span class=\"math inline\">\\(\\frac{\\partial E}{\\partial O}\\)</span>记作<span class=\"math inline\">\\(\\delta_O\\)</span>，其大小和<span class=\"math inline\">\\(O\\)</span>相同。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\frac{\\partial E}{\\partial I} &amp;= padpad(\\delta_O) \\star rot_{180}trans(W)\r\n\\end{aligned}\r\n\\]</span> 其中<span class=\"math inline\">\\(padpad(\\delta_O)\\)</span>是将<span class=\"math inline\">\\(\\delta_O\\)</span>进行外部padding和内部padding得到的，其外部padding大小：<span class=\"math inline\">\\(P&#39;_H = H_k - P_H - 1\\)</span>和<span class=\"math inline\">\\(P&#39;_W = W_k - P_W - 1\\)</span>，内部padding大小为<span class=\"math inline\">\\(S_H - 1\\)</span>和<span class=\"math inline\">\\(S_W - 1\\)</span>。</p>\r\n<p><span class=\"math inline\">\\(rot_{180}trans(W)\\)</span>则首先需要将<span class=\"math inline\">\\(W\\)</span>在<span class=\"math inline\">\\(H_k\\times W_k\\)</span>大小上，旋转180度，然后对其中每个像素（其实每个像素都是个<span class=\"math inline\">\\(C_o\\times C_i\\)</span>大小的矩阵）求转置得到，最终的形状是<span class=\"math inline\">\\(H_k\\times W_k\\times C_i\\times C_o\\)</span>，其中<span class=\"math inline\">\\(rot_{180}trans(W)_{i,j,k,l} = W_{H_k-i+1,W_k-j+1,l,k},\\ i=1,2,...,H_k,\\ j=1,2,...,W_k,\\ k=1,2,...,C_o,\\ l=1,2,...,C_i\\)</span></p>\r\n<p>则有: <span class=\"math display\">\\[\\delta_{i} = padpad(\\delta_{i+1}) \\star rot_{180}trans(w_{i+1}) \\odot\\sigma_{i}&#39;(z_i)\\]</span></p>\r\n<p>其中<span class=\"math inline\">\\(\\odot\\)</span>表示逐元素乘法，由于这里不是向量和向量乘积，因此不能像之前一样表示成<span class=\"math inline\">\\(diag\\)</span>矩阵乘以向量的形式。</p>\r\n<p><span class=\"math inline\">\\(\\delta_i,\\ i=1,2,...,l\\)</span>能够求出来了，接下来的问题是如何根据<span class=\"math inline\">\\(\\delta_i\\)</span>求出<span class=\"math inline\">\\(\\frac{\\partial E}{\\partial w_i}\\)</span>。</p>\r\n<p>因为<span class=\"math inline\">\\(a_i =l_{i-1} \\star w_i + b_i\\)</span>，将<span class=\"math inline\">\\(a_i\\)</span>、<span class=\"math inline\">\\(l_{i-1}\\)</span>和<span class=\"math inline\">\\(w_i\\)</span>都旋转180度之后，可以看成<span class=\"math inline\">\\(rot180(a_i) = rot180(w_i) \\star rot180(l_{i-1}) + b_i\\)</span>，这里将<span class=\"math inline\">\\(l_{i-1}\\)</span>看成卷积核，卷积核大小<span class=\"math inline\">\\(\\hat{W}_k = W, \\hat{H}_k = H\\)</span>，Padding变成了<span class=\"math inline\">\\(\\hat{P}_H = H - H_k + P_H\\)</span>和<span class=\"math inline\">\\(\\hat{P}_W = W - W_k + P_W\\)</span>，stride不变，则按照上面的分析结果，对<span class=\"math inline\">\\(rot_{180}(\\delta_i)\\)</span>和<span class=\"math inline\">\\(rot_{180}(l_{i-1})\\)</span>做同样的变换：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\frac{\\partial E}{\\partial w_i} &amp;= rot_{180}(padpad(rot_{180}(\\delta_i)) \\star rot_{180}trans(rot_{180}(l_{i-1})))\\\\\r\n    &amp;= rot_{180}(padpad(rot_{180}(\\delta_i)) \\star trans(l_{i-1}))\\\\\r\n    &amp;= padpad(\\delta_i) \\star rot_{180}trans(l_{i-1})\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>其中<span class=\"math inline\">\\(padpad(rot180(\\delta_i))\\)</span>是将<span class=\"math inline\">\\(rot180(\\delta_i)\\)</span>进行外部padding和内部padding得到的，其外部padding大小：<span class=\"math inline\">\\(P&#39;_H = \\hat{H}_k - \\hat{P}_H - 1 = H - H + H_k - P_H - 1 = H_k - P_H - 1\\)</span>和<span class=\"math inline\">\\(P&#39;_W = \\hat{W}_k - \\hat{P}_W - 1 = W_k - P_W - 1\\)</span>，内部padding大小为<span class=\"math inline\">\\(S_H - 1\\)</span>和<span class=\"math inline\">\\(S_W - 1\\)</span>，可以看出来这里的padpad操作和之前的padpad操作是一样的，两个地方完美等价。</p>\r\n<p>因此如果知道了<span class=\"math inline\">\\(\\delta_i\\)</span>，那么： <span class=\"math display\">\\[\r\n\\frac{\\partial E}{\\partial w_i} = padpad(\\delta_i) \\star rot_{180}trans(l_{i-1})\r\n\\]</span></p>\r\n<p>因此如果第<span class=\"math inline\">\\(i\\)</span>层是个卷积层，那么这一层的反向传播核心公式如下： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\delta_{i-1} &amp;= padpad(\\delta_i) \\star rot_{180}trans(w_i) \\odot\\sigma_{i-1}&#39;(z_{i-1})\\\\\r\n    \\frac{\\partial E}{\\partial w_i} &amp;= padpad(\\delta_i) \\star rot_{180}trans(l_{i-1})\\\\\r\n    \\frac{\\partial E}{\\partial b_i} &amp;= \\delta_{i}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<h1 id=\"池化层的反向传播\">池化层的反向传播</h1>\r\n<p>池化层没有参数，如果第<span class=\"math inline\">\\(i\\)</span>层是池化层，其反向传播主要需要计算<span class=\"math inline\">\\(\\delta_i\\)</span>和<span class=\"math inline\">\\(\\delta_{i-1}\\)</span>的关系即可。</p>\r\n<p>池化层的<span class=\"math inline\">\\(\\delta_i\\)</span>和<span class=\"math inline\">\\(\\delta_{i-1}\\)</span>的关系取决于池化的类型，如果是最大池化，则需要构造一个非0即1的掩码矩阵，用于标记哪些位置被向前传播，如果是平均池化，则将权重1平均分配到池化核大小的窗口中，以此来构造掩码矩阵</p>\r\n<p>首先上采样<span class=\"math inline\">\\(\\delta_i\\)</span>，使其和<span class=\"math inline\">\\(\\delta_{i-1}\\)</span>的大小相同，然后根据池化类型，构造掩码矩阵<span class=\"math inline\">\\(M\\)</span>，则<span class=\"math inline\">\\(\\delta_{i-1} = M\\odot upsample(\\delta_i)\\)</span></p>\r\n<h1 id=\"cnn反向传播的总结\">CNN反向传播的总结</h1>\r\n<p>对于共<span class=\"math inline\">\\(l\\)</span>层的CNN，如果第<span class=\"math inline\">\\(i\\)</span>层是全连接层，则其权重表示为<span class=\"math inline\">\\(w_i \\in R^{O_i \\times I_i},O_l = c\\)</span>，第<span class=\"math inline\">\\(i\\)</span>层偏移表示为<span class=\"math inline\">\\(b_i \\in R^{O_i}\\)</span>，第<span class=\"math inline\">\\(i\\)</span>层激活函数表示为<span class=\"math inline\">\\(\\sigma_i\\)</span>，这一般是个逐元素函数，第<span class=\"math inline\">\\(i\\)</span>层输入即第<span class=\"math inline\">\\(i-1\\)</span>层的输出，表示为<span class=\"math inline\">\\(l_{i-1}\\)</span>，其中<span class=\"math inline\">\\(z_i = w_i l_{i-1} + b_i, l_i = \\sigma_i(z_i)\\)</span></p>\r\n<p>如果第<span class=\"math inline\">\\(i\\)</span>层是卷积层，则其表达式变为：<span class=\"math inline\">\\(z_i =l_{i-1} \\star w_i + b_i, l_i = \\sigma_i(z_i)\\)</span>，其中<span class=\"math inline\">\\(\\star\\)</span>表示卷积操作，<span class=\"math inline\">\\(w_i \\in R^{H\\times W\\times C_{i}\\times C_{i-1}}\\)</span>，<span class=\"math inline\">\\(x\\in R^{H\\times W\\times C_0}\\)</span>，<span class=\"math inline\">\\(z_i,l_i \\in R^{H\\times W\\times C_i}\\)</span>，<span class=\"math inline\">\\(b_i \\in R^{C_i}\\)</span>。</p>\r\n<p>loss函数记为<span class=\"math inline\">\\(E(l_l, y)\\)</span>。</p>\r\n<p>这里默认最后一层即第<span class=\"math inline\">\\(l\\)</span>层为全连接层。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\delta_l = \\frac{\\partial E}{\\partial z_l} &amp;= diag(\\sigma_l&#39;(z_l))E&#39;\\\\\r\n    \\delta_{i-1} &amp;= \\begin{cases}\r\n    diag(\\sigma_{i-1}&#39;(z_{i-1}))w_i^T\\delta_i &amp;如果第i层是全连接层\\\\\r\n    padpad(\\delta_i) \\star rot_{180}trans(w_i) \\odot\\sigma_{i-1}&#39;(z_{i-1}) &amp;如果第i层是卷积层\\\\\r\n    M\\odot upsample(\\delta_i) &amp;如果第i层是池化层\r\n    \\end{cases}\\\\\r\n    \\frac{\\partial E}{\\partial w_i} &amp;= \\begin{cases}\r\n    \\delta_il_{i-1}^T &amp;如果第i层是全连接层\\\\\r\n    padpad(\\delta_i) \\star rot_{180}trans(l_{i-1}) &amp;如果第i层是卷积层\\\\\r\n    \\end{cases}\\\\\r\n    \\frac{\\partial E}{\\partial b_i} &amp;= \\delta_i,\\ i=1,2,...,l\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>其中<span class=\"math inline\">\\(padpad\\)</span>表示内部padding（列和行中每两个元素之间）：<span class=\"math inline\">\\((S_H - 1, S_W - 1)\\)</span>和外部padding：<span class=\"math inline\">\\((H_k - P_H - 1, W_k - P_W - 1)\\)</span>，<span class=\"math inline\">\\(rot_{180}\\)</span>表示在空间维度(H,W)上旋转180度，<span class=\"math inline\">\\(trans\\)</span>表示在通道维度上转置，<span class=\"math inline\">\\(M\\)</span>表示池化操作的掩码。</p>\r\n"},{"title":"CNN相关知识","date":"2020-05-07T03:12:22.000Z","mathjax":true,"_content":"\n使用CNN这么久了，最近抽点时间来复习一下CNN相关的一些基础内容。\n\n# CNN中的基本操作\n## 卷积层\n一般的神经网络中，每一层是上层输出到该层输出的全连接，但是对于图像数据，如果使用全连接操作，参数量会变得非常大，难以计算和优化而且容易过拟合。\n\n卷积层有一维卷积、二维卷积、三维卷积几种常见的类型，这里主要讨论二维卷积，其他的卷积只是输入的维度数量不同，并无特殊。\n\n在Pytorch框架下的CNN实现中，卷积层的输入一般是$(B\\times C\\times H\\times W)$的形状（其他框架例如Tensorflow，使用的输入形状是$(B\\times H\\times W\\times C)$，只是维度顺序不同），其中$B$是代表batch size，表示不同的图片样本，$C$代表通道数量，例如原始彩色图片一般是三个通道：R、G、B，最后的$H,\\ W$是特征图的像素高度和像素宽度。\n\n卷积层的参数主要有两个：Kernel、Bias。\n\n其中Kernel的形状是$(H_k\\times W_k\\times C_o\\times C_i)$，$H_k, W_k$表示卷积核的大小，例如下图的卷积核大小为$3\\times 3$，$C_i$表示卷积核的通道数，如果输入的特征图是彩色图像，即一个三通道图片，那么$C_i=3$即表示在每一个像素点，对其三个通道进行全连接操作，$C_o$则是对每个像素所有通道进行全连接操作的输出通道数，也可以理解为卷积核个数（一个卷积核输出一个通道）。\n\nBias一般是个$R^{C_o}$的向量，在卷积操作结束之后，对输出的特征图进行偏移，每个通道共用其中的一个偏置参数。\n\n卷积操作如图所示：\n{%asset_img Conv.gif 卷积操作示意%}\n\n### 卷积层的参数量和计算量\n参数量即卷积核的大小加上偏置的大小：$H_k\\times W_k\\times C_o\\times C_i + C_o$\n计算量受到步长，padding方式等影响，从输入的大小来看不好计算，但是可以根据输出的特征图大小来计算：$H' \\times W' \\times (H_k\\times W_k\\times C_o\\times C_i + C_o)$\n\n### 对卷积操作的理解\n卷积操作的本质就是在特征图上使用相同的参数滑动进行局部的全连接，这样做能够减少参数量和计算量，并且因为共享权值，在进行优化时更加容易。\n\n卷积层有一些主要特点：\n- 权值共享：在图像的不同部分，使用相同的特征提取是有意义的，这也是卷积层具备平移不变性的原因。\n- 稀疏连接：对于一个输出值只和输入的一部分相关。\n\n卷积操作可以看做一种局部特征的提取，例如一些边缘检测算子，实际上就是在图片上使用相同的参数滑动进行乘法操作（匹配特征方向），将代表边缘的像素点赋予更大的像素值，从而在结果图片中将边缘表示为亮的线条，这里就是在提取局部的边缘特征并保留其位置信息，最终组成整幅图片的边缘表示。\n\n### CNN的反向传播\n这部分内容可见我的另一篇文章{% post_link CNN的反向传播 CNN的反向传播 %}\n\n## 激活函数\n激活函数一般是一个非线性的逐元素的函数，例如Sigmoid和ReLU。\n### 为什么要使用激活函数\n在NN中，如果不使用一些非线性函数作为激活函数，那么模型的深度将毫无意义（仿射变换之后，在进行仿射变换，依旧等效于一个仿射变换）。\n在CNN中也有类似的问题，如果多层卷积堆叠，中间没有非线性处理，那么完全可以用一个大核卷积层来进行替换，虽然会导致运算量上升，但是由于小核卷积的参数共享更严重，所以多个小核卷积层的表达能力还不如一个大核卷积层。\n\n激活函数不论是加在NN中还是CNN中，其主要目的是引入非线性操作，提升模型的表达能力。\n## 池化层\n池化层的本质是采样操作，用于对上一层得到的结果进行压缩，减少模型计算量，同时有助于滤除噪声。\n\n池化层主要有两类：\n- 最大池化：保留池化核范围内最大的元素，注重于纹理信息的提取、过滤噪声，更适合浅层特征。\n- 平均池化：将池化核范围内的元素取平均，注重保留背景信息、混合不同空间位置的信息，更适合深层特征。\n","source":"_posts/学习笔记/CNN相关知识.md","raw":"---\ntitle: CNN相关知识\ndate: 2020-05-07 11:12:22\ntags: [深度学习]\nmathjax: true\n---\n\n使用CNN这么久了，最近抽点时间来复习一下CNN相关的一些基础内容。\n\n# CNN中的基本操作\n## 卷积层\n一般的神经网络中，每一层是上层输出到该层输出的全连接，但是对于图像数据，如果使用全连接操作，参数量会变得非常大，难以计算和优化而且容易过拟合。\n\n卷积层有一维卷积、二维卷积、三维卷积几种常见的类型，这里主要讨论二维卷积，其他的卷积只是输入的维度数量不同，并无特殊。\n\n在Pytorch框架下的CNN实现中，卷积层的输入一般是$(B\\times C\\times H\\times W)$的形状（其他框架例如Tensorflow，使用的输入形状是$(B\\times H\\times W\\times C)$，只是维度顺序不同），其中$B$是代表batch size，表示不同的图片样本，$C$代表通道数量，例如原始彩色图片一般是三个通道：R、G、B，最后的$H,\\ W$是特征图的像素高度和像素宽度。\n\n卷积层的参数主要有两个：Kernel、Bias。\n\n其中Kernel的形状是$(H_k\\times W_k\\times C_o\\times C_i)$，$H_k, W_k$表示卷积核的大小，例如下图的卷积核大小为$3\\times 3$，$C_i$表示卷积核的通道数，如果输入的特征图是彩色图像，即一个三通道图片，那么$C_i=3$即表示在每一个像素点，对其三个通道进行全连接操作，$C_o$则是对每个像素所有通道进行全连接操作的输出通道数，也可以理解为卷积核个数（一个卷积核输出一个通道）。\n\nBias一般是个$R^{C_o}$的向量，在卷积操作结束之后，对输出的特征图进行偏移，每个通道共用其中的一个偏置参数。\n\n卷积操作如图所示：\n{%asset_img Conv.gif 卷积操作示意%}\n\n### 卷积层的参数量和计算量\n参数量即卷积核的大小加上偏置的大小：$H_k\\times W_k\\times C_o\\times C_i + C_o$\n计算量受到步长，padding方式等影响，从输入的大小来看不好计算，但是可以根据输出的特征图大小来计算：$H' \\times W' \\times (H_k\\times W_k\\times C_o\\times C_i + C_o)$\n\n### 对卷积操作的理解\n卷积操作的本质就是在特征图上使用相同的参数滑动进行局部的全连接，这样做能够减少参数量和计算量，并且因为共享权值，在进行优化时更加容易。\n\n卷积层有一些主要特点：\n- 权值共享：在图像的不同部分，使用相同的特征提取是有意义的，这也是卷积层具备平移不变性的原因。\n- 稀疏连接：对于一个输出值只和输入的一部分相关。\n\n卷积操作可以看做一种局部特征的提取，例如一些边缘检测算子，实际上就是在图片上使用相同的参数滑动进行乘法操作（匹配特征方向），将代表边缘的像素点赋予更大的像素值，从而在结果图片中将边缘表示为亮的线条，这里就是在提取局部的边缘特征并保留其位置信息，最终组成整幅图片的边缘表示。\n\n### CNN的反向传播\n这部分内容可见我的另一篇文章{% post_link CNN的反向传播 CNN的反向传播 %}\n\n## 激活函数\n激活函数一般是一个非线性的逐元素的函数，例如Sigmoid和ReLU。\n### 为什么要使用激活函数\n在NN中，如果不使用一些非线性函数作为激活函数，那么模型的深度将毫无意义（仿射变换之后，在进行仿射变换，依旧等效于一个仿射变换）。\n在CNN中也有类似的问题，如果多层卷积堆叠，中间没有非线性处理，那么完全可以用一个大核卷积层来进行替换，虽然会导致运算量上升，但是由于小核卷积的参数共享更严重，所以多个小核卷积层的表达能力还不如一个大核卷积层。\n\n激活函数不论是加在NN中还是CNN中，其主要目的是引入非线性操作，提升模型的表达能力。\n## 池化层\n池化层的本质是采样操作，用于对上一层得到的结果进行压缩，减少模型计算量，同时有助于滤除噪声。\n\n池化层主要有两类：\n- 最大池化：保留池化核范围内最大的元素，注重于纹理信息的提取、过滤噪声，更适合浅层特征。\n- 平均池化：将池化核范围内的元素取平均，注重保留背景信息、混合不同空间位置的信息，更适合深层特征。\n","slug":"学习笔记/CNN相关知识","published":1,"updated":"2020-08-31T06:39:20.738Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckjxpc3rj000844mq0i0q3iwk","content":"<p>使用CNN这么久了，最近抽点时间来复习一下CNN相关的一些基础内容。</p>\r\n<h1 id=\"cnn中的基本操作\">CNN中的基本操作</h1>\r\n<h2 id=\"卷积层\">卷积层</h2>\r\n<p>一般的神经网络中，每一层是上层输出到该层输出的全连接，但是对于图像数据，如果使用全连接操作，参数量会变得非常大，难以计算和优化而且容易过拟合。</p>\r\n<p>卷积层有一维卷积、二维卷积、三维卷积几种常见的类型，这里主要讨论二维卷积，其他的卷积只是输入的维度数量不同，并无特殊。</p>\r\n<p>在Pytorch框架下的CNN实现中，卷积层的输入一般是<span class=\"math inline\">\\((B\\times C\\times H\\times W)\\)</span>的形状（其他框架例如Tensorflow，使用的输入形状是<span class=\"math inline\">\\((B\\times H\\times W\\times C)\\)</span>，只是维度顺序不同），其中<span class=\"math inline\">\\(B\\)</span>是代表batch size，表示不同的图片样本，<span class=\"math inline\">\\(C\\)</span>代表通道数量，例如原始彩色图片一般是三个通道：R、G、B，最后的<span class=\"math inline\">\\(H,\\ W\\)</span>是特征图的像素高度和像素宽度。</p>\r\n<p>卷积层的参数主要有两个：Kernel、Bias。</p>\r\n<p>其中Kernel的形状是<span class=\"math inline\">\\((H_k\\times W_k\\times C_o\\times C_i)\\)</span>，<span class=\"math inline\">\\(H_k, W_k\\)</span>表示卷积核的大小，例如下图的卷积核大小为<span class=\"math inline\">\\(3\\times 3\\)</span>，<span class=\"math inline\">\\(C_i\\)</span>表示卷积核的通道数，如果输入的特征图是彩色图像，即一个三通道图片，那么<span class=\"math inline\">\\(C_i=3\\)</span>即表示在每一个像素点，对其三个通道进行全连接操作，<span class=\"math inline\">\\(C_o\\)</span>则是对每个像素所有通道进行全连接操作的输出通道数，也可以理解为卷积核个数（一个卷积核输出一个通道）。</p>\r\n<p>Bias一般是个<span class=\"math inline\">\\(R^{C_o}\\)</span>的向量，在卷积操作结束之后，对输出的特征图进行偏移，每个通道共用其中的一个偏置参数。</p>\r\n<p>卷积操作如图所示： <img src=\"/2020/05/07/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/CNN%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86/Conv.gif\" class=\"\" title=\"卷积操作示意\"></p>\r\n<h3 id=\"卷积层的参数量和计算量\">卷积层的参数量和计算量</h3>\r\n<p>参数量即卷积核的大小加上偏置的大小：<span class=\"math inline\">\\(H_k\\times W_k\\times C_o\\times C_i + C_o\\)</span> 计算量受到步长，padding方式等影响，从输入的大小来看不好计算，但是可以根据输出的特征图大小来计算：<span class=\"math inline\">\\(H&#39; \\times W&#39; \\times (H_k\\times W_k\\times C_o\\times C_i + C_o)\\)</span></p>\r\n<h3 id=\"对卷积操作的理解\">对卷积操作的理解</h3>\r\n<p>卷积操作的本质就是在特征图上使用相同的参数滑动进行局部的全连接，这样做能够减少参数量和计算量，并且因为共享权值，在进行优化时更加容易。</p>\r\n<p>卷积层有一些主要特点： - 权值共享：在图像的不同部分，使用相同的特征提取是有意义的，这也是卷积层具备平移不变性的原因。 - 稀疏连接：对于一个输出值只和输入的一部分相关。</p>\r\n<p>卷积操作可以看做一种局部特征的提取，例如一些边缘检测算子，实际上就是在图片上使用相同的参数滑动进行乘法操作（匹配特征方向），将代表边缘的像素点赋予更大的像素值，从而在结果图片中将边缘表示为亮的线条，这里就是在提取局部的边缘特征并保留其位置信息，最终组成整幅图片的边缘表示。</p>\r\n<h3 id=\"cnn的反向传播\">CNN的反向传播</h3>\r\n<p>这部分内容可见我的另一篇文章<a href=\"#\">Post not found: CNN的反向传播 CNN的反向传播</a></p>\r\n<h2 id=\"激活函数\">激活函数</h2>\r\n<p>激活函数一般是一个非线性的逐元素的函数，例如Sigmoid和ReLU。 ### 为什么要使用激活函数 在NN中，如果不使用一些非线性函数作为激活函数，那么模型的深度将毫无意义（仿射变换之后，在进行仿射变换，依旧等效于一个仿射变换）。 在CNN中也有类似的问题，如果多层卷积堆叠，中间没有非线性处理，那么完全可以用一个大核卷积层来进行替换，虽然会导致运算量上升，但是由于小核卷积的参数共享更严重，所以多个小核卷积层的表达能力还不如一个大核卷积层。</p>\r\n<p>激活函数不论是加在NN中还是CNN中，其主要目的是引入非线性操作，提升模型的表达能力。 ## 池化层 池化层的本质是采样操作，用于对上一层得到的结果进行压缩，减少模型计算量，同时有助于滤除噪声。</p>\r\n<p>池化层主要有两类： - 最大池化：保留池化核范围内最大的元素，注重于纹理信息的提取、过滤噪声，更适合浅层特征。 - 平均池化：将池化核范围内的元素取平均，注重保留背景信息、混合不同空间位置的信息，更适合深层特征。</p>\r\n","site":{"data":{}},"excerpt":"","more":"<p>使用CNN这么久了，最近抽点时间来复习一下CNN相关的一些基础内容。</p>\r\n<h1 id=\"cnn中的基本操作\">CNN中的基本操作</h1>\r\n<h2 id=\"卷积层\">卷积层</h2>\r\n<p>一般的神经网络中，每一层是上层输出到该层输出的全连接，但是对于图像数据，如果使用全连接操作，参数量会变得非常大，难以计算和优化而且容易过拟合。</p>\r\n<p>卷积层有一维卷积、二维卷积、三维卷积几种常见的类型，这里主要讨论二维卷积，其他的卷积只是输入的维度数量不同，并无特殊。</p>\r\n<p>在Pytorch框架下的CNN实现中，卷积层的输入一般是<span class=\"math inline\">\\((B\\times C\\times H\\times W)\\)</span>的形状（其他框架例如Tensorflow，使用的输入形状是<span class=\"math inline\">\\((B\\times H\\times W\\times C)\\)</span>，只是维度顺序不同），其中<span class=\"math inline\">\\(B\\)</span>是代表batch size，表示不同的图片样本，<span class=\"math inline\">\\(C\\)</span>代表通道数量，例如原始彩色图片一般是三个通道：R、G、B，最后的<span class=\"math inline\">\\(H,\\ W\\)</span>是特征图的像素高度和像素宽度。</p>\r\n<p>卷积层的参数主要有两个：Kernel、Bias。</p>\r\n<p>其中Kernel的形状是<span class=\"math inline\">\\((H_k\\times W_k\\times C_o\\times C_i)\\)</span>，<span class=\"math inline\">\\(H_k, W_k\\)</span>表示卷积核的大小，例如下图的卷积核大小为<span class=\"math inline\">\\(3\\times 3\\)</span>，<span class=\"math inline\">\\(C_i\\)</span>表示卷积核的通道数，如果输入的特征图是彩色图像，即一个三通道图片，那么<span class=\"math inline\">\\(C_i=3\\)</span>即表示在每一个像素点，对其三个通道进行全连接操作，<span class=\"math inline\">\\(C_o\\)</span>则是对每个像素所有通道进行全连接操作的输出通道数，也可以理解为卷积核个数（一个卷积核输出一个通道）。</p>\r\n<p>Bias一般是个<span class=\"math inline\">\\(R^{C_o}\\)</span>的向量，在卷积操作结束之后，对输出的特征图进行偏移，每个通道共用其中的一个偏置参数。</p>\r\n<p>卷积操作如图所示： <img src=\"/2020/05/07/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/CNN%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86/Conv.gif\" class=\"\" title=\"卷积操作示意\"></p>\r\n<h3 id=\"卷积层的参数量和计算量\">卷积层的参数量和计算量</h3>\r\n<p>参数量即卷积核的大小加上偏置的大小：<span class=\"math inline\">\\(H_k\\times W_k\\times C_o\\times C_i + C_o\\)</span> 计算量受到步长，padding方式等影响，从输入的大小来看不好计算，但是可以根据输出的特征图大小来计算：<span class=\"math inline\">\\(H&#39; \\times W&#39; \\times (H_k\\times W_k\\times C_o\\times C_i + C_o)\\)</span></p>\r\n<h3 id=\"对卷积操作的理解\">对卷积操作的理解</h3>\r\n<p>卷积操作的本质就是在特征图上使用相同的参数滑动进行局部的全连接，这样做能够减少参数量和计算量，并且因为共享权值，在进行优化时更加容易。</p>\r\n<p>卷积层有一些主要特点： - 权值共享：在图像的不同部分，使用相同的特征提取是有意义的，这也是卷积层具备平移不变性的原因。 - 稀疏连接：对于一个输出值只和输入的一部分相关。</p>\r\n<p>卷积操作可以看做一种局部特征的提取，例如一些边缘检测算子，实际上就是在图片上使用相同的参数滑动进行乘法操作（匹配特征方向），将代表边缘的像素点赋予更大的像素值，从而在结果图片中将边缘表示为亮的线条，这里就是在提取局部的边缘特征并保留其位置信息，最终组成整幅图片的边缘表示。</p>\r\n<h3 id=\"cnn的反向传播\">CNN的反向传播</h3>\r\n<p>这部分内容可见我的另一篇文章<a href=\"#\">Post not found: CNN的反向传播 CNN的反向传播</a></p>\r\n<h2 id=\"激活函数\">激活函数</h2>\r\n<p>激活函数一般是一个非线性的逐元素的函数，例如Sigmoid和ReLU。 ### 为什么要使用激活函数 在NN中，如果不使用一些非线性函数作为激活函数，那么模型的深度将毫无意义（仿射变换之后，在进行仿射变换，依旧等效于一个仿射变换）。 在CNN中也有类似的问题，如果多层卷积堆叠，中间没有非线性处理，那么完全可以用一个大核卷积层来进行替换，虽然会导致运算量上升，但是由于小核卷积的参数共享更严重，所以多个小核卷积层的表达能力还不如一个大核卷积层。</p>\r\n<p>激活函数不论是加在NN中还是CNN中，其主要目的是引入非线性操作，提升模型的表达能力。 ## 池化层 池化层的本质是采样操作，用于对上一层得到的结果进行压缩，减少模型计算量，同时有助于滤除噪声。</p>\r\n<p>池化层主要有两类： - 最大池化：保留池化核范围内最大的元素，注重于纹理信息的提取、过滤噪声，更适合浅层特征。 - 平均池化：将池化核范围内的元素取平均，注重保留背景信息、混合不同空间位置的信息，更适合深层特征。</p>\r\n"},{"title":"EM算法学习笔记","date":"2020-01-02T07:27:25.000Z","mathjax":true,"_content":"\n## 问题描述\n先看一个简单问题，例如现在有$m$个独立同分布样本$X=(x^{1}, x^{2}, \\dots,x^{m})$，我们需要估计数据$X$的分布，那么我们只需要找到一个概率分布$P(x^{(i)}|\\theta)$使得数据$X$的对数似然最大就行了，即只需要估计出参数$\\theta=\\mathop{\\arg\\max}_{\\theta}L(\\theta)=\\mathop{\\arg\\max}_{\\theta}\\sum_{i=1}^{m}\\log P(x^{(i)}|\\theta)$\n\n接下来问题变得复杂一些，还有一个不可观测的隐含变量$Z$会影响到$X$的分布，这个时候为了估计$X$的分布，所进行的工作将变为找到$\\theta, Z = \\mathop{\\arg\\max}_{\\theta, Z}L(\\theta, Z) = \\mathop{\\arg\\max}_{\\theta, Z}\\sum_{i=1}^{m}logP(x^{(i)}|\\theta) = \\mathop{\\arg\\max}_{\\theta, Z}\\sum_{i=1}^{m}\\log \\sum_{z^{(i)}}P(x^{(i)}, z^{(i)}|\\theta)$\n这里$P(x^{(i)}|\\theta)$其实是$P(x^{(i)}, z^{(i)}|\\theta)$的边缘分布。\n\n对于这个问题，直接对$Z$和$\\theta$求导过于复杂，可能无法计算，因此需要EM算法来进行优化，得到一个可以接受的局部最优解。\n\n## EM算法\n\nEM算法分为两个重复的步骤，Expectation和Maximization\n\n### Expectation\n\n首先，重写上式如下，这里多出了一个关于$Z$的概率分布$Q$，对于$z^{(i)}$的所有取值，可知$\\sum_{z^{(i)}}Q(z^{(i)}) = 1$。\n$$\n\\begin{aligned}\n&\\mathop{\\arg\\max}_{\\theta, Z}\\sum_{i=1}^{m}\\log \\sum_{z^{(i)}}P(x^{(i)}, z^{(i)}|\\theta)\\\\\n&= \\mathop{\\arg\\max}_{\\theta, Z} \\sum_{i=1}^{m}\\log \\sum_{z^{(i)}} Q(z^{(i)}) \\frac{P(x^{(i)}, z^{(i)}|\\theta)}{Q(z^{(i)})}\\\\\n\\end{aligned}\n$$\n\n由于$\\sum_{z^{(i)}}Q(z^{(i)}) = 1$，因此$\\log \\sum_{z^{(i)}} Q(z^{(i)}) \\frac{P(x^{(i)}, z^{(i)}|\\theta)}{Q(z^{(i)})} = \\log E(\\frac{P(x^{(i)}, z^{(i)}|\\theta)}{Q(z^{(i)})})$，又由于$\\log$为凹函数，因此$\\log E(\\frac{P(x^{(i)}, z^{(i)}|\\theta)}{Q(z^{(i)})}) \\ge E(\\log \\frac{P(x^{(i)}, z^{(i)}|\\theta)}{Q(z^{(i)})})$，即：\n\n$$\n\\sum_{i=1}^{m}\\log \\sum_{z^{(i)}} Q(z^{(i)}) \\frac{P(x^{(i)}, z^{(i)}|\\theta)}{Q(z^{(i)})} \\ge \\sum_{i=1}^{m} \\sum_{z^{(i)}} Q(z^{(i)}) \\log \\frac{P(x^{(i)}, z^{(i)}|\\theta)}{Q(z^{(i)})}\n$$\n\n此时，假设$\\theta$已经固定，那么要使得下面的等式成立\n\n$\\log E(\\frac{P(x^{(i)}, z^{(i)}|\\theta)}{Q(z^{(i)})}) = E(\\log \\frac{P(x^{(i)}, z^{(i)}|\\theta)}{Q(z^{(i)})})$\n\n我们需要让$\\frac{P(x^{(i)}, z^{(i)}|\\theta)}{Q(z^{(i)})}$等于一个常数$c$，因此对于所有的$i$，都满足\n\n$c \\times Q(z^{(i)}) = P(x^{(i)}, z^{(i)}|\\theta)$\n\n两边同时求和，得到\n\n$c \\times \\sum_{z} Q(z^{(i)}) = \\sum_{z} P(x^{(i)}, z^{(i)}|\\theta)$\n\n即：\n\n$c = \\sum_{z} P(x^{(i)}, z^{(i)}|\\theta)$\n\n因此：\n\n$Q(z^{(i)}) = \\frac{P(x^{(i)}, z^{(i)}|\\theta)}{\\sum_{z} P(x^{(i)}, z^{(i)}|\\theta)} = \\frac{P(x^{(i)}, z^{(i)}|\\theta)}{P(x^{(i)}|\\theta)} = P(z^{(i)}|x^{(i)},\\theta)$\n\n这样一来，确定了$Q(z^{(i)})$的值，等式可以成立，对数似然的下确界就被找到了，这个下确界即$\\sum_{i=1}^{m} \\sum_{z^{(i)}} Q(z^{(i)}) \\log \\frac{P(x^{(i)}, z^{(i)}|\\theta)}{Q(z^{(i)})}$，接下来在Maximization步骤，固定$Q(z^{(i)})$，通过$\\theta$来最大化这个下确界，可以间接的达到优化对数似然的目的。\n\n### Maximization\n由于$Q(z^{(i)}) = P(z^{(i)}|x^{(i)},\\theta^{old})$被固定为常量，不再和$\\theta$相关，因此问题变为\n$$\n\\begin{aligned}\n&\\mathop{\\arg\\max}_{\\theta} \\sum_{i=1}^{m} \\sum_{z^{(i)}} Q(z^{(i)}) \\log \\frac{P(x^{(i)}, z^{(i)}|\\theta)}{Q(z^{(i)})}\\\\\n&=\\mathop{\\arg\\max}_{\\theta} \\sum_{i=1}^{m} \\sum_{z^{(i)}} Q(z^{(i)}) \\log P(x^{(i)}, z^{(i)}|\\theta)\n\\end{aligned}\n$$\n\n总的来说，只需要首先初始化$\\theta$，然后重复循环Expectation和Maximization两个步骤，不断更新$Q(z^{(i)})$和$\\theta$，EM算法被证明最终可以收敛到一个局部最优解。\n","source":"_posts/学习笔记/EM算法学习笔记.md","raw":"---\ntitle: EM算法学习笔记\ndate: 2020-01-02 15:27:25\ntags: 机器学习\nmathjax: true\n---\n\n## 问题描述\n先看一个简单问题，例如现在有$m$个独立同分布样本$X=(x^{1}, x^{2}, \\dots,x^{m})$，我们需要估计数据$X$的分布，那么我们只需要找到一个概率分布$P(x^{(i)}|\\theta)$使得数据$X$的对数似然最大就行了，即只需要估计出参数$\\theta=\\mathop{\\arg\\max}_{\\theta}L(\\theta)=\\mathop{\\arg\\max}_{\\theta}\\sum_{i=1}^{m}\\log P(x^{(i)}|\\theta)$\n\n接下来问题变得复杂一些，还有一个不可观测的隐含变量$Z$会影响到$X$的分布，这个时候为了估计$X$的分布，所进行的工作将变为找到$\\theta, Z = \\mathop{\\arg\\max}_{\\theta, Z}L(\\theta, Z) = \\mathop{\\arg\\max}_{\\theta, Z}\\sum_{i=1}^{m}logP(x^{(i)}|\\theta) = \\mathop{\\arg\\max}_{\\theta, Z}\\sum_{i=1}^{m}\\log \\sum_{z^{(i)}}P(x^{(i)}, z^{(i)}|\\theta)$\n这里$P(x^{(i)}|\\theta)$其实是$P(x^{(i)}, z^{(i)}|\\theta)$的边缘分布。\n\n对于这个问题，直接对$Z$和$\\theta$求导过于复杂，可能无法计算，因此需要EM算法来进行优化，得到一个可以接受的局部最优解。\n\n## EM算法\n\nEM算法分为两个重复的步骤，Expectation和Maximization\n\n### Expectation\n\n首先，重写上式如下，这里多出了一个关于$Z$的概率分布$Q$，对于$z^{(i)}$的所有取值，可知$\\sum_{z^{(i)}}Q(z^{(i)}) = 1$。\n$$\n\\begin{aligned}\n&\\mathop{\\arg\\max}_{\\theta, Z}\\sum_{i=1}^{m}\\log \\sum_{z^{(i)}}P(x^{(i)}, z^{(i)}|\\theta)\\\\\n&= \\mathop{\\arg\\max}_{\\theta, Z} \\sum_{i=1}^{m}\\log \\sum_{z^{(i)}} Q(z^{(i)}) \\frac{P(x^{(i)}, z^{(i)}|\\theta)}{Q(z^{(i)})}\\\\\n\\end{aligned}\n$$\n\n由于$\\sum_{z^{(i)}}Q(z^{(i)}) = 1$，因此$\\log \\sum_{z^{(i)}} Q(z^{(i)}) \\frac{P(x^{(i)}, z^{(i)}|\\theta)}{Q(z^{(i)})} = \\log E(\\frac{P(x^{(i)}, z^{(i)}|\\theta)}{Q(z^{(i)})})$，又由于$\\log$为凹函数，因此$\\log E(\\frac{P(x^{(i)}, z^{(i)}|\\theta)}{Q(z^{(i)})}) \\ge E(\\log \\frac{P(x^{(i)}, z^{(i)}|\\theta)}{Q(z^{(i)})})$，即：\n\n$$\n\\sum_{i=1}^{m}\\log \\sum_{z^{(i)}} Q(z^{(i)}) \\frac{P(x^{(i)}, z^{(i)}|\\theta)}{Q(z^{(i)})} \\ge \\sum_{i=1}^{m} \\sum_{z^{(i)}} Q(z^{(i)}) \\log \\frac{P(x^{(i)}, z^{(i)}|\\theta)}{Q(z^{(i)})}\n$$\n\n此时，假设$\\theta$已经固定，那么要使得下面的等式成立\n\n$\\log E(\\frac{P(x^{(i)}, z^{(i)}|\\theta)}{Q(z^{(i)})}) = E(\\log \\frac{P(x^{(i)}, z^{(i)}|\\theta)}{Q(z^{(i)})})$\n\n我们需要让$\\frac{P(x^{(i)}, z^{(i)}|\\theta)}{Q(z^{(i)})}$等于一个常数$c$，因此对于所有的$i$，都满足\n\n$c \\times Q(z^{(i)}) = P(x^{(i)}, z^{(i)}|\\theta)$\n\n两边同时求和，得到\n\n$c \\times \\sum_{z} Q(z^{(i)}) = \\sum_{z} P(x^{(i)}, z^{(i)}|\\theta)$\n\n即：\n\n$c = \\sum_{z} P(x^{(i)}, z^{(i)}|\\theta)$\n\n因此：\n\n$Q(z^{(i)}) = \\frac{P(x^{(i)}, z^{(i)}|\\theta)}{\\sum_{z} P(x^{(i)}, z^{(i)}|\\theta)} = \\frac{P(x^{(i)}, z^{(i)}|\\theta)}{P(x^{(i)}|\\theta)} = P(z^{(i)}|x^{(i)},\\theta)$\n\n这样一来，确定了$Q(z^{(i)})$的值，等式可以成立，对数似然的下确界就被找到了，这个下确界即$\\sum_{i=1}^{m} \\sum_{z^{(i)}} Q(z^{(i)}) \\log \\frac{P(x^{(i)}, z^{(i)}|\\theta)}{Q(z^{(i)})}$，接下来在Maximization步骤，固定$Q(z^{(i)})$，通过$\\theta$来最大化这个下确界，可以间接的达到优化对数似然的目的。\n\n### Maximization\n由于$Q(z^{(i)}) = P(z^{(i)}|x^{(i)},\\theta^{old})$被固定为常量，不再和$\\theta$相关，因此问题变为\n$$\n\\begin{aligned}\n&\\mathop{\\arg\\max}_{\\theta} \\sum_{i=1}^{m} \\sum_{z^{(i)}} Q(z^{(i)}) \\log \\frac{P(x^{(i)}, z^{(i)}|\\theta)}{Q(z^{(i)})}\\\\\n&=\\mathop{\\arg\\max}_{\\theta} \\sum_{i=1}^{m} \\sum_{z^{(i)}} Q(z^{(i)}) \\log P(x^{(i)}, z^{(i)}|\\theta)\n\\end{aligned}\n$$\n\n总的来说，只需要首先初始化$\\theta$，然后重复循环Expectation和Maximization两个步骤，不断更新$Q(z^{(i)})$和$\\theta$，EM算法被证明最终可以收敛到一个局部最优解。\n","slug":"学习笔记/EM算法学习笔记","published":1,"updated":"2020-08-31T06:39:20.741Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckjxpc3rk000a44mq7i11hr7x","content":"<h2 id=\"问题描述\">问题描述</h2>\r\n<p>先看一个简单问题，例如现在有<span class=\"math inline\">\\(m\\)</span>个独立同分布样本<span class=\"math inline\">\\(X=(x^{1}, x^{2}, \\dots,x^{m})\\)</span>，我们需要估计数据<span class=\"math inline\">\\(X\\)</span>的分布，那么我们只需要找到一个概率分布<span class=\"math inline\">\\(P(x^{(i)}|\\theta)\\)</span>使得数据<span class=\"math inline\">\\(X\\)</span>的对数似然最大就行了，即只需要估计出参数<span class=\"math inline\">\\(\\theta=\\mathop{\\arg\\max}_{\\theta}L(\\theta)=\\mathop{\\arg\\max}_{\\theta}\\sum_{i=1}^{m}\\log P(x^{(i)}|\\theta)\\)</span></p>\r\n<p>接下来问题变得复杂一些，还有一个不可观测的隐含变量<span class=\"math inline\">\\(Z\\)</span>会影响到<span class=\"math inline\">\\(X\\)</span>的分布，这个时候为了估计<span class=\"math inline\">\\(X\\)</span>的分布，所进行的工作将变为找到<span class=\"math inline\">\\(\\theta, Z = \\mathop{\\arg\\max}_{\\theta, Z}L(\\theta, Z) = \\mathop{\\arg\\max}_{\\theta, Z}\\sum_{i=1}^{m}logP(x^{(i)}|\\theta) = \\mathop{\\arg\\max}_{\\theta, Z}\\sum_{i=1}^{m}\\log \\sum_{z^{(i)}}P(x^{(i)}, z^{(i)}|\\theta)\\)</span> 这里<span class=\"math inline\">\\(P(x^{(i)}|\\theta)\\)</span>其实是<span class=\"math inline\">\\(P(x^{(i)}, z^{(i)}|\\theta)\\)</span>的边缘分布。</p>\r\n<p>对于这个问题，直接对<span class=\"math inline\">\\(Z\\)</span>和<span class=\"math inline\">\\(\\theta\\)</span>求导过于复杂，可能无法计算，因此需要EM算法来进行优化，得到一个可以接受的局部最优解。</p>\r\n<h2 id=\"em算法\">EM算法</h2>\r\n<p>EM算法分为两个重复的步骤，Expectation和Maximization</p>\r\n<h3 id=\"expectation\">Expectation</h3>\r\n<p>首先，重写上式如下，这里多出了一个关于<span class=\"math inline\">\\(Z\\)</span>的概率分布<span class=\"math inline\">\\(Q\\)</span>，对于<span class=\"math inline\">\\(z^{(i)}\\)</span>的所有取值，可知<span class=\"math inline\">\\(\\sum_{z^{(i)}}Q(z^{(i)}) = 1\\)</span>。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n&amp;\\mathop{\\arg\\max}_{\\theta, Z}\\sum_{i=1}^{m}\\log \\sum_{z^{(i)}}P(x^{(i)}, z^{(i)}|\\theta)\\\\\r\n&amp;= \\mathop{\\arg\\max}_{\\theta, Z} \\sum_{i=1}^{m}\\log \\sum_{z^{(i)}} Q(z^{(i)}) \\frac{P(x^{(i)}, z^{(i)}|\\theta)}{Q(z^{(i)})}\\\\\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>由于<span class=\"math inline\">\\(\\sum_{z^{(i)}}Q(z^{(i)}) = 1\\)</span>，因此<span class=\"math inline\">\\(\\log \\sum_{z^{(i)}} Q(z^{(i)}) \\frac{P(x^{(i)}, z^{(i)}|\\theta)}{Q(z^{(i)})} = \\log E(\\frac{P(x^{(i)}, z^{(i)}|\\theta)}{Q(z^{(i)})})\\)</span>，又由于<span class=\"math inline\">\\(\\log\\)</span>为凹函数，因此<span class=\"math inline\">\\(\\log E(\\frac{P(x^{(i)}, z^{(i)}|\\theta)}{Q(z^{(i)})}) \\ge E(\\log \\frac{P(x^{(i)}, z^{(i)}|\\theta)}{Q(z^{(i)})})\\)</span>，即：</p>\r\n<p><span class=\"math display\">\\[\r\n\\sum_{i=1}^{m}\\log \\sum_{z^{(i)}} Q(z^{(i)}) \\frac{P(x^{(i)}, z^{(i)}|\\theta)}{Q(z^{(i)})} \\ge \\sum_{i=1}^{m} \\sum_{z^{(i)}} Q(z^{(i)}) \\log \\frac{P(x^{(i)}, z^{(i)}|\\theta)}{Q(z^{(i)})}\r\n\\]</span></p>\r\n<p>此时，假设<span class=\"math inline\">\\(\\theta\\)</span>已经固定，那么要使得下面的等式成立</p>\r\n<p><span class=\"math inline\">\\(\\log E(\\frac{P(x^{(i)}, z^{(i)}|\\theta)}{Q(z^{(i)})}) = E(\\log \\frac{P(x^{(i)}, z^{(i)}|\\theta)}{Q(z^{(i)})})\\)</span></p>\r\n<p>我们需要让<span class=\"math inline\">\\(\\frac{P(x^{(i)}, z^{(i)}|\\theta)}{Q(z^{(i)})}\\)</span>等于一个常数<span class=\"math inline\">\\(c\\)</span>，因此对于所有的<span class=\"math inline\">\\(i\\)</span>，都满足</p>\r\n<p><span class=\"math inline\">\\(c \\times Q(z^{(i)}) = P(x^{(i)}, z^{(i)}|\\theta)\\)</span></p>\r\n<p>两边同时求和，得到</p>\r\n<p><span class=\"math inline\">\\(c \\times \\sum_{z} Q(z^{(i)}) = \\sum_{z} P(x^{(i)}, z^{(i)}|\\theta)\\)</span></p>\r\n<p>即：</p>\r\n<p><span class=\"math inline\">\\(c = \\sum_{z} P(x^{(i)}, z^{(i)}|\\theta)\\)</span></p>\r\n<p>因此：</p>\r\n<p><span class=\"math inline\">\\(Q(z^{(i)}) = \\frac{P(x^{(i)}, z^{(i)}|\\theta)}{\\sum_{z} P(x^{(i)}, z^{(i)}|\\theta)} = \\frac{P(x^{(i)}, z^{(i)}|\\theta)}{P(x^{(i)}|\\theta)} = P(z^{(i)}|x^{(i)},\\theta)\\)</span></p>\r\n<p>这样一来，确定了<span class=\"math inline\">\\(Q(z^{(i)})\\)</span>的值，等式可以成立，对数似然的下确界就被找到了，这个下确界即<span class=\"math inline\">\\(\\sum_{i=1}^{m} \\sum_{z^{(i)}} Q(z^{(i)}) \\log \\frac{P(x^{(i)}, z^{(i)}|\\theta)}{Q(z^{(i)})}\\)</span>，接下来在Maximization步骤，固定<span class=\"math inline\">\\(Q(z^{(i)})\\)</span>，通过<span class=\"math inline\">\\(\\theta\\)</span>来最大化这个下确界，可以间接的达到优化对数似然的目的。</p>\r\n<h3 id=\"maximization\">Maximization</h3>\r\n<p>由于<span class=\"math inline\">\\(Q(z^{(i)}) = P(z^{(i)}|x^{(i)},\\theta^{old})\\)</span>被固定为常量，不再和<span class=\"math inline\">\\(\\theta\\)</span>相关，因此问题变为 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n&amp;\\mathop{\\arg\\max}_{\\theta} \\sum_{i=1}^{m} \\sum_{z^{(i)}} Q(z^{(i)}) \\log \\frac{P(x^{(i)}, z^{(i)}|\\theta)}{Q(z^{(i)})}\\\\\r\n&amp;=\\mathop{\\arg\\max}_{\\theta} \\sum_{i=1}^{m} \\sum_{z^{(i)}} Q(z^{(i)}) \\log P(x^{(i)}, z^{(i)}|\\theta)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>总的来说，只需要首先初始化<span class=\"math inline\">\\(\\theta\\)</span>，然后重复循环Expectation和Maximization两个步骤，不断更新<span class=\"math inline\">\\(Q(z^{(i)})\\)</span>和<span class=\"math inline\">\\(\\theta\\)</span>，EM算法被证明最终可以收敛到一个局部最优解。</p>\r\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"问题描述\">问题描述</h2>\r\n<p>先看一个简单问题，例如现在有<span class=\"math inline\">\\(m\\)</span>个独立同分布样本<span class=\"math inline\">\\(X=(x^{1}, x^{2}, \\dots,x^{m})\\)</span>，我们需要估计数据<span class=\"math inline\">\\(X\\)</span>的分布，那么我们只需要找到一个概率分布<span class=\"math inline\">\\(P(x^{(i)}|\\theta)\\)</span>使得数据<span class=\"math inline\">\\(X\\)</span>的对数似然最大就行了，即只需要估计出参数<span class=\"math inline\">\\(\\theta=\\mathop{\\arg\\max}_{\\theta}L(\\theta)=\\mathop{\\arg\\max}_{\\theta}\\sum_{i=1}^{m}\\log P(x^{(i)}|\\theta)\\)</span></p>\r\n<p>接下来问题变得复杂一些，还有一个不可观测的隐含变量<span class=\"math inline\">\\(Z\\)</span>会影响到<span class=\"math inline\">\\(X\\)</span>的分布，这个时候为了估计<span class=\"math inline\">\\(X\\)</span>的分布，所进行的工作将变为找到<span class=\"math inline\">\\(\\theta, Z = \\mathop{\\arg\\max}_{\\theta, Z}L(\\theta, Z) = \\mathop{\\arg\\max}_{\\theta, Z}\\sum_{i=1}^{m}logP(x^{(i)}|\\theta) = \\mathop{\\arg\\max}_{\\theta, Z}\\sum_{i=1}^{m}\\log \\sum_{z^{(i)}}P(x^{(i)}, z^{(i)}|\\theta)\\)</span> 这里<span class=\"math inline\">\\(P(x^{(i)}|\\theta)\\)</span>其实是<span class=\"math inline\">\\(P(x^{(i)}, z^{(i)}|\\theta)\\)</span>的边缘分布。</p>\r\n<p>对于这个问题，直接对<span class=\"math inline\">\\(Z\\)</span>和<span class=\"math inline\">\\(\\theta\\)</span>求导过于复杂，可能无法计算，因此需要EM算法来进行优化，得到一个可以接受的局部最优解。</p>\r\n<h2 id=\"em算法\">EM算法</h2>\r\n<p>EM算法分为两个重复的步骤，Expectation和Maximization</p>\r\n<h3 id=\"expectation\">Expectation</h3>\r\n<p>首先，重写上式如下，这里多出了一个关于<span class=\"math inline\">\\(Z\\)</span>的概率分布<span class=\"math inline\">\\(Q\\)</span>，对于<span class=\"math inline\">\\(z^{(i)}\\)</span>的所有取值，可知<span class=\"math inline\">\\(\\sum_{z^{(i)}}Q(z^{(i)}) = 1\\)</span>。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n&amp;\\mathop{\\arg\\max}_{\\theta, Z}\\sum_{i=1}^{m}\\log \\sum_{z^{(i)}}P(x^{(i)}, z^{(i)}|\\theta)\\\\\r\n&amp;= \\mathop{\\arg\\max}_{\\theta, Z} \\sum_{i=1}^{m}\\log \\sum_{z^{(i)}} Q(z^{(i)}) \\frac{P(x^{(i)}, z^{(i)}|\\theta)}{Q(z^{(i)})}\\\\\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>由于<span class=\"math inline\">\\(\\sum_{z^{(i)}}Q(z^{(i)}) = 1\\)</span>，因此<span class=\"math inline\">\\(\\log \\sum_{z^{(i)}} Q(z^{(i)}) \\frac{P(x^{(i)}, z^{(i)}|\\theta)}{Q(z^{(i)})} = \\log E(\\frac{P(x^{(i)}, z^{(i)}|\\theta)}{Q(z^{(i)})})\\)</span>，又由于<span class=\"math inline\">\\(\\log\\)</span>为凹函数，因此<span class=\"math inline\">\\(\\log E(\\frac{P(x^{(i)}, z^{(i)}|\\theta)}{Q(z^{(i)})}) \\ge E(\\log \\frac{P(x^{(i)}, z^{(i)}|\\theta)}{Q(z^{(i)})})\\)</span>，即：</p>\r\n<p><span class=\"math display\">\\[\r\n\\sum_{i=1}^{m}\\log \\sum_{z^{(i)}} Q(z^{(i)}) \\frac{P(x^{(i)}, z^{(i)}|\\theta)}{Q(z^{(i)})} \\ge \\sum_{i=1}^{m} \\sum_{z^{(i)}} Q(z^{(i)}) \\log \\frac{P(x^{(i)}, z^{(i)}|\\theta)}{Q(z^{(i)})}\r\n\\]</span></p>\r\n<p>此时，假设<span class=\"math inline\">\\(\\theta\\)</span>已经固定，那么要使得下面的等式成立</p>\r\n<p><span class=\"math inline\">\\(\\log E(\\frac{P(x^{(i)}, z^{(i)}|\\theta)}{Q(z^{(i)})}) = E(\\log \\frac{P(x^{(i)}, z^{(i)}|\\theta)}{Q(z^{(i)})})\\)</span></p>\r\n<p>我们需要让<span class=\"math inline\">\\(\\frac{P(x^{(i)}, z^{(i)}|\\theta)}{Q(z^{(i)})}\\)</span>等于一个常数<span class=\"math inline\">\\(c\\)</span>，因此对于所有的<span class=\"math inline\">\\(i\\)</span>，都满足</p>\r\n<p><span class=\"math inline\">\\(c \\times Q(z^{(i)}) = P(x^{(i)}, z^{(i)}|\\theta)\\)</span></p>\r\n<p>两边同时求和，得到</p>\r\n<p><span class=\"math inline\">\\(c \\times \\sum_{z} Q(z^{(i)}) = \\sum_{z} P(x^{(i)}, z^{(i)}|\\theta)\\)</span></p>\r\n<p>即：</p>\r\n<p><span class=\"math inline\">\\(c = \\sum_{z} P(x^{(i)}, z^{(i)}|\\theta)\\)</span></p>\r\n<p>因此：</p>\r\n<p><span class=\"math inline\">\\(Q(z^{(i)}) = \\frac{P(x^{(i)}, z^{(i)}|\\theta)}{\\sum_{z} P(x^{(i)}, z^{(i)}|\\theta)} = \\frac{P(x^{(i)}, z^{(i)}|\\theta)}{P(x^{(i)}|\\theta)} = P(z^{(i)}|x^{(i)},\\theta)\\)</span></p>\r\n<p>这样一来，确定了<span class=\"math inline\">\\(Q(z^{(i)})\\)</span>的值，等式可以成立，对数似然的下确界就被找到了，这个下确界即<span class=\"math inline\">\\(\\sum_{i=1}^{m} \\sum_{z^{(i)}} Q(z^{(i)}) \\log \\frac{P(x^{(i)}, z^{(i)}|\\theta)}{Q(z^{(i)})}\\)</span>，接下来在Maximization步骤，固定<span class=\"math inline\">\\(Q(z^{(i)})\\)</span>，通过<span class=\"math inline\">\\(\\theta\\)</span>来最大化这个下确界，可以间接的达到优化对数似然的目的。</p>\r\n<h3 id=\"maximization\">Maximization</h3>\r\n<p>由于<span class=\"math inline\">\\(Q(z^{(i)}) = P(z^{(i)}|x^{(i)},\\theta^{old})\\)</span>被固定为常量，不再和<span class=\"math inline\">\\(\\theta\\)</span>相关，因此问题变为 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n&amp;\\mathop{\\arg\\max}_{\\theta} \\sum_{i=1}^{m} \\sum_{z^{(i)}} Q(z^{(i)}) \\log \\frac{P(x^{(i)}, z^{(i)}|\\theta)}{Q(z^{(i)})}\\\\\r\n&amp;=\\mathop{\\arg\\max}_{\\theta} \\sum_{i=1}^{m} \\sum_{z^{(i)}} Q(z^{(i)}) \\log P(x^{(i)}, z^{(i)}|\\theta)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>总的来说，只需要首先初始化<span class=\"math inline\">\\(\\theta\\)</span>，然后重复循环Expectation和Maximization两个步骤，不断更新<span class=\"math inline\">\\(Q(z^{(i)})\\)</span>和<span class=\"math inline\">\\(\\theta\\)</span>，EM算法被证明最终可以收敛到一个局部最优解。</p>\r\n"},{"title":"ResNet总结","date":"2020-05-15T05:18:26.000Z","mathjax":true,"_content":"\n# ResNet\n在论文《Deep Residual Learning for Image Recognition》中提出了ResNet结构，其主要目的是使更深层的模型训练变得更加容易。\n\nResNet的主要特点在于其残差块（Residual Block）结构，如下图所示，其中左边为ResNet-18和ResNet-34中使用的残差块结构，右边则是ResNet-50、ResNet-101、ResNet-152使用的残差快结构，两者不同之处在于瓶颈层（BottleNeck）的使用，瓶颈层的作用主要是减少参数量和运算量。\n\n![Residual Block](Residual_block.png)\n\n在原论文中，上图的曲线连接被称为“identity mapping”，通过这样的短连接，显式地改变了模型所模拟的映射函数，从$H(x)$变成$H(x) + x$，这里的相加操作有两个细节：1、如果$H(x)$的通道数和$x$不一样，那么$x$需要调整通道数，论文中尝试了三种方案，第一种是使用0填充，第二种是仅仅在通道数不一样的层使用$1\\times 1$卷积来调整通道，第三种是所有的residual block中都使用$1\\times 1$卷积，实验结果发现第三种方案最好，但是作者解释说这主要是因为参数量上升带来的性能增加，而且和第二种方案相比效果提升不大，因此最终还是用的第二种方案。2、如果除了通道不一样，$H(x)$的大小和$x$也不一样，那么在$x$上使用的$1\\times 1$卷积的stride设置为2。\n\n## 对ResNet的理解\nResNet中的短连接主要有两个好处：短连接使得梯度的传递更加稳定，短连接显式的增加了特征利用率，使得模型的深层仍然可以看到浅层特征。\n\nResidual Block还有一个好处是可以让模型自由的选择深度，例如模型在最后几层如果觉得在加深深度没有效果，完全可以把最后几层的$H(x)$中的参数学习为全0，这样最后几层相当于没有任何的操作，从而让学习过程来决定模型的深度。\n\nResNet在设计的过程中，可以借鉴的一个思路是：**设计模型结构的时候，如果能够保证模型参数在某个情况下能够使得当前模型和没有改变结构时的原始模型相同，那么设计出来的模型结构效果下限也就是原来的模型了。**\n\n# ResNetV2\n在论文《Identity Mappings in Deep Residual Networks》中，提出了ResNetV2结构，这里主要是将Residual Block的结构做了一些修改，如下图所示，其中（a）图为原始的Residual Block结构，（e）图为ResNetV2的Residual Block结构，其余结构在论文中也有实验，但是最终发现（e）图所示结构效果最佳。\n\n![Residual Block v2](Residual_block_v2.png)\n\n上图五种结构的实验结果如下表，其中数字代表分类错误率。\n\n![各结构效果对比](exp_result.png)\n\n以上结构中，（b）图的结构表现非常差，我认为其原因在于addition之前两个分支的分布不统一，导致在addition之后经过BN层时对identity mapping的输出分布有一定程度上的扰乱（也可以理解为identity mapping因为已经经过BN层，因此方差小，但是另一个分支方差大，导致经过BN层之后，基本上只保留了另外一个分支的信号）。\n\n（c）图中效果差的原因我认为主要是ReLU在addition之前，导致每次相加都是加的一个非负值，因此限制了模型的信号强度只能越来越大，模型的表达能力受限。\n\n（d）图的结果其实和（a）图接近，我认为这两个结构比（e）图稍差的地方在于：在经过权重层之前，信号的分布已经被打乱且没有经过BN层的整合，减弱了BN层在整个模型中的作用。\n","source":"_posts/学习笔记/ResNet总结.md","raw":"---\ntitle: ResNet总结\ndate: 2020-05-15 13:18:26\ntags: [深度学习]\nmathjax: true\n---\n\n# ResNet\n在论文《Deep Residual Learning for Image Recognition》中提出了ResNet结构，其主要目的是使更深层的模型训练变得更加容易。\n\nResNet的主要特点在于其残差块（Residual Block）结构，如下图所示，其中左边为ResNet-18和ResNet-34中使用的残差块结构，右边则是ResNet-50、ResNet-101、ResNet-152使用的残差快结构，两者不同之处在于瓶颈层（BottleNeck）的使用，瓶颈层的作用主要是减少参数量和运算量。\n\n![Residual Block](Residual_block.png)\n\n在原论文中，上图的曲线连接被称为“identity mapping”，通过这样的短连接，显式地改变了模型所模拟的映射函数，从$H(x)$变成$H(x) + x$，这里的相加操作有两个细节：1、如果$H(x)$的通道数和$x$不一样，那么$x$需要调整通道数，论文中尝试了三种方案，第一种是使用0填充，第二种是仅仅在通道数不一样的层使用$1\\times 1$卷积来调整通道，第三种是所有的residual block中都使用$1\\times 1$卷积，实验结果发现第三种方案最好，但是作者解释说这主要是因为参数量上升带来的性能增加，而且和第二种方案相比效果提升不大，因此最终还是用的第二种方案。2、如果除了通道不一样，$H(x)$的大小和$x$也不一样，那么在$x$上使用的$1\\times 1$卷积的stride设置为2。\n\n## 对ResNet的理解\nResNet中的短连接主要有两个好处：短连接使得梯度的传递更加稳定，短连接显式的增加了特征利用率，使得模型的深层仍然可以看到浅层特征。\n\nResidual Block还有一个好处是可以让模型自由的选择深度，例如模型在最后几层如果觉得在加深深度没有效果，完全可以把最后几层的$H(x)$中的参数学习为全0，这样最后几层相当于没有任何的操作，从而让学习过程来决定模型的深度。\n\nResNet在设计的过程中，可以借鉴的一个思路是：**设计模型结构的时候，如果能够保证模型参数在某个情况下能够使得当前模型和没有改变结构时的原始模型相同，那么设计出来的模型结构效果下限也就是原来的模型了。**\n\n# ResNetV2\n在论文《Identity Mappings in Deep Residual Networks》中，提出了ResNetV2结构，这里主要是将Residual Block的结构做了一些修改，如下图所示，其中（a）图为原始的Residual Block结构，（e）图为ResNetV2的Residual Block结构，其余结构在论文中也有实验，但是最终发现（e）图所示结构效果最佳。\n\n![Residual Block v2](Residual_block_v2.png)\n\n上图五种结构的实验结果如下表，其中数字代表分类错误率。\n\n![各结构效果对比](exp_result.png)\n\n以上结构中，（b）图的结构表现非常差，我认为其原因在于addition之前两个分支的分布不统一，导致在addition之后经过BN层时对identity mapping的输出分布有一定程度上的扰乱（也可以理解为identity mapping因为已经经过BN层，因此方差小，但是另一个分支方差大，导致经过BN层之后，基本上只保留了另外一个分支的信号）。\n\n（c）图中效果差的原因我认为主要是ReLU在addition之前，导致每次相加都是加的一个非负值，因此限制了模型的信号强度只能越来越大，模型的表达能力受限。\n\n（d）图的结果其实和（a）图接近，我认为这两个结构比（e）图稍差的地方在于：在经过权重层之前，信号的分布已经被打乱且没有经过BN层的整合，减弱了BN层在整个模型中的作用。\n","slug":"学习笔记/ResNet总结","published":1,"updated":"2020-08-31T06:39:20.746Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckjxpc3rl000d44mq7mar3up3","content":"<h1 id=\"resnet\">ResNet</h1>\r\n<p>在论文《Deep Residual Learning for Image Recognition》中提出了ResNet结构，其主要目的是使更深层的模型训练变得更加容易。</p>\r\n<p>ResNet的主要特点在于其残差块（Residual Block）结构，如下图所示，其中左边为ResNet-18和ResNet-34中使用的残差块结构，右边则是ResNet-50、ResNet-101、ResNet-152使用的残差快结构，两者不同之处在于瓶颈层（BottleNeck）的使用，瓶颈层的作用主要是减少参数量和运算量。</p>\r\n<figure>\r\n<img src=\"Residual_block.png\" alt=\"Residual Block\" /><figcaption aria-hidden=\"true\">Residual Block</figcaption>\r\n</figure>\r\n<p>在原论文中，上图的曲线连接被称为“identity mapping”，通过这样的短连接，显式地改变了模型所模拟的映射函数，从<span class=\"math inline\">\\(H(x)\\)</span>变成<span class=\"math inline\">\\(H(x) + x\\)</span>，这里的相加操作有两个细节：1、如果<span class=\"math inline\">\\(H(x)\\)</span>的通道数和<span class=\"math inline\">\\(x\\)</span>不一样，那么<span class=\"math inline\">\\(x\\)</span>需要调整通道数，论文中尝试了三种方案，第一种是使用0填充，第二种是仅仅在通道数不一样的层使用<span class=\"math inline\">\\(1\\times 1\\)</span>卷积来调整通道，第三种是所有的residual block中都使用<span class=\"math inline\">\\(1\\times 1\\)</span>卷积，实验结果发现第三种方案最好，但是作者解释说这主要是因为参数量上升带来的性能增加，而且和第二种方案相比效果提升不大，因此最终还是用的第二种方案。2、如果除了通道不一样，<span class=\"math inline\">\\(H(x)\\)</span>的大小和<span class=\"math inline\">\\(x\\)</span>也不一样，那么在<span class=\"math inline\">\\(x\\)</span>上使用的<span class=\"math inline\">\\(1\\times 1\\)</span>卷积的stride设置为2。</p>\r\n<h2 id=\"对resnet的理解\">对ResNet的理解</h2>\r\n<p>ResNet中的短连接主要有两个好处：短连接使得梯度的传递更加稳定，短连接显式的增加了特征利用率，使得模型的深层仍然可以看到浅层特征。</p>\r\n<p>Residual Block还有一个好处是可以让模型自由的选择深度，例如模型在最后几层如果觉得在加深深度没有效果，完全可以把最后几层的<span class=\"math inline\">\\(H(x)\\)</span>中的参数学习为全0，这样最后几层相当于没有任何的操作，从而让学习过程来决定模型的深度。</p>\r\n<p>ResNet在设计的过程中，可以借鉴的一个思路是：<strong>设计模型结构的时候，如果能够保证模型参数在某个情况下能够使得当前模型和没有改变结构时的原始模型相同，那么设计出来的模型结构效果下限也就是原来的模型了。</strong></p>\r\n<h1 id=\"resnetv2\">ResNetV2</h1>\r\n<p>在论文《Identity Mappings in Deep Residual Networks》中，提出了ResNetV2结构，这里主要是将Residual Block的结构做了一些修改，如下图所示，其中（a）图为原始的Residual Block结构，（e）图为ResNetV2的Residual Block结构，其余结构在论文中也有实验，但是最终发现（e）图所示结构效果最佳。</p>\r\n<figure>\r\n<img src=\"Residual_block_v2.png\" alt=\"Residual Block v2\" /><figcaption aria-hidden=\"true\">Residual Block v2</figcaption>\r\n</figure>\r\n<p>上图五种结构的实验结果如下表，其中数字代表分类错误率。</p>\r\n<figure>\r\n<img src=\"exp_result.png\" alt=\"各结构效果对比\" /><figcaption aria-hidden=\"true\">各结构效果对比</figcaption>\r\n</figure>\r\n<p>以上结构中，（b）图的结构表现非常差，我认为其原因在于addition之前两个分支的分布不统一，导致在addition之后经过BN层时对identity mapping的输出分布有一定程度上的扰乱（也可以理解为identity mapping因为已经经过BN层，因此方差小，但是另一个分支方差大，导致经过BN层之后，基本上只保留了另外一个分支的信号）。</p>\r\n<p>（c）图中效果差的原因我认为主要是ReLU在addition之前，导致每次相加都是加的一个非负值，因此限制了模型的信号强度只能越来越大，模型的表达能力受限。</p>\r\n<p>（d）图的结果其实和（a）图接近，我认为这两个结构比（e）图稍差的地方在于：在经过权重层之前，信号的分布已经被打乱且没有经过BN层的整合，减弱了BN层在整个模型中的作用。</p>\r\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"resnet\">ResNet</h1>\r\n<p>在论文《Deep Residual Learning for Image Recognition》中提出了ResNet结构，其主要目的是使更深层的模型训练变得更加容易。</p>\r\n<p>ResNet的主要特点在于其残差块（Residual Block）结构，如下图所示，其中左边为ResNet-18和ResNet-34中使用的残差块结构，右边则是ResNet-50、ResNet-101、ResNet-152使用的残差快结构，两者不同之处在于瓶颈层（BottleNeck）的使用，瓶颈层的作用主要是减少参数量和运算量。</p>\r\n<figure>\r\n<img src=\"Residual_block.png\" alt=\"Residual Block\" /><figcaption aria-hidden=\"true\">Residual Block</figcaption>\r\n</figure>\r\n<p>在原论文中，上图的曲线连接被称为“identity mapping”，通过这样的短连接，显式地改变了模型所模拟的映射函数，从<span class=\"math inline\">\\(H(x)\\)</span>变成<span class=\"math inline\">\\(H(x) + x\\)</span>，这里的相加操作有两个细节：1、如果<span class=\"math inline\">\\(H(x)\\)</span>的通道数和<span class=\"math inline\">\\(x\\)</span>不一样，那么<span class=\"math inline\">\\(x\\)</span>需要调整通道数，论文中尝试了三种方案，第一种是使用0填充，第二种是仅仅在通道数不一样的层使用<span class=\"math inline\">\\(1\\times 1\\)</span>卷积来调整通道，第三种是所有的residual block中都使用<span class=\"math inline\">\\(1\\times 1\\)</span>卷积，实验结果发现第三种方案最好，但是作者解释说这主要是因为参数量上升带来的性能增加，而且和第二种方案相比效果提升不大，因此最终还是用的第二种方案。2、如果除了通道不一样，<span class=\"math inline\">\\(H(x)\\)</span>的大小和<span class=\"math inline\">\\(x\\)</span>也不一样，那么在<span class=\"math inline\">\\(x\\)</span>上使用的<span class=\"math inline\">\\(1\\times 1\\)</span>卷积的stride设置为2。</p>\r\n<h2 id=\"对resnet的理解\">对ResNet的理解</h2>\r\n<p>ResNet中的短连接主要有两个好处：短连接使得梯度的传递更加稳定，短连接显式的增加了特征利用率，使得模型的深层仍然可以看到浅层特征。</p>\r\n<p>Residual Block还有一个好处是可以让模型自由的选择深度，例如模型在最后几层如果觉得在加深深度没有效果，完全可以把最后几层的<span class=\"math inline\">\\(H(x)\\)</span>中的参数学习为全0，这样最后几层相当于没有任何的操作，从而让学习过程来决定模型的深度。</p>\r\n<p>ResNet在设计的过程中，可以借鉴的一个思路是：<strong>设计模型结构的时候，如果能够保证模型参数在某个情况下能够使得当前模型和没有改变结构时的原始模型相同，那么设计出来的模型结构效果下限也就是原来的模型了。</strong></p>\r\n<h1 id=\"resnetv2\">ResNetV2</h1>\r\n<p>在论文《Identity Mappings in Deep Residual Networks》中，提出了ResNetV2结构，这里主要是将Residual Block的结构做了一些修改，如下图所示，其中（a）图为原始的Residual Block结构，（e）图为ResNetV2的Residual Block结构，其余结构在论文中也有实验，但是最终发现（e）图所示结构效果最佳。</p>\r\n<figure>\r\n<img src=\"Residual_block_v2.png\" alt=\"Residual Block v2\" /><figcaption aria-hidden=\"true\">Residual Block v2</figcaption>\r\n</figure>\r\n<p>上图五种结构的实验结果如下表，其中数字代表分类错误率。</p>\r\n<figure>\r\n<img src=\"exp_result.png\" alt=\"各结构效果对比\" /><figcaption aria-hidden=\"true\">各结构效果对比</figcaption>\r\n</figure>\r\n<p>以上结构中，（b）图的结构表现非常差，我认为其原因在于addition之前两个分支的分布不统一，导致在addition之后经过BN层时对identity mapping的输出分布有一定程度上的扰乱（也可以理解为identity mapping因为已经经过BN层，因此方差小，但是另一个分支方差大，导致经过BN层之后，基本上只保留了另外一个分支的信号）。</p>\r\n<p>（c）图中效果差的原因我认为主要是ReLU在addition之前，导致每次相加都是加的一个非负值，因此限制了模型的信号强度只能越来越大，模型的表达能力受限。</p>\r\n<p>（d）图的结果其实和（a）图接近，我认为这两个结构比（e）图稍差的地方在于：在经过权重层之前，信号的分布已经被打乱且没有经过BN层的整合，减弱了BN层在整个模型中的作用。</p>\r\n"},{"title":"SVM学习笔记","date":"2020-04-23T07:57:32.000Z","mathjax":true,"_content":"# 线性SVM详细推导\n首先，一个点$p \\in \\mathbb{R}^d$到超平面$w^Tx+b=0$的距离可以表示为$\\frac{1}{||w||}|w^Tp + b|$。\n\n对于一个两类别数据集$X\\in\\mathbb{R}^d, Y\\in\\{-1, 1\\}$，定义间隔$\\gamma=2\\min_i\\frac{1}{||w||}|w^Tx_i + b|$\n\n线性支持向量机的目标即找到一组适合的参数$(w, b)$使得\n\n$$\n\\max_{w,b}\\gamma = \\max_{w,b} 2\\min_i\\frac{1}{||w||}|w^Tx_i + b|\\\\\ns.t. \\ y_i(w^Tx_i + b) > 0, i = 1,2,...,m\n$$\n\n若一组$(w^*, b^*)$是支持向量机的一个解，那么对于$\\lambda > 0$，$(\\lambda w^*, \\lambda b^*)$也是该优化问题的一个解，因为间隔不会变化。\n\n所以这里添加一个约束条件，让$\\min_i |w^T x_i + b| = 1$，因此支持向量机的优化目标可以进一步化成如下，即支持向量机的基本型：\n\n$$\n\\max_{w,b} 2\\min_i\\frac{1}{||w||}|w^Tx_i + b| \\\\\n= \\max_{w,b} \\frac{2}{||w||}\\\\\n= \\min_{w,b} \\frac{1}{2}w^T w\\\\\ns.t. \\ y_i(w^Tx_i + b) \\ge 1, i = 1,2,...,m\n$$\n\n可以看出，$w^T w$是一个正定二次型，如此一来，支持向量机的可以看做一个凸二次优化问题：\n\n$$\n\\min_{u} \\frac{1}{2} u^T Q u + t^T u \\\\\ns.t. \\ c_i^T u \\ge d_i, i = 1, 2,...,m\n$$\n\n其中$u=\\begin{bmatrix}w\\\\b\\end{bmatrix}$, $Q=\\begin{bmatrix}I &\\mathbf{0}\\\\\\mathbf{0}&0\\end{bmatrix}$, $t=\\mathbf{0}$, $c_i = y_i\\begin{bmatrix}x_i\\\\ 1\\end{bmatrix}$, $d_i = 1$\n\n也可以运用拉格朗日法来求解支持向量机，定义其拉格朗日函数$\\mathcal{L}(w,b,\\alpha) = \\frac{1}{2}w^T w + \\sum_{i=1}^{m} \\alpha_i(1 - y_i(w^T x_i + b))$\n原问题可以表示为\n$$\n\\min_{w,b} \\max_\\alpha \\mathcal{L}(w, b, \\alpha)\\\\\ns.t. \\ \\alpha_i \\ge 0, i=1,2,...,m\n$$\n\n其对偶问题可以表示为:\n\n$$\n\\max_\\alpha \\min_{w,b} \\mathcal{L}(w, b, \\alpha)\\\\\ns.t. \\ \\alpha_i \\ge 0, i=1,2,...,m\n$$\n\n其KKT条件表示为:\n\n$$\n\\begin{aligned}\n&\\triangledown_w\\mathcal{L}(w,b,\\alpha) = 0 \\\\\n&\\triangledown_b\\mathcal{L}(w,b,\\alpha) = 0 \\\\\n&1 - y_i(w^T x_i + b) \\le 0 \\\\\n&\\alpha_i \\ge 0 \\\\\n&\\alpha_i(1 - y_i(w^Tx_i + b)) = 0\n\\end{aligned}\n$$\n\n这里对支持向量机的对偶问题进行第一步求解:$\\min_{w,b} \\mathcal{L}(w, b, \\alpha)$，直接令一阶导数等于0：\n$$\n\\frac{\\partial\\mathcal{L}}{\\partial w} = 0 \\Rightarrow w = \\sum_{i=1}^m \\alpha_i y_i x_i \\\\\n\\frac{\\partial\\mathcal{L}}{\\partial b} = 0 \\Rightarrow \\sum_{i=1}^m \\alpha_i y_i = 0\n$$\n\n这里可以看出，$w$仅和$\\alpha_i > 0$的样本有关，而根据KKT条件，$\\alpha_i > 0$的地方必须满足$(1 - y_i(w^Tx_i + b)) = 0$，即这些$x_i$在最大间隔边界上，这样的样本称为支持向量，支持向量机的解仅仅和支持向量有关。\n\n将求得结果代入$\\mathcal{L}$：\n$$\n\\begin{aligned}\n\\mathcal{L} &= \\frac{1}{2}\\sum_{i=1}^m\\sum_{j=1}^m \\alpha_i \\alpha_j y_i y_j x_i^T x_j + \\sum_{i=1}^m \\alpha_i - \\sum_{i=1}^m \\alpha_i y_i(w^Tx_i + b)\\\\\n&= \\frac{1}{2}\\sum_{i=1}^m\\sum_{j=1}^m \\alpha_i \\alpha_j y_i y_j x_i^T x_j + \\sum_{i=1}^m \\alpha_i - \\sum_{i=1}^m \\alpha_i y_iw^Tx_i\\\\\n&= \\sum_{i=1}^m \\alpha_i - \\frac{1}{2}\\sum_{i=1}^m\\sum_{j=1}^m \\alpha_i \\alpha_j y_i y_j x_i^T x_j\n\\end{aligned}\n$$\n\n此时，再求解\n$$\n\\max_{\\alpha} \\mathcal{L}(w, b, \\alpha)\\\\\ns.t. \\ \\alpha_i \\ge 0,i=1,2,...,m\\\\\n\\ \\sum_{i=1}^m \\alpha_i y_i = 0\n$$\n这个问题的求解使用SMO(序列最小优化)算法，大致思路和坐标上升法类似，迭代进行，每次选取两个$\\alpha$进行更新，同时更新参数$b$，具体步骤后面有时间再详细学习。\n\n# SVM的核技巧\n线性SVM基于一个基本假设：数据在空间$\\mathbb{R}^d$中线性可分，但这个假设在实际应用中，基本不满足。\n\n但是存在一个定理：当$d$有限时，一定存在$\\hat{d}$，使得样本在空间$\\mathbb{R}^{\\hat{d}}$中线性可分。\n\n因此我们可以构造一种映射：$x_i \\rightarrow \\phi(x_i)$，然后在这个映射的空间中使用线性SVM进行分类。\n\n这样最终需要求解的拉格朗日函数可以写成：$\\mathcal{L} = \\sum_{i=1}^m \\alpha_i - \\frac{1}{2}\\sum_{i=1}^m\\sum_{j=1}^m \\alpha_i \\alpha_j y_i y_j \\phi(x_i)^T \\phi(x_j)$，看起来很简单，只需要换一种计算方式就行，但是这里存在一个问题：$\\hat{d}$可能非常大，导致计算困难。\n\n针对上述问题，需要使用核技巧：构造一个计算复杂度为$O(d)$的$k(x_i, x_j)$使得：$k(x_i, x_j)=\\phi(x_i)^T \\phi(x_j)$，即我们只需要一种快速的计算内积的方式，并不关心其他运算。\n\n核函数的选择：当数据维数$d$超过了样本数量的时候，一般选用线性核，当数据维数$d$较小，而样本量$m$中等时，可以选择RBF核，但是当数据维数$d$较小，而样本量$m$特别大时，不需要选择了，直接使用深度神经网络吧。\n\n核函数的定义需要满足Mercer条件：核函数矩阵必须是半正定的。可以理解为内积大于等于0。\n\n核函数有一些性质：如果$k_1$、$k_2$是核函数，那么下列函数也是核函数：\n\n$c_1k_1(x_i, x_j) + c_2k_2(x_i, x_j), \\ c1,c2 > 0$\n\n$k_1(x_i, x_j)k_2(x_i, x_j)$\n\n$f(x_i)k_1(x_i, x_j)f(x_j)$\n\n# 软间隔SVM\n数据中不能总是找到线性可分的空间，而且数据存在噪声或者错误标注，这个时候我们如果按照SVM的优化方式，很可能造成过拟合的问题，因此可以允许少量分类错误出现，定义松弛变量$\\epsilon_i = \\begin{cases}\n    0 & y_i(w^T\\phi(x_i) + b) \\ge 1\\\\\n    1 - y_i(w^T\\phi(x_i) + b) & y_i(w^T\\phi(x_i) + b) < 1\n\\end{cases}$，由此定义软间隔支持向量机的基本型：\n$$\n\\min_{w,b,\\epsilon} \\frac{1}{2}w^T w + C\\sum_{i=1}^m \\epsilon_i\\\\\ns.t. y_i(w^T\\phi(x_i) + b) \\ge 1 - \\epsilon_i, i = 1, 2,...,m\\\\\n\\epsilon_i \\ge 0, i=1,2,...,m\n$$\n其中$C$是一个可调节参数，用于调节错误分类的惩罚。\n\n软间隔支持向量机的求解方式和支持向量机类似，不过从一个约束变成了两个约束。\n\n另外$\\epsilon_i$也可以表示成$\\max(0, 1 - y_i(w^T\\phi(x_i) + b))$，因此软间隔支持向量机的基本型的对偶问题可以表示为$\\min_{w, b} \\frac{1}{m}\\sum_{i=1}^m \\max(0, 1 - y_i(w^T\\phi(x_i) + b)) + \\frac{1}{2mC}w^Tw$，其中第一项称为经验风险，度量模型对数据的拟合程度，第二项称为结构风险，度量模型的复杂程度，也可以称为正则化项。因此还衍生出一种损失函数：hinge loss：$\\mathbb{l}(s) = max(0, 1-s)$。\n","source":"_posts/学习笔记/SVM学习笔记.md","raw":"---\ntitle: SVM学习笔记\ndate: 2020-04-23 15:57:32\ntags: [机器学习]\nmathjax: true\n---\n# 线性SVM详细推导\n首先，一个点$p \\in \\mathbb{R}^d$到超平面$w^Tx+b=0$的距离可以表示为$\\frac{1}{||w||}|w^Tp + b|$。\n\n对于一个两类别数据集$X\\in\\mathbb{R}^d, Y\\in\\{-1, 1\\}$，定义间隔$\\gamma=2\\min_i\\frac{1}{||w||}|w^Tx_i + b|$\n\n线性支持向量机的目标即找到一组适合的参数$(w, b)$使得\n\n$$\n\\max_{w,b}\\gamma = \\max_{w,b} 2\\min_i\\frac{1}{||w||}|w^Tx_i + b|\\\\\ns.t. \\ y_i(w^Tx_i + b) > 0, i = 1,2,...,m\n$$\n\n若一组$(w^*, b^*)$是支持向量机的一个解，那么对于$\\lambda > 0$，$(\\lambda w^*, \\lambda b^*)$也是该优化问题的一个解，因为间隔不会变化。\n\n所以这里添加一个约束条件，让$\\min_i |w^T x_i + b| = 1$，因此支持向量机的优化目标可以进一步化成如下，即支持向量机的基本型：\n\n$$\n\\max_{w,b} 2\\min_i\\frac{1}{||w||}|w^Tx_i + b| \\\\\n= \\max_{w,b} \\frac{2}{||w||}\\\\\n= \\min_{w,b} \\frac{1}{2}w^T w\\\\\ns.t. \\ y_i(w^Tx_i + b) \\ge 1, i = 1,2,...,m\n$$\n\n可以看出，$w^T w$是一个正定二次型，如此一来，支持向量机的可以看做一个凸二次优化问题：\n\n$$\n\\min_{u} \\frac{1}{2} u^T Q u + t^T u \\\\\ns.t. \\ c_i^T u \\ge d_i, i = 1, 2,...,m\n$$\n\n其中$u=\\begin{bmatrix}w\\\\b\\end{bmatrix}$, $Q=\\begin{bmatrix}I &\\mathbf{0}\\\\\\mathbf{0}&0\\end{bmatrix}$, $t=\\mathbf{0}$, $c_i = y_i\\begin{bmatrix}x_i\\\\ 1\\end{bmatrix}$, $d_i = 1$\n\n也可以运用拉格朗日法来求解支持向量机，定义其拉格朗日函数$\\mathcal{L}(w,b,\\alpha) = \\frac{1}{2}w^T w + \\sum_{i=1}^{m} \\alpha_i(1 - y_i(w^T x_i + b))$\n原问题可以表示为\n$$\n\\min_{w,b} \\max_\\alpha \\mathcal{L}(w, b, \\alpha)\\\\\ns.t. \\ \\alpha_i \\ge 0, i=1,2,...,m\n$$\n\n其对偶问题可以表示为:\n\n$$\n\\max_\\alpha \\min_{w,b} \\mathcal{L}(w, b, \\alpha)\\\\\ns.t. \\ \\alpha_i \\ge 0, i=1,2,...,m\n$$\n\n其KKT条件表示为:\n\n$$\n\\begin{aligned}\n&\\triangledown_w\\mathcal{L}(w,b,\\alpha) = 0 \\\\\n&\\triangledown_b\\mathcal{L}(w,b,\\alpha) = 0 \\\\\n&1 - y_i(w^T x_i + b) \\le 0 \\\\\n&\\alpha_i \\ge 0 \\\\\n&\\alpha_i(1 - y_i(w^Tx_i + b)) = 0\n\\end{aligned}\n$$\n\n这里对支持向量机的对偶问题进行第一步求解:$\\min_{w,b} \\mathcal{L}(w, b, \\alpha)$，直接令一阶导数等于0：\n$$\n\\frac{\\partial\\mathcal{L}}{\\partial w} = 0 \\Rightarrow w = \\sum_{i=1}^m \\alpha_i y_i x_i \\\\\n\\frac{\\partial\\mathcal{L}}{\\partial b} = 0 \\Rightarrow \\sum_{i=1}^m \\alpha_i y_i = 0\n$$\n\n这里可以看出，$w$仅和$\\alpha_i > 0$的样本有关，而根据KKT条件，$\\alpha_i > 0$的地方必须满足$(1 - y_i(w^Tx_i + b)) = 0$，即这些$x_i$在最大间隔边界上，这样的样本称为支持向量，支持向量机的解仅仅和支持向量有关。\n\n将求得结果代入$\\mathcal{L}$：\n$$\n\\begin{aligned}\n\\mathcal{L} &= \\frac{1}{2}\\sum_{i=1}^m\\sum_{j=1}^m \\alpha_i \\alpha_j y_i y_j x_i^T x_j + \\sum_{i=1}^m \\alpha_i - \\sum_{i=1}^m \\alpha_i y_i(w^Tx_i + b)\\\\\n&= \\frac{1}{2}\\sum_{i=1}^m\\sum_{j=1}^m \\alpha_i \\alpha_j y_i y_j x_i^T x_j + \\sum_{i=1}^m \\alpha_i - \\sum_{i=1}^m \\alpha_i y_iw^Tx_i\\\\\n&= \\sum_{i=1}^m \\alpha_i - \\frac{1}{2}\\sum_{i=1}^m\\sum_{j=1}^m \\alpha_i \\alpha_j y_i y_j x_i^T x_j\n\\end{aligned}\n$$\n\n此时，再求解\n$$\n\\max_{\\alpha} \\mathcal{L}(w, b, \\alpha)\\\\\ns.t. \\ \\alpha_i \\ge 0,i=1,2,...,m\\\\\n\\ \\sum_{i=1}^m \\alpha_i y_i = 0\n$$\n这个问题的求解使用SMO(序列最小优化)算法，大致思路和坐标上升法类似，迭代进行，每次选取两个$\\alpha$进行更新，同时更新参数$b$，具体步骤后面有时间再详细学习。\n\n# SVM的核技巧\n线性SVM基于一个基本假设：数据在空间$\\mathbb{R}^d$中线性可分，但这个假设在实际应用中，基本不满足。\n\n但是存在一个定理：当$d$有限时，一定存在$\\hat{d}$，使得样本在空间$\\mathbb{R}^{\\hat{d}}$中线性可分。\n\n因此我们可以构造一种映射：$x_i \\rightarrow \\phi(x_i)$，然后在这个映射的空间中使用线性SVM进行分类。\n\n这样最终需要求解的拉格朗日函数可以写成：$\\mathcal{L} = \\sum_{i=1}^m \\alpha_i - \\frac{1}{2}\\sum_{i=1}^m\\sum_{j=1}^m \\alpha_i \\alpha_j y_i y_j \\phi(x_i)^T \\phi(x_j)$，看起来很简单，只需要换一种计算方式就行，但是这里存在一个问题：$\\hat{d}$可能非常大，导致计算困难。\n\n针对上述问题，需要使用核技巧：构造一个计算复杂度为$O(d)$的$k(x_i, x_j)$使得：$k(x_i, x_j)=\\phi(x_i)^T \\phi(x_j)$，即我们只需要一种快速的计算内积的方式，并不关心其他运算。\n\n核函数的选择：当数据维数$d$超过了样本数量的时候，一般选用线性核，当数据维数$d$较小，而样本量$m$中等时，可以选择RBF核，但是当数据维数$d$较小，而样本量$m$特别大时，不需要选择了，直接使用深度神经网络吧。\n\n核函数的定义需要满足Mercer条件：核函数矩阵必须是半正定的。可以理解为内积大于等于0。\n\n核函数有一些性质：如果$k_1$、$k_2$是核函数，那么下列函数也是核函数：\n\n$c_1k_1(x_i, x_j) + c_2k_2(x_i, x_j), \\ c1,c2 > 0$\n\n$k_1(x_i, x_j)k_2(x_i, x_j)$\n\n$f(x_i)k_1(x_i, x_j)f(x_j)$\n\n# 软间隔SVM\n数据中不能总是找到线性可分的空间，而且数据存在噪声或者错误标注，这个时候我们如果按照SVM的优化方式，很可能造成过拟合的问题，因此可以允许少量分类错误出现，定义松弛变量$\\epsilon_i = \\begin{cases}\n    0 & y_i(w^T\\phi(x_i) + b) \\ge 1\\\\\n    1 - y_i(w^T\\phi(x_i) + b) & y_i(w^T\\phi(x_i) + b) < 1\n\\end{cases}$，由此定义软间隔支持向量机的基本型：\n$$\n\\min_{w,b,\\epsilon} \\frac{1}{2}w^T w + C\\sum_{i=1}^m \\epsilon_i\\\\\ns.t. y_i(w^T\\phi(x_i) + b) \\ge 1 - \\epsilon_i, i = 1, 2,...,m\\\\\n\\epsilon_i \\ge 0, i=1,2,...,m\n$$\n其中$C$是一个可调节参数，用于调节错误分类的惩罚。\n\n软间隔支持向量机的求解方式和支持向量机类似，不过从一个约束变成了两个约束。\n\n另外$\\epsilon_i$也可以表示成$\\max(0, 1 - y_i(w^T\\phi(x_i) + b))$，因此软间隔支持向量机的基本型的对偶问题可以表示为$\\min_{w, b} \\frac{1}{m}\\sum_{i=1}^m \\max(0, 1 - y_i(w^T\\phi(x_i) + b)) + \\frac{1}{2mC}w^Tw$，其中第一项称为经验风险，度量模型对数据的拟合程度，第二项称为结构风险，度量模型的复杂程度，也可以称为正则化项。因此还衍生出一种损失函数：hinge loss：$\\mathbb{l}(s) = max(0, 1-s)$。\n","slug":"学习笔记/SVM学习笔记","published":1,"updated":"2020-08-31T06:39:20.750Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckjxpc3rm000f44mq4wmo0spb","content":"<h1 id=\"线性svm详细推导\">线性SVM详细推导</h1>\r\n<p>首先，一个点<span class=\"math inline\">\\(p \\in \\mathbb{R}^d\\)</span>到超平面<span class=\"math inline\">\\(w^Tx+b=0\\)</span>的距离可以表示为<span class=\"math inline\">\\(\\frac{1}{||w||}|w^Tp + b|\\)</span>。</p>\r\n<p>对于一个两类别数据集<span class=\"math inline\">\\(X\\in\\mathbb{R}^d, Y\\in\\{-1, 1\\}\\)</span>，定义间隔<span class=\"math inline\">\\(\\gamma=2\\min_i\\frac{1}{||w||}|w^Tx_i + b|\\)</span></p>\r\n<p>线性支持向量机的目标即找到一组适合的参数<span class=\"math inline\">\\((w, b)\\)</span>使得</p>\r\n<p><span class=\"math display\">\\[\r\n\\max_{w,b}\\gamma = \\max_{w,b} 2\\min_i\\frac{1}{||w||}|w^Tx_i + b|\\\\\r\ns.t. \\ y_i(w^Tx_i + b) &gt; 0, i = 1,2,...,m\r\n\\]</span></p>\r\n<p>若一组<span class=\"math inline\">\\((w^*, b^*)\\)</span>是支持向量机的一个解，那么对于<span class=\"math inline\">\\(\\lambda &gt; 0\\)</span>，<span class=\"math inline\">\\((\\lambda w^*, \\lambda b^*)\\)</span>也是该优化问题的一个解，因为间隔不会变化。</p>\r\n<p>所以这里添加一个约束条件，让<span class=\"math inline\">\\(\\min_i |w^T x_i + b| = 1\\)</span>，因此支持向量机的优化目标可以进一步化成如下，即支持向量机的基本型：</p>\r\n<p><span class=\"math display\">\\[\r\n\\max_{w,b} 2\\min_i\\frac{1}{||w||}|w^Tx_i + b| \\\\\r\n= \\max_{w,b} \\frac{2}{||w||}\\\\\r\n= \\min_{w,b} \\frac{1}{2}w^T w\\\\\r\ns.t. \\ y_i(w^Tx_i + b) \\ge 1, i = 1,2,...,m\r\n\\]</span></p>\r\n<p>可以看出，<span class=\"math inline\">\\(w^T w\\)</span>是一个正定二次型，如此一来，支持向量机的可以看做一个凸二次优化问题：</p>\r\n<p><span class=\"math display\">\\[\r\n\\min_{u} \\frac{1}{2} u^T Q u + t^T u \\\\\r\ns.t. \\ c_i^T u \\ge d_i, i = 1, 2,...,m\r\n\\]</span></p>\r\n<p>其中<span class=\"math inline\">\\(u=\\begin{bmatrix}w\\\\b\\end{bmatrix}\\)</span>, <span class=\"math inline\">\\(Q=\\begin{bmatrix}I &amp;\\mathbf{0}\\\\\\mathbf{0}&amp;0\\end{bmatrix}\\)</span>, <span class=\"math inline\">\\(t=\\mathbf{0}\\)</span>, <span class=\"math inline\">\\(c_i = y_i\\begin{bmatrix}x_i\\\\ 1\\end{bmatrix}\\)</span>, <span class=\"math inline\">\\(d_i = 1\\)</span></p>\r\n<p>也可以运用拉格朗日法来求解支持向量机，定义其拉格朗日函数<span class=\"math inline\">\\(\\mathcal{L}(w,b,\\alpha) = \\frac{1}{2}w^T w + \\sum_{i=1}^{m} \\alpha_i(1 - y_i(w^T x_i + b))\\)</span> 原问题可以表示为 <span class=\"math display\">\\[\r\n\\min_{w,b} \\max_\\alpha \\mathcal{L}(w, b, \\alpha)\\\\\r\ns.t. \\ \\alpha_i \\ge 0, i=1,2,...,m\r\n\\]</span></p>\r\n<p>其对偶问题可以表示为:</p>\r\n<p><span class=\"math display\">\\[\r\n\\max_\\alpha \\min_{w,b} \\mathcal{L}(w, b, \\alpha)\\\\\r\ns.t. \\ \\alpha_i \\ge 0, i=1,2,...,m\r\n\\]</span></p>\r\n<p>其KKT条件表示为:</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n&amp;\\triangledown_w\\mathcal{L}(w,b,\\alpha) = 0 \\\\\r\n&amp;\\triangledown_b\\mathcal{L}(w,b,\\alpha) = 0 \\\\\r\n&amp;1 - y_i(w^T x_i + b) \\le 0 \\\\\r\n&amp;\\alpha_i \\ge 0 \\\\\r\n&amp;\\alpha_i(1 - y_i(w^Tx_i + b)) = 0\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>这里对支持向量机的对偶问题进行第一步求解:<span class=\"math inline\">\\(\\min_{w,b} \\mathcal{L}(w, b, \\alpha)\\)</span>，直接令一阶导数等于0： <span class=\"math display\">\\[\r\n\\frac{\\partial\\mathcal{L}}{\\partial w} = 0 \\Rightarrow w = \\sum_{i=1}^m \\alpha_i y_i x_i \\\\\r\n\\frac{\\partial\\mathcal{L}}{\\partial b} = 0 \\Rightarrow \\sum_{i=1}^m \\alpha_i y_i = 0\r\n\\]</span></p>\r\n<p>这里可以看出，<span class=\"math inline\">\\(w\\)</span>仅和<span class=\"math inline\">\\(\\alpha_i &gt; 0\\)</span>的样本有关，而根据KKT条件，<span class=\"math inline\">\\(\\alpha_i &gt; 0\\)</span>的地方必须满足<span class=\"math inline\">\\((1 - y_i(w^Tx_i + b)) = 0\\)</span>，即这些<span class=\"math inline\">\\(x_i\\)</span>在最大间隔边界上，这样的样本称为支持向量，支持向量机的解仅仅和支持向量有关。</p>\r\n<p>将求得结果代入<span class=\"math inline\">\\(\\mathcal{L}\\)</span>： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n\\mathcal{L} &amp;= \\frac{1}{2}\\sum_{i=1}^m\\sum_{j=1}^m \\alpha_i \\alpha_j y_i y_j x_i^T x_j + \\sum_{i=1}^m \\alpha_i - \\sum_{i=1}^m \\alpha_i y_i(w^Tx_i + b)\\\\\r\n&amp;= \\frac{1}{2}\\sum_{i=1}^m\\sum_{j=1}^m \\alpha_i \\alpha_j y_i y_j x_i^T x_j + \\sum_{i=1}^m \\alpha_i - \\sum_{i=1}^m \\alpha_i y_iw^Tx_i\\\\\r\n&amp;= \\sum_{i=1}^m \\alpha_i - \\frac{1}{2}\\sum_{i=1}^m\\sum_{j=1}^m \\alpha_i \\alpha_j y_i y_j x_i^T x_j\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>此时，再求解 <span class=\"math display\">\\[\r\n\\max_{\\alpha} \\mathcal{L}(w, b, \\alpha)\\\\\r\ns.t. \\ \\alpha_i \\ge 0,i=1,2,...,m\\\\\r\n\\ \\sum_{i=1}^m \\alpha_i y_i = 0\r\n\\]</span> 这个问题的求解使用SMO(序列最小优化)算法，大致思路和坐标上升法类似，迭代进行，每次选取两个<span class=\"math inline\">\\(\\alpha\\)</span>进行更新，同时更新参数<span class=\"math inline\">\\(b\\)</span>，具体步骤后面有时间再详细学习。</p>\r\n<h1 id=\"svm的核技巧\">SVM的核技巧</h1>\r\n<p>线性SVM基于一个基本假设：数据在空间<span class=\"math inline\">\\(\\mathbb{R}^d\\)</span>中线性可分，但这个假设在实际应用中，基本不满足。</p>\r\n<p>但是存在一个定理：当<span class=\"math inline\">\\(d\\)</span>有限时，一定存在<span class=\"math inline\">\\(\\hat{d}\\)</span>，使得样本在空间<span class=\"math inline\">\\(\\mathbb{R}^{\\hat{d}}\\)</span>中线性可分。</p>\r\n<p>因此我们可以构造一种映射：<span class=\"math inline\">\\(x_i \\rightarrow \\phi(x_i)\\)</span>，然后在这个映射的空间中使用线性SVM进行分类。</p>\r\n<p>这样最终需要求解的拉格朗日函数可以写成：<span class=\"math inline\">\\(\\mathcal{L} = \\sum_{i=1}^m \\alpha_i - \\frac{1}{2}\\sum_{i=1}^m\\sum_{j=1}^m \\alpha_i \\alpha_j y_i y_j \\phi(x_i)^T \\phi(x_j)\\)</span>，看起来很简单，只需要换一种计算方式就行，但是这里存在一个问题：<span class=\"math inline\">\\(\\hat{d}\\)</span>可能非常大，导致计算困难。</p>\r\n<p>针对上述问题，需要使用核技巧：构造一个计算复杂度为<span class=\"math inline\">\\(O(d)\\)</span>的<span class=\"math inline\">\\(k(x_i, x_j)\\)</span>使得：<span class=\"math inline\">\\(k(x_i, x_j)=\\phi(x_i)^T \\phi(x_j)\\)</span>，即我们只需要一种快速的计算内积的方式，并不关心其他运算。</p>\r\n<p>核函数的选择：当数据维数<span class=\"math inline\">\\(d\\)</span>超过了样本数量的时候，一般选用线性核，当数据维数<span class=\"math inline\">\\(d\\)</span>较小，而样本量<span class=\"math inline\">\\(m\\)</span>中等时，可以选择RBF核，但是当数据维数<span class=\"math inline\">\\(d\\)</span>较小，而样本量<span class=\"math inline\">\\(m\\)</span>特别大时，不需要选择了，直接使用深度神经网络吧。</p>\r\n<p>核函数的定义需要满足Mercer条件：核函数矩阵必须是半正定的。可以理解为内积大于等于0。</p>\r\n<p>核函数有一些性质：如果<span class=\"math inline\">\\(k_1\\)</span>、<span class=\"math inline\">\\(k_2\\)</span>是核函数，那么下列函数也是核函数：</p>\r\n<p><span class=\"math inline\">\\(c_1k_1(x_i, x_j) + c_2k_2(x_i, x_j), \\ c1,c2 &gt; 0\\)</span></p>\r\n<p><span class=\"math inline\">\\(k_1(x_i, x_j)k_2(x_i, x_j)\\)</span></p>\r\n<p><span class=\"math inline\">\\(f(x_i)k_1(x_i, x_j)f(x_j)\\)</span></p>\r\n<h1 id=\"软间隔svm\">软间隔SVM</h1>\r\n<p>数据中不能总是找到线性可分的空间，而且数据存在噪声或者错误标注，这个时候我们如果按照SVM的优化方式，很可能造成过拟合的问题，因此可以允许少量分类错误出现，定义松弛变量<span class=\"math inline\">\\(\\epsilon_i = \\begin{cases}  0 &amp; y_i(w^T\\phi(x_i) + b) \\ge 1\\\\  1 - y_i(w^T\\phi(x_i) + b) &amp; y_i(w^T\\phi(x_i) + b) &lt; 1 \\end{cases}\\)</span>，由此定义软间隔支持向量机的基本型： <span class=\"math display\">\\[\r\n\\min_{w,b,\\epsilon} \\frac{1}{2}w^T w + C\\sum_{i=1}^m \\epsilon_i\\\\\r\ns.t. y_i(w^T\\phi(x_i) + b) \\ge 1 - \\epsilon_i, i = 1, 2,...,m\\\\\r\n\\epsilon_i \\ge 0, i=1,2,...,m\r\n\\]</span> 其中<span class=\"math inline\">\\(C\\)</span>是一个可调节参数，用于调节错误分类的惩罚。</p>\r\n<p>软间隔支持向量机的求解方式和支持向量机类似，不过从一个约束变成了两个约束。</p>\r\n<p>另外<span class=\"math inline\">\\(\\epsilon_i\\)</span>也可以表示成<span class=\"math inline\">\\(\\max(0, 1 - y_i(w^T\\phi(x_i) + b))\\)</span>，因此软间隔支持向量机的基本型的对偶问题可以表示为<span class=\"math inline\">\\(\\min_{w, b} \\frac{1}{m}\\sum_{i=1}^m \\max(0, 1 - y_i(w^T\\phi(x_i) + b)) + \\frac{1}{2mC}w^Tw\\)</span>，其中第一项称为经验风险，度量模型对数据的拟合程度，第二项称为结构风险，度量模型的复杂程度，也可以称为正则化项。因此还衍生出一种损失函数：hinge loss：<span class=\"math inline\">\\(\\mathbb{l}(s) = max(0, 1-s)\\)</span>。</p>\r\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"线性svm详细推导\">线性SVM详细推导</h1>\r\n<p>首先，一个点<span class=\"math inline\">\\(p \\in \\mathbb{R}^d\\)</span>到超平面<span class=\"math inline\">\\(w^Tx+b=0\\)</span>的距离可以表示为<span class=\"math inline\">\\(\\frac{1}{||w||}|w^Tp + b|\\)</span>。</p>\r\n<p>对于一个两类别数据集<span class=\"math inline\">\\(X\\in\\mathbb{R}^d, Y\\in\\{-1, 1\\}\\)</span>，定义间隔<span class=\"math inline\">\\(\\gamma=2\\min_i\\frac{1}{||w||}|w^Tx_i + b|\\)</span></p>\r\n<p>线性支持向量机的目标即找到一组适合的参数<span class=\"math inline\">\\((w, b)\\)</span>使得</p>\r\n<p><span class=\"math display\">\\[\r\n\\max_{w,b}\\gamma = \\max_{w,b} 2\\min_i\\frac{1}{||w||}|w^Tx_i + b|\\\\\r\ns.t. \\ y_i(w^Tx_i + b) &gt; 0, i = 1,2,...,m\r\n\\]</span></p>\r\n<p>若一组<span class=\"math inline\">\\((w^*, b^*)\\)</span>是支持向量机的一个解，那么对于<span class=\"math inline\">\\(\\lambda &gt; 0\\)</span>，<span class=\"math inline\">\\((\\lambda w^*, \\lambda b^*)\\)</span>也是该优化问题的一个解，因为间隔不会变化。</p>\r\n<p>所以这里添加一个约束条件，让<span class=\"math inline\">\\(\\min_i |w^T x_i + b| = 1\\)</span>，因此支持向量机的优化目标可以进一步化成如下，即支持向量机的基本型：</p>\r\n<p><span class=\"math display\">\\[\r\n\\max_{w,b} 2\\min_i\\frac{1}{||w||}|w^Tx_i + b| \\\\\r\n= \\max_{w,b} \\frac{2}{||w||}\\\\\r\n= \\min_{w,b} \\frac{1}{2}w^T w\\\\\r\ns.t. \\ y_i(w^Tx_i + b) \\ge 1, i = 1,2,...,m\r\n\\]</span></p>\r\n<p>可以看出，<span class=\"math inline\">\\(w^T w\\)</span>是一个正定二次型，如此一来，支持向量机的可以看做一个凸二次优化问题：</p>\r\n<p><span class=\"math display\">\\[\r\n\\min_{u} \\frac{1}{2} u^T Q u + t^T u \\\\\r\ns.t. \\ c_i^T u \\ge d_i, i = 1, 2,...,m\r\n\\]</span></p>\r\n<p>其中<span class=\"math inline\">\\(u=\\begin{bmatrix}w\\\\b\\end{bmatrix}\\)</span>, <span class=\"math inline\">\\(Q=\\begin{bmatrix}I &amp;\\mathbf{0}\\\\\\mathbf{0}&amp;0\\end{bmatrix}\\)</span>, <span class=\"math inline\">\\(t=\\mathbf{0}\\)</span>, <span class=\"math inline\">\\(c_i = y_i\\begin{bmatrix}x_i\\\\ 1\\end{bmatrix}\\)</span>, <span class=\"math inline\">\\(d_i = 1\\)</span></p>\r\n<p>也可以运用拉格朗日法来求解支持向量机，定义其拉格朗日函数<span class=\"math inline\">\\(\\mathcal{L}(w,b,\\alpha) = \\frac{1}{2}w^T w + \\sum_{i=1}^{m} \\alpha_i(1 - y_i(w^T x_i + b))\\)</span> 原问题可以表示为 <span class=\"math display\">\\[\r\n\\min_{w,b} \\max_\\alpha \\mathcal{L}(w, b, \\alpha)\\\\\r\ns.t. \\ \\alpha_i \\ge 0, i=1,2,...,m\r\n\\]</span></p>\r\n<p>其对偶问题可以表示为:</p>\r\n<p><span class=\"math display\">\\[\r\n\\max_\\alpha \\min_{w,b} \\mathcal{L}(w, b, \\alpha)\\\\\r\ns.t. \\ \\alpha_i \\ge 0, i=1,2,...,m\r\n\\]</span></p>\r\n<p>其KKT条件表示为:</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n&amp;\\triangledown_w\\mathcal{L}(w,b,\\alpha) = 0 \\\\\r\n&amp;\\triangledown_b\\mathcal{L}(w,b,\\alpha) = 0 \\\\\r\n&amp;1 - y_i(w^T x_i + b) \\le 0 \\\\\r\n&amp;\\alpha_i \\ge 0 \\\\\r\n&amp;\\alpha_i(1 - y_i(w^Tx_i + b)) = 0\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>这里对支持向量机的对偶问题进行第一步求解:<span class=\"math inline\">\\(\\min_{w,b} \\mathcal{L}(w, b, \\alpha)\\)</span>，直接令一阶导数等于0： <span class=\"math display\">\\[\r\n\\frac{\\partial\\mathcal{L}}{\\partial w} = 0 \\Rightarrow w = \\sum_{i=1}^m \\alpha_i y_i x_i \\\\\r\n\\frac{\\partial\\mathcal{L}}{\\partial b} = 0 \\Rightarrow \\sum_{i=1}^m \\alpha_i y_i = 0\r\n\\]</span></p>\r\n<p>这里可以看出，<span class=\"math inline\">\\(w\\)</span>仅和<span class=\"math inline\">\\(\\alpha_i &gt; 0\\)</span>的样本有关，而根据KKT条件，<span class=\"math inline\">\\(\\alpha_i &gt; 0\\)</span>的地方必须满足<span class=\"math inline\">\\((1 - y_i(w^Tx_i + b)) = 0\\)</span>，即这些<span class=\"math inline\">\\(x_i\\)</span>在最大间隔边界上，这样的样本称为支持向量，支持向量机的解仅仅和支持向量有关。</p>\r\n<p>将求得结果代入<span class=\"math inline\">\\(\\mathcal{L}\\)</span>： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n\\mathcal{L} &amp;= \\frac{1}{2}\\sum_{i=1}^m\\sum_{j=1}^m \\alpha_i \\alpha_j y_i y_j x_i^T x_j + \\sum_{i=1}^m \\alpha_i - \\sum_{i=1}^m \\alpha_i y_i(w^Tx_i + b)\\\\\r\n&amp;= \\frac{1}{2}\\sum_{i=1}^m\\sum_{j=1}^m \\alpha_i \\alpha_j y_i y_j x_i^T x_j + \\sum_{i=1}^m \\alpha_i - \\sum_{i=1}^m \\alpha_i y_iw^Tx_i\\\\\r\n&amp;= \\sum_{i=1}^m \\alpha_i - \\frac{1}{2}\\sum_{i=1}^m\\sum_{j=1}^m \\alpha_i \\alpha_j y_i y_j x_i^T x_j\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>此时，再求解 <span class=\"math display\">\\[\r\n\\max_{\\alpha} \\mathcal{L}(w, b, \\alpha)\\\\\r\ns.t. \\ \\alpha_i \\ge 0,i=1,2,...,m\\\\\r\n\\ \\sum_{i=1}^m \\alpha_i y_i = 0\r\n\\]</span> 这个问题的求解使用SMO(序列最小优化)算法，大致思路和坐标上升法类似，迭代进行，每次选取两个<span class=\"math inline\">\\(\\alpha\\)</span>进行更新，同时更新参数<span class=\"math inline\">\\(b\\)</span>，具体步骤后面有时间再详细学习。</p>\r\n<h1 id=\"svm的核技巧\">SVM的核技巧</h1>\r\n<p>线性SVM基于一个基本假设：数据在空间<span class=\"math inline\">\\(\\mathbb{R}^d\\)</span>中线性可分，但这个假设在实际应用中，基本不满足。</p>\r\n<p>但是存在一个定理：当<span class=\"math inline\">\\(d\\)</span>有限时，一定存在<span class=\"math inline\">\\(\\hat{d}\\)</span>，使得样本在空间<span class=\"math inline\">\\(\\mathbb{R}^{\\hat{d}}\\)</span>中线性可分。</p>\r\n<p>因此我们可以构造一种映射：<span class=\"math inline\">\\(x_i \\rightarrow \\phi(x_i)\\)</span>，然后在这个映射的空间中使用线性SVM进行分类。</p>\r\n<p>这样最终需要求解的拉格朗日函数可以写成：<span class=\"math inline\">\\(\\mathcal{L} = \\sum_{i=1}^m \\alpha_i - \\frac{1}{2}\\sum_{i=1}^m\\sum_{j=1}^m \\alpha_i \\alpha_j y_i y_j \\phi(x_i)^T \\phi(x_j)\\)</span>，看起来很简单，只需要换一种计算方式就行，但是这里存在一个问题：<span class=\"math inline\">\\(\\hat{d}\\)</span>可能非常大，导致计算困难。</p>\r\n<p>针对上述问题，需要使用核技巧：构造一个计算复杂度为<span class=\"math inline\">\\(O(d)\\)</span>的<span class=\"math inline\">\\(k(x_i, x_j)\\)</span>使得：<span class=\"math inline\">\\(k(x_i, x_j)=\\phi(x_i)^T \\phi(x_j)\\)</span>，即我们只需要一种快速的计算内积的方式，并不关心其他运算。</p>\r\n<p>核函数的选择：当数据维数<span class=\"math inline\">\\(d\\)</span>超过了样本数量的时候，一般选用线性核，当数据维数<span class=\"math inline\">\\(d\\)</span>较小，而样本量<span class=\"math inline\">\\(m\\)</span>中等时，可以选择RBF核，但是当数据维数<span class=\"math inline\">\\(d\\)</span>较小，而样本量<span class=\"math inline\">\\(m\\)</span>特别大时，不需要选择了，直接使用深度神经网络吧。</p>\r\n<p>核函数的定义需要满足Mercer条件：核函数矩阵必须是半正定的。可以理解为内积大于等于0。</p>\r\n<p>核函数有一些性质：如果<span class=\"math inline\">\\(k_1\\)</span>、<span class=\"math inline\">\\(k_2\\)</span>是核函数，那么下列函数也是核函数：</p>\r\n<p><span class=\"math inline\">\\(c_1k_1(x_i, x_j) + c_2k_2(x_i, x_j), \\ c1,c2 &gt; 0\\)</span></p>\r\n<p><span class=\"math inline\">\\(k_1(x_i, x_j)k_2(x_i, x_j)\\)</span></p>\r\n<p><span class=\"math inline\">\\(f(x_i)k_1(x_i, x_j)f(x_j)\\)</span></p>\r\n<h1 id=\"软间隔svm\">软间隔SVM</h1>\r\n<p>数据中不能总是找到线性可分的空间，而且数据存在噪声或者错误标注，这个时候我们如果按照SVM的优化方式，很可能造成过拟合的问题，因此可以允许少量分类错误出现，定义松弛变量<span class=\"math inline\">\\(\\epsilon_i = \\begin{cases}  0 &amp; y_i(w^T\\phi(x_i) + b) \\ge 1\\\\  1 - y_i(w^T\\phi(x_i) + b) &amp; y_i(w^T\\phi(x_i) + b) &lt; 1 \\end{cases}\\)</span>，由此定义软间隔支持向量机的基本型： <span class=\"math display\">\\[\r\n\\min_{w,b,\\epsilon} \\frac{1}{2}w^T w + C\\sum_{i=1}^m \\epsilon_i\\\\\r\ns.t. y_i(w^T\\phi(x_i) + b) \\ge 1 - \\epsilon_i, i = 1, 2,...,m\\\\\r\n\\epsilon_i \\ge 0, i=1,2,...,m\r\n\\]</span> 其中<span class=\"math inline\">\\(C\\)</span>是一个可调节参数，用于调节错误分类的惩罚。</p>\r\n<p>软间隔支持向量机的求解方式和支持向量机类似，不过从一个约束变成了两个约束。</p>\r\n<p>另外<span class=\"math inline\">\\(\\epsilon_i\\)</span>也可以表示成<span class=\"math inline\">\\(\\max(0, 1 - y_i(w^T\\phi(x_i) + b))\\)</span>，因此软间隔支持向量机的基本型的对偶问题可以表示为<span class=\"math inline\">\\(\\min_{w, b} \\frac{1}{m}\\sum_{i=1}^m \\max(0, 1 - y_i(w^T\\phi(x_i) + b)) + \\frac{1}{2mC}w^Tw\\)</span>，其中第一项称为经验风险，度量模型对数据的拟合程度，第二项称为结构风险，度量模型的复杂程度，也可以称为正则化项。因此还衍生出一种损失函数：hinge loss：<span class=\"math inline\">\\(\\mathbb{l}(s) = max(0, 1-s)\\)</span>。</p>\r\n"},{"title":"attention机制总结","date":"2020-06-09T06:50:26.000Z","mathjax":true,"_content":"Attention机制最初是在自然语言处理领域取得了巨大成功，现在CNN中也在越来越多的使用Attention机制，这里就CNN中使用的Soft Attention（其中Attention weight是模型产生，可微可学习）做一些总结。\n\n# 对Attention的理解\nAttention使得神经网络（NN）可以更多的关注当前任务相关的输入（不一定是原始输入，也可能是提取出的特征），同时忽略和当前任务不相关的输入，本质上是希望对不同的信息计算得到不同的权重，以代表不同信息的重要性程度，并通过加权和的方式聚合这些信息。\n\n更抽象来说，在给定一个query元素的情况下，Attention机制的作用是：根据query元素将不同的key元素按照相关程度（或者重要程度）排序，在产生一个输出时，让模型可以选择哪些输入更加重要。这里query元素和key元素暂时没有合适的定义，不过可以通过两个例子来理解，例如在语言翻译问题中，这里的query element可以理解为输出语句中的某个单词，key element则是输入的语句中的某个单词，对于某个翻译得到的单词，不是每个输入单词都和其相关（或者说不是每个输入单词和这个翻译得到的单词的相关程度都一样），又例如在CNN中，query和key都可以理解为图像上某个像素或者某个区域。\n\n在论文《An Empirical Study of Spatial Attention Mechanisms in Deep Networks》中提出了广义注意力机制的表达式（Generalized attention formulation），如下所示，其中$y_q$表示最终得到的Attention feature（通过Attention机制聚合得到的特征或者信息），$q$表示query元素的位置，$k$表示key元素的位置，$M$表示Attention Head的个数（不同的Attention Head可以理解为不同的Attention计算方式），$z_q$表示位置为$q$的query元素的内容，$x_k$表示位置为$k$的key元素的内容，$A_m$表示计算Attention weight的函数，$W_m$和$W'_m$表示两个可学习参数，这两个参数也可以去掉（即设置为固定值），$\\Omega_q$表示query元素可以涉及到的key元素的范围（例如翻译任务中，一般这个范围是整个输入语句的所有单词）\n$$\n    y_q = \\sum\\limits_{m=1}^M W_m[\\sum\\limits_{k\\in \\Omega_q}A_m(q, k, z_q, x_k)\\odot W'_m x_k]\n$$\n\n上面的公式中$W'_m x_k$可以理解为首先对key元素做一个投影变换，得到合适的特征，然后$A_m$函数针对范围$\\Omega_q$中的每个key元素$x_k$计算出其Attention weight（这个attention weight一般和q, k, z_q, x_k这几个值有关，但有些计算方式不会全部使用），按照Attention weight对key元素的投影进行一个加权求和，最后再进行一次投影变换，得到合适的Attention 特征，这就完成了一种Attention的过程，但是有些时候可以多种Attention一起使用，因此可以$M$种Attention feature求和得到最终的Attention feature。\n\n根据上面的广义注意力机制表达式，设计一个注意力机制时，需要确定的内容就是一下几点：\n- $\\Omega_q$，一个query元素可以涉及到的key元素的范围。\n- $A_m$，计算Attention weight的函数。\n- $W_m$、$W'_m$，是否需要进行特征投影变换。\n\n## Self-Attention\nSelf-Attention是指query元素和key元素来自于同一个集合的Attention，CNN中的Attention基本上都是Self-Attention，例如《Non-local Neural Networks》、《CBAM: Convolutional Block Attention Module》这些论文中使用的Attention，其query和key元素都是特征图上的像素，属于同一个集合，因此都属于Self-Attention的范围。\n\n在论文《An Empirical Study of Spatial Attention Mechanisms in Deep Networks》中提到了几种Self-Attention的计算方式，如下图所示，这里有四种Attention weight的计算方式：\n\n- query and key content表示仅仅使用$z_q$和$x_k$来计算Attention weight，一般其Attention weight正比于query content和 key content之间的相似性。\n- query content and relative position表示使用$z_q$和$k,q$来计算Attention weight。\n- key content only表示仅使用$x_k$来计算Attention weight。\n- relative position only表示仅使用$k,q$来计算Attention weight。\n\n![Attention计算方式](Attention计算方式.png)\n\n## 不同的Attention机制在广义Attention表达式上的表示\n### Transformer attention\n在论文《Transformer-xl: Attentive lan-guage models beyond a fixed-length context》中使用了4个Attention Head，包含了上面提到的Self-Attention的四种计算方式，论文《An Empirical Study of Spatial Attention Mechanisms in Deep Networks》中对其进行了总结，如下：\n\n- query and key content方式计算得到的Attention weight可以表示为$z_q^TU^T_mV^C_mx_k$，其中$U_m$和$V^C_m$表示可学习的嵌入矩阵，用于将query content和key content转换到合适的空间方便计算，或者缩减维度以减少计算量。\n- query content and relative position方式计算得到的Attention weight可以表示为$z_q^TU^T_mV^R_mR_{k-q}$，这里的$R_{k-q}$表示相对位置$k-q$的一种编码，$V^R_m$同样是一个可学习的嵌入矩阵。\n- key content only方式计算得到的Attention weight可以表示为$u_m^TV_m^Cx_k$，其中$u_m$是个可学习的向量，用于捕获对当前任务贡献突出的key content的编码。\n- relative position only方式计算得到的Attention weight可以表示为$v^T_mV^R_mR_{k-q}$，这里的$v_m$也是个可学习的向量，用于key元素和query元素的全局位置偏差。\n\n### Regular and deformable convolution\n常规卷积和可变形卷积都可以看做一种注意力机制。\n\n在常规卷积中，Attention weight的计算函数$A_m$属于上面提到的relative position only方式，可以表达为如下形式，这里的$Q$表示卷积核覆盖的区域。这样的$A_m$实际上是一种基于query和key元素相对位置的函数，计算Attention weight的过程不需要$z_q$和$x_k$的值，计算出来的Attention weight只是指定哪些位置和query相关。\n\n$$\nA_m(k, q) = \\begin{cases}\n    1 & if \\ k \\in Q\\\\\n    0 & otherwise\n\\end{cases}\n$$\n\n另外，常规卷积相当于只有一个Attention head，因此$M=1$，$\\Omega_q$可以是整个特征图上的所有像素，的卷积核参数可以看成是$W_0$，常规卷积没有$W'_0$，key元素则是输入的特征图上的像素点$x_k$，因此常规卷积可以看做Attention的一种特例。\n\n对于可变形卷积，其$A_m$函数可以表达如下，其中$w_m^T$表示用于计算偏移的$1\\times 1$卷积的参数，$p_m$表示正常卷积相对于卷积核中心的偏移，$q + p_m + w_m^T x_q$实际上就是可变形卷积首先计算得到的卷积核偏移值，而$G$是插值函数，表示在插值过程中，对于需要插值的位置$q + p_m + w_m^T x_q$计算得到的位置$k$的权重，因此可变形卷积的插值权重可以理解为Attention weight，所以可变形卷积核常规卷积一样，也可以看做是一种Attention。\n\n$$\nA_m(q,k,x_q) = G(k, q + p_m + w_m^T x_q)\n$$\n\n# CNN中的一些Attention\n## Non-local Neural Networks\n在论文《Non-local Neural Networks》中，提出了一种称为Non-local Neural Networks的结构，通过将某一位置的响应计算为所有位置特征的加权和的方式，用于捕获远程依赖，其表达式如下，其中$i$是响应的位置，$j$则遍历所有输入的位置，（这里的位置可以包含空间和时间，因此Non-local Neural Networks可以用于处理视频），$x_i$表示响应位置的输入值，$x_j$则可以遍历所有输入元素的值，$f(x_i, x_j)$计算得到一个标量，用于表示$x_i, x_j$之间的关系，例如相关程度，$g(x_j)$则表示对$x_j$的一种表示，例如一种投影，$C(x)$表示归一化系数，$y_i$是最终得到的输出。\n\n$$\ny_i = \\frac{1}{C(x)}\\sum\\limits_{\\forall j}f(x_i, x_j)g(x_j)\n$$\n\nNon-local Neural Networks的结构示意图如下所示，其中$H,W$表示空间上的范围，$T$表示时间上的范围，这里的示例输入形状是$T\\times H \\times W \\times 1024$，如果是视频处理的话，即表示一共处理相邻的$T$帧特征图，每幅特征图高$H$，宽$W$，channel个数为1024，上面表达式中的$f(x_i, x_j)$由下图中的$\\theta(x) \\times \\phi(x)$来完成，归一化则由softmax操作来完成，softmax的结果和$g(x)$的结果相乘，就完成了“所有位置特征的加权和操作”，得到响应$y$，表示捕获到的远程依赖，这样一来，$y$中每个特征值都聚合了全局的信息，再进行一些处理转换，最终和原始输入相加得到输出Z。\n\n![Non-local Neural Networks 结构示意图](Non_local_NN.png)\n\n上图中的$y$最终还经过了一次$1\\times 1\\times 1$卷积得到$z$，因此可以表达为：$z_i = W_z y_i + x_i$，这里论文中将$W_z$初始化为0，且加上$x_i$这一项，其原因是为了在改变网络结构之后，还可以利用之前的预训练权重。\n\n整个Non-local Neural Networks过程虽然简单，但是也是Self-Attention的一种特例，类比之前提到的广义注意力机制表达式，这里只有一个Attention Head，$M=1$，$i$可以看做query元素的位置，$j$则是key元素的位置，query元素和key元素都属于同一个集合（即输入x），$x_i$表示query content，$x_j$则表示key content，$f(x_i, x_j)$则是query and key content方式的$A_m$函数（$A_m$计算得到的Attention weight是归一化的，因此$A_m$还包括$\\frac{1}{C(x)}$用于归一化）。$g(x_j)$是对key content的一种变换，其参数就是前面的$W'_m$，最后这里没有$W_m$，softmax的结果和$g(x)$的结果进行的矩阵乘法完成了特征加权和的操作。\n\n## Squeeze-and-Excitation Networks（SE Network）\n\n在论文《Squeeze-and-Excitation Networks》中，提出了一种用于channel attention的Squeeze-and-Excitation（SE）结构，通过显式地建模通道之间的相互依赖关系，自适应地重新修正不同通道的特征响应幅度，其结构示意图如下所示。其中$F_{sq}$表示Squeeze操作，在论文中使用一个全局平均池化来实现，用于在空间维度上来聚合全局的信息，变成一种逐通道的表示状态。之后的$F_{ex}$表示Excitation操作，其操作顺序为（全连接-ReLU-全连接-Sigmoid，这里没看懂可以参考下面在InceptionNet和ResNet中的详细结构），用于捕获通道之间的依赖性关系，并且形成通道的权重。最终将得到的各通道权重分别乘以原来的通道，达到对不同通道的响应幅度进行自适应修正的目的。\n\n![Squeeze-and-Excitation Networks 结构示意图](Squeeze_and_Excitation.png)\n\n论文中在InceptionNet以及ResNet中都尝试加入SE模块，其加入之后的结构如下图所示。其中Global pooling即Squeeze操作，用于聚合超出像素点感受野之外的信息并形成逐通道的表示，后面紧接着Excitation操作，首先通过全连接对其进行降维，减少计算量，之后使用ReLU激活函数来增加Excitation操作的非线性性质，之后再通过一个全连接层得到逐通道的channel weight，并通过Sigmoid函数将channel weight限制在0到1之间，最后将channel weight与原始特征图相乘，每个通道一个权重，用于调整不同通道的响应幅值。\n\n![InceptionNet以及ResNet中的SE结构](SEInceptionNet_SEResNet.png)\n\nExcitation操作很容易和之前的广义注意力机制表达式类比，其本质上就是一种简单的Self-Attention，只不过Excitation操作计算出的Attention weight除了query位置以外全是0，这样就不是按照权重去聚合所有通道的信息，而是使用空间维度的全局信息来调整通道的响应幅值。\n\n## Convolutional Block Attention Module\n论文《CBAM: Convolutional Block Attention Module》中提出了一种同时进行channel和spatial attention的attention模块，称为CBAM。\n\nCBAM中包含了两种Attention，如下图所示，上面的结构是channel Attention部分，其接收输入之后分别通过空间维度上的全局最大池化和全局平均池化，和SE模块中的Squeeze类似，只不过这里使用两种池化，得到了两个结果，这两个结果再送到同一个MLP（多层感知机），这里相当于两个共享权重的Excitation操作，最后两个结果相加在进行sigmoid操作，这里可以看出来，整个Channel Attention过程其实就是稍微改变的SE模块。图片下面的结构是Spatial Attention的结构，这里的最大池化和平均池化是在Channel维度进行的，即对每个像素求出所有Channel的最大值或者平均值，最终得到两个Channel数量为1的特征图，两个特征图进行Concatenate之后经过一个$7\\times7$卷积，最后使用sigmoid函数处理得到结果，这两个模块得到的结果都是用于和原始特征图相乘，一个是逐通道的调整特征响应的幅值，一个是逐像素的调整特征响应的幅值。\n\n![CBAM中的两种Attention结构](CBAM_detail.png)\n\n上面展示的两种结构在模型中先后使用，可以使得模型包含对不同channel响应幅度的调整能力的同时也包含对不同位置响应幅度的调整能力，在ResBlock中添加CBAM结构之后，模型的结构如下图所示。\n\n![ResBlock中使用CBAM结构](Resnet_CBAM.png)\n\n在该论文的实验中，CBAM模块在图像分类任务上提升不大，但是使用轻量级模型（MobileNet，ShuffleNet）试验时错误率相对于SE模块有一个百分点左右的下降（可能是参数增加，模型容量增大导致？），在目标检测任务上，ResNet+CBAM相比于原始的ResNet有一点几个百分点的mAP提升。","source":"_posts/学习笔记/attention机制总结.md","raw":"---\ntitle: attention机制总结\ndate: 2020-06-09 14:50:26\ntags: [深度学习]\nmathjax: true\n---\nAttention机制最初是在自然语言处理领域取得了巨大成功，现在CNN中也在越来越多的使用Attention机制，这里就CNN中使用的Soft Attention（其中Attention weight是模型产生，可微可学习）做一些总结。\n\n# 对Attention的理解\nAttention使得神经网络（NN）可以更多的关注当前任务相关的输入（不一定是原始输入，也可能是提取出的特征），同时忽略和当前任务不相关的输入，本质上是希望对不同的信息计算得到不同的权重，以代表不同信息的重要性程度，并通过加权和的方式聚合这些信息。\n\n更抽象来说，在给定一个query元素的情况下，Attention机制的作用是：根据query元素将不同的key元素按照相关程度（或者重要程度）排序，在产生一个输出时，让模型可以选择哪些输入更加重要。这里query元素和key元素暂时没有合适的定义，不过可以通过两个例子来理解，例如在语言翻译问题中，这里的query element可以理解为输出语句中的某个单词，key element则是输入的语句中的某个单词，对于某个翻译得到的单词，不是每个输入单词都和其相关（或者说不是每个输入单词和这个翻译得到的单词的相关程度都一样），又例如在CNN中，query和key都可以理解为图像上某个像素或者某个区域。\n\n在论文《An Empirical Study of Spatial Attention Mechanisms in Deep Networks》中提出了广义注意力机制的表达式（Generalized attention formulation），如下所示，其中$y_q$表示最终得到的Attention feature（通过Attention机制聚合得到的特征或者信息），$q$表示query元素的位置，$k$表示key元素的位置，$M$表示Attention Head的个数（不同的Attention Head可以理解为不同的Attention计算方式），$z_q$表示位置为$q$的query元素的内容，$x_k$表示位置为$k$的key元素的内容，$A_m$表示计算Attention weight的函数，$W_m$和$W'_m$表示两个可学习参数，这两个参数也可以去掉（即设置为固定值），$\\Omega_q$表示query元素可以涉及到的key元素的范围（例如翻译任务中，一般这个范围是整个输入语句的所有单词）\n$$\n    y_q = \\sum\\limits_{m=1}^M W_m[\\sum\\limits_{k\\in \\Omega_q}A_m(q, k, z_q, x_k)\\odot W'_m x_k]\n$$\n\n上面的公式中$W'_m x_k$可以理解为首先对key元素做一个投影变换，得到合适的特征，然后$A_m$函数针对范围$\\Omega_q$中的每个key元素$x_k$计算出其Attention weight（这个attention weight一般和q, k, z_q, x_k这几个值有关，但有些计算方式不会全部使用），按照Attention weight对key元素的投影进行一个加权求和，最后再进行一次投影变换，得到合适的Attention 特征，这就完成了一种Attention的过程，但是有些时候可以多种Attention一起使用，因此可以$M$种Attention feature求和得到最终的Attention feature。\n\n根据上面的广义注意力机制表达式，设计一个注意力机制时，需要确定的内容就是一下几点：\n- $\\Omega_q$，一个query元素可以涉及到的key元素的范围。\n- $A_m$，计算Attention weight的函数。\n- $W_m$、$W'_m$，是否需要进行特征投影变换。\n\n## Self-Attention\nSelf-Attention是指query元素和key元素来自于同一个集合的Attention，CNN中的Attention基本上都是Self-Attention，例如《Non-local Neural Networks》、《CBAM: Convolutional Block Attention Module》这些论文中使用的Attention，其query和key元素都是特征图上的像素，属于同一个集合，因此都属于Self-Attention的范围。\n\n在论文《An Empirical Study of Spatial Attention Mechanisms in Deep Networks》中提到了几种Self-Attention的计算方式，如下图所示，这里有四种Attention weight的计算方式：\n\n- query and key content表示仅仅使用$z_q$和$x_k$来计算Attention weight，一般其Attention weight正比于query content和 key content之间的相似性。\n- query content and relative position表示使用$z_q$和$k,q$来计算Attention weight。\n- key content only表示仅使用$x_k$来计算Attention weight。\n- relative position only表示仅使用$k,q$来计算Attention weight。\n\n![Attention计算方式](Attention计算方式.png)\n\n## 不同的Attention机制在广义Attention表达式上的表示\n### Transformer attention\n在论文《Transformer-xl: Attentive lan-guage models beyond a fixed-length context》中使用了4个Attention Head，包含了上面提到的Self-Attention的四种计算方式，论文《An Empirical Study of Spatial Attention Mechanisms in Deep Networks》中对其进行了总结，如下：\n\n- query and key content方式计算得到的Attention weight可以表示为$z_q^TU^T_mV^C_mx_k$，其中$U_m$和$V^C_m$表示可学习的嵌入矩阵，用于将query content和key content转换到合适的空间方便计算，或者缩减维度以减少计算量。\n- query content and relative position方式计算得到的Attention weight可以表示为$z_q^TU^T_mV^R_mR_{k-q}$，这里的$R_{k-q}$表示相对位置$k-q$的一种编码，$V^R_m$同样是一个可学习的嵌入矩阵。\n- key content only方式计算得到的Attention weight可以表示为$u_m^TV_m^Cx_k$，其中$u_m$是个可学习的向量，用于捕获对当前任务贡献突出的key content的编码。\n- relative position only方式计算得到的Attention weight可以表示为$v^T_mV^R_mR_{k-q}$，这里的$v_m$也是个可学习的向量，用于key元素和query元素的全局位置偏差。\n\n### Regular and deformable convolution\n常规卷积和可变形卷积都可以看做一种注意力机制。\n\n在常规卷积中，Attention weight的计算函数$A_m$属于上面提到的relative position only方式，可以表达为如下形式，这里的$Q$表示卷积核覆盖的区域。这样的$A_m$实际上是一种基于query和key元素相对位置的函数，计算Attention weight的过程不需要$z_q$和$x_k$的值，计算出来的Attention weight只是指定哪些位置和query相关。\n\n$$\nA_m(k, q) = \\begin{cases}\n    1 & if \\ k \\in Q\\\\\n    0 & otherwise\n\\end{cases}\n$$\n\n另外，常规卷积相当于只有一个Attention head，因此$M=1$，$\\Omega_q$可以是整个特征图上的所有像素，的卷积核参数可以看成是$W_0$，常规卷积没有$W'_0$，key元素则是输入的特征图上的像素点$x_k$，因此常规卷积可以看做Attention的一种特例。\n\n对于可变形卷积，其$A_m$函数可以表达如下，其中$w_m^T$表示用于计算偏移的$1\\times 1$卷积的参数，$p_m$表示正常卷积相对于卷积核中心的偏移，$q + p_m + w_m^T x_q$实际上就是可变形卷积首先计算得到的卷积核偏移值，而$G$是插值函数，表示在插值过程中，对于需要插值的位置$q + p_m + w_m^T x_q$计算得到的位置$k$的权重，因此可变形卷积的插值权重可以理解为Attention weight，所以可变形卷积核常规卷积一样，也可以看做是一种Attention。\n\n$$\nA_m(q,k,x_q) = G(k, q + p_m + w_m^T x_q)\n$$\n\n# CNN中的一些Attention\n## Non-local Neural Networks\n在论文《Non-local Neural Networks》中，提出了一种称为Non-local Neural Networks的结构，通过将某一位置的响应计算为所有位置特征的加权和的方式，用于捕获远程依赖，其表达式如下，其中$i$是响应的位置，$j$则遍历所有输入的位置，（这里的位置可以包含空间和时间，因此Non-local Neural Networks可以用于处理视频），$x_i$表示响应位置的输入值，$x_j$则可以遍历所有输入元素的值，$f(x_i, x_j)$计算得到一个标量，用于表示$x_i, x_j$之间的关系，例如相关程度，$g(x_j)$则表示对$x_j$的一种表示，例如一种投影，$C(x)$表示归一化系数，$y_i$是最终得到的输出。\n\n$$\ny_i = \\frac{1}{C(x)}\\sum\\limits_{\\forall j}f(x_i, x_j)g(x_j)\n$$\n\nNon-local Neural Networks的结构示意图如下所示，其中$H,W$表示空间上的范围，$T$表示时间上的范围，这里的示例输入形状是$T\\times H \\times W \\times 1024$，如果是视频处理的话，即表示一共处理相邻的$T$帧特征图，每幅特征图高$H$，宽$W$，channel个数为1024，上面表达式中的$f(x_i, x_j)$由下图中的$\\theta(x) \\times \\phi(x)$来完成，归一化则由softmax操作来完成，softmax的结果和$g(x)$的结果相乘，就完成了“所有位置特征的加权和操作”，得到响应$y$，表示捕获到的远程依赖，这样一来，$y$中每个特征值都聚合了全局的信息，再进行一些处理转换，最终和原始输入相加得到输出Z。\n\n![Non-local Neural Networks 结构示意图](Non_local_NN.png)\n\n上图中的$y$最终还经过了一次$1\\times 1\\times 1$卷积得到$z$，因此可以表达为：$z_i = W_z y_i + x_i$，这里论文中将$W_z$初始化为0，且加上$x_i$这一项，其原因是为了在改变网络结构之后，还可以利用之前的预训练权重。\n\n整个Non-local Neural Networks过程虽然简单，但是也是Self-Attention的一种特例，类比之前提到的广义注意力机制表达式，这里只有一个Attention Head，$M=1$，$i$可以看做query元素的位置，$j$则是key元素的位置，query元素和key元素都属于同一个集合（即输入x），$x_i$表示query content，$x_j$则表示key content，$f(x_i, x_j)$则是query and key content方式的$A_m$函数（$A_m$计算得到的Attention weight是归一化的，因此$A_m$还包括$\\frac{1}{C(x)}$用于归一化）。$g(x_j)$是对key content的一种变换，其参数就是前面的$W'_m$，最后这里没有$W_m$，softmax的结果和$g(x)$的结果进行的矩阵乘法完成了特征加权和的操作。\n\n## Squeeze-and-Excitation Networks（SE Network）\n\n在论文《Squeeze-and-Excitation Networks》中，提出了一种用于channel attention的Squeeze-and-Excitation（SE）结构，通过显式地建模通道之间的相互依赖关系，自适应地重新修正不同通道的特征响应幅度，其结构示意图如下所示。其中$F_{sq}$表示Squeeze操作，在论文中使用一个全局平均池化来实现，用于在空间维度上来聚合全局的信息，变成一种逐通道的表示状态。之后的$F_{ex}$表示Excitation操作，其操作顺序为（全连接-ReLU-全连接-Sigmoid，这里没看懂可以参考下面在InceptionNet和ResNet中的详细结构），用于捕获通道之间的依赖性关系，并且形成通道的权重。最终将得到的各通道权重分别乘以原来的通道，达到对不同通道的响应幅度进行自适应修正的目的。\n\n![Squeeze-and-Excitation Networks 结构示意图](Squeeze_and_Excitation.png)\n\n论文中在InceptionNet以及ResNet中都尝试加入SE模块，其加入之后的结构如下图所示。其中Global pooling即Squeeze操作，用于聚合超出像素点感受野之外的信息并形成逐通道的表示，后面紧接着Excitation操作，首先通过全连接对其进行降维，减少计算量，之后使用ReLU激活函数来增加Excitation操作的非线性性质，之后再通过一个全连接层得到逐通道的channel weight，并通过Sigmoid函数将channel weight限制在0到1之间，最后将channel weight与原始特征图相乘，每个通道一个权重，用于调整不同通道的响应幅值。\n\n![InceptionNet以及ResNet中的SE结构](SEInceptionNet_SEResNet.png)\n\nExcitation操作很容易和之前的广义注意力机制表达式类比，其本质上就是一种简单的Self-Attention，只不过Excitation操作计算出的Attention weight除了query位置以外全是0，这样就不是按照权重去聚合所有通道的信息，而是使用空间维度的全局信息来调整通道的响应幅值。\n\n## Convolutional Block Attention Module\n论文《CBAM: Convolutional Block Attention Module》中提出了一种同时进行channel和spatial attention的attention模块，称为CBAM。\n\nCBAM中包含了两种Attention，如下图所示，上面的结构是channel Attention部分，其接收输入之后分别通过空间维度上的全局最大池化和全局平均池化，和SE模块中的Squeeze类似，只不过这里使用两种池化，得到了两个结果，这两个结果再送到同一个MLP（多层感知机），这里相当于两个共享权重的Excitation操作，最后两个结果相加在进行sigmoid操作，这里可以看出来，整个Channel Attention过程其实就是稍微改变的SE模块。图片下面的结构是Spatial Attention的结构，这里的最大池化和平均池化是在Channel维度进行的，即对每个像素求出所有Channel的最大值或者平均值，最终得到两个Channel数量为1的特征图，两个特征图进行Concatenate之后经过一个$7\\times7$卷积，最后使用sigmoid函数处理得到结果，这两个模块得到的结果都是用于和原始特征图相乘，一个是逐通道的调整特征响应的幅值，一个是逐像素的调整特征响应的幅值。\n\n![CBAM中的两种Attention结构](CBAM_detail.png)\n\n上面展示的两种结构在模型中先后使用，可以使得模型包含对不同channel响应幅度的调整能力的同时也包含对不同位置响应幅度的调整能力，在ResBlock中添加CBAM结构之后，模型的结构如下图所示。\n\n![ResBlock中使用CBAM结构](Resnet_CBAM.png)\n\n在该论文的实验中，CBAM模块在图像分类任务上提升不大，但是使用轻量级模型（MobileNet，ShuffleNet）试验时错误率相对于SE模块有一个百分点左右的下降（可能是参数增加，模型容量增大导致？），在目标检测任务上，ResNet+CBAM相比于原始的ResNet有一点几个百分点的mAP提升。","slug":"学习笔记/attention机制总结","published":1,"updated":"2020-08-31T06:39:20.751Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckjxpc3ro000i44mq549cd5m4","content":"<p>Attention机制最初是在自然语言处理领域取得了巨大成功，现在CNN中也在越来越多的使用Attention机制，这里就CNN中使用的Soft Attention（其中Attention weight是模型产生，可微可学习）做一些总结。</p>\r\n<h1 id=\"对attention的理解\">对Attention的理解</h1>\r\n<p>Attention使得神经网络（NN）可以更多的关注当前任务相关的输入（不一定是原始输入，也可能是提取出的特征），同时忽略和当前任务不相关的输入，本质上是希望对不同的信息计算得到不同的权重，以代表不同信息的重要性程度，并通过加权和的方式聚合这些信息。</p>\r\n<p>更抽象来说，在给定一个query元素的情况下，Attention机制的作用是：根据query元素将不同的key元素按照相关程度（或者重要程度）排序，在产生一个输出时，让模型可以选择哪些输入更加重要。这里query元素和key元素暂时没有合适的定义，不过可以通过两个例子来理解，例如在语言翻译问题中，这里的query element可以理解为输出语句中的某个单词，key element则是输入的语句中的某个单词，对于某个翻译得到的单词，不是每个输入单词都和其相关（或者说不是每个输入单词和这个翻译得到的单词的相关程度都一样），又例如在CNN中，query和key都可以理解为图像上某个像素或者某个区域。</p>\r\n<p>在论文《An Empirical Study of Spatial Attention Mechanisms in Deep Networks》中提出了广义注意力机制的表达式（Generalized attention formulation），如下所示，其中<span class=\"math inline\">\\(y_q\\)</span>表示最终得到的Attention feature（通过Attention机制聚合得到的特征或者信息），<span class=\"math inline\">\\(q\\)</span>表示query元素的位置，<span class=\"math inline\">\\(k\\)</span>表示key元素的位置，<span class=\"math inline\">\\(M\\)</span>表示Attention Head的个数（不同的Attention Head可以理解为不同的Attention计算方式），<span class=\"math inline\">\\(z_q\\)</span>表示位置为<span class=\"math inline\">\\(q\\)</span>的query元素的内容，<span class=\"math inline\">\\(x_k\\)</span>表示位置为<span class=\"math inline\">\\(k\\)</span>的key元素的内容，<span class=\"math inline\">\\(A_m\\)</span>表示计算Attention weight的函数，<span class=\"math inline\">\\(W_m\\)</span>和<span class=\"math inline\">\\(W&#39;_m\\)</span>表示两个可学习参数，这两个参数也可以去掉（即设置为固定值），<span class=\"math inline\">\\(\\Omega_q\\)</span>表示query元素可以涉及到的key元素的范围（例如翻译任务中，一般这个范围是整个输入语句的所有单词） <span class=\"math display\">\\[\r\n    y_q = \\sum\\limits_{m=1}^M W_m[\\sum\\limits_{k\\in \\Omega_q}A_m(q, k, z_q, x_k)\\odot W&#39;_m x_k]\r\n\\]</span></p>\r\n<p>上面的公式中<span class=\"math inline\">\\(W&#39;_m x_k\\)</span>可以理解为首先对key元素做一个投影变换，得到合适的特征，然后<span class=\"math inline\">\\(A_m\\)</span>函数针对范围<span class=\"math inline\">\\(\\Omega_q\\)</span>中的每个key元素<span class=\"math inline\">\\(x_k\\)</span>计算出其Attention weight（这个attention weight一般和q, k, z_q, x_k这几个值有关，但有些计算方式不会全部使用），按照Attention weight对key元素的投影进行一个加权求和，最后再进行一次投影变换，得到合适的Attention 特征，这就完成了一种Attention的过程，但是有些时候可以多种Attention一起使用，因此可以<span class=\"math inline\">\\(M\\)</span>种Attention feature求和得到最终的Attention feature。</p>\r\n<p>根据上面的广义注意力机制表达式，设计一个注意力机制时，需要确定的内容就是一下几点： - <span class=\"math inline\">\\(\\Omega_q\\)</span>，一个query元素可以涉及到的key元素的范围。 - <span class=\"math inline\">\\(A_m\\)</span>，计算Attention weight的函数。 - <span class=\"math inline\">\\(W_m\\)</span>、<span class=\"math inline\">\\(W&#39;_m\\)</span>，是否需要进行特征投影变换。</p>\r\n<h2 id=\"self-attention\">Self-Attention</h2>\r\n<p>Self-Attention是指query元素和key元素来自于同一个集合的Attention，CNN中的Attention基本上都是Self-Attention，例如《Non-local Neural Networks》、《CBAM: Convolutional Block Attention Module》这些论文中使用的Attention，其query和key元素都是特征图上的像素，属于同一个集合，因此都属于Self-Attention的范围。</p>\r\n<p>在论文《An Empirical Study of Spatial Attention Mechanisms in Deep Networks》中提到了几种Self-Attention的计算方式，如下图所示，这里有四种Attention weight的计算方式：</p>\r\n<ul>\r\n<li>query and key content表示仅仅使用<span class=\"math inline\">\\(z_q\\)</span>和<span class=\"math inline\">\\(x_k\\)</span>来计算Attention weight，一般其Attention weight正比于query content和 key content之间的相似性。</li>\r\n<li>query content and relative position表示使用<span class=\"math inline\">\\(z_q\\)</span>和<span class=\"math inline\">\\(k,q\\)</span>来计算Attention weight。</li>\r\n<li>key content only表示仅使用<span class=\"math inline\">\\(x_k\\)</span>来计算Attention weight。</li>\r\n<li>relative position only表示仅使用<span class=\"math inline\">\\(k,q\\)</span>来计算Attention weight。</li>\r\n</ul>\r\n<figure>\r\n<img src=\"Attention计算方式.png\" alt=\"Attention计算方式\" /><figcaption aria-hidden=\"true\">Attention计算方式</figcaption>\r\n</figure>\r\n<h2 id=\"不同的attention机制在广义attention表达式上的表示\">不同的Attention机制在广义Attention表达式上的表示</h2>\r\n<h3 id=\"transformer-attention\">Transformer attention</h3>\r\n<p>在论文《Transformer-xl: Attentive lan-guage models beyond a fixed-length context》中使用了4个Attention Head，包含了上面提到的Self-Attention的四种计算方式，论文《An Empirical Study of Spatial Attention Mechanisms in Deep Networks》中对其进行了总结，如下：</p>\r\n<ul>\r\n<li>query and key content方式计算得到的Attention weight可以表示为<span class=\"math inline\">\\(z_q^TU^T_mV^C_mx_k\\)</span>，其中<span class=\"math inline\">\\(U_m\\)</span>和<span class=\"math inline\">\\(V^C_m\\)</span>表示可学习的嵌入矩阵，用于将query content和key content转换到合适的空间方便计算，或者缩减维度以减少计算量。</li>\r\n<li>query content and relative position方式计算得到的Attention weight可以表示为<span class=\"math inline\">\\(z_q^TU^T_mV^R_mR_{k-q}\\)</span>，这里的<span class=\"math inline\">\\(R_{k-q}\\)</span>表示相对位置<span class=\"math inline\">\\(k-q\\)</span>的一种编码，<span class=\"math inline\">\\(V^R_m\\)</span>同样是一个可学习的嵌入矩阵。</li>\r\n<li>key content only方式计算得到的Attention weight可以表示为<span class=\"math inline\">\\(u_m^TV_m^Cx_k\\)</span>，其中<span class=\"math inline\">\\(u_m\\)</span>是个可学习的向量，用于捕获对当前任务贡献突出的key content的编码。</li>\r\n<li>relative position only方式计算得到的Attention weight可以表示为<span class=\"math inline\">\\(v^T_mV^R_mR_{k-q}\\)</span>，这里的<span class=\"math inline\">\\(v_m\\)</span>也是个可学习的向量，用于key元素和query元素的全局位置偏差。</li>\r\n</ul>\r\n<h3 id=\"regular-and-deformable-convolution\">Regular and deformable convolution</h3>\r\n<p>常规卷积和可变形卷积都可以看做一种注意力机制。</p>\r\n<p>在常规卷积中，Attention weight的计算函数<span class=\"math inline\">\\(A_m\\)</span>属于上面提到的relative position only方式，可以表达为如下形式，这里的<span class=\"math inline\">\\(Q\\)</span>表示卷积核覆盖的区域。这样的<span class=\"math inline\">\\(A_m\\)</span>实际上是一种基于query和key元素相对位置的函数，计算Attention weight的过程不需要<span class=\"math inline\">\\(z_q\\)</span>和<span class=\"math inline\">\\(x_k\\)</span>的值，计算出来的Attention weight只是指定哪些位置和query相关。</p>\r\n<p><span class=\"math display\">\\[\r\nA_m(k, q) = \\begin{cases}\r\n    1 &amp; if \\ k \\in Q\\\\\r\n    0 &amp; otherwise\r\n\\end{cases}\r\n\\]</span></p>\r\n<p>另外，常规卷积相当于只有一个Attention head，因此<span class=\"math inline\">\\(M=1\\)</span>，<span class=\"math inline\">\\(\\Omega_q\\)</span>可以是整个特征图上的所有像素，的卷积核参数可以看成是<span class=\"math inline\">\\(W_0\\)</span>，常规卷积没有<span class=\"math inline\">\\(W&#39;_0\\)</span>，key元素则是输入的特征图上的像素点<span class=\"math inline\">\\(x_k\\)</span>，因此常规卷积可以看做Attention的一种特例。</p>\r\n<p>对于可变形卷积，其<span class=\"math inline\">\\(A_m\\)</span>函数可以表达如下，其中<span class=\"math inline\">\\(w_m^T\\)</span>表示用于计算偏移的<span class=\"math inline\">\\(1\\times 1\\)</span>卷积的参数，<span class=\"math inline\">\\(p_m\\)</span>表示正常卷积相对于卷积核中心的偏移，<span class=\"math inline\">\\(q + p_m + w_m^T x_q\\)</span>实际上就是可变形卷积首先计算得到的卷积核偏移值，而<span class=\"math inline\">\\(G\\)</span>是插值函数，表示在插值过程中，对于需要插值的位置<span class=\"math inline\">\\(q + p_m + w_m^T x_q\\)</span>计算得到的位置<span class=\"math inline\">\\(k\\)</span>的权重，因此可变形卷积的插值权重可以理解为Attention weight，所以可变形卷积核常规卷积一样，也可以看做是一种Attention。</p>\r\n<p><span class=\"math display\">\\[\r\nA_m(q,k,x_q) = G(k, q + p_m + w_m^T x_q)\r\n\\]</span></p>\r\n<h1 id=\"cnn中的一些attention\">CNN中的一些Attention</h1>\r\n<h2 id=\"non-local-neural-networks\">Non-local Neural Networks</h2>\r\n<p>在论文《Non-local Neural Networks》中，提出了一种称为Non-local Neural Networks的结构，通过将某一位置的响应计算为所有位置特征的加权和的方式，用于捕获远程依赖，其表达式如下，其中<span class=\"math inline\">\\(i\\)</span>是响应的位置，<span class=\"math inline\">\\(j\\)</span>则遍历所有输入的位置，（这里的位置可以包含空间和时间，因此Non-local Neural Networks可以用于处理视频），<span class=\"math inline\">\\(x_i\\)</span>表示响应位置的输入值，<span class=\"math inline\">\\(x_j\\)</span>则可以遍历所有输入元素的值，<span class=\"math inline\">\\(f(x_i, x_j)\\)</span>计算得到一个标量，用于表示<span class=\"math inline\">\\(x_i, x_j\\)</span>之间的关系，例如相关程度，<span class=\"math inline\">\\(g(x_j)\\)</span>则表示对<span class=\"math inline\">\\(x_j\\)</span>的一种表示，例如一种投影，<span class=\"math inline\">\\(C(x)\\)</span>表示归一化系数，<span class=\"math inline\">\\(y_i\\)</span>是最终得到的输出。</p>\r\n<p><span class=\"math display\">\\[\r\ny_i = \\frac{1}{C(x)}\\sum\\limits_{\\forall j}f(x_i, x_j)g(x_j)\r\n\\]</span></p>\r\n<p>Non-local Neural Networks的结构示意图如下所示，其中<span class=\"math inline\">\\(H,W\\)</span>表示空间上的范围，<span class=\"math inline\">\\(T\\)</span>表示时间上的范围，这里的示例输入形状是<span class=\"math inline\">\\(T\\times H \\times W \\times 1024\\)</span>，如果是视频处理的话，即表示一共处理相邻的<span class=\"math inline\">\\(T\\)</span>帧特征图，每幅特征图高<span class=\"math inline\">\\(H\\)</span>，宽<span class=\"math inline\">\\(W\\)</span>，channel个数为1024，上面表达式中的<span class=\"math inline\">\\(f(x_i, x_j)\\)</span>由下图中的<span class=\"math inline\">\\(\\theta(x) \\times \\phi(x)\\)</span>来完成，归一化则由softmax操作来完成，softmax的结果和<span class=\"math inline\">\\(g(x)\\)</span>的结果相乘，就完成了“所有位置特征的加权和操作”，得到响应<span class=\"math inline\">\\(y\\)</span>，表示捕获到的远程依赖，这样一来，<span class=\"math inline\">\\(y\\)</span>中每个特征值都聚合了全局的信息，再进行一些处理转换，最终和原始输入相加得到输出Z。</p>\r\n<figure>\r\n<img src=\"Non_local_NN.png\" alt=\"Non-local Neural Networks 结构示意图\" /><figcaption aria-hidden=\"true\">Non-local Neural Networks 结构示意图</figcaption>\r\n</figure>\r\n<p>上图中的<span class=\"math inline\">\\(y\\)</span>最终还经过了一次<span class=\"math inline\">\\(1\\times 1\\times 1\\)</span>卷积得到<span class=\"math inline\">\\(z\\)</span>，因此可以表达为：<span class=\"math inline\">\\(z_i = W_z y_i + x_i\\)</span>，这里论文中将<span class=\"math inline\">\\(W_z\\)</span>初始化为0，且加上<span class=\"math inline\">\\(x_i\\)</span>这一项，其原因是为了在改变网络结构之后，还可以利用之前的预训练权重。</p>\r\n<p>整个Non-local Neural Networks过程虽然简单，但是也是Self-Attention的一种特例，类比之前提到的广义注意力机制表达式，这里只有一个Attention Head，<span class=\"math inline\">\\(M=1\\)</span>，<span class=\"math inline\">\\(i\\)</span>可以看做query元素的位置，<span class=\"math inline\">\\(j\\)</span>则是key元素的位置，query元素和key元素都属于同一个集合（即输入x），<span class=\"math inline\">\\(x_i\\)</span>表示query content，<span class=\"math inline\">\\(x_j\\)</span>则表示key content，<span class=\"math inline\">\\(f(x_i, x_j)\\)</span>则是query and key content方式的<span class=\"math inline\">\\(A_m\\)</span>函数（<span class=\"math inline\">\\(A_m\\)</span>计算得到的Attention weight是归一化的，因此<span class=\"math inline\">\\(A_m\\)</span>还包括<span class=\"math inline\">\\(\\frac{1}{C(x)}\\)</span>用于归一化）。<span class=\"math inline\">\\(g(x_j)\\)</span>是对key content的一种变换，其参数就是前面的<span class=\"math inline\">\\(W&#39;_m\\)</span>，最后这里没有<span class=\"math inline\">\\(W_m\\)</span>，softmax的结果和<span class=\"math inline\">\\(g(x)\\)</span>的结果进行的矩阵乘法完成了特征加权和的操作。</p>\r\n<h2 id=\"squeeze-and-excitation-networksse-network\">Squeeze-and-Excitation Networks（SE Network）</h2>\r\n<p>在论文《Squeeze-and-Excitation Networks》中，提出了一种用于channel attention的Squeeze-and-Excitation（SE）结构，通过显式地建模通道之间的相互依赖关系，自适应地重新修正不同通道的特征响应幅度，其结构示意图如下所示。其中<span class=\"math inline\">\\(F_{sq}\\)</span>表示Squeeze操作，在论文中使用一个全局平均池化来实现，用于在空间维度上来聚合全局的信息，变成一种逐通道的表示状态。之后的<span class=\"math inline\">\\(F_{ex}\\)</span>表示Excitation操作，其操作顺序为（全连接-ReLU-全连接-Sigmoid，这里没看懂可以参考下面在InceptionNet和ResNet中的详细结构），用于捕获通道之间的依赖性关系，并且形成通道的权重。最终将得到的各通道权重分别乘以原来的通道，达到对不同通道的响应幅度进行自适应修正的目的。</p>\r\n<figure>\r\n<img src=\"Squeeze_and_Excitation.png\" alt=\"Squeeze-and-Excitation Networks 结构示意图\" /><figcaption aria-hidden=\"true\">Squeeze-and-Excitation Networks 结构示意图</figcaption>\r\n</figure>\r\n<p>论文中在InceptionNet以及ResNet中都尝试加入SE模块，其加入之后的结构如下图所示。其中Global pooling即Squeeze操作，用于聚合超出像素点感受野之外的信息并形成逐通道的表示，后面紧接着Excitation操作，首先通过全连接对其进行降维，减少计算量，之后使用ReLU激活函数来增加Excitation操作的非线性性质，之后再通过一个全连接层得到逐通道的channel weight，并通过Sigmoid函数将channel weight限制在0到1之间，最后将channel weight与原始特征图相乘，每个通道一个权重，用于调整不同通道的响应幅值。</p>\r\n<figure>\r\n<img src=\"SEInceptionNet_SEResNet.png\" alt=\"InceptionNet以及ResNet中的SE结构\" /><figcaption aria-hidden=\"true\">InceptionNet以及ResNet中的SE结构</figcaption>\r\n</figure>\r\n<p>Excitation操作很容易和之前的广义注意力机制表达式类比，其本质上就是一种简单的Self-Attention，只不过Excitation操作计算出的Attention weight除了query位置以外全是0，这样就不是按照权重去聚合所有通道的信息，而是使用空间维度的全局信息来调整通道的响应幅值。</p>\r\n<h2 id=\"convolutional-block-attention-module\">Convolutional Block Attention Module</h2>\r\n<p>论文《CBAM: Convolutional Block Attention Module》中提出了一种同时进行channel和spatial attention的attention模块，称为CBAM。</p>\r\n<p>CBAM中包含了两种Attention，如下图所示，上面的结构是channel Attention部分，其接收输入之后分别通过空间维度上的全局最大池化和全局平均池化，和SE模块中的Squeeze类似，只不过这里使用两种池化，得到了两个结果，这两个结果再送到同一个MLP（多层感知机），这里相当于两个共享权重的Excitation操作，最后两个结果相加在进行sigmoid操作，这里可以看出来，整个Channel Attention过程其实就是稍微改变的SE模块。图片下面的结构是Spatial Attention的结构，这里的最大池化和平均池化是在Channel维度进行的，即对每个像素求出所有Channel的最大值或者平均值，最终得到两个Channel数量为1的特征图，两个特征图进行Concatenate之后经过一个<span class=\"math inline\">\\(7\\times7\\)</span>卷积，最后使用sigmoid函数处理得到结果，这两个模块得到的结果都是用于和原始特征图相乘，一个是逐通道的调整特征响应的幅值，一个是逐像素的调整特征响应的幅值。</p>\r\n<figure>\r\n<img src=\"CBAM_detail.png\" alt=\"CBAM中的两种Attention结构\" /><figcaption aria-hidden=\"true\">CBAM中的两种Attention结构</figcaption>\r\n</figure>\r\n<p>上面展示的两种结构在模型中先后使用，可以使得模型包含对不同channel响应幅度的调整能力的同时也包含对不同位置响应幅度的调整能力，在ResBlock中添加CBAM结构之后，模型的结构如下图所示。</p>\r\n<figure>\r\n<img src=\"Resnet_CBAM.png\" alt=\"ResBlock中使用CBAM结构\" /><figcaption aria-hidden=\"true\">ResBlock中使用CBAM结构</figcaption>\r\n</figure>\r\n<p>在该论文的实验中，CBAM模块在图像分类任务上提升不大，但是使用轻量级模型（MobileNet，ShuffleNet）试验时错误率相对于SE模块有一个百分点左右的下降（可能是参数增加，模型容量增大导致？），在目标检测任务上，ResNet+CBAM相比于原始的ResNet有一点几个百分点的mAP提升。</p>\r\n","site":{"data":{}},"excerpt":"","more":"<p>Attention机制最初是在自然语言处理领域取得了巨大成功，现在CNN中也在越来越多的使用Attention机制，这里就CNN中使用的Soft Attention（其中Attention weight是模型产生，可微可学习）做一些总结。</p>\r\n<h1 id=\"对attention的理解\">对Attention的理解</h1>\r\n<p>Attention使得神经网络（NN）可以更多的关注当前任务相关的输入（不一定是原始输入，也可能是提取出的特征），同时忽略和当前任务不相关的输入，本质上是希望对不同的信息计算得到不同的权重，以代表不同信息的重要性程度，并通过加权和的方式聚合这些信息。</p>\r\n<p>更抽象来说，在给定一个query元素的情况下，Attention机制的作用是：根据query元素将不同的key元素按照相关程度（或者重要程度）排序，在产生一个输出时，让模型可以选择哪些输入更加重要。这里query元素和key元素暂时没有合适的定义，不过可以通过两个例子来理解，例如在语言翻译问题中，这里的query element可以理解为输出语句中的某个单词，key element则是输入的语句中的某个单词，对于某个翻译得到的单词，不是每个输入单词都和其相关（或者说不是每个输入单词和这个翻译得到的单词的相关程度都一样），又例如在CNN中，query和key都可以理解为图像上某个像素或者某个区域。</p>\r\n<p>在论文《An Empirical Study of Spatial Attention Mechanisms in Deep Networks》中提出了广义注意力机制的表达式（Generalized attention formulation），如下所示，其中<span class=\"math inline\">\\(y_q\\)</span>表示最终得到的Attention feature（通过Attention机制聚合得到的特征或者信息），<span class=\"math inline\">\\(q\\)</span>表示query元素的位置，<span class=\"math inline\">\\(k\\)</span>表示key元素的位置，<span class=\"math inline\">\\(M\\)</span>表示Attention Head的个数（不同的Attention Head可以理解为不同的Attention计算方式），<span class=\"math inline\">\\(z_q\\)</span>表示位置为<span class=\"math inline\">\\(q\\)</span>的query元素的内容，<span class=\"math inline\">\\(x_k\\)</span>表示位置为<span class=\"math inline\">\\(k\\)</span>的key元素的内容，<span class=\"math inline\">\\(A_m\\)</span>表示计算Attention weight的函数，<span class=\"math inline\">\\(W_m\\)</span>和<span class=\"math inline\">\\(W&#39;_m\\)</span>表示两个可学习参数，这两个参数也可以去掉（即设置为固定值），<span class=\"math inline\">\\(\\Omega_q\\)</span>表示query元素可以涉及到的key元素的范围（例如翻译任务中，一般这个范围是整个输入语句的所有单词） <span class=\"math display\">\\[\r\n    y_q = \\sum\\limits_{m=1}^M W_m[\\sum\\limits_{k\\in \\Omega_q}A_m(q, k, z_q, x_k)\\odot W&#39;_m x_k]\r\n\\]</span></p>\r\n<p>上面的公式中<span class=\"math inline\">\\(W&#39;_m x_k\\)</span>可以理解为首先对key元素做一个投影变换，得到合适的特征，然后<span class=\"math inline\">\\(A_m\\)</span>函数针对范围<span class=\"math inline\">\\(\\Omega_q\\)</span>中的每个key元素<span class=\"math inline\">\\(x_k\\)</span>计算出其Attention weight（这个attention weight一般和q, k, z_q, x_k这几个值有关，但有些计算方式不会全部使用），按照Attention weight对key元素的投影进行一个加权求和，最后再进行一次投影变换，得到合适的Attention 特征，这就完成了一种Attention的过程，但是有些时候可以多种Attention一起使用，因此可以<span class=\"math inline\">\\(M\\)</span>种Attention feature求和得到最终的Attention feature。</p>\r\n<p>根据上面的广义注意力机制表达式，设计一个注意力机制时，需要确定的内容就是一下几点： - <span class=\"math inline\">\\(\\Omega_q\\)</span>，一个query元素可以涉及到的key元素的范围。 - <span class=\"math inline\">\\(A_m\\)</span>，计算Attention weight的函数。 - <span class=\"math inline\">\\(W_m\\)</span>、<span class=\"math inline\">\\(W&#39;_m\\)</span>，是否需要进行特征投影变换。</p>\r\n<h2 id=\"self-attention\">Self-Attention</h2>\r\n<p>Self-Attention是指query元素和key元素来自于同一个集合的Attention，CNN中的Attention基本上都是Self-Attention，例如《Non-local Neural Networks》、《CBAM: Convolutional Block Attention Module》这些论文中使用的Attention，其query和key元素都是特征图上的像素，属于同一个集合，因此都属于Self-Attention的范围。</p>\r\n<p>在论文《An Empirical Study of Spatial Attention Mechanisms in Deep Networks》中提到了几种Self-Attention的计算方式，如下图所示，这里有四种Attention weight的计算方式：</p>\r\n<ul>\r\n<li>query and key content表示仅仅使用<span class=\"math inline\">\\(z_q\\)</span>和<span class=\"math inline\">\\(x_k\\)</span>来计算Attention weight，一般其Attention weight正比于query content和 key content之间的相似性。</li>\r\n<li>query content and relative position表示使用<span class=\"math inline\">\\(z_q\\)</span>和<span class=\"math inline\">\\(k,q\\)</span>来计算Attention weight。</li>\r\n<li>key content only表示仅使用<span class=\"math inline\">\\(x_k\\)</span>来计算Attention weight。</li>\r\n<li>relative position only表示仅使用<span class=\"math inline\">\\(k,q\\)</span>来计算Attention weight。</li>\r\n</ul>\r\n<figure>\r\n<img src=\"Attention计算方式.png\" alt=\"Attention计算方式\" /><figcaption aria-hidden=\"true\">Attention计算方式</figcaption>\r\n</figure>\r\n<h2 id=\"不同的attention机制在广义attention表达式上的表示\">不同的Attention机制在广义Attention表达式上的表示</h2>\r\n<h3 id=\"transformer-attention\">Transformer attention</h3>\r\n<p>在论文《Transformer-xl: Attentive lan-guage models beyond a fixed-length context》中使用了4个Attention Head，包含了上面提到的Self-Attention的四种计算方式，论文《An Empirical Study of Spatial Attention Mechanisms in Deep Networks》中对其进行了总结，如下：</p>\r\n<ul>\r\n<li>query and key content方式计算得到的Attention weight可以表示为<span class=\"math inline\">\\(z_q^TU^T_mV^C_mx_k\\)</span>，其中<span class=\"math inline\">\\(U_m\\)</span>和<span class=\"math inline\">\\(V^C_m\\)</span>表示可学习的嵌入矩阵，用于将query content和key content转换到合适的空间方便计算，或者缩减维度以减少计算量。</li>\r\n<li>query content and relative position方式计算得到的Attention weight可以表示为<span class=\"math inline\">\\(z_q^TU^T_mV^R_mR_{k-q}\\)</span>，这里的<span class=\"math inline\">\\(R_{k-q}\\)</span>表示相对位置<span class=\"math inline\">\\(k-q\\)</span>的一种编码，<span class=\"math inline\">\\(V^R_m\\)</span>同样是一个可学习的嵌入矩阵。</li>\r\n<li>key content only方式计算得到的Attention weight可以表示为<span class=\"math inline\">\\(u_m^TV_m^Cx_k\\)</span>，其中<span class=\"math inline\">\\(u_m\\)</span>是个可学习的向量，用于捕获对当前任务贡献突出的key content的编码。</li>\r\n<li>relative position only方式计算得到的Attention weight可以表示为<span class=\"math inline\">\\(v^T_mV^R_mR_{k-q}\\)</span>，这里的<span class=\"math inline\">\\(v_m\\)</span>也是个可学习的向量，用于key元素和query元素的全局位置偏差。</li>\r\n</ul>\r\n<h3 id=\"regular-and-deformable-convolution\">Regular and deformable convolution</h3>\r\n<p>常规卷积和可变形卷积都可以看做一种注意力机制。</p>\r\n<p>在常规卷积中，Attention weight的计算函数<span class=\"math inline\">\\(A_m\\)</span>属于上面提到的relative position only方式，可以表达为如下形式，这里的<span class=\"math inline\">\\(Q\\)</span>表示卷积核覆盖的区域。这样的<span class=\"math inline\">\\(A_m\\)</span>实际上是一种基于query和key元素相对位置的函数，计算Attention weight的过程不需要<span class=\"math inline\">\\(z_q\\)</span>和<span class=\"math inline\">\\(x_k\\)</span>的值，计算出来的Attention weight只是指定哪些位置和query相关。</p>\r\n<p><span class=\"math display\">\\[\r\nA_m(k, q) = \\begin{cases}\r\n    1 &amp; if \\ k \\in Q\\\\\r\n    0 &amp; otherwise\r\n\\end{cases}\r\n\\]</span></p>\r\n<p>另外，常规卷积相当于只有一个Attention head，因此<span class=\"math inline\">\\(M=1\\)</span>，<span class=\"math inline\">\\(\\Omega_q\\)</span>可以是整个特征图上的所有像素，的卷积核参数可以看成是<span class=\"math inline\">\\(W_0\\)</span>，常规卷积没有<span class=\"math inline\">\\(W&#39;_0\\)</span>，key元素则是输入的特征图上的像素点<span class=\"math inline\">\\(x_k\\)</span>，因此常规卷积可以看做Attention的一种特例。</p>\r\n<p>对于可变形卷积，其<span class=\"math inline\">\\(A_m\\)</span>函数可以表达如下，其中<span class=\"math inline\">\\(w_m^T\\)</span>表示用于计算偏移的<span class=\"math inline\">\\(1\\times 1\\)</span>卷积的参数，<span class=\"math inline\">\\(p_m\\)</span>表示正常卷积相对于卷积核中心的偏移，<span class=\"math inline\">\\(q + p_m + w_m^T x_q\\)</span>实际上就是可变形卷积首先计算得到的卷积核偏移值，而<span class=\"math inline\">\\(G\\)</span>是插值函数，表示在插值过程中，对于需要插值的位置<span class=\"math inline\">\\(q + p_m + w_m^T x_q\\)</span>计算得到的位置<span class=\"math inline\">\\(k\\)</span>的权重，因此可变形卷积的插值权重可以理解为Attention weight，所以可变形卷积核常规卷积一样，也可以看做是一种Attention。</p>\r\n<p><span class=\"math display\">\\[\r\nA_m(q,k,x_q) = G(k, q + p_m + w_m^T x_q)\r\n\\]</span></p>\r\n<h1 id=\"cnn中的一些attention\">CNN中的一些Attention</h1>\r\n<h2 id=\"non-local-neural-networks\">Non-local Neural Networks</h2>\r\n<p>在论文《Non-local Neural Networks》中，提出了一种称为Non-local Neural Networks的结构，通过将某一位置的响应计算为所有位置特征的加权和的方式，用于捕获远程依赖，其表达式如下，其中<span class=\"math inline\">\\(i\\)</span>是响应的位置，<span class=\"math inline\">\\(j\\)</span>则遍历所有输入的位置，（这里的位置可以包含空间和时间，因此Non-local Neural Networks可以用于处理视频），<span class=\"math inline\">\\(x_i\\)</span>表示响应位置的输入值，<span class=\"math inline\">\\(x_j\\)</span>则可以遍历所有输入元素的值，<span class=\"math inline\">\\(f(x_i, x_j)\\)</span>计算得到一个标量，用于表示<span class=\"math inline\">\\(x_i, x_j\\)</span>之间的关系，例如相关程度，<span class=\"math inline\">\\(g(x_j)\\)</span>则表示对<span class=\"math inline\">\\(x_j\\)</span>的一种表示，例如一种投影，<span class=\"math inline\">\\(C(x)\\)</span>表示归一化系数，<span class=\"math inline\">\\(y_i\\)</span>是最终得到的输出。</p>\r\n<p><span class=\"math display\">\\[\r\ny_i = \\frac{1}{C(x)}\\sum\\limits_{\\forall j}f(x_i, x_j)g(x_j)\r\n\\]</span></p>\r\n<p>Non-local Neural Networks的结构示意图如下所示，其中<span class=\"math inline\">\\(H,W\\)</span>表示空间上的范围，<span class=\"math inline\">\\(T\\)</span>表示时间上的范围，这里的示例输入形状是<span class=\"math inline\">\\(T\\times H \\times W \\times 1024\\)</span>，如果是视频处理的话，即表示一共处理相邻的<span class=\"math inline\">\\(T\\)</span>帧特征图，每幅特征图高<span class=\"math inline\">\\(H\\)</span>，宽<span class=\"math inline\">\\(W\\)</span>，channel个数为1024，上面表达式中的<span class=\"math inline\">\\(f(x_i, x_j)\\)</span>由下图中的<span class=\"math inline\">\\(\\theta(x) \\times \\phi(x)\\)</span>来完成，归一化则由softmax操作来完成，softmax的结果和<span class=\"math inline\">\\(g(x)\\)</span>的结果相乘，就完成了“所有位置特征的加权和操作”，得到响应<span class=\"math inline\">\\(y\\)</span>，表示捕获到的远程依赖，这样一来，<span class=\"math inline\">\\(y\\)</span>中每个特征值都聚合了全局的信息，再进行一些处理转换，最终和原始输入相加得到输出Z。</p>\r\n<figure>\r\n<img src=\"Non_local_NN.png\" alt=\"Non-local Neural Networks 结构示意图\" /><figcaption aria-hidden=\"true\">Non-local Neural Networks 结构示意图</figcaption>\r\n</figure>\r\n<p>上图中的<span class=\"math inline\">\\(y\\)</span>最终还经过了一次<span class=\"math inline\">\\(1\\times 1\\times 1\\)</span>卷积得到<span class=\"math inline\">\\(z\\)</span>，因此可以表达为：<span class=\"math inline\">\\(z_i = W_z y_i + x_i\\)</span>，这里论文中将<span class=\"math inline\">\\(W_z\\)</span>初始化为0，且加上<span class=\"math inline\">\\(x_i\\)</span>这一项，其原因是为了在改变网络结构之后，还可以利用之前的预训练权重。</p>\r\n<p>整个Non-local Neural Networks过程虽然简单，但是也是Self-Attention的一种特例，类比之前提到的广义注意力机制表达式，这里只有一个Attention Head，<span class=\"math inline\">\\(M=1\\)</span>，<span class=\"math inline\">\\(i\\)</span>可以看做query元素的位置，<span class=\"math inline\">\\(j\\)</span>则是key元素的位置，query元素和key元素都属于同一个集合（即输入x），<span class=\"math inline\">\\(x_i\\)</span>表示query content，<span class=\"math inline\">\\(x_j\\)</span>则表示key content，<span class=\"math inline\">\\(f(x_i, x_j)\\)</span>则是query and key content方式的<span class=\"math inline\">\\(A_m\\)</span>函数（<span class=\"math inline\">\\(A_m\\)</span>计算得到的Attention weight是归一化的，因此<span class=\"math inline\">\\(A_m\\)</span>还包括<span class=\"math inline\">\\(\\frac{1}{C(x)}\\)</span>用于归一化）。<span class=\"math inline\">\\(g(x_j)\\)</span>是对key content的一种变换，其参数就是前面的<span class=\"math inline\">\\(W&#39;_m\\)</span>，最后这里没有<span class=\"math inline\">\\(W_m\\)</span>，softmax的结果和<span class=\"math inline\">\\(g(x)\\)</span>的结果进行的矩阵乘法完成了特征加权和的操作。</p>\r\n<h2 id=\"squeeze-and-excitation-networksse-network\">Squeeze-and-Excitation Networks（SE Network）</h2>\r\n<p>在论文《Squeeze-and-Excitation Networks》中，提出了一种用于channel attention的Squeeze-and-Excitation（SE）结构，通过显式地建模通道之间的相互依赖关系，自适应地重新修正不同通道的特征响应幅度，其结构示意图如下所示。其中<span class=\"math inline\">\\(F_{sq}\\)</span>表示Squeeze操作，在论文中使用一个全局平均池化来实现，用于在空间维度上来聚合全局的信息，变成一种逐通道的表示状态。之后的<span class=\"math inline\">\\(F_{ex}\\)</span>表示Excitation操作，其操作顺序为（全连接-ReLU-全连接-Sigmoid，这里没看懂可以参考下面在InceptionNet和ResNet中的详细结构），用于捕获通道之间的依赖性关系，并且形成通道的权重。最终将得到的各通道权重分别乘以原来的通道，达到对不同通道的响应幅度进行自适应修正的目的。</p>\r\n<figure>\r\n<img src=\"Squeeze_and_Excitation.png\" alt=\"Squeeze-and-Excitation Networks 结构示意图\" /><figcaption aria-hidden=\"true\">Squeeze-and-Excitation Networks 结构示意图</figcaption>\r\n</figure>\r\n<p>论文中在InceptionNet以及ResNet中都尝试加入SE模块，其加入之后的结构如下图所示。其中Global pooling即Squeeze操作，用于聚合超出像素点感受野之外的信息并形成逐通道的表示，后面紧接着Excitation操作，首先通过全连接对其进行降维，减少计算量，之后使用ReLU激活函数来增加Excitation操作的非线性性质，之后再通过一个全连接层得到逐通道的channel weight，并通过Sigmoid函数将channel weight限制在0到1之间，最后将channel weight与原始特征图相乘，每个通道一个权重，用于调整不同通道的响应幅值。</p>\r\n<figure>\r\n<img src=\"SEInceptionNet_SEResNet.png\" alt=\"InceptionNet以及ResNet中的SE结构\" /><figcaption aria-hidden=\"true\">InceptionNet以及ResNet中的SE结构</figcaption>\r\n</figure>\r\n<p>Excitation操作很容易和之前的广义注意力机制表达式类比，其本质上就是一种简单的Self-Attention，只不过Excitation操作计算出的Attention weight除了query位置以外全是0，这样就不是按照权重去聚合所有通道的信息，而是使用空间维度的全局信息来调整通道的响应幅值。</p>\r\n<h2 id=\"convolutional-block-attention-module\">Convolutional Block Attention Module</h2>\r\n<p>论文《CBAM: Convolutional Block Attention Module》中提出了一种同时进行channel和spatial attention的attention模块，称为CBAM。</p>\r\n<p>CBAM中包含了两种Attention，如下图所示，上面的结构是channel Attention部分，其接收输入之后分别通过空间维度上的全局最大池化和全局平均池化，和SE模块中的Squeeze类似，只不过这里使用两种池化，得到了两个结果，这两个结果再送到同一个MLP（多层感知机），这里相当于两个共享权重的Excitation操作，最后两个结果相加在进行sigmoid操作，这里可以看出来，整个Channel Attention过程其实就是稍微改变的SE模块。图片下面的结构是Spatial Attention的结构，这里的最大池化和平均池化是在Channel维度进行的，即对每个像素求出所有Channel的最大值或者平均值，最终得到两个Channel数量为1的特征图，两个特征图进行Concatenate之后经过一个<span class=\"math inline\">\\(7\\times7\\)</span>卷积，最后使用sigmoid函数处理得到结果，这两个模块得到的结果都是用于和原始特征图相乘，一个是逐通道的调整特征响应的幅值，一个是逐像素的调整特征响应的幅值。</p>\r\n<figure>\r\n<img src=\"CBAM_detail.png\" alt=\"CBAM中的两种Attention结构\" /><figcaption aria-hidden=\"true\">CBAM中的两种Attention结构</figcaption>\r\n</figure>\r\n<p>上面展示的两种结构在模型中先后使用，可以使得模型包含对不同channel响应幅度的调整能力的同时也包含对不同位置响应幅度的调整能力，在ResBlock中添加CBAM结构之后，模型的结构如下图所示。</p>\r\n<figure>\r\n<img src=\"Resnet_CBAM.png\" alt=\"ResBlock中使用CBAM结构\" /><figcaption aria-hidden=\"true\">ResBlock中使用CBAM结构</figcaption>\r\n</figure>\r\n<p>在该论文的实验中，CBAM模块在图像分类任务上提升不大，但是使用轻量级模型（MobileNet，ShuffleNet）试验时错误率相对于SE模块有一个百分点左右的下降（可能是参数增加，模型容量增大导致？），在目标检测任务上，ResNet+CBAM相比于原始的ResNet有一点几个百分点的mAP提升。</p>\r\n"},{"title":"refineDet模型结构要点","date":"2019-04-15T01:21:39.000Z","_content":"因为需要实现keras版本的refineDet模型，因此考虑从keras-retinanet代码上进行修改，这里记录refineDet模型结构中的一些关键实现方式。\n\n- 为了获取更高层的信息，在backbone网络中，加深了一层，对于ResNet-101，添加了一个额外的residual block.\n- 在anchor大小选择方面，根据《S3FD: Single Shot Scale-invariant Face Detector》、《Understanding the Effective Receptive Field inDeep Convolutional Neural Networks》这两篇论文，anchor大小的选择应该要比理论感受小很多，使用有效感受野来估计每一个检测层的anchor大小，大约使用检测层stride的4倍作为anchor大小，并在anchor基础大小之上使用三种比例0.5,1.0,2.0\n- anchor匹配时，首先对于每个ground truth，分别匹配一个jaccard overlap最大的anchor，之后再将剩下的每一个anchor分配给jaccard overlap大于0.5的ground truth。\n- 计算loss时，对anchor进行hard negative mining，保证negative anchor和positive anchor的比例是3：1。","source":"_posts/学习笔记/refineDet模型结构要点.md","raw":"---\ntitle: refineDet模型结构要点\ndate: 2019-04-15 09:21:39\ntags: 深度学习\ncategories: 工程实践\n---\n因为需要实现keras版本的refineDet模型，因此考虑从keras-retinanet代码上进行修改，这里记录refineDet模型结构中的一些关键实现方式。\n\n- 为了获取更高层的信息，在backbone网络中，加深了一层，对于ResNet-101，添加了一个额外的residual block.\n- 在anchor大小选择方面，根据《S3FD: Single Shot Scale-invariant Face Detector》、《Understanding the Effective Receptive Field inDeep Convolutional Neural Networks》这两篇论文，anchor大小的选择应该要比理论感受小很多，使用有效感受野来估计每一个检测层的anchor大小，大约使用检测层stride的4倍作为anchor大小，并在anchor基础大小之上使用三种比例0.5,1.0,2.0\n- anchor匹配时，首先对于每个ground truth，分别匹配一个jaccard overlap最大的anchor，之后再将剩下的每一个anchor分配给jaccard overlap大于0.5的ground truth。\n- 计算loss时，对anchor进行hard negative mining，保证negative anchor和positive anchor的比例是3：1。","slug":"学习笔记/refineDet模型结构要点","published":1,"updated":"2020-08-31T06:39:20.760Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckjxpc3rp000k44mq9sdrdxmp","content":"<p>因为需要实现keras版本的refineDet模型，因此考虑从keras-retinanet代码上进行修改，这里记录refineDet模型结构中的一些关键实现方式。</p>\r\n<ul>\r\n<li>为了获取更高层的信息，在backbone网络中，加深了一层，对于ResNet-101，添加了一个额外的residual block.</li>\r\n<li>在anchor大小选择方面，根据《S3FD: Single Shot Scale-invariant Face Detector》、《Understanding the Effective Receptive Field inDeep Convolutional Neural Networks》这两篇论文，anchor大小的选择应该要比理论感受小很多，使用有效感受野来估计每一个检测层的anchor大小，大约使用检测层stride的4倍作为anchor大小，并在anchor基础大小之上使用三种比例0.5,1.0,2.0</li>\r\n<li>anchor匹配时，首先对于每个ground truth，分别匹配一个jaccard overlap最大的anchor，之后再将剩下的每一个anchor分配给jaccard overlap大于0.5的ground truth。</li>\r\n<li>计算loss时，对anchor进行hard negative mining，保证negative anchor和positive anchor的比例是3：1。</li>\r\n</ul>\r\n","site":{"data":{}},"excerpt":"","more":"<p>因为需要实现keras版本的refineDet模型，因此考虑从keras-retinanet代码上进行修改，这里记录refineDet模型结构中的一些关键实现方式。</p>\r\n<ul>\r\n<li>为了获取更高层的信息，在backbone网络中，加深了一层，对于ResNet-101，添加了一个额外的residual block.</li>\r\n<li>在anchor大小选择方面，根据《S3FD: Single Shot Scale-invariant Face Detector》、《Understanding the Effective Receptive Field inDeep Convolutional Neural Networks》这两篇论文，anchor大小的选择应该要比理论感受小很多，使用有效感受野来估计每一个检测层的anchor大小，大约使用检测层stride的4倍作为anchor大小，并在anchor基础大小之上使用三种比例0.5,1.0,2.0</li>\r\n<li>anchor匹配时，首先对于每个ground truth，分别匹配一个jaccard overlap最大的anchor，之后再将剩下的每一个anchor分配给jaccard overlap大于0.5的ground truth。</li>\r\n<li>计算loss时，对anchor进行hard negative mining，保证negative anchor和positive anchor的比例是3：1。</li>\r\n</ul>\r\n"},{"title":"《DeepLearning》读书笔记","date":"2020-05-02T09:59:47.000Z","mathjax":true,"_content":"\n\n","source":"_posts/学习笔记/《DeepLearning》读书笔记.md","raw":"---\ntitle: 《DeepLearning》读书笔记\ndate: 2020-05-02 17:59:47\ntags: [深度学习, 读书笔记, 杂项]\nmathjax: true\n---\n\n\n","slug":"学习笔记/《DeepLearning》读书笔记","published":1,"updated":"2020-08-31T06:39:20.761Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckjxpc3rq000n44mq895uc74p","content":"\r\n","site":{"data":{}},"excerpt":"","more":"\r\n"},{"title":"决策树总结","date":"2020-05-29T08:57:57.000Z","mathjax":true,"_content":"\n# 决策树基础知识\n\n## 信息熵\n如果某个事件，有$C$种可能，其中第$k$中可能状态发生概率为$p_k$，那么该事件的信息熵定义为$Ent(D) = -\\sum\\limits_{k=1}^C p_k\\log_2p_k$，表示的是所有状态的信息的期望，以2为底数这样的定义的信息熵单位是bit。\n\n在决策树方法中，也类似的定义信息熵：样本集合$D$中第$k$类样本所占比例为$p_k$，则信息熵定义为$Ent(D) = -\\sum\\limits_{k=1}^C p_k\\log_2p_k$，其中$C$为类别个数，其刻画了样本集合的纯度。\n\n## 信息增益\n假设离散属性$a$有$v$个取值：$a_1, a_2, ..., a_v$，可以将当前数据集合分成$V$个子集：$D^1, D^2, ..., D^V$，那么使用属性$a$划分样本集的信息增益定义为$Gain(D,a)=Ent(D)-\\sum\\limits_{v=1}^V\\frac{|D^v|}{|D|}Ent(D^v)$\n\n## 增益率\n信息增益的定义导致其对数量较多的$D^v$更加敏感，因此又提出了信息增益率的概念：$Gain\\_ratio(D,a)=\\frac{Gain(D,a)}{IV(a)}$，其中，$IV(a)=-\\sum\\limits_{v=1}^V \\frac{|D^v|}{|D|} \\log_2\\frac{|D^v|}{|D|}$称为属性$a$的固有值（或者叫内在价值）。\n\n## 基尼指数\n基尼值定义为$Gini(D) = \\sum\\limits_{k=1}^C\\sum\\limits_{k' \\ne k}p_k p_{k'} = 1-\\sum\\limits_{k=1}^Cp_k^2$，其反映了在$D$中随机抽取两个样本，不属于同一类别的概率。\n和信息熵增益类似，对于在属性$a$上划分出$V$个区间的操作，定义划分后的基尼指数为$Gini\\_index(D, a) = \\sum\\limits_{v=1}^V\\frac{|D^v|}{|D|}Gini(D^v)$，构造决策树时的最优划分属性选择也可以使用能够使基尼指数最小的属性。\n\n# 决策树的构造算法\n## ID3算法\n每次选择一个信息增益最大的属性$a$构造节点，这个节点将数据划分为$V$个子集，然后在每个子集中进行同样的操作（不能选择已经使用过的属性），若无属性可用或者当前子集类别相同，则当前子集标记为类别数量最多的类别，停止继续划分。\n\n## C4.5算法\n由于ID3算法在选择属性时使用信息增益最大值来选取，因此这样构造的决策树偏向于先选择特征取值更多的属性，另外ID3算法还有一个问题是处理连续值属性，这两个问题在C4.5算法中都给出了解决方案。\n\nC4.5算法则是对ID3算法的一个改进版本，其中使用增益率来进行属性选择。\n\n### 对连续值属性的处理\n在遇到连续值时，如果连续属性$a$在$D$中出现$n$个取值，则将其从小到大排序为$\\begin{bmatrix}a_1, a_2, ... a_n\\end{bmatrix}$，这样产生$n-1$个离散值$T_a = \\{\\frac{a_i + a_{i+1}}{2}|1 \\le i \\le n-1\\}$则$Gain(D,a)= \\max\\limits_{t \\in T_a}Gain(D, a, t)$，其中$Gain(D, a, t)$表示将$a$属性使用$t$划分为两部分，这样连续值的决策树构造和离散值类似了，不过连续值属性在构造决策树的过程中可以使用多次。\n\n## 分类回归树(Classification And Regression Tree, CART)算法\n对于分类树，CART算法使用基尼指数最小化准则进行特征选择，CART的主要思想是每次将数据划分成两个单元，例如，对于当前的数据集合$D$，根据属性$a$上的取值$s$数据划分成$D^1$和$D^2$两个子集（对于连续变量，划分数据就用大于和小于关系，对于离散变量就是用等于或不等于关系），对于这样的划分，可以得到基尼指数$Gini\\_index(D, a, s) = \\frac{|D^1|}{|D|}Gini(D^1) + \\frac{|D^2|}{|D|}Gini(D^2)$，每次划分都选择使得$Gini\\_index(D, a, s)$最小的$a$和$s$。如此循环，即得到了CART分类树。\n\n不同于ID3和C4.5算法，CART算法还能够处理回归树。对于回归树，其样本标签是连续值，回归树的一般形式表示如下，其中$H(x)$表示回归树的预测函数，$\\mathbf{1}$表示指示函数，其基本思想是根据数据的属性，将数据划分成$M$个部分$R_m, m=1,2,...,M$，每个部分对应一个输出值。\n$$\nH(x) = \\sum\\limits_{i=1}^M c_i \\mathbf{1}(X \\in R_i)\n$$\nCART回归树使用平方误差作为每个部分的损失函数，每次利用属性$a$上的取值$s$将数据划分成两个部分：$R^1(a, s)$和$R^2(a, s)$，按照下式的规则来选择每次划分的$a$和$s$：\n$$\n\\min\\limits_{a, s}[\\min\\limits_{c_1} \\sum\\limits_{x_i \\in R^1(a,s)} (y_i - c_1)^2 + \\min\\limits_{c_2} \\sum\\limits_{x_j \\in R^2(a,s)} (y_j - c_2)^2]\n$$\n其中c的值非常好计算，直接使用该数据子集对应标签的平均数即可。\n\n## 属性缺失的处理\n令$\\tilde{D}$是所有没有缺失属性a的样本集合，对于样本$x$，有样本权重$w_x$，定义如下参数。\n$$\n\\rho = \\frac{\\sum_{x\\in \\tilde{D}} w_x}{\\sum_{x\\in D}w_x}\\\\\n\\tilde{p}_k = \\frac{\\sum_{x\\in \\tilde{D}_k w_x}}{\\sum_{x\\in \\tilde{D}}w_x}, (1\\le k \\le C)\\\\\n\\tilde{r}_v = \\frac{\\sum_{x\\in \\tilde{D}^v}w_x}{\\sum_{x \\in \\tilde{D}} w_x}, (1 \\le v \\le V)\n$$\n显然，$\\rho$表示属性无缺失样本所占比例，$\\tilde{p}_k$表示属性无缺失样本中第$k$类所占比例，$\\tilde{r}_v$表示属性无缺失样本中在属性$a$上取值$a^v$的样本比例。\n\n由此推广信息增益为：\n$$\n\\begin{aligned}\nGain(D, a) &= \\rho \\times Gain(\\tilde{D}, a)\\\\\n&=\\rho \\times (Ent(\\tilde{D}) - \\sum\\limits_{v=1}^V \\tilde{r}_v Ent(\\tilde{D}^v))\n\\end{aligned}\n$$\n其中：\n$$\nEnt(\\tilde{D}) = -\\sum\\limits_{k=1}^C \\tilde{p}_k log_2 \\tilde{p}_k\n$$\n这样解决了最优划分的属性选择问题，在构造子树时，如果样本$x$在属性$a$上的取值已知，那么$x$划分到相应子节点，且权重保持为$w_x$，如果属性$a$未知，则将$s$划分入所有的子节点，且权重调整为$\\tilde{r}_v w_x$。\n\n## 多变量决策树\n叶节点不再针对某个属性，而是针对属性的线性组合进行划分。\n\n# 决策树的剪枝\n\n## 预剪枝\n在生成决策树的过程中，利用验证集验证当前划分是否能提升泛化效果，如果不能，则停止继续划分，直接将当前样本集作为决策树叶节点，并指定数量最多的类别做为叶节点类别。预剪枝训练时间短，但是由于其是贪心策略，容易引起欠拟合。\n\n## 后剪枝\n在决策树生成完成之后，自底向上的对非叶节点进行考察，观察将该节点直接指定为叶节点是否能提升验证集泛化效果，如果能则将此节点的子树一起合并为一个叶节点。后剪枝的训练时间比预剪枝长很多，但是其欠拟合风险小，而且泛化性能往往优于预剪枝。\n","source":"_posts/学习笔记/决策树总结.md","raw":"---\ntitle: 决策树总结\ndate: 2020-05-29 16:57:57\ntags: [机器学习]\nmathjax: true\n---\n\n# 决策树基础知识\n\n## 信息熵\n如果某个事件，有$C$种可能，其中第$k$中可能状态发生概率为$p_k$，那么该事件的信息熵定义为$Ent(D) = -\\sum\\limits_{k=1}^C p_k\\log_2p_k$，表示的是所有状态的信息的期望，以2为底数这样的定义的信息熵单位是bit。\n\n在决策树方法中，也类似的定义信息熵：样本集合$D$中第$k$类样本所占比例为$p_k$，则信息熵定义为$Ent(D) = -\\sum\\limits_{k=1}^C p_k\\log_2p_k$，其中$C$为类别个数，其刻画了样本集合的纯度。\n\n## 信息增益\n假设离散属性$a$有$v$个取值：$a_1, a_2, ..., a_v$，可以将当前数据集合分成$V$个子集：$D^1, D^2, ..., D^V$，那么使用属性$a$划分样本集的信息增益定义为$Gain(D,a)=Ent(D)-\\sum\\limits_{v=1}^V\\frac{|D^v|}{|D|}Ent(D^v)$\n\n## 增益率\n信息增益的定义导致其对数量较多的$D^v$更加敏感，因此又提出了信息增益率的概念：$Gain\\_ratio(D,a)=\\frac{Gain(D,a)}{IV(a)}$，其中，$IV(a)=-\\sum\\limits_{v=1}^V \\frac{|D^v|}{|D|} \\log_2\\frac{|D^v|}{|D|}$称为属性$a$的固有值（或者叫内在价值）。\n\n## 基尼指数\n基尼值定义为$Gini(D) = \\sum\\limits_{k=1}^C\\sum\\limits_{k' \\ne k}p_k p_{k'} = 1-\\sum\\limits_{k=1}^Cp_k^2$，其反映了在$D$中随机抽取两个样本，不属于同一类别的概率。\n和信息熵增益类似，对于在属性$a$上划分出$V$个区间的操作，定义划分后的基尼指数为$Gini\\_index(D, a) = \\sum\\limits_{v=1}^V\\frac{|D^v|}{|D|}Gini(D^v)$，构造决策树时的最优划分属性选择也可以使用能够使基尼指数最小的属性。\n\n# 决策树的构造算法\n## ID3算法\n每次选择一个信息增益最大的属性$a$构造节点，这个节点将数据划分为$V$个子集，然后在每个子集中进行同样的操作（不能选择已经使用过的属性），若无属性可用或者当前子集类别相同，则当前子集标记为类别数量最多的类别，停止继续划分。\n\n## C4.5算法\n由于ID3算法在选择属性时使用信息增益最大值来选取，因此这样构造的决策树偏向于先选择特征取值更多的属性，另外ID3算法还有一个问题是处理连续值属性，这两个问题在C4.5算法中都给出了解决方案。\n\nC4.5算法则是对ID3算法的一个改进版本，其中使用增益率来进行属性选择。\n\n### 对连续值属性的处理\n在遇到连续值时，如果连续属性$a$在$D$中出现$n$个取值，则将其从小到大排序为$\\begin{bmatrix}a_1, a_2, ... a_n\\end{bmatrix}$，这样产生$n-1$个离散值$T_a = \\{\\frac{a_i + a_{i+1}}{2}|1 \\le i \\le n-1\\}$则$Gain(D,a)= \\max\\limits_{t \\in T_a}Gain(D, a, t)$，其中$Gain(D, a, t)$表示将$a$属性使用$t$划分为两部分，这样连续值的决策树构造和离散值类似了，不过连续值属性在构造决策树的过程中可以使用多次。\n\n## 分类回归树(Classification And Regression Tree, CART)算法\n对于分类树，CART算法使用基尼指数最小化准则进行特征选择，CART的主要思想是每次将数据划分成两个单元，例如，对于当前的数据集合$D$，根据属性$a$上的取值$s$数据划分成$D^1$和$D^2$两个子集（对于连续变量，划分数据就用大于和小于关系，对于离散变量就是用等于或不等于关系），对于这样的划分，可以得到基尼指数$Gini\\_index(D, a, s) = \\frac{|D^1|}{|D|}Gini(D^1) + \\frac{|D^2|}{|D|}Gini(D^2)$，每次划分都选择使得$Gini\\_index(D, a, s)$最小的$a$和$s$。如此循环，即得到了CART分类树。\n\n不同于ID3和C4.5算法，CART算法还能够处理回归树。对于回归树，其样本标签是连续值，回归树的一般形式表示如下，其中$H(x)$表示回归树的预测函数，$\\mathbf{1}$表示指示函数，其基本思想是根据数据的属性，将数据划分成$M$个部分$R_m, m=1,2,...,M$，每个部分对应一个输出值。\n$$\nH(x) = \\sum\\limits_{i=1}^M c_i \\mathbf{1}(X \\in R_i)\n$$\nCART回归树使用平方误差作为每个部分的损失函数，每次利用属性$a$上的取值$s$将数据划分成两个部分：$R^1(a, s)$和$R^2(a, s)$，按照下式的规则来选择每次划分的$a$和$s$：\n$$\n\\min\\limits_{a, s}[\\min\\limits_{c_1} \\sum\\limits_{x_i \\in R^1(a,s)} (y_i - c_1)^2 + \\min\\limits_{c_2} \\sum\\limits_{x_j \\in R^2(a,s)} (y_j - c_2)^2]\n$$\n其中c的值非常好计算，直接使用该数据子集对应标签的平均数即可。\n\n## 属性缺失的处理\n令$\\tilde{D}$是所有没有缺失属性a的样本集合，对于样本$x$，有样本权重$w_x$，定义如下参数。\n$$\n\\rho = \\frac{\\sum_{x\\in \\tilde{D}} w_x}{\\sum_{x\\in D}w_x}\\\\\n\\tilde{p}_k = \\frac{\\sum_{x\\in \\tilde{D}_k w_x}}{\\sum_{x\\in \\tilde{D}}w_x}, (1\\le k \\le C)\\\\\n\\tilde{r}_v = \\frac{\\sum_{x\\in \\tilde{D}^v}w_x}{\\sum_{x \\in \\tilde{D}} w_x}, (1 \\le v \\le V)\n$$\n显然，$\\rho$表示属性无缺失样本所占比例，$\\tilde{p}_k$表示属性无缺失样本中第$k$类所占比例，$\\tilde{r}_v$表示属性无缺失样本中在属性$a$上取值$a^v$的样本比例。\n\n由此推广信息增益为：\n$$\n\\begin{aligned}\nGain(D, a) &= \\rho \\times Gain(\\tilde{D}, a)\\\\\n&=\\rho \\times (Ent(\\tilde{D}) - \\sum\\limits_{v=1}^V \\tilde{r}_v Ent(\\tilde{D}^v))\n\\end{aligned}\n$$\n其中：\n$$\nEnt(\\tilde{D}) = -\\sum\\limits_{k=1}^C \\tilde{p}_k log_2 \\tilde{p}_k\n$$\n这样解决了最优划分的属性选择问题，在构造子树时，如果样本$x$在属性$a$上的取值已知，那么$x$划分到相应子节点，且权重保持为$w_x$，如果属性$a$未知，则将$s$划分入所有的子节点，且权重调整为$\\tilde{r}_v w_x$。\n\n## 多变量决策树\n叶节点不再针对某个属性，而是针对属性的线性组合进行划分。\n\n# 决策树的剪枝\n\n## 预剪枝\n在生成决策树的过程中，利用验证集验证当前划分是否能提升泛化效果，如果不能，则停止继续划分，直接将当前样本集作为决策树叶节点，并指定数量最多的类别做为叶节点类别。预剪枝训练时间短，但是由于其是贪心策略，容易引起欠拟合。\n\n## 后剪枝\n在决策树生成完成之后，自底向上的对非叶节点进行考察，观察将该节点直接指定为叶节点是否能提升验证集泛化效果，如果能则将此节点的子树一起合并为一个叶节点。后剪枝的训练时间比预剪枝长很多，但是其欠拟合风险小，而且泛化性能往往优于预剪枝。\n","slug":"学习笔记/决策树总结","published":1,"updated":"2020-08-31T06:39:20.762Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckjxpc3rs000q44mqc1wq6akq","content":"<h1 id=\"决策树基础知识\">决策树基础知识</h1>\r\n<h2 id=\"信息熵\">信息熵</h2>\r\n<p>如果某个事件，有<span class=\"math inline\">\\(C\\)</span>种可能，其中第<span class=\"math inline\">\\(k\\)</span>中可能状态发生概率为<span class=\"math inline\">\\(p_k\\)</span>，那么该事件的信息熵定义为<span class=\"math inline\">\\(Ent(D) = -\\sum\\limits_{k=1}^C p_k\\log_2p_k\\)</span>，表示的是所有状态的信息的期望，以2为底数这样的定义的信息熵单位是bit。</p>\r\n<p>在决策树方法中，也类似的定义信息熵：样本集合<span class=\"math inline\">\\(D\\)</span>中第<span class=\"math inline\">\\(k\\)</span>类样本所占比例为<span class=\"math inline\">\\(p_k\\)</span>，则信息熵定义为<span class=\"math inline\">\\(Ent(D) = -\\sum\\limits_{k=1}^C p_k\\log_2p_k\\)</span>，其中<span class=\"math inline\">\\(C\\)</span>为类别个数，其刻画了样本集合的纯度。</p>\r\n<h2 id=\"信息增益\">信息增益</h2>\r\n<p>假设离散属性<span class=\"math inline\">\\(a\\)</span>有<span class=\"math inline\">\\(v\\)</span>个取值：<span class=\"math inline\">\\(a_1, a_2, ..., a_v\\)</span>，可以将当前数据集合分成<span class=\"math inline\">\\(V\\)</span>个子集：<span class=\"math inline\">\\(D^1, D^2, ..., D^V\\)</span>，那么使用属性<span class=\"math inline\">\\(a\\)</span>划分样本集的信息增益定义为<span class=\"math inline\">\\(Gain(D,a)=Ent(D)-\\sum\\limits_{v=1}^V\\frac{|D^v|}{|D|}Ent(D^v)\\)</span></p>\r\n<h2 id=\"增益率\">增益率</h2>\r\n<p>信息增益的定义导致其对数量较多的<span class=\"math inline\">\\(D^v\\)</span>更加敏感，因此又提出了信息增益率的概念：<span class=\"math inline\">\\(Gain\\_ratio(D,a)=\\frac{Gain(D,a)}{IV(a)}\\)</span>，其中，<span class=\"math inline\">\\(IV(a)=-\\sum\\limits_{v=1}^V \\frac{|D^v|}{|D|} \\log_2\\frac{|D^v|}{|D|}\\)</span>称为属性<span class=\"math inline\">\\(a\\)</span>的固有值（或者叫内在价值）。</p>\r\n<h2 id=\"基尼指数\">基尼指数</h2>\r\n<p>基尼值定义为<span class=\"math inline\">\\(Gini(D) = \\sum\\limits_{k=1}^C\\sum\\limits_{k&#39; \\ne k}p_k p_{k&#39;} = 1-\\sum\\limits_{k=1}^Cp_k^2\\)</span>，其反映了在<span class=\"math inline\">\\(D\\)</span>中随机抽取两个样本，不属于同一类别的概率。 和信息熵增益类似，对于在属性<span class=\"math inline\">\\(a\\)</span>上划分出<span class=\"math inline\">\\(V\\)</span>个区间的操作，定义划分后的基尼指数为<span class=\"math inline\">\\(Gini\\_index(D, a) = \\sum\\limits_{v=1}^V\\frac{|D^v|}{|D|}Gini(D^v)\\)</span>，构造决策树时的最优划分属性选择也可以使用能够使基尼指数最小的属性。</p>\r\n<h1 id=\"决策树的构造算法\">决策树的构造算法</h1>\r\n<h2 id=\"id3算法\">ID3算法</h2>\r\n<p>每次选择一个信息增益最大的属性<span class=\"math inline\">\\(a\\)</span>构造节点，这个节点将数据划分为<span class=\"math inline\">\\(V\\)</span>个子集，然后在每个子集中进行同样的操作（不能选择已经使用过的属性），若无属性可用或者当前子集类别相同，则当前子集标记为类别数量最多的类别，停止继续划分。</p>\r\n<h2 id=\"c4.5算法\">C4.5算法</h2>\r\n<p>由于ID3算法在选择属性时使用信息增益最大值来选取，因此这样构造的决策树偏向于先选择特征取值更多的属性，另外ID3算法还有一个问题是处理连续值属性，这两个问题在C4.5算法中都给出了解决方案。</p>\r\n<p>C4.5算法则是对ID3算法的一个改进版本，其中使用增益率来进行属性选择。</p>\r\n<h3 id=\"对连续值属性的处理\">对连续值属性的处理</h3>\r\n<p>在遇到连续值时，如果连续属性<span class=\"math inline\">\\(a\\)</span>在<span class=\"math inline\">\\(D\\)</span>中出现<span class=\"math inline\">\\(n\\)</span>个取值，则将其从小到大排序为<span class=\"math inline\">\\(\\begin{bmatrix}a_1, a_2, ... a_n\\end{bmatrix}\\)</span>，这样产生<span class=\"math inline\">\\(n-1\\)</span>个离散值<span class=\"math inline\">\\(T_a = \\{\\frac{a_i + a_{i+1}}{2}|1 \\le i \\le n-1\\}\\)</span>则<span class=\"math inline\">\\(Gain(D,a)= \\max\\limits_{t \\in T_a}Gain(D, a, t)\\)</span>，其中<span class=\"math inline\">\\(Gain(D, a, t)\\)</span>表示将<span class=\"math inline\">\\(a\\)</span>属性使用<span class=\"math inline\">\\(t\\)</span>划分为两部分，这样连续值的决策树构造和离散值类似了，不过连续值属性在构造决策树的过程中可以使用多次。</p>\r\n<h2 id=\"分类回归树classification-and-regression-tree-cart算法\">分类回归树(Classification And Regression Tree, CART)算法</h2>\r\n<p>对于分类树，CART算法使用基尼指数最小化准则进行特征选择，CART的主要思想是每次将数据划分成两个单元，例如，对于当前的数据集合<span class=\"math inline\">\\(D\\)</span>，根据属性<span class=\"math inline\">\\(a\\)</span>上的取值<span class=\"math inline\">\\(s\\)</span>数据划分成<span class=\"math inline\">\\(D^1\\)</span>和<span class=\"math inline\">\\(D^2\\)</span>两个子集（对于连续变量，划分数据就用大于和小于关系，对于离散变量就是用等于或不等于关系），对于这样的划分，可以得到基尼指数<span class=\"math inline\">\\(Gini\\_index(D, a, s) = \\frac{|D^1|}{|D|}Gini(D^1) + \\frac{|D^2|}{|D|}Gini(D^2)\\)</span>，每次划分都选择使得<span class=\"math inline\">\\(Gini\\_index(D, a, s)\\)</span>最小的<span class=\"math inline\">\\(a\\)</span>和<span class=\"math inline\">\\(s\\)</span>。如此循环，即得到了CART分类树。</p>\r\n<p>不同于ID3和C4.5算法，CART算法还能够处理回归树。对于回归树，其样本标签是连续值，回归树的一般形式表示如下，其中<span class=\"math inline\">\\(H(x)\\)</span>表示回归树的预测函数，<span class=\"math inline\">\\(\\mathbf{1}\\)</span>表示指示函数，其基本思想是根据数据的属性，将数据划分成<span class=\"math inline\">\\(M\\)</span>个部分<span class=\"math inline\">\\(R_m, m=1,2,...,M\\)</span>，每个部分对应一个输出值。 <span class=\"math display\">\\[\r\nH(x) = \\sum\\limits_{i=1}^M c_i \\mathbf{1}(X \\in R_i)\r\n\\]</span> CART回归树使用平方误差作为每个部分的损失函数，每次利用属性<span class=\"math inline\">\\(a\\)</span>上的取值<span class=\"math inline\">\\(s\\)</span>将数据划分成两个部分：<span class=\"math inline\">\\(R^1(a, s)\\)</span>和<span class=\"math inline\">\\(R^2(a, s)\\)</span>，按照下式的规则来选择每次划分的<span class=\"math inline\">\\(a\\)</span>和<span class=\"math inline\">\\(s\\)</span>： <span class=\"math display\">\\[\r\n\\min\\limits_{a, s}[\\min\\limits_{c_1} \\sum\\limits_{x_i \\in R^1(a,s)} (y_i - c_1)^2 + \\min\\limits_{c_2} \\sum\\limits_{x_j \\in R^2(a,s)} (y_j - c_2)^2]\r\n\\]</span> 其中c的值非常好计算，直接使用该数据子集对应标签的平均数即可。</p>\r\n<h2 id=\"属性缺失的处理\">属性缺失的处理</h2>\r\n<p>令<span class=\"math inline\">\\(\\tilde{D}\\)</span>是所有没有缺失属性a的样本集合，对于样本<span class=\"math inline\">\\(x\\)</span>，有样本权重<span class=\"math inline\">\\(w_x\\)</span>，定义如下参数。 <span class=\"math display\">\\[\r\n\\rho = \\frac{\\sum_{x\\in \\tilde{D}} w_x}{\\sum_{x\\in D}w_x}\\\\\r\n\\tilde{p}_k = \\frac{\\sum_{x\\in \\tilde{D}_k w_x}}{\\sum_{x\\in \\tilde{D}}w_x}, (1\\le k \\le C)\\\\\r\n\\tilde{r}_v = \\frac{\\sum_{x\\in \\tilde{D}^v}w_x}{\\sum_{x \\in \\tilde{D}} w_x}, (1 \\le v \\le V)\r\n\\]</span> 显然，<span class=\"math inline\">\\(\\rho\\)</span>表示属性无缺失样本所占比例，<span class=\"math inline\">\\(\\tilde{p}_k\\)</span>表示属性无缺失样本中第<span class=\"math inline\">\\(k\\)</span>类所占比例，<span class=\"math inline\">\\(\\tilde{r}_v\\)</span>表示属性无缺失样本中在属性<span class=\"math inline\">\\(a\\)</span>上取值<span class=\"math inline\">\\(a^v\\)</span>的样本比例。</p>\r\n<p>由此推广信息增益为： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\nGain(D, a) &amp;= \\rho \\times Gain(\\tilde{D}, a)\\\\\r\n&amp;=\\rho \\times (Ent(\\tilde{D}) - \\sum\\limits_{v=1}^V \\tilde{r}_v Ent(\\tilde{D}^v))\r\n\\end{aligned}\r\n\\]</span> 其中： <span class=\"math display\">\\[\r\nEnt(\\tilde{D}) = -\\sum\\limits_{k=1}^C \\tilde{p}_k log_2 \\tilde{p}_k\r\n\\]</span> 这样解决了最优划分的属性选择问题，在构造子树时，如果样本<span class=\"math inline\">\\(x\\)</span>在属性<span class=\"math inline\">\\(a\\)</span>上的取值已知，那么<span class=\"math inline\">\\(x\\)</span>划分到相应子节点，且权重保持为<span class=\"math inline\">\\(w_x\\)</span>，如果属性<span class=\"math inline\">\\(a\\)</span>未知，则将<span class=\"math inline\">\\(s\\)</span>划分入所有的子节点，且权重调整为<span class=\"math inline\">\\(\\tilde{r}_v w_x\\)</span>。</p>\r\n<h2 id=\"多变量决策树\">多变量决策树</h2>\r\n<p>叶节点不再针对某个属性，而是针对属性的线性组合进行划分。</p>\r\n<h1 id=\"决策树的剪枝\">决策树的剪枝</h1>\r\n<h2 id=\"预剪枝\">预剪枝</h2>\r\n<p>在生成决策树的过程中，利用验证集验证当前划分是否能提升泛化效果，如果不能，则停止继续划分，直接将当前样本集作为决策树叶节点，并指定数量最多的类别做为叶节点类别。预剪枝训练时间短，但是由于其是贪心策略，容易引起欠拟合。</p>\r\n<h2 id=\"后剪枝\">后剪枝</h2>\r\n<p>在决策树生成完成之后，自底向上的对非叶节点进行考察，观察将该节点直接指定为叶节点是否能提升验证集泛化效果，如果能则将此节点的子树一起合并为一个叶节点。后剪枝的训练时间比预剪枝长很多，但是其欠拟合风险小，而且泛化性能往往优于预剪枝。</p>\r\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"决策树基础知识\">决策树基础知识</h1>\r\n<h2 id=\"信息熵\">信息熵</h2>\r\n<p>如果某个事件，有<span class=\"math inline\">\\(C\\)</span>种可能，其中第<span class=\"math inline\">\\(k\\)</span>中可能状态发生概率为<span class=\"math inline\">\\(p_k\\)</span>，那么该事件的信息熵定义为<span class=\"math inline\">\\(Ent(D) = -\\sum\\limits_{k=1}^C p_k\\log_2p_k\\)</span>，表示的是所有状态的信息的期望，以2为底数这样的定义的信息熵单位是bit。</p>\r\n<p>在决策树方法中，也类似的定义信息熵：样本集合<span class=\"math inline\">\\(D\\)</span>中第<span class=\"math inline\">\\(k\\)</span>类样本所占比例为<span class=\"math inline\">\\(p_k\\)</span>，则信息熵定义为<span class=\"math inline\">\\(Ent(D) = -\\sum\\limits_{k=1}^C p_k\\log_2p_k\\)</span>，其中<span class=\"math inline\">\\(C\\)</span>为类别个数，其刻画了样本集合的纯度。</p>\r\n<h2 id=\"信息增益\">信息增益</h2>\r\n<p>假设离散属性<span class=\"math inline\">\\(a\\)</span>有<span class=\"math inline\">\\(v\\)</span>个取值：<span class=\"math inline\">\\(a_1, a_2, ..., a_v\\)</span>，可以将当前数据集合分成<span class=\"math inline\">\\(V\\)</span>个子集：<span class=\"math inline\">\\(D^1, D^2, ..., D^V\\)</span>，那么使用属性<span class=\"math inline\">\\(a\\)</span>划分样本集的信息增益定义为<span class=\"math inline\">\\(Gain(D,a)=Ent(D)-\\sum\\limits_{v=1}^V\\frac{|D^v|}{|D|}Ent(D^v)\\)</span></p>\r\n<h2 id=\"增益率\">增益率</h2>\r\n<p>信息增益的定义导致其对数量较多的<span class=\"math inline\">\\(D^v\\)</span>更加敏感，因此又提出了信息增益率的概念：<span class=\"math inline\">\\(Gain\\_ratio(D,a)=\\frac{Gain(D,a)}{IV(a)}\\)</span>，其中，<span class=\"math inline\">\\(IV(a)=-\\sum\\limits_{v=1}^V \\frac{|D^v|}{|D|} \\log_2\\frac{|D^v|}{|D|}\\)</span>称为属性<span class=\"math inline\">\\(a\\)</span>的固有值（或者叫内在价值）。</p>\r\n<h2 id=\"基尼指数\">基尼指数</h2>\r\n<p>基尼值定义为<span class=\"math inline\">\\(Gini(D) = \\sum\\limits_{k=1}^C\\sum\\limits_{k&#39; \\ne k}p_k p_{k&#39;} = 1-\\sum\\limits_{k=1}^Cp_k^2\\)</span>，其反映了在<span class=\"math inline\">\\(D\\)</span>中随机抽取两个样本，不属于同一类别的概率。 和信息熵增益类似，对于在属性<span class=\"math inline\">\\(a\\)</span>上划分出<span class=\"math inline\">\\(V\\)</span>个区间的操作，定义划分后的基尼指数为<span class=\"math inline\">\\(Gini\\_index(D, a) = \\sum\\limits_{v=1}^V\\frac{|D^v|}{|D|}Gini(D^v)\\)</span>，构造决策树时的最优划分属性选择也可以使用能够使基尼指数最小的属性。</p>\r\n<h1 id=\"决策树的构造算法\">决策树的构造算法</h1>\r\n<h2 id=\"id3算法\">ID3算法</h2>\r\n<p>每次选择一个信息增益最大的属性<span class=\"math inline\">\\(a\\)</span>构造节点，这个节点将数据划分为<span class=\"math inline\">\\(V\\)</span>个子集，然后在每个子集中进行同样的操作（不能选择已经使用过的属性），若无属性可用或者当前子集类别相同，则当前子集标记为类别数量最多的类别，停止继续划分。</p>\r\n<h2 id=\"c4.5算法\">C4.5算法</h2>\r\n<p>由于ID3算法在选择属性时使用信息增益最大值来选取，因此这样构造的决策树偏向于先选择特征取值更多的属性，另外ID3算法还有一个问题是处理连续值属性，这两个问题在C4.5算法中都给出了解决方案。</p>\r\n<p>C4.5算法则是对ID3算法的一个改进版本，其中使用增益率来进行属性选择。</p>\r\n<h3 id=\"对连续值属性的处理\">对连续值属性的处理</h3>\r\n<p>在遇到连续值时，如果连续属性<span class=\"math inline\">\\(a\\)</span>在<span class=\"math inline\">\\(D\\)</span>中出现<span class=\"math inline\">\\(n\\)</span>个取值，则将其从小到大排序为<span class=\"math inline\">\\(\\begin{bmatrix}a_1, a_2, ... a_n\\end{bmatrix}\\)</span>，这样产生<span class=\"math inline\">\\(n-1\\)</span>个离散值<span class=\"math inline\">\\(T_a = \\{\\frac{a_i + a_{i+1}}{2}|1 \\le i \\le n-1\\}\\)</span>则<span class=\"math inline\">\\(Gain(D,a)= \\max\\limits_{t \\in T_a}Gain(D, a, t)\\)</span>，其中<span class=\"math inline\">\\(Gain(D, a, t)\\)</span>表示将<span class=\"math inline\">\\(a\\)</span>属性使用<span class=\"math inline\">\\(t\\)</span>划分为两部分，这样连续值的决策树构造和离散值类似了，不过连续值属性在构造决策树的过程中可以使用多次。</p>\r\n<h2 id=\"分类回归树classification-and-regression-tree-cart算法\">分类回归树(Classification And Regression Tree, CART)算法</h2>\r\n<p>对于分类树，CART算法使用基尼指数最小化准则进行特征选择，CART的主要思想是每次将数据划分成两个单元，例如，对于当前的数据集合<span class=\"math inline\">\\(D\\)</span>，根据属性<span class=\"math inline\">\\(a\\)</span>上的取值<span class=\"math inline\">\\(s\\)</span>数据划分成<span class=\"math inline\">\\(D^1\\)</span>和<span class=\"math inline\">\\(D^2\\)</span>两个子集（对于连续变量，划分数据就用大于和小于关系，对于离散变量就是用等于或不等于关系），对于这样的划分，可以得到基尼指数<span class=\"math inline\">\\(Gini\\_index(D, a, s) = \\frac{|D^1|}{|D|}Gini(D^1) + \\frac{|D^2|}{|D|}Gini(D^2)\\)</span>，每次划分都选择使得<span class=\"math inline\">\\(Gini\\_index(D, a, s)\\)</span>最小的<span class=\"math inline\">\\(a\\)</span>和<span class=\"math inline\">\\(s\\)</span>。如此循环，即得到了CART分类树。</p>\r\n<p>不同于ID3和C4.5算法，CART算法还能够处理回归树。对于回归树，其样本标签是连续值，回归树的一般形式表示如下，其中<span class=\"math inline\">\\(H(x)\\)</span>表示回归树的预测函数，<span class=\"math inline\">\\(\\mathbf{1}\\)</span>表示指示函数，其基本思想是根据数据的属性，将数据划分成<span class=\"math inline\">\\(M\\)</span>个部分<span class=\"math inline\">\\(R_m, m=1,2,...,M\\)</span>，每个部分对应一个输出值。 <span class=\"math display\">\\[\r\nH(x) = \\sum\\limits_{i=1}^M c_i \\mathbf{1}(X \\in R_i)\r\n\\]</span> CART回归树使用平方误差作为每个部分的损失函数，每次利用属性<span class=\"math inline\">\\(a\\)</span>上的取值<span class=\"math inline\">\\(s\\)</span>将数据划分成两个部分：<span class=\"math inline\">\\(R^1(a, s)\\)</span>和<span class=\"math inline\">\\(R^2(a, s)\\)</span>，按照下式的规则来选择每次划分的<span class=\"math inline\">\\(a\\)</span>和<span class=\"math inline\">\\(s\\)</span>： <span class=\"math display\">\\[\r\n\\min\\limits_{a, s}[\\min\\limits_{c_1} \\sum\\limits_{x_i \\in R^1(a,s)} (y_i - c_1)^2 + \\min\\limits_{c_2} \\sum\\limits_{x_j \\in R^2(a,s)} (y_j - c_2)^2]\r\n\\]</span> 其中c的值非常好计算，直接使用该数据子集对应标签的平均数即可。</p>\r\n<h2 id=\"属性缺失的处理\">属性缺失的处理</h2>\r\n<p>令<span class=\"math inline\">\\(\\tilde{D}\\)</span>是所有没有缺失属性a的样本集合，对于样本<span class=\"math inline\">\\(x\\)</span>，有样本权重<span class=\"math inline\">\\(w_x\\)</span>，定义如下参数。 <span class=\"math display\">\\[\r\n\\rho = \\frac{\\sum_{x\\in \\tilde{D}} w_x}{\\sum_{x\\in D}w_x}\\\\\r\n\\tilde{p}_k = \\frac{\\sum_{x\\in \\tilde{D}_k w_x}}{\\sum_{x\\in \\tilde{D}}w_x}, (1\\le k \\le C)\\\\\r\n\\tilde{r}_v = \\frac{\\sum_{x\\in \\tilde{D}^v}w_x}{\\sum_{x \\in \\tilde{D}} w_x}, (1 \\le v \\le V)\r\n\\]</span> 显然，<span class=\"math inline\">\\(\\rho\\)</span>表示属性无缺失样本所占比例，<span class=\"math inline\">\\(\\tilde{p}_k\\)</span>表示属性无缺失样本中第<span class=\"math inline\">\\(k\\)</span>类所占比例，<span class=\"math inline\">\\(\\tilde{r}_v\\)</span>表示属性无缺失样本中在属性<span class=\"math inline\">\\(a\\)</span>上取值<span class=\"math inline\">\\(a^v\\)</span>的样本比例。</p>\r\n<p>由此推广信息增益为： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\nGain(D, a) &amp;= \\rho \\times Gain(\\tilde{D}, a)\\\\\r\n&amp;=\\rho \\times (Ent(\\tilde{D}) - \\sum\\limits_{v=1}^V \\tilde{r}_v Ent(\\tilde{D}^v))\r\n\\end{aligned}\r\n\\]</span> 其中： <span class=\"math display\">\\[\r\nEnt(\\tilde{D}) = -\\sum\\limits_{k=1}^C \\tilde{p}_k log_2 \\tilde{p}_k\r\n\\]</span> 这样解决了最优划分的属性选择问题，在构造子树时，如果样本<span class=\"math inline\">\\(x\\)</span>在属性<span class=\"math inline\">\\(a\\)</span>上的取值已知，那么<span class=\"math inline\">\\(x\\)</span>划分到相应子节点，且权重保持为<span class=\"math inline\">\\(w_x\\)</span>，如果属性<span class=\"math inline\">\\(a\\)</span>未知，则将<span class=\"math inline\">\\(s\\)</span>划分入所有的子节点，且权重调整为<span class=\"math inline\">\\(\\tilde{r}_v w_x\\)</span>。</p>\r\n<h2 id=\"多变量决策树\">多变量决策树</h2>\r\n<p>叶节点不再针对某个属性，而是针对属性的线性组合进行划分。</p>\r\n<h1 id=\"决策树的剪枝\">决策树的剪枝</h1>\r\n<h2 id=\"预剪枝\">预剪枝</h2>\r\n<p>在生成决策树的过程中，利用验证集验证当前划分是否能提升泛化效果，如果不能，则停止继续划分，直接将当前样本集作为决策树叶节点，并指定数量最多的类别做为叶节点类别。预剪枝训练时间短，但是由于其是贪心策略，容易引起欠拟合。</p>\r\n<h2 id=\"后剪枝\">后剪枝</h2>\r\n<p>在决策树生成完成之后，自底向上的对非叶节点进行考察，观察将该节点直接指定为叶节点是否能提升验证集泛化效果，如果能则将此节点的子树一起合并为一个叶节点。后剪枝的训练时间比预剪枝长很多，但是其欠拟合风险小，而且泛化性能往往优于预剪枝。</p>\r\n"},{"title":"各种优化算法总结","date":"2020-05-12T07:05:12.000Z","mathjax":true,"_content":"\n梯度下降（Gradient Descnet, GD）是目前DNN（Deep Neural Network）中使用最多的优化算法，相同的数据，相同的DNN结构，相同的初始化参数值，使用不同的优化算法会得到截然不同的结果，因此优化算法非常重要，但是梯度下降法有一些缺点，为了对这些缺点进行修正，GD优化算法经历了很多演进，但是目前仍然没有一种普适的优化算法，这里根据论文《An overview of gradient descent optimization algorithms》的描述，将GD算法的的各种改进做了一些总结。\n\n# 最初的GD算法\n对于待优化的函数$J(\\theta)$，其中$\\theta$是待优化参数，GD算法的思路非常简洁，即指定学习速率$\\eta$和参数初始值$\\theta^0$，按照如下方式迭代的更新参数，其中$\\nabla_\\theta J(\\theta)$表示梯度，$\\theta^i$表示第$i$次更新后的参数值，\n$$\n\\theta^{i+1} = \\theta^i - \\eta \\nabla_\\theta J(\\theta)\n$$\n\n对于凸问题或者拟凸问题，这样的迭代最终会使得参数在最优点附近震荡，对于非凸且非拟凸的问题，这样的迭代大概率会最终在一个局部极小值附近震荡。\n\n这里还有个问题是$J(\\theta)$往往无法准确地计算，因为$J(\\theta)$和所有的训练数据相关，如果每一次迭代都去完整的遍历一遍所有数据，那么时间上可能不可接受，因此有了最初的两个GD算法的变种：随机梯度下降（Stochastic gradient descent，SGD）、批梯度下降（Mini-batch gradient descent）\n\n# 随机梯度下降（Stochastic gradient descent，SGD）\n针对$J(\\theta)$和所有的训练数据相关从而导致计算耗时长的问题，随机梯度下降的思想非常朴素：每次迭代时随机选取一个样本进行训练。如下所示，其中$\\nabla_\\theta J(\\theta;x^{(i+1)},y^{(i+1)})$表示根据第$i+1$次随机选择到的样本标签对$(x^{(i+1)},y^{(i+1)})$计算得到的损失函数。\n$$\n\\theta^{i+1} = \\theta^i - \\eta \\nabla_\\theta J(\\theta;x^{(i+1)},y^{(i+1)})\n$$\n\n# 小批梯度下降（Mini-batch gradient descent）\n和随机梯度下降的动机相同，为了简化损失函数的计算，这里每一次使用一批数据（n个样本）来进行损失函数的计算（本质上是将多个损失函数进行求和或者求平均），如下所示：\n$$\n\\theta^{i+1} = \\theta^i - \\eta \\nabla_\\theta J(\\theta;x^{((i+1)\\times n:(i+2)\\times n)},y^{((i+1)\\times n:(i+2)\\times n)})\n$$\n\n这里需要注意的是，很多深度学习框架的实现中，SGD代表的不是随机梯度下降，而是随机小批梯度下降，即随机打乱数据之后，再使用小批梯度下降进行优化。\n\n这几个方法没有什么本质上的区别，在数据的选择上，如果每次迭代时计算梯度所用到的数据越多，那么梯度的计算自然更加准确，但是数据过多，又会引起计算时间增加导致迭代缓慢，因此每次选择多少个数据来进行计算，即批大小（batch size）的选择就是在这两个问题上进行权衡。至于为什么要随机选择，自然是为了将数据充分混合，在pytorch的dataloader实现中，一般是在最初将数据shuffle一次，然后每次迭代都顺序选择下一批数据。\n\n梯度下降算法的主要难点：\n- 学习速率的选择，过大的学习速率会导致算法不能很好的收敛（在局部极小值附近大幅度震荡），因此一般使用初始大学习速率，然后学习速率逐渐衰减的方式，但是学习速率的初始值和衰减速度同样难以设置。\n- 对于存在很多鞍点的问题，梯度下降算法理论上无法正确的处理在鞍点的情况（但实际上每次随机使用不同的数据计算梯度，很难得到一致的鞍点，总有可能随机到一些数据计算出来在原来的鞍点处又有梯度的情况，因此后面的一些说能够有助于逃离鞍点的动量化方法，我认为其实在这方面意义不是非常大，这可能也是很多论文中依旧在使用SGD，而且效果也还可以的原因）\n- 在狭长的损失函数区域中，梯度下降算法非常容易震荡，如下图所示。\n\n![狭长形状的损失函数下SGD的运行示意图](狭长形状的损失函数下SGD的运行示意图.png)\n\n# Momentum\n为了解决上面的最后一个问题，在SGD中可以使用动量项（Momentum term）进行改进，类似于物理上的动量概念：运动方向的改变不是瞬间变化的，而是缓慢改变，平滑过渡。动量项可以表达为：\n$$\nm^t = \\gamma m^{t-1} + \\nabla_\\theta J(\\theta^{t-1})\n$$\n\n有些地方也写成\n$$\nm^t = \\gamma m^{t-1} + (1 - \\gamma)\\nabla_\\theta J(\\theta^{t-1})\n$$\n\n其中$m^0$一般初始化为0，$0 < \\gamma <1$，$\\gamma$一般被设置为$0.9$，这个表达式的意思就是将之前的梯度方向累积下来，优化算法在迭代时，不再是直接向着梯度方向更新，而是使用动量来更新：\n$$\n\\theta^t = \\theta^{t-1}- \\eta m^t\n$$\n这样的好处是可以避免更新方向的频繁变化，减少震荡，加速收敛。\n\nMomentum可以抑制更新过程中的震荡，以狭长形状的损失函数为例，使用动量项之后，其更新轨迹类似下图：\n\n![狭长形状的损失函数下Momentum的运行示意图](狭长形状的损失函数下Momentum的运行示意图.png)\n\n# Nesterov accelerated gradient（NAG）\nNAG方法对Momentum方法进行了改进，其思想非常简单：既然需要沿着动量方向进行参数更新，那么梯度的计也可以在动量更新之后再进行，即沿着动量方向进行参数更新之后再计算梯度来修正动量，最后再使用修正后的动量来更新。\n\n可以通俗的理解一定程度上预测了Momentum下一步的位置，然后根据未来的位置计算梯度来对动量项进行修正。\n\n因此这里将动量定义为：\n$$\nm^t = \\gamma m^{t-1} + \\nabla_\\theta J(\\theta^{t-1} - \\eta \\gamma m^{t-1})\n$$\n这样的动量项也被称为Nesterov Momentum，参数更新过程依旧不变：\n\n$$\n\\theta^t = \\theta^{t-1} - \\eta m^t\n$$\n\nMomentum和NAG的运行差别如下图所示（图片来自于[《路遥知马力——Momentum - 无痛的机器学习 - 知乎专栏》](https://zhuanlan.zhihu.com/p/21486826)），上面的是Momentum方法，下面的是NAG方法。\n\n![Momentum和NAG的运行差别](Momentum和NAG的运行差别.png)\n\nNAG的另外等价一种表达方式可以写成：\n$$\n\\begin{aligned}\nd^t &= \\gamma d^{t-1} + \\nabla_\\theta J(\\theta^{t-1}) + \\gamma(\\nabla_\\theta J(\\theta^{t-1}) - \\nabla_\\theta J(\\theta^{t-2}))\\\\\n\\theta^t &= \\theta^{t-1} - \\eta d^t    \n\\end{aligned}\n$$\n这个等价的推理过程可见[比Momentum更快：揭开Nesterov Accelerated Gradient的真面目](https://zhuanlan.zhihu.com/p/22810533/)，这里表达的意思就是“如果这次的梯度比上次的梯度变大了，那么有理由相信它会继续变大下去，那就把预计要增大的部分提前加进来；如果相比上次变小了，那么需要动量项上需要做相应的减少”，$\\gamma(\\nabla_\\theta J(\\theta^{t-1}) - \\nabla_\\theta J(\\theta^{t-2}))$这一项其实是利用了近似二阶导数信息。因此NAG相比于Momentum要快些。\n\n\n# Adagrad\n\nAdagrad的主要思想是让学习速率能够自动调整，使得更新频繁的参数有更小的学习速率，更新少的参数有更大的学习速率。\n\n这里的意思是每个参数的学习速率各不相同，使用每个参数每次的梯度平方和来计算，例如对于参数$\\theta$，其第$i$次更新的梯度$g^i$，初始学习速率为$\\eta$，那么第$t$次更新的时候，该参数的学习速率$\\eta^t$的计算方式如下：\n$$\n\\begin{aligned}\n    v^t &= \\sum\\limits_{i=1}^t (g^i)^2\\\\\n    \\eta^t &= \\frac{\\eta}{\\sqrt{v^t + \\epsilon}}\\\\\n\\end{aligned}\n$$\n如果每次梯度都很大，那么这个参数就被认为是频繁更新，学习速率衰减比较快，否则学习速率衰减比较慢，可以说Adagrad是一种学习速率衰减方法。\n\n# Adadelta\n在Adagrad中，由于$v^t$是单调递增的，因此Adagrad很容易导致提前停止训练，所以在Adadelta中将其改成不关心全局的梯度，而是只关心最近一段时间的梯度，这里将$v^t$改成如下计算方式：\n\n$$\nv^t = \\gamma v_{t-1, j} + (1 - \\gamma)(g^t)^2\n$$\n\n# RMSProp\nRMSProp算法是Adadelta的一个特列：$\\gamma=0.9$\n\n# Adam（Adaptive Moment Estimation）\nAdam其实就是在Adadelta的基础上添加了momentum，然后稍微做了些修正，修正是因为考虑到其初值是0，会对动量和二阶动量的估计造成偏差，其实这里可以看出，修正之后$\\hat{m}_{1,j}$其实就是$g_{1, j}$，$\\hat{v}_{1,j}$也是类似的。\n$$\n\\begin{aligned}\nm^t &= \\beta_1 m^{t-1} + (1 - \\beta_1)g^t\\\\\nv^t &= \\beta_2 v^{t-1} + (1 - \\beta_2)(g^t)^2\\\\\n\\hat{m}^t &= \\frac{m^t}{1 - \\beta^t_1}\\\\\n\\hat{v}^t &= \\frac{v^t}{1 - \\beta^t_2}\n\\end{aligned}\n$$\n参数$\\theta$的更新变成：\n$$\n\\theta^t = \\theta^t - \\frac{\\eta}{\\sqrt{\\hat{v}^t}+\\epsilon}\\hat{m}^t\n$$\nAdam在实际使用中也是非常常见的优化方法，既有动量方法的优点，又能自动调整学习速率。\n\n# NAdam\n既然Adam都使用了动量方法，那肯定要试一下将NAG（Nesterov accelerated gradient）方法加入进来，因此就有了NAdam。\n\n对于NAG，这里做了一些变化，首先令$\\hat{\\theta}^{t-1} = \\theta^{t-1} - \\eta m^{t-1}$，那么Nesterov Momentum可以写为：\n$$\n\\begin{aligned}\nm^t &= \\beta_1 m^{t-1} + (1 - \\beta_1)\\nabla_\\theta J(\\theta^{t-1} - \\eta m^{t-1})\\\\\n&=\\beta_1 m^{t-1} + (1 - \\beta_1)\\nabla_\\theta J(\\hat{\\theta}^t)\n\\end{aligned}\n$$\n则NAG的更新可以写成：\n$$\n\\begin{aligned}\n    \\theta^t &= \\theta^{t-1} - \\eta m^t\\\\\n    \\hat{\\theta}^t &= \\theta^t - \\eta \\beta_1 m^t\\\\\n    &=\\theta^{t-1} - \\eta m^t - \\eta \\beta_1 m^t\\\\\n    &=\\theta^{t-1} - \\eta (\\beta_1 m^{t-1} + (1 - \\beta_1)\\nabla_\\theta J(\\hat{\\theta}^t)) - \\eta \\beta_1 m^t\\\\\n    &=\\hat{\\theta}^{t-1} - \\eta (\\beta_1 m^t + (1 - \\beta_1)\\nabla_\\theta J(\\hat{\\theta}^t))\n\\end{aligned}\n$$\n\n根据NAG的思路得到了$m^t$，那么在Adam中得到$\\hat{m}^t = \\frac{m^t}{1 - \\beta^t_1} = \\frac{\\beta_1 m^{t-1} + (1 - \\beta_1)\\nabla_\\theta J(\\hat{\\theta}^t)}{1 - \\beta^t_1}$之后，$\\hat{\\theta}^t$的计算变为：\n$$\n\\begin{aligned}\n    \\hat{\\theta}^t &=\\hat{\\theta}^{t-1} - \\frac{\\eta}{\\sqrt{\\hat{v}^t}+\\epsilon}\\hat{m}^t\\\\\n    &=\\hat{\\theta}^{t-1} - \\frac{\\eta}{\\sqrt{\\hat{v}^t}+\\epsilon}\\frac{\\beta_1 m^{t-1} + (1 - \\beta_1)\\nabla_\\theta J(\\hat{\\theta}^t)}{1 - \\beta^t_1}\\\\\n\\end{aligned}\n$$\n\n这里如果用$\\beta_1\\hat{m}^{t-1}$来替换$\\frac{\\beta_1 m^{t-1}}{1 - \\beta^t_1}$，则可以简化为：\n$$\n\\hat{\\theta}^t = \\hat{\\theta}^{t-1} - \\frac{\\eta}{\\sqrt{\\hat{v}^t}+\\epsilon}(\\beta_1\\hat{m}^{t-1} + \\frac{(1 - \\beta_1)\\nabla_\\theta J(\\hat{\\theta}^t)}{1 - \\beta^t_1})\n$$\n\n完整的NAdam表达如下：\n$$\n\\begin{aligned}\n\\hat{\\theta}^{t-1} &= \\theta^{t-1} - \\eta m^{t-1}\\\\\ng^t &= \\nabla_\\theta J(\\hat{\\theta}^t)\\\\\nm^t &= \\beta_1 m^{t-1} + (1 - \\beta_1)g^t\\\\\nv^t &= \\beta_2 v^{t-1} + (1 - \\beta_2)(g^t)^2\\\\\n\\hat{m}^t &= \\frac{m^t}{1 - \\beta^t_1}\\\\\n\\hat{v}^t &= \\frac{v^t}{1 - \\beta^t_2}\\\\\n\\hat{\\theta}^t &= \\hat{\\theta}^{t-1} - \\frac{\\eta}{\\sqrt{\\hat{v}^t}+\\epsilon}(\\beta_1\\hat{m}^{t-1} + \\frac{(1 - \\beta_1)}{1 - \\beta^t_1} g^t)\n\\end{aligned}\n$$\n\n一般如果使用Adam有效的场景，都可以使用NAdam来得到更好的效果。","source":"_posts/学习笔记/各种优化算法总结.md","raw":"---\ntitle: 各种优化算法总结\ndate: 2020-05-12 15:05:12\ntags: [深度学习]\nmathjax: true\n---\n\n梯度下降（Gradient Descnet, GD）是目前DNN（Deep Neural Network）中使用最多的优化算法，相同的数据，相同的DNN结构，相同的初始化参数值，使用不同的优化算法会得到截然不同的结果，因此优化算法非常重要，但是梯度下降法有一些缺点，为了对这些缺点进行修正，GD优化算法经历了很多演进，但是目前仍然没有一种普适的优化算法，这里根据论文《An overview of gradient descent optimization algorithms》的描述，将GD算法的的各种改进做了一些总结。\n\n# 最初的GD算法\n对于待优化的函数$J(\\theta)$，其中$\\theta$是待优化参数，GD算法的思路非常简洁，即指定学习速率$\\eta$和参数初始值$\\theta^0$，按照如下方式迭代的更新参数，其中$\\nabla_\\theta J(\\theta)$表示梯度，$\\theta^i$表示第$i$次更新后的参数值，\n$$\n\\theta^{i+1} = \\theta^i - \\eta \\nabla_\\theta J(\\theta)\n$$\n\n对于凸问题或者拟凸问题，这样的迭代最终会使得参数在最优点附近震荡，对于非凸且非拟凸的问题，这样的迭代大概率会最终在一个局部极小值附近震荡。\n\n这里还有个问题是$J(\\theta)$往往无法准确地计算，因为$J(\\theta)$和所有的训练数据相关，如果每一次迭代都去完整的遍历一遍所有数据，那么时间上可能不可接受，因此有了最初的两个GD算法的变种：随机梯度下降（Stochastic gradient descent，SGD）、批梯度下降（Mini-batch gradient descent）\n\n# 随机梯度下降（Stochastic gradient descent，SGD）\n针对$J(\\theta)$和所有的训练数据相关从而导致计算耗时长的问题，随机梯度下降的思想非常朴素：每次迭代时随机选取一个样本进行训练。如下所示，其中$\\nabla_\\theta J(\\theta;x^{(i+1)},y^{(i+1)})$表示根据第$i+1$次随机选择到的样本标签对$(x^{(i+1)},y^{(i+1)})$计算得到的损失函数。\n$$\n\\theta^{i+1} = \\theta^i - \\eta \\nabla_\\theta J(\\theta;x^{(i+1)},y^{(i+1)})\n$$\n\n# 小批梯度下降（Mini-batch gradient descent）\n和随机梯度下降的动机相同，为了简化损失函数的计算，这里每一次使用一批数据（n个样本）来进行损失函数的计算（本质上是将多个损失函数进行求和或者求平均），如下所示：\n$$\n\\theta^{i+1} = \\theta^i - \\eta \\nabla_\\theta J(\\theta;x^{((i+1)\\times n:(i+2)\\times n)},y^{((i+1)\\times n:(i+2)\\times n)})\n$$\n\n这里需要注意的是，很多深度学习框架的实现中，SGD代表的不是随机梯度下降，而是随机小批梯度下降，即随机打乱数据之后，再使用小批梯度下降进行优化。\n\n这几个方法没有什么本质上的区别，在数据的选择上，如果每次迭代时计算梯度所用到的数据越多，那么梯度的计算自然更加准确，但是数据过多，又会引起计算时间增加导致迭代缓慢，因此每次选择多少个数据来进行计算，即批大小（batch size）的选择就是在这两个问题上进行权衡。至于为什么要随机选择，自然是为了将数据充分混合，在pytorch的dataloader实现中，一般是在最初将数据shuffle一次，然后每次迭代都顺序选择下一批数据。\n\n梯度下降算法的主要难点：\n- 学习速率的选择，过大的学习速率会导致算法不能很好的收敛（在局部极小值附近大幅度震荡），因此一般使用初始大学习速率，然后学习速率逐渐衰减的方式，但是学习速率的初始值和衰减速度同样难以设置。\n- 对于存在很多鞍点的问题，梯度下降算法理论上无法正确的处理在鞍点的情况（但实际上每次随机使用不同的数据计算梯度，很难得到一致的鞍点，总有可能随机到一些数据计算出来在原来的鞍点处又有梯度的情况，因此后面的一些说能够有助于逃离鞍点的动量化方法，我认为其实在这方面意义不是非常大，这可能也是很多论文中依旧在使用SGD，而且效果也还可以的原因）\n- 在狭长的损失函数区域中，梯度下降算法非常容易震荡，如下图所示。\n\n![狭长形状的损失函数下SGD的运行示意图](狭长形状的损失函数下SGD的运行示意图.png)\n\n# Momentum\n为了解决上面的最后一个问题，在SGD中可以使用动量项（Momentum term）进行改进，类似于物理上的动量概念：运动方向的改变不是瞬间变化的，而是缓慢改变，平滑过渡。动量项可以表达为：\n$$\nm^t = \\gamma m^{t-1} + \\nabla_\\theta J(\\theta^{t-1})\n$$\n\n有些地方也写成\n$$\nm^t = \\gamma m^{t-1} + (1 - \\gamma)\\nabla_\\theta J(\\theta^{t-1})\n$$\n\n其中$m^0$一般初始化为0，$0 < \\gamma <1$，$\\gamma$一般被设置为$0.9$，这个表达式的意思就是将之前的梯度方向累积下来，优化算法在迭代时，不再是直接向着梯度方向更新，而是使用动量来更新：\n$$\n\\theta^t = \\theta^{t-1}- \\eta m^t\n$$\n这样的好处是可以避免更新方向的频繁变化，减少震荡，加速收敛。\n\nMomentum可以抑制更新过程中的震荡，以狭长形状的损失函数为例，使用动量项之后，其更新轨迹类似下图：\n\n![狭长形状的损失函数下Momentum的运行示意图](狭长形状的损失函数下Momentum的运行示意图.png)\n\n# Nesterov accelerated gradient（NAG）\nNAG方法对Momentum方法进行了改进，其思想非常简单：既然需要沿着动量方向进行参数更新，那么梯度的计也可以在动量更新之后再进行，即沿着动量方向进行参数更新之后再计算梯度来修正动量，最后再使用修正后的动量来更新。\n\n可以通俗的理解一定程度上预测了Momentum下一步的位置，然后根据未来的位置计算梯度来对动量项进行修正。\n\n因此这里将动量定义为：\n$$\nm^t = \\gamma m^{t-1} + \\nabla_\\theta J(\\theta^{t-1} - \\eta \\gamma m^{t-1})\n$$\n这样的动量项也被称为Nesterov Momentum，参数更新过程依旧不变：\n\n$$\n\\theta^t = \\theta^{t-1} - \\eta m^t\n$$\n\nMomentum和NAG的运行差别如下图所示（图片来自于[《路遥知马力——Momentum - 无痛的机器学习 - 知乎专栏》](https://zhuanlan.zhihu.com/p/21486826)），上面的是Momentum方法，下面的是NAG方法。\n\n![Momentum和NAG的运行差别](Momentum和NAG的运行差别.png)\n\nNAG的另外等价一种表达方式可以写成：\n$$\n\\begin{aligned}\nd^t &= \\gamma d^{t-1} + \\nabla_\\theta J(\\theta^{t-1}) + \\gamma(\\nabla_\\theta J(\\theta^{t-1}) - \\nabla_\\theta J(\\theta^{t-2}))\\\\\n\\theta^t &= \\theta^{t-1} - \\eta d^t    \n\\end{aligned}\n$$\n这个等价的推理过程可见[比Momentum更快：揭开Nesterov Accelerated Gradient的真面目](https://zhuanlan.zhihu.com/p/22810533/)，这里表达的意思就是“如果这次的梯度比上次的梯度变大了，那么有理由相信它会继续变大下去，那就把预计要增大的部分提前加进来；如果相比上次变小了，那么需要动量项上需要做相应的减少”，$\\gamma(\\nabla_\\theta J(\\theta^{t-1}) - \\nabla_\\theta J(\\theta^{t-2}))$这一项其实是利用了近似二阶导数信息。因此NAG相比于Momentum要快些。\n\n\n# Adagrad\n\nAdagrad的主要思想是让学习速率能够自动调整，使得更新频繁的参数有更小的学习速率，更新少的参数有更大的学习速率。\n\n这里的意思是每个参数的学习速率各不相同，使用每个参数每次的梯度平方和来计算，例如对于参数$\\theta$，其第$i$次更新的梯度$g^i$，初始学习速率为$\\eta$，那么第$t$次更新的时候，该参数的学习速率$\\eta^t$的计算方式如下：\n$$\n\\begin{aligned}\n    v^t &= \\sum\\limits_{i=1}^t (g^i)^2\\\\\n    \\eta^t &= \\frac{\\eta}{\\sqrt{v^t + \\epsilon}}\\\\\n\\end{aligned}\n$$\n如果每次梯度都很大，那么这个参数就被认为是频繁更新，学习速率衰减比较快，否则学习速率衰减比较慢，可以说Adagrad是一种学习速率衰减方法。\n\n# Adadelta\n在Adagrad中，由于$v^t$是单调递增的，因此Adagrad很容易导致提前停止训练，所以在Adadelta中将其改成不关心全局的梯度，而是只关心最近一段时间的梯度，这里将$v^t$改成如下计算方式：\n\n$$\nv^t = \\gamma v_{t-1, j} + (1 - \\gamma)(g^t)^2\n$$\n\n# RMSProp\nRMSProp算法是Adadelta的一个特列：$\\gamma=0.9$\n\n# Adam（Adaptive Moment Estimation）\nAdam其实就是在Adadelta的基础上添加了momentum，然后稍微做了些修正，修正是因为考虑到其初值是0，会对动量和二阶动量的估计造成偏差，其实这里可以看出，修正之后$\\hat{m}_{1,j}$其实就是$g_{1, j}$，$\\hat{v}_{1,j}$也是类似的。\n$$\n\\begin{aligned}\nm^t &= \\beta_1 m^{t-1} + (1 - \\beta_1)g^t\\\\\nv^t &= \\beta_2 v^{t-1} + (1 - \\beta_2)(g^t)^2\\\\\n\\hat{m}^t &= \\frac{m^t}{1 - \\beta^t_1}\\\\\n\\hat{v}^t &= \\frac{v^t}{1 - \\beta^t_2}\n\\end{aligned}\n$$\n参数$\\theta$的更新变成：\n$$\n\\theta^t = \\theta^t - \\frac{\\eta}{\\sqrt{\\hat{v}^t}+\\epsilon}\\hat{m}^t\n$$\nAdam在实际使用中也是非常常见的优化方法，既有动量方法的优点，又能自动调整学习速率。\n\n# NAdam\n既然Adam都使用了动量方法，那肯定要试一下将NAG（Nesterov accelerated gradient）方法加入进来，因此就有了NAdam。\n\n对于NAG，这里做了一些变化，首先令$\\hat{\\theta}^{t-1} = \\theta^{t-1} - \\eta m^{t-1}$，那么Nesterov Momentum可以写为：\n$$\n\\begin{aligned}\nm^t &= \\beta_1 m^{t-1} + (1 - \\beta_1)\\nabla_\\theta J(\\theta^{t-1} - \\eta m^{t-1})\\\\\n&=\\beta_1 m^{t-1} + (1 - \\beta_1)\\nabla_\\theta J(\\hat{\\theta}^t)\n\\end{aligned}\n$$\n则NAG的更新可以写成：\n$$\n\\begin{aligned}\n    \\theta^t &= \\theta^{t-1} - \\eta m^t\\\\\n    \\hat{\\theta}^t &= \\theta^t - \\eta \\beta_1 m^t\\\\\n    &=\\theta^{t-1} - \\eta m^t - \\eta \\beta_1 m^t\\\\\n    &=\\theta^{t-1} - \\eta (\\beta_1 m^{t-1} + (1 - \\beta_1)\\nabla_\\theta J(\\hat{\\theta}^t)) - \\eta \\beta_1 m^t\\\\\n    &=\\hat{\\theta}^{t-1} - \\eta (\\beta_1 m^t + (1 - \\beta_1)\\nabla_\\theta J(\\hat{\\theta}^t))\n\\end{aligned}\n$$\n\n根据NAG的思路得到了$m^t$，那么在Adam中得到$\\hat{m}^t = \\frac{m^t}{1 - \\beta^t_1} = \\frac{\\beta_1 m^{t-1} + (1 - \\beta_1)\\nabla_\\theta J(\\hat{\\theta}^t)}{1 - \\beta^t_1}$之后，$\\hat{\\theta}^t$的计算变为：\n$$\n\\begin{aligned}\n    \\hat{\\theta}^t &=\\hat{\\theta}^{t-1} - \\frac{\\eta}{\\sqrt{\\hat{v}^t}+\\epsilon}\\hat{m}^t\\\\\n    &=\\hat{\\theta}^{t-1} - \\frac{\\eta}{\\sqrt{\\hat{v}^t}+\\epsilon}\\frac{\\beta_1 m^{t-1} + (1 - \\beta_1)\\nabla_\\theta J(\\hat{\\theta}^t)}{1 - \\beta^t_1}\\\\\n\\end{aligned}\n$$\n\n这里如果用$\\beta_1\\hat{m}^{t-1}$来替换$\\frac{\\beta_1 m^{t-1}}{1 - \\beta^t_1}$，则可以简化为：\n$$\n\\hat{\\theta}^t = \\hat{\\theta}^{t-1} - \\frac{\\eta}{\\sqrt{\\hat{v}^t}+\\epsilon}(\\beta_1\\hat{m}^{t-1} + \\frac{(1 - \\beta_1)\\nabla_\\theta J(\\hat{\\theta}^t)}{1 - \\beta^t_1})\n$$\n\n完整的NAdam表达如下：\n$$\n\\begin{aligned}\n\\hat{\\theta}^{t-1} &= \\theta^{t-1} - \\eta m^{t-1}\\\\\ng^t &= \\nabla_\\theta J(\\hat{\\theta}^t)\\\\\nm^t &= \\beta_1 m^{t-1} + (1 - \\beta_1)g^t\\\\\nv^t &= \\beta_2 v^{t-1} + (1 - \\beta_2)(g^t)^2\\\\\n\\hat{m}^t &= \\frac{m^t}{1 - \\beta^t_1}\\\\\n\\hat{v}^t &= \\frac{v^t}{1 - \\beta^t_2}\\\\\n\\hat{\\theta}^t &= \\hat{\\theta}^{t-1} - \\frac{\\eta}{\\sqrt{\\hat{v}^t}+\\epsilon}(\\beta_1\\hat{m}^{t-1} + \\frac{(1 - \\beta_1)}{1 - \\beta^t_1} g^t)\n\\end{aligned}\n$$\n\n一般如果使用Adam有效的场景，都可以使用NAdam来得到更好的效果。","slug":"学习笔记/各种优化算法总结","published":1,"updated":"2020-08-31T09:03:14.844Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckjxpc3rt000t44mqhtda54gb","content":"<p>梯度下降（Gradient Descnet, GD）是目前DNN（Deep Neural Network）中使用最多的优化算法，相同的数据，相同的DNN结构，相同的初始化参数值，使用不同的优化算法会得到截然不同的结果，因此优化算法非常重要，但是梯度下降法有一些缺点，为了对这些缺点进行修正，GD优化算法经历了很多演进，但是目前仍然没有一种普适的优化算法，这里根据论文《An overview of gradient descent optimization algorithms》的描述，将GD算法的的各种改进做了一些总结。</p>\r\n<h1 id=\"最初的gd算法\">最初的GD算法</h1>\r\n<p>对于待优化的函数<span class=\"math inline\">\\(J(\\theta)\\)</span>，其中<span class=\"math inline\">\\(\\theta\\)</span>是待优化参数，GD算法的思路非常简洁，即指定学习速率<span class=\"math inline\">\\(\\eta\\)</span>和参数初始值<span class=\"math inline\">\\(\\theta^0\\)</span>，按照如下方式迭代的更新参数，其中<span class=\"math inline\">\\(\\nabla_\\theta J(\\theta)\\)</span>表示梯度，<span class=\"math inline\">\\(\\theta^i\\)</span>表示第<span class=\"math inline\">\\(i\\)</span>次更新后的参数值， <span class=\"math display\">\\[\r\n\\theta^{i+1} = \\theta^i - \\eta \\nabla_\\theta J(\\theta)\r\n\\]</span></p>\r\n<p>对于凸问题或者拟凸问题，这样的迭代最终会使得参数在最优点附近震荡，对于非凸且非拟凸的问题，这样的迭代大概率会最终在一个局部极小值附近震荡。</p>\r\n<p>这里还有个问题是<span class=\"math inline\">\\(J(\\theta)\\)</span>往往无法准确地计算，因为<span class=\"math inline\">\\(J(\\theta)\\)</span>和所有的训练数据相关，如果每一次迭代都去完整的遍历一遍所有数据，那么时间上可能不可接受，因此有了最初的两个GD算法的变种：随机梯度下降（Stochastic gradient descent，SGD）、批梯度下降（Mini-batch gradient descent）</p>\r\n<h1 id=\"随机梯度下降stochastic-gradient-descentsgd\">随机梯度下降（Stochastic gradient descent，SGD）</h1>\r\n<p>针对<span class=\"math inline\">\\(J(\\theta)\\)</span>和所有的训练数据相关从而导致计算耗时长的问题，随机梯度下降的思想非常朴素：每次迭代时随机选取一个样本进行训练。如下所示，其中<span class=\"math inline\">\\(\\nabla_\\theta J(\\theta;x^{(i+1)},y^{(i+1)})\\)</span>表示根据第<span class=\"math inline\">\\(i+1\\)</span>次随机选择到的样本标签对<span class=\"math inline\">\\((x^{(i+1)},y^{(i+1)})\\)</span>计算得到的损失函数。 <span class=\"math display\">\\[\r\n\\theta^{i+1} = \\theta^i - \\eta \\nabla_\\theta J(\\theta;x^{(i+1)},y^{(i+1)})\r\n\\]</span></p>\r\n<h1 id=\"小批梯度下降mini-batch-gradient-descent\">小批梯度下降（Mini-batch gradient descent）</h1>\r\n<p>和随机梯度下降的动机相同，为了简化损失函数的计算，这里每一次使用一批数据（n个样本）来进行损失函数的计算（本质上是将多个损失函数进行求和或者求平均），如下所示： <span class=\"math display\">\\[\r\n\\theta^{i+1} = \\theta^i - \\eta \\nabla_\\theta J(\\theta;x^{((i+1)\\times n:(i+2)\\times n)},y^{((i+1)\\times n:(i+2)\\times n)})\r\n\\]</span></p>\r\n<p>这里需要注意的是，很多深度学习框架的实现中，SGD代表的不是随机梯度下降，而是随机小批梯度下降，即随机打乱数据之后，再使用小批梯度下降进行优化。</p>\r\n<p>这几个方法没有什么本质上的区别，在数据的选择上，如果每次迭代时计算梯度所用到的数据越多，那么梯度的计算自然更加准确，但是数据过多，又会引起计算时间增加导致迭代缓慢，因此每次选择多少个数据来进行计算，即批大小（batch size）的选择就是在这两个问题上进行权衡。至于为什么要随机选择，自然是为了将数据充分混合，在pytorch的dataloader实现中，一般是在最初将数据shuffle一次，然后每次迭代都顺序选择下一批数据。</p>\r\n<p>梯度下降算法的主要难点： - 学习速率的选择，过大的学习速率会导致算法不能很好的收敛（在局部极小值附近大幅度震荡），因此一般使用初始大学习速率，然后学习速率逐渐衰减的方式，但是学习速率的初始值和衰减速度同样难以设置。 - 对于存在很多鞍点的问题，梯度下降算法理论上无法正确的处理在鞍点的情况（但实际上每次随机使用不同的数据计算梯度，很难得到一致的鞍点，总有可能随机到一些数据计算出来在原来的鞍点处又有梯度的情况，因此后面的一些说能够有助于逃离鞍点的动量化方法，我认为其实在这方面意义不是非常大，这可能也是很多论文中依旧在使用SGD，而且效果也还可以的原因） - 在狭长的损失函数区域中，梯度下降算法非常容易震荡，如下图所示。</p>\r\n<figure>\r\n<img src=\"狭长形状的损失函数下SGD的运行示意图.png\" alt=\"狭长形状的损失函数下SGD的运行示意图\" /><figcaption aria-hidden=\"true\">狭长形状的损失函数下SGD的运行示意图</figcaption>\r\n</figure>\r\n<h1 id=\"momentum\">Momentum</h1>\r\n<p>为了解决上面的最后一个问题，在SGD中可以使用动量项（Momentum term）进行改进，类似于物理上的动量概念：运动方向的改变不是瞬间变化的，而是缓慢改变，平滑过渡。动量项可以表达为： <span class=\"math display\">\\[\r\nm^t = \\gamma m^{t-1} + \\nabla_\\theta J(\\theta^{t-1})\r\n\\]</span></p>\r\n<p>有些地方也写成 <span class=\"math display\">\\[\r\nm^t = \\gamma m^{t-1} + (1 - \\gamma)\\nabla_\\theta J(\\theta^{t-1})\r\n\\]</span></p>\r\n<p>其中<span class=\"math inline\">\\(m^0\\)</span>一般初始化为0，<span class=\"math inline\">\\(0 &lt; \\gamma &lt;1\\)</span>，<span class=\"math inline\">\\(\\gamma\\)</span>一般被设置为<span class=\"math inline\">\\(0.9\\)</span>，这个表达式的意思就是将之前的梯度方向累积下来，优化算法在迭代时，不再是直接向着梯度方向更新，而是使用动量来更新： <span class=\"math display\">\\[\r\n\\theta^t = \\theta^{t-1}- \\eta m^t\r\n\\]</span> 这样的好处是可以避免更新方向的频繁变化，减少震荡，加速收敛。</p>\r\n<p>Momentum可以抑制更新过程中的震荡，以狭长形状的损失函数为例，使用动量项之后，其更新轨迹类似下图：</p>\r\n<figure>\r\n<img src=\"狭长形状的损失函数下Momentum的运行示意图.png\" alt=\"狭长形状的损失函数下Momentum的运行示意图\" /><figcaption aria-hidden=\"true\">狭长形状的损失函数下Momentum的运行示意图</figcaption>\r\n</figure>\r\n<h1 id=\"nesterov-accelerated-gradientnag\">Nesterov accelerated gradient（NAG）</h1>\r\n<p>NAG方法对Momentum方法进行了改进，其思想非常简单：既然需要沿着动量方向进行参数更新，那么梯度的计也可以在动量更新之后再进行，即沿着动量方向进行参数更新之后再计算梯度来修正动量，最后再使用修正后的动量来更新。</p>\r\n<p>可以通俗的理解一定程度上预测了Momentum下一步的位置，然后根据未来的位置计算梯度来对动量项进行修正。</p>\r\n<p>因此这里将动量定义为： <span class=\"math display\">\\[\r\nm^t = \\gamma m^{t-1} + \\nabla_\\theta J(\\theta^{t-1} - \\eta \\gamma m^{t-1})\r\n\\]</span> 这样的动量项也被称为Nesterov Momentum，参数更新过程依旧不变：</p>\r\n<p><span class=\"math display\">\\[\r\n\\theta^t = \\theta^{t-1} - \\eta m^t\r\n\\]</span></p>\r\n<p>Momentum和NAG的运行差别如下图所示（图片来自于<a href=\"https://zhuanlan.zhihu.com/p/21486826\">《路遥知马力——Momentum - 无痛的机器学习 - 知乎专栏》</a>），上面的是Momentum方法，下面的是NAG方法。</p>\r\n<figure>\r\n<img src=\"Momentum和NAG的运行差别.png\" alt=\"Momentum和NAG的运行差别\" /><figcaption aria-hidden=\"true\">Momentum和NAG的运行差别</figcaption>\r\n</figure>\r\n<p>NAG的另外等价一种表达方式可以写成： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\nd^t &amp;= \\gamma d^{t-1} + \\nabla_\\theta J(\\theta^{t-1}) + \\gamma(\\nabla_\\theta J(\\theta^{t-1}) - \\nabla_\\theta J(\\theta^{t-2}))\\\\\r\n\\theta^t &amp;= \\theta^{t-1} - \\eta d^t    \r\n\\end{aligned}\r\n\\]</span> 这个等价的推理过程可见<a href=\"https://zhuanlan.zhihu.com/p/22810533/\">比Momentum更快：揭开Nesterov Accelerated Gradient的真面目</a>，这里表达的意思就是“如果这次的梯度比上次的梯度变大了，那么有理由相信它会继续变大下去，那就把预计要增大的部分提前加进来；如果相比上次变小了，那么需要动量项上需要做相应的减少”，<span class=\"math inline\">\\(\\gamma(\\nabla_\\theta J(\\theta^{t-1}) - \\nabla_\\theta J(\\theta^{t-2}))\\)</span>这一项其实是利用了近似二阶导数信息。因此NAG相比于Momentum要快些。</p>\r\n<h1 id=\"adagrad\">Adagrad</h1>\r\n<p>Adagrad的主要思想是让学习速率能够自动调整，使得更新频繁的参数有更小的学习速率，更新少的参数有更大的学习速率。</p>\r\n<p>这里的意思是每个参数的学习速率各不相同，使用每个参数每次的梯度平方和来计算，例如对于参数<span class=\"math inline\">\\(\\theta\\)</span>，其第<span class=\"math inline\">\\(i\\)</span>次更新的梯度<span class=\"math inline\">\\(g^i\\)</span>，初始学习速率为<span class=\"math inline\">\\(\\eta\\)</span>，那么第<span class=\"math inline\">\\(t\\)</span>次更新的时候，该参数的学习速率<span class=\"math inline\">\\(\\eta^t\\)</span>的计算方式如下： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    v^t &amp;= \\sum\\limits_{i=1}^t (g^i)^2\\\\\r\n    \\eta^t &amp;= \\frac{\\eta}{\\sqrt{v^t + \\epsilon}}\\\\\r\n\\end{aligned}\r\n\\]</span> 如果每次梯度都很大，那么这个参数就被认为是频繁更新，学习速率衰减比较快，否则学习速率衰减比较慢，可以说Adagrad是一种学习速率衰减方法。</p>\r\n<h1 id=\"adadelta\">Adadelta</h1>\r\n<p>在Adagrad中，由于<span class=\"math inline\">\\(v^t\\)</span>是单调递增的，因此Adagrad很容易导致提前停止训练，所以在Adadelta中将其改成不关心全局的梯度，而是只关心最近一段时间的梯度，这里将<span class=\"math inline\">\\(v^t\\)</span>改成如下计算方式：</p>\r\n<p><span class=\"math display\">\\[\r\nv^t = \\gamma v_{t-1, j} + (1 - \\gamma)(g^t)^2\r\n\\]</span></p>\r\n<h1 id=\"rmsprop\">RMSProp</h1>\r\n<p>RMSProp算法是Adadelta的一个特列：<span class=\"math inline\">\\(\\gamma=0.9\\)</span></p>\r\n<h1 id=\"adamadaptive-moment-estimation\">Adam（Adaptive Moment Estimation）</h1>\r\n<p>Adam其实就是在Adadelta的基础上添加了momentum，然后稍微做了些修正，修正是因为考虑到其初值是0，会对动量和二阶动量的估计造成偏差，其实这里可以看出，修正之后<span class=\"math inline\">\\(\\hat{m}_{1,j}\\)</span>其实就是<span class=\"math inline\">\\(g_{1, j}\\)</span>，<span class=\"math inline\">\\(\\hat{v}_{1,j}\\)</span>也是类似的。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\nm^t &amp;= \\beta_1 m^{t-1} + (1 - \\beta_1)g^t\\\\\r\nv^t &amp;= \\beta_2 v^{t-1} + (1 - \\beta_2)(g^t)^2\\\\\r\n\\hat{m}^t &amp;= \\frac{m^t}{1 - \\beta^t_1}\\\\\r\n\\hat{v}^t &amp;= \\frac{v^t}{1 - \\beta^t_2}\r\n\\end{aligned}\r\n\\]</span> 参数<span class=\"math inline\">\\(\\theta\\)</span>的更新变成： <span class=\"math display\">\\[\r\n\\theta^t = \\theta^t - \\frac{\\eta}{\\sqrt{\\hat{v}^t}+\\epsilon}\\hat{m}^t\r\n\\]</span> Adam在实际使用中也是非常常见的优化方法，既有动量方法的优点，又能自动调整学习速率。</p>\r\n<h1 id=\"nadam\">NAdam</h1>\r\n<p>既然Adam都使用了动量方法，那肯定要试一下将NAG（Nesterov accelerated gradient）方法加入进来，因此就有了NAdam。</p>\r\n<p>对于NAG，这里做了一些变化，首先令<span class=\"math inline\">\\(\\hat{\\theta}^{t-1} = \\theta^{t-1} - \\eta m^{t-1}\\)</span>，那么Nesterov Momentum可以写为： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\nm^t &amp;= \\beta_1 m^{t-1} + (1 - \\beta_1)\\nabla_\\theta J(\\theta^{t-1} - \\eta m^{t-1})\\\\\r\n&amp;=\\beta_1 m^{t-1} + (1 - \\beta_1)\\nabla_\\theta J(\\hat{\\theta}^t)\r\n\\end{aligned}\r\n\\]</span> 则NAG的更新可以写成： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\theta^t &amp;= \\theta^{t-1} - \\eta m^t\\\\\r\n    \\hat{\\theta}^t &amp;= \\theta^t - \\eta \\beta_1 m^t\\\\\r\n    &amp;=\\theta^{t-1} - \\eta m^t - \\eta \\beta_1 m^t\\\\\r\n    &amp;=\\theta^{t-1} - \\eta (\\beta_1 m^{t-1} + (1 - \\beta_1)\\nabla_\\theta J(\\hat{\\theta}^t)) - \\eta \\beta_1 m^t\\\\\r\n    &amp;=\\hat{\\theta}^{t-1} - \\eta (\\beta_1 m^t + (1 - \\beta_1)\\nabla_\\theta J(\\hat{\\theta}^t))\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>根据NAG的思路得到了<span class=\"math inline\">\\(m^t\\)</span>，那么在Adam中得到<span class=\"math inline\">\\(\\hat{m}^t = \\frac{m^t}{1 - \\beta^t_1} = \\frac{\\beta_1 m^{t-1} + (1 - \\beta_1)\\nabla_\\theta J(\\hat{\\theta}^t)}{1 - \\beta^t_1}\\)</span>之后，<span class=\"math inline\">\\(\\hat{\\theta}^t\\)</span>的计算变为： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\hat{\\theta}^t &amp;=\\hat{\\theta}^{t-1} - \\frac{\\eta}{\\sqrt{\\hat{v}^t}+\\epsilon}\\hat{m}^t\\\\\r\n    &amp;=\\hat{\\theta}^{t-1} - \\frac{\\eta}{\\sqrt{\\hat{v}^t}+\\epsilon}\\frac{\\beta_1 m^{t-1} + (1 - \\beta_1)\\nabla_\\theta J(\\hat{\\theta}^t)}{1 - \\beta^t_1}\\\\\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>这里如果用<span class=\"math inline\">\\(\\beta_1\\hat{m}^{t-1}\\)</span>来替换<span class=\"math inline\">\\(\\frac{\\beta_1 m^{t-1}}{1 - \\beta^t_1}\\)</span>，则可以简化为： <span class=\"math display\">\\[\r\n\\hat{\\theta}^t = \\hat{\\theta}^{t-1} - \\frac{\\eta}{\\sqrt{\\hat{v}^t}+\\epsilon}(\\beta_1\\hat{m}^{t-1} + \\frac{(1 - \\beta_1)\\nabla_\\theta J(\\hat{\\theta}^t)}{1 - \\beta^t_1})\r\n\\]</span></p>\r\n<p>完整的NAdam表达如下： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n\\hat{\\theta}^{t-1} &amp;= \\theta^{t-1} - \\eta m^{t-1}\\\\\r\ng^t &amp;= \\nabla_\\theta J(\\hat{\\theta}^t)\\\\\r\nm^t &amp;= \\beta_1 m^{t-1} + (1 - \\beta_1)g^t\\\\\r\nv^t &amp;= \\beta_2 v^{t-1} + (1 - \\beta_2)(g^t)^2\\\\\r\n\\hat{m}^t &amp;= \\frac{m^t}{1 - \\beta^t_1}\\\\\r\n\\hat{v}^t &amp;= \\frac{v^t}{1 - \\beta^t_2}\\\\\r\n\\hat{\\theta}^t &amp;= \\hat{\\theta}^{t-1} - \\frac{\\eta}{\\sqrt{\\hat{v}^t}+\\epsilon}(\\beta_1\\hat{m}^{t-1} + \\frac{(1 - \\beta_1)}{1 - \\beta^t_1} g^t)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>一般如果使用Adam有效的场景，都可以使用NAdam来得到更好的效果。</p>\r\n","site":{"data":{}},"excerpt":"","more":"<p>梯度下降（Gradient Descnet, GD）是目前DNN（Deep Neural Network）中使用最多的优化算法，相同的数据，相同的DNN结构，相同的初始化参数值，使用不同的优化算法会得到截然不同的结果，因此优化算法非常重要，但是梯度下降法有一些缺点，为了对这些缺点进行修正，GD优化算法经历了很多演进，但是目前仍然没有一种普适的优化算法，这里根据论文《An overview of gradient descent optimization algorithms》的描述，将GD算法的的各种改进做了一些总结。</p>\r\n<h1 id=\"最初的gd算法\">最初的GD算法</h1>\r\n<p>对于待优化的函数<span class=\"math inline\">\\(J(\\theta)\\)</span>，其中<span class=\"math inline\">\\(\\theta\\)</span>是待优化参数，GD算法的思路非常简洁，即指定学习速率<span class=\"math inline\">\\(\\eta\\)</span>和参数初始值<span class=\"math inline\">\\(\\theta^0\\)</span>，按照如下方式迭代的更新参数，其中<span class=\"math inline\">\\(\\nabla_\\theta J(\\theta)\\)</span>表示梯度，<span class=\"math inline\">\\(\\theta^i\\)</span>表示第<span class=\"math inline\">\\(i\\)</span>次更新后的参数值， <span class=\"math display\">\\[\r\n\\theta^{i+1} = \\theta^i - \\eta \\nabla_\\theta J(\\theta)\r\n\\]</span></p>\r\n<p>对于凸问题或者拟凸问题，这样的迭代最终会使得参数在最优点附近震荡，对于非凸且非拟凸的问题，这样的迭代大概率会最终在一个局部极小值附近震荡。</p>\r\n<p>这里还有个问题是<span class=\"math inline\">\\(J(\\theta)\\)</span>往往无法准确地计算，因为<span class=\"math inline\">\\(J(\\theta)\\)</span>和所有的训练数据相关，如果每一次迭代都去完整的遍历一遍所有数据，那么时间上可能不可接受，因此有了最初的两个GD算法的变种：随机梯度下降（Stochastic gradient descent，SGD）、批梯度下降（Mini-batch gradient descent）</p>\r\n<h1 id=\"随机梯度下降stochastic-gradient-descentsgd\">随机梯度下降（Stochastic gradient descent，SGD）</h1>\r\n<p>针对<span class=\"math inline\">\\(J(\\theta)\\)</span>和所有的训练数据相关从而导致计算耗时长的问题，随机梯度下降的思想非常朴素：每次迭代时随机选取一个样本进行训练。如下所示，其中<span class=\"math inline\">\\(\\nabla_\\theta J(\\theta;x^{(i+1)},y^{(i+1)})\\)</span>表示根据第<span class=\"math inline\">\\(i+1\\)</span>次随机选择到的样本标签对<span class=\"math inline\">\\((x^{(i+1)},y^{(i+1)})\\)</span>计算得到的损失函数。 <span class=\"math display\">\\[\r\n\\theta^{i+1} = \\theta^i - \\eta \\nabla_\\theta J(\\theta;x^{(i+1)},y^{(i+1)})\r\n\\]</span></p>\r\n<h1 id=\"小批梯度下降mini-batch-gradient-descent\">小批梯度下降（Mini-batch gradient descent）</h1>\r\n<p>和随机梯度下降的动机相同，为了简化损失函数的计算，这里每一次使用一批数据（n个样本）来进行损失函数的计算（本质上是将多个损失函数进行求和或者求平均），如下所示： <span class=\"math display\">\\[\r\n\\theta^{i+1} = \\theta^i - \\eta \\nabla_\\theta J(\\theta;x^{((i+1)\\times n:(i+2)\\times n)},y^{((i+1)\\times n:(i+2)\\times n)})\r\n\\]</span></p>\r\n<p>这里需要注意的是，很多深度学习框架的实现中，SGD代表的不是随机梯度下降，而是随机小批梯度下降，即随机打乱数据之后，再使用小批梯度下降进行优化。</p>\r\n<p>这几个方法没有什么本质上的区别，在数据的选择上，如果每次迭代时计算梯度所用到的数据越多，那么梯度的计算自然更加准确，但是数据过多，又会引起计算时间增加导致迭代缓慢，因此每次选择多少个数据来进行计算，即批大小（batch size）的选择就是在这两个问题上进行权衡。至于为什么要随机选择，自然是为了将数据充分混合，在pytorch的dataloader实现中，一般是在最初将数据shuffle一次，然后每次迭代都顺序选择下一批数据。</p>\r\n<p>梯度下降算法的主要难点： - 学习速率的选择，过大的学习速率会导致算法不能很好的收敛（在局部极小值附近大幅度震荡），因此一般使用初始大学习速率，然后学习速率逐渐衰减的方式，但是学习速率的初始值和衰减速度同样难以设置。 - 对于存在很多鞍点的问题，梯度下降算法理论上无法正确的处理在鞍点的情况（但实际上每次随机使用不同的数据计算梯度，很难得到一致的鞍点，总有可能随机到一些数据计算出来在原来的鞍点处又有梯度的情况，因此后面的一些说能够有助于逃离鞍点的动量化方法，我认为其实在这方面意义不是非常大，这可能也是很多论文中依旧在使用SGD，而且效果也还可以的原因） - 在狭长的损失函数区域中，梯度下降算法非常容易震荡，如下图所示。</p>\r\n<figure>\r\n<img src=\"狭长形状的损失函数下SGD的运行示意图.png\" alt=\"狭长形状的损失函数下SGD的运行示意图\" /><figcaption aria-hidden=\"true\">狭长形状的损失函数下SGD的运行示意图</figcaption>\r\n</figure>\r\n<h1 id=\"momentum\">Momentum</h1>\r\n<p>为了解决上面的最后一个问题，在SGD中可以使用动量项（Momentum term）进行改进，类似于物理上的动量概念：运动方向的改变不是瞬间变化的，而是缓慢改变，平滑过渡。动量项可以表达为： <span class=\"math display\">\\[\r\nm^t = \\gamma m^{t-1} + \\nabla_\\theta J(\\theta^{t-1})\r\n\\]</span></p>\r\n<p>有些地方也写成 <span class=\"math display\">\\[\r\nm^t = \\gamma m^{t-1} + (1 - \\gamma)\\nabla_\\theta J(\\theta^{t-1})\r\n\\]</span></p>\r\n<p>其中<span class=\"math inline\">\\(m^0\\)</span>一般初始化为0，<span class=\"math inline\">\\(0 &lt; \\gamma &lt;1\\)</span>，<span class=\"math inline\">\\(\\gamma\\)</span>一般被设置为<span class=\"math inline\">\\(0.9\\)</span>，这个表达式的意思就是将之前的梯度方向累积下来，优化算法在迭代时，不再是直接向着梯度方向更新，而是使用动量来更新： <span class=\"math display\">\\[\r\n\\theta^t = \\theta^{t-1}- \\eta m^t\r\n\\]</span> 这样的好处是可以避免更新方向的频繁变化，减少震荡，加速收敛。</p>\r\n<p>Momentum可以抑制更新过程中的震荡，以狭长形状的损失函数为例，使用动量项之后，其更新轨迹类似下图：</p>\r\n<figure>\r\n<img src=\"狭长形状的损失函数下Momentum的运行示意图.png\" alt=\"狭长形状的损失函数下Momentum的运行示意图\" /><figcaption aria-hidden=\"true\">狭长形状的损失函数下Momentum的运行示意图</figcaption>\r\n</figure>\r\n<h1 id=\"nesterov-accelerated-gradientnag\">Nesterov accelerated gradient（NAG）</h1>\r\n<p>NAG方法对Momentum方法进行了改进，其思想非常简单：既然需要沿着动量方向进行参数更新，那么梯度的计也可以在动量更新之后再进行，即沿着动量方向进行参数更新之后再计算梯度来修正动量，最后再使用修正后的动量来更新。</p>\r\n<p>可以通俗的理解一定程度上预测了Momentum下一步的位置，然后根据未来的位置计算梯度来对动量项进行修正。</p>\r\n<p>因此这里将动量定义为： <span class=\"math display\">\\[\r\nm^t = \\gamma m^{t-1} + \\nabla_\\theta J(\\theta^{t-1} - \\eta \\gamma m^{t-1})\r\n\\]</span> 这样的动量项也被称为Nesterov Momentum，参数更新过程依旧不变：</p>\r\n<p><span class=\"math display\">\\[\r\n\\theta^t = \\theta^{t-1} - \\eta m^t\r\n\\]</span></p>\r\n<p>Momentum和NAG的运行差别如下图所示（图片来自于<a href=\"https://zhuanlan.zhihu.com/p/21486826\">《路遥知马力——Momentum - 无痛的机器学习 - 知乎专栏》</a>），上面的是Momentum方法，下面的是NAG方法。</p>\r\n<figure>\r\n<img src=\"Momentum和NAG的运行差别.png\" alt=\"Momentum和NAG的运行差别\" /><figcaption aria-hidden=\"true\">Momentum和NAG的运行差别</figcaption>\r\n</figure>\r\n<p>NAG的另外等价一种表达方式可以写成： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\nd^t &amp;= \\gamma d^{t-1} + \\nabla_\\theta J(\\theta^{t-1}) + \\gamma(\\nabla_\\theta J(\\theta^{t-1}) - \\nabla_\\theta J(\\theta^{t-2}))\\\\\r\n\\theta^t &amp;= \\theta^{t-1} - \\eta d^t    \r\n\\end{aligned}\r\n\\]</span> 这个等价的推理过程可见<a href=\"https://zhuanlan.zhihu.com/p/22810533/\">比Momentum更快：揭开Nesterov Accelerated Gradient的真面目</a>，这里表达的意思就是“如果这次的梯度比上次的梯度变大了，那么有理由相信它会继续变大下去，那就把预计要增大的部分提前加进来；如果相比上次变小了，那么需要动量项上需要做相应的减少”，<span class=\"math inline\">\\(\\gamma(\\nabla_\\theta J(\\theta^{t-1}) - \\nabla_\\theta J(\\theta^{t-2}))\\)</span>这一项其实是利用了近似二阶导数信息。因此NAG相比于Momentum要快些。</p>\r\n<h1 id=\"adagrad\">Adagrad</h1>\r\n<p>Adagrad的主要思想是让学习速率能够自动调整，使得更新频繁的参数有更小的学习速率，更新少的参数有更大的学习速率。</p>\r\n<p>这里的意思是每个参数的学习速率各不相同，使用每个参数每次的梯度平方和来计算，例如对于参数<span class=\"math inline\">\\(\\theta\\)</span>，其第<span class=\"math inline\">\\(i\\)</span>次更新的梯度<span class=\"math inline\">\\(g^i\\)</span>，初始学习速率为<span class=\"math inline\">\\(\\eta\\)</span>，那么第<span class=\"math inline\">\\(t\\)</span>次更新的时候，该参数的学习速率<span class=\"math inline\">\\(\\eta^t\\)</span>的计算方式如下： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    v^t &amp;= \\sum\\limits_{i=1}^t (g^i)^2\\\\\r\n    \\eta^t &amp;= \\frac{\\eta}{\\sqrt{v^t + \\epsilon}}\\\\\r\n\\end{aligned}\r\n\\]</span> 如果每次梯度都很大，那么这个参数就被认为是频繁更新，学习速率衰减比较快，否则学习速率衰减比较慢，可以说Adagrad是一种学习速率衰减方法。</p>\r\n<h1 id=\"adadelta\">Adadelta</h1>\r\n<p>在Adagrad中，由于<span class=\"math inline\">\\(v^t\\)</span>是单调递增的，因此Adagrad很容易导致提前停止训练，所以在Adadelta中将其改成不关心全局的梯度，而是只关心最近一段时间的梯度，这里将<span class=\"math inline\">\\(v^t\\)</span>改成如下计算方式：</p>\r\n<p><span class=\"math display\">\\[\r\nv^t = \\gamma v_{t-1, j} + (1 - \\gamma)(g^t)^2\r\n\\]</span></p>\r\n<h1 id=\"rmsprop\">RMSProp</h1>\r\n<p>RMSProp算法是Adadelta的一个特列：<span class=\"math inline\">\\(\\gamma=0.9\\)</span></p>\r\n<h1 id=\"adamadaptive-moment-estimation\">Adam（Adaptive Moment Estimation）</h1>\r\n<p>Adam其实就是在Adadelta的基础上添加了momentum，然后稍微做了些修正，修正是因为考虑到其初值是0，会对动量和二阶动量的估计造成偏差，其实这里可以看出，修正之后<span class=\"math inline\">\\(\\hat{m}_{1,j}\\)</span>其实就是<span class=\"math inline\">\\(g_{1, j}\\)</span>，<span class=\"math inline\">\\(\\hat{v}_{1,j}\\)</span>也是类似的。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\nm^t &amp;= \\beta_1 m^{t-1} + (1 - \\beta_1)g^t\\\\\r\nv^t &amp;= \\beta_2 v^{t-1} + (1 - \\beta_2)(g^t)^2\\\\\r\n\\hat{m}^t &amp;= \\frac{m^t}{1 - \\beta^t_1}\\\\\r\n\\hat{v}^t &amp;= \\frac{v^t}{1 - \\beta^t_2}\r\n\\end{aligned}\r\n\\]</span> 参数<span class=\"math inline\">\\(\\theta\\)</span>的更新变成： <span class=\"math display\">\\[\r\n\\theta^t = \\theta^t - \\frac{\\eta}{\\sqrt{\\hat{v}^t}+\\epsilon}\\hat{m}^t\r\n\\]</span> Adam在实际使用中也是非常常见的优化方法，既有动量方法的优点，又能自动调整学习速率。</p>\r\n<h1 id=\"nadam\">NAdam</h1>\r\n<p>既然Adam都使用了动量方法，那肯定要试一下将NAG（Nesterov accelerated gradient）方法加入进来，因此就有了NAdam。</p>\r\n<p>对于NAG，这里做了一些变化，首先令<span class=\"math inline\">\\(\\hat{\\theta}^{t-1} = \\theta^{t-1} - \\eta m^{t-1}\\)</span>，那么Nesterov Momentum可以写为： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\nm^t &amp;= \\beta_1 m^{t-1} + (1 - \\beta_1)\\nabla_\\theta J(\\theta^{t-1} - \\eta m^{t-1})\\\\\r\n&amp;=\\beta_1 m^{t-1} + (1 - \\beta_1)\\nabla_\\theta J(\\hat{\\theta}^t)\r\n\\end{aligned}\r\n\\]</span> 则NAG的更新可以写成： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\theta^t &amp;= \\theta^{t-1} - \\eta m^t\\\\\r\n    \\hat{\\theta}^t &amp;= \\theta^t - \\eta \\beta_1 m^t\\\\\r\n    &amp;=\\theta^{t-1} - \\eta m^t - \\eta \\beta_1 m^t\\\\\r\n    &amp;=\\theta^{t-1} - \\eta (\\beta_1 m^{t-1} + (1 - \\beta_1)\\nabla_\\theta J(\\hat{\\theta}^t)) - \\eta \\beta_1 m^t\\\\\r\n    &amp;=\\hat{\\theta}^{t-1} - \\eta (\\beta_1 m^t + (1 - \\beta_1)\\nabla_\\theta J(\\hat{\\theta}^t))\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>根据NAG的思路得到了<span class=\"math inline\">\\(m^t\\)</span>，那么在Adam中得到<span class=\"math inline\">\\(\\hat{m}^t = \\frac{m^t}{1 - \\beta^t_1} = \\frac{\\beta_1 m^{t-1} + (1 - \\beta_1)\\nabla_\\theta J(\\hat{\\theta}^t)}{1 - \\beta^t_1}\\)</span>之后，<span class=\"math inline\">\\(\\hat{\\theta}^t\\)</span>的计算变为： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\hat{\\theta}^t &amp;=\\hat{\\theta}^{t-1} - \\frac{\\eta}{\\sqrt{\\hat{v}^t}+\\epsilon}\\hat{m}^t\\\\\r\n    &amp;=\\hat{\\theta}^{t-1} - \\frac{\\eta}{\\sqrt{\\hat{v}^t}+\\epsilon}\\frac{\\beta_1 m^{t-1} + (1 - \\beta_1)\\nabla_\\theta J(\\hat{\\theta}^t)}{1 - \\beta^t_1}\\\\\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>这里如果用<span class=\"math inline\">\\(\\beta_1\\hat{m}^{t-1}\\)</span>来替换<span class=\"math inline\">\\(\\frac{\\beta_1 m^{t-1}}{1 - \\beta^t_1}\\)</span>，则可以简化为： <span class=\"math display\">\\[\r\n\\hat{\\theta}^t = \\hat{\\theta}^{t-1} - \\frac{\\eta}{\\sqrt{\\hat{v}^t}+\\epsilon}(\\beta_1\\hat{m}^{t-1} + \\frac{(1 - \\beta_1)\\nabla_\\theta J(\\hat{\\theta}^t)}{1 - \\beta^t_1})\r\n\\]</span></p>\r\n<p>完整的NAdam表达如下： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n\\hat{\\theta}^{t-1} &amp;= \\theta^{t-1} - \\eta m^{t-1}\\\\\r\ng^t &amp;= \\nabla_\\theta J(\\hat{\\theta}^t)\\\\\r\nm^t &amp;= \\beta_1 m^{t-1} + (1 - \\beta_1)g^t\\\\\r\nv^t &amp;= \\beta_2 v^{t-1} + (1 - \\beta_2)(g^t)^2\\\\\r\n\\hat{m}^t &amp;= \\frac{m^t}{1 - \\beta^t_1}\\\\\r\n\\hat{v}^t &amp;= \\frac{v^t}{1 - \\beta^t_2}\\\\\r\n\\hat{\\theta}^t &amp;= \\hat{\\theta}^{t-1} - \\frac{\\eta}{\\sqrt{\\hat{v}^t}+\\epsilon}(\\beta_1\\hat{m}^{t-1} + \\frac{(1 - \\beta_1)}{1 - \\beta^t_1} g^t)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>一般如果使用Adam有效的场景，都可以使用NAdam来得到更好的效果。</p>\r\n"},{"title":"回归方法总结","date":"2020-05-06T02:15:58.000Z","mathjax":true,"_content":"\n在机器学习中，回归方法主要有线性回归和逻辑回归两种，其中线性回归是真的用于回归任务，而逻辑回归主要用于分类任务。\n\n# 线性回归（Linear Regression, LR）\n线性回归的表达方式很简单：$\\hat{y} = x^Tw + b$，其中$x$是一个表示为$R^n$的样本，$\\hat{y}$表示对样本$x$的回归值，$w,\\ b$是线性回归的参数。\n\n如果令$X=\\begin{bmatrix}x_1 & 1\\\\ x_2 & 1\\\\ \\vdots & \\vdots \\\\ x_m & 1 \\end{bmatrix} \\in R^{m \\times (n+1)}$表示所有数据的扩展矩阵，其中$m$是数据数量，$Y=\\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_m\\end{bmatrix} \\in R^{m \\times 1}$表示数据的标签，$W \\in R^{(n+1) \\times 1}$表示线性回归的参数，其中$n$是数据特征维数，则线性回归可以表示为$\\hat{Y} = XW$。\n\n如果首先对数据和标签进行了中心化，即$\\sum_i x_i=0,\\ \\sum_i y_i=0$，那么可以令$X=\\begin{bmatrix}x_1\\\\ x_2\\\\ \\vdots\\\\ x_m \\end{bmatrix} \\in R^{m \\times n}$表示所有数据的矩阵，其中$m$是数据数量，$W \\in R^{n \\times 1}$表示线性回归的参数。\n\n## 线性回归的最小二乘法求解\n如果定义线性回归的损失函数为$L = (\\hat{Y} - Y)^T(\\hat{Y} - Y)$，虽小化损失函数即最小化均方误差。\n\n损失函数可以进一步表达为：\n$$\n\\begin{aligned}\nL(W) &= (XW - Y)^T (XW - Y) \\\\\n&= W^TX^TXW - YXW - XWY + Y^TY\\\\\n\\end{aligned}\n$$\n求损失函数对$w$的偏导：\n$$\n\\begin{aligned}\ndL(W) &= Z^TdZ + dZ^TZ,\\ Z = XW-Y\\\\\n&=2Z^TdZ\\\\\n&=2Z^Td(XW-Y)\\\\\n&=2Z^TXdW\\\\\n\\frac{\\partial L(W)}{\\partial W} &= X^TZ\\\\\n&=X^T(XW-Y)\n\\end{aligned}\n$$\n如果$X^TX$可逆，则令$\\frac{\\partial L(W)}{\\partial W} = 0$可得$W = (X^TX)^{-1}X^TY$，如果$X^TX$不可逆，则可以利用梯度下降法进行求解。\n\n## 最小二乘法线性回归的理解\n如果假设数据标签服从高斯分布，则可以根据最大似然法推导得出最小二乘法，这里不再赘述。\n\n如果首先对数据进行了中心化，则$X^TY$是表示各个属性和回归值相关性的一个向量，$X^TX$表示的是数据的协方差矩阵。\n\n如果数据量小于特征个数（$X$中肯定有列相关的情况），那么$X^TX$不可逆，且容易导致过拟合。\n\n如果一些属性之间存在近似线性关系甚至线性关系，即$X^TX$是个病态矩阵甚至$X^TX$中存在相关（$X^TX$不可逆，可以看做病态矩阵的一种极端情况），根据$X^TXW = X^TY$，当$X^TY$稍作变动，所求得的$W$变化会非常大，一般情况下，没有做特征工程的数据，都会存在特征之间有近似线性关系的情况，因此线性回归的一个问题是对噪声非常敏感。\n\n如果$X^TX$不可逆，则$X^TXW = X^TY$的解不止一个，这种时候，为了防止过拟合，同时也防止对噪声数据过于敏感，可以在线性回归的损失函数上加一个正则化项，因此就有了岭回归和LASSO回归。\n\n## 岭回归（ridge regression, RR）\n岭回归在线性回归的损失函数中加入了L2正则化，即$L(W) = (XW - Y)^T (XW - Y) + \\lambda W^TW$。\n对该损失函数求梯度如下：\n$$\n\\begin{aligned}\n    dL(W) &= 2(XW-Y)^TXdW + 2\\lambda W^TdW\\\\\n    &=(2(XW-Y)^TX + 2\\lambda W^T)dW\\\\\n    \\frac{\\partial L(W)}{\\partial W} &= 2X^T(XW-Y) + 2\\lambda W\n\\end{aligned}\n$$\n令$\\frac{\\partial L(W)}{\\partial W} = 0$可得$(X^TX + \\lambda I)W = X^TY$\n\n## 对岭回归的理解\n这里和线性回归的区别在于$\\lambda I$这一项。\n\n由于$X^TX$是实对称矩阵，其特征分解可以表示为$X^TX = Q\\Lambda Q^T$，其中$Q$是由特征向量构成的正交矩阵。\n\n那么$X^TX + \\lambda I = Q\\Lambda Q^T + \\lambda I = Q (\\Lambda + \\lambda I) Q^T$，可以看出加入了L2正则化之后，可以理解为一定程度上可以防止特征值为0，也可以理解为减小了$X^TX$为病态矩阵的影响。\n\n## LASSO回归（Least absolute shrinkage and selection operator Regression）\n类似于岭回归，LASSO回归在线性回归的损失函数中加入了L1正则化，即$L(W) = (XW - Y)^T (XW - Y) + \\lambda ||W||_1$。\n\n如果将$W$分解成$W^+,\\ W^-$两部分，其中$W^+$只包含非负数，$W^-$只包含非正数，$W = W^+ - W^-$，那么$L(W^+, W^-) = (XW - Y)^T (XW - Y) + \\lambda (I^n)^TW^+ + \\lambda (I^n)^TW^-$\n\n为了最小化$L(W^+, W^-)$，求导可得：\n$$\n\\begin{aligned}\n    dL(W^+, W^-) &= 2Z^TdZ + \\lambda (I^n)^TdW^+ + \\lambda (I^n)^TdW^-,\\ Z = XW-Y\\\\\n    &=2Z^TXdW + \\lambda (I^n)^TdW^+ + \\lambda (I^n)^TdW^-\\\\\n    &=2Z^TXdW^+ - 2Z^TXdW^- + \\lambda (I^n)^TdW^+ + \\lambda (I^n)^TdW^-\\\\\n    &=(\\lambda (I^n)^T + 2Z^TX)dW^+ + (\\lambda (I^n)^T - 2Z^TX)dW^-\\\\\n    \\frac{\\partial L(W^+, W^-)}{\\partial W^+} &= \\lambda I^n + 2X^TZ\\\\\n    &=\\lambda I^n + 2X^T(XW-Y)\\\\\n    \\frac{\\partial L(W^+, W^-)}{\\partial W^-} &= \\lambda I^n - 2X^TZ\\\\\n    &=\\lambda I^n - 2X^T(XW-Y)\\\\\n\\end{aligned}\n$$\n\n令导数为0：\n$$\n\\begin{aligned}\n    \\lambda I^n + 2X^T(XW-Y) = 0\\\\\n    \\Rightarrow 2X^TXW = 2X^TY - \\lambda I^n\\\\\n\n    \\lambda I^n - 2X^T(XW-Y) = 0\\\\\n    \\Rightarrow 2X^TXW = 2X^TY + \\lambda I^n \\\\\n\\end{aligned}\n$$\n可以发现，如果不是$\\lambda$为0，则两个导数不可能同时为0，所以希望通过导数为0得到损失函数极小值的办法行不通。\n\nLASSO回归可以考虑坐标下降法或者是转换为带约束条件的优化问题来解决。\n\n## 对LASSO回归的理解\n如果是基于梯度的方法来求解LASSO回归，则可以发现，如果$W$中某个元素当前的值为负数，则其梯度中包含$-\\lambda$这一项，向着梯度相反的方向更新的话，则需要加上$\\alpha \\lambda$这一项，其实是在让$W$中的元素趋近于0，即让参数变得稀疏，有助于模型的可解释性，同时$\\alpha \\lambda$这一项也倾向于使得这个元素的绝对值变小，有助于缓解过拟合。\n\n# 逻辑回归\n逻辑回归表面上是个回归方法，其实是用于二分类任务的，只不过需要回归的值变成了$ln\\frac{P(y=1)}{P(y=0)}$，其中$P(y=1)$表示样本类别为1的概率，$P(y=0)$表示样本类别为0的概率，因为是二分类：$P(y=1) = 1 - P(y=0)$，所以$\\frac{P(y=1)}{P(y=0)}$其实就是$\\frac{P(y=1)}{1 - P(y=1)}$即类别为1的对数几率，因此逻辑回归也叫对数几率回归。\n\n如果令$x \\in R^n$表示一个样本，$w \\in R^n,\\ b\\in R^n$表示逻辑回归的参数，其中$n$是数据特征维数，则逻辑回归可以表示为$ln\\frac{P(y=1)}{P(y=0)} = z,\\ z = w^Tx + b$，即使用$w^Tx + b$去回归类别为1的对数几率。\n\n因为$ln\\frac{P(y=1)}{1 - P(y=1)} = z,\\ P(y=1) = 1 - P(y=0)$，所以可以得出$P(y=1) = \\frac{1}{1 + e^{-z}},\\ P(y=0) = \\frac{1}{1+e^z}$。\n\n其对数似然函数可以写成$\\sum\\limits_X ylnP(y=1) + (1-y)lnP(y=0)$，这个表达式就是负的二值交叉熵，因此极大化其对数似然，就是极小化二值交叉熵损失。\n\n在分类CNN中线性回归的部分被替换为卷积神经网络，最后一层的激活函数使用Sigmoid或者Softmax，将回归输出转换为（多）类别概率，如果使用（二值）交叉熵损失函数，其实就是在最大化对数似然。\n\n\n\n\n\n\n","source":"_posts/学习笔记/回归方法总结.md","raw":"---\ntitle: 回归方法总结\ndate: 2020-05-06 10:15:58\ntags: [机器学习]\nmathjax: true\n---\n\n在机器学习中，回归方法主要有线性回归和逻辑回归两种，其中线性回归是真的用于回归任务，而逻辑回归主要用于分类任务。\n\n# 线性回归（Linear Regression, LR）\n线性回归的表达方式很简单：$\\hat{y} = x^Tw + b$，其中$x$是一个表示为$R^n$的样本，$\\hat{y}$表示对样本$x$的回归值，$w,\\ b$是线性回归的参数。\n\n如果令$X=\\begin{bmatrix}x_1 & 1\\\\ x_2 & 1\\\\ \\vdots & \\vdots \\\\ x_m & 1 \\end{bmatrix} \\in R^{m \\times (n+1)}$表示所有数据的扩展矩阵，其中$m$是数据数量，$Y=\\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_m\\end{bmatrix} \\in R^{m \\times 1}$表示数据的标签，$W \\in R^{(n+1) \\times 1}$表示线性回归的参数，其中$n$是数据特征维数，则线性回归可以表示为$\\hat{Y} = XW$。\n\n如果首先对数据和标签进行了中心化，即$\\sum_i x_i=0,\\ \\sum_i y_i=0$，那么可以令$X=\\begin{bmatrix}x_1\\\\ x_2\\\\ \\vdots\\\\ x_m \\end{bmatrix} \\in R^{m \\times n}$表示所有数据的矩阵，其中$m$是数据数量，$W \\in R^{n \\times 1}$表示线性回归的参数。\n\n## 线性回归的最小二乘法求解\n如果定义线性回归的损失函数为$L = (\\hat{Y} - Y)^T(\\hat{Y} - Y)$，虽小化损失函数即最小化均方误差。\n\n损失函数可以进一步表达为：\n$$\n\\begin{aligned}\nL(W) &= (XW - Y)^T (XW - Y) \\\\\n&= W^TX^TXW - YXW - XWY + Y^TY\\\\\n\\end{aligned}\n$$\n求损失函数对$w$的偏导：\n$$\n\\begin{aligned}\ndL(W) &= Z^TdZ + dZ^TZ,\\ Z = XW-Y\\\\\n&=2Z^TdZ\\\\\n&=2Z^Td(XW-Y)\\\\\n&=2Z^TXdW\\\\\n\\frac{\\partial L(W)}{\\partial W} &= X^TZ\\\\\n&=X^T(XW-Y)\n\\end{aligned}\n$$\n如果$X^TX$可逆，则令$\\frac{\\partial L(W)}{\\partial W} = 0$可得$W = (X^TX)^{-1}X^TY$，如果$X^TX$不可逆，则可以利用梯度下降法进行求解。\n\n## 最小二乘法线性回归的理解\n如果假设数据标签服从高斯分布，则可以根据最大似然法推导得出最小二乘法，这里不再赘述。\n\n如果首先对数据进行了中心化，则$X^TY$是表示各个属性和回归值相关性的一个向量，$X^TX$表示的是数据的协方差矩阵。\n\n如果数据量小于特征个数（$X$中肯定有列相关的情况），那么$X^TX$不可逆，且容易导致过拟合。\n\n如果一些属性之间存在近似线性关系甚至线性关系，即$X^TX$是个病态矩阵甚至$X^TX$中存在相关（$X^TX$不可逆，可以看做病态矩阵的一种极端情况），根据$X^TXW = X^TY$，当$X^TY$稍作变动，所求得的$W$变化会非常大，一般情况下，没有做特征工程的数据，都会存在特征之间有近似线性关系的情况，因此线性回归的一个问题是对噪声非常敏感。\n\n如果$X^TX$不可逆，则$X^TXW = X^TY$的解不止一个，这种时候，为了防止过拟合，同时也防止对噪声数据过于敏感，可以在线性回归的损失函数上加一个正则化项，因此就有了岭回归和LASSO回归。\n\n## 岭回归（ridge regression, RR）\n岭回归在线性回归的损失函数中加入了L2正则化，即$L(W) = (XW - Y)^T (XW - Y) + \\lambda W^TW$。\n对该损失函数求梯度如下：\n$$\n\\begin{aligned}\n    dL(W) &= 2(XW-Y)^TXdW + 2\\lambda W^TdW\\\\\n    &=(2(XW-Y)^TX + 2\\lambda W^T)dW\\\\\n    \\frac{\\partial L(W)}{\\partial W} &= 2X^T(XW-Y) + 2\\lambda W\n\\end{aligned}\n$$\n令$\\frac{\\partial L(W)}{\\partial W} = 0$可得$(X^TX + \\lambda I)W = X^TY$\n\n## 对岭回归的理解\n这里和线性回归的区别在于$\\lambda I$这一项。\n\n由于$X^TX$是实对称矩阵，其特征分解可以表示为$X^TX = Q\\Lambda Q^T$，其中$Q$是由特征向量构成的正交矩阵。\n\n那么$X^TX + \\lambda I = Q\\Lambda Q^T + \\lambda I = Q (\\Lambda + \\lambda I) Q^T$，可以看出加入了L2正则化之后，可以理解为一定程度上可以防止特征值为0，也可以理解为减小了$X^TX$为病态矩阵的影响。\n\n## LASSO回归（Least absolute shrinkage and selection operator Regression）\n类似于岭回归，LASSO回归在线性回归的损失函数中加入了L1正则化，即$L(W) = (XW - Y)^T (XW - Y) + \\lambda ||W||_1$。\n\n如果将$W$分解成$W^+,\\ W^-$两部分，其中$W^+$只包含非负数，$W^-$只包含非正数，$W = W^+ - W^-$，那么$L(W^+, W^-) = (XW - Y)^T (XW - Y) + \\lambda (I^n)^TW^+ + \\lambda (I^n)^TW^-$\n\n为了最小化$L(W^+, W^-)$，求导可得：\n$$\n\\begin{aligned}\n    dL(W^+, W^-) &= 2Z^TdZ + \\lambda (I^n)^TdW^+ + \\lambda (I^n)^TdW^-,\\ Z = XW-Y\\\\\n    &=2Z^TXdW + \\lambda (I^n)^TdW^+ + \\lambda (I^n)^TdW^-\\\\\n    &=2Z^TXdW^+ - 2Z^TXdW^- + \\lambda (I^n)^TdW^+ + \\lambda (I^n)^TdW^-\\\\\n    &=(\\lambda (I^n)^T + 2Z^TX)dW^+ + (\\lambda (I^n)^T - 2Z^TX)dW^-\\\\\n    \\frac{\\partial L(W^+, W^-)}{\\partial W^+} &= \\lambda I^n + 2X^TZ\\\\\n    &=\\lambda I^n + 2X^T(XW-Y)\\\\\n    \\frac{\\partial L(W^+, W^-)}{\\partial W^-} &= \\lambda I^n - 2X^TZ\\\\\n    &=\\lambda I^n - 2X^T(XW-Y)\\\\\n\\end{aligned}\n$$\n\n令导数为0：\n$$\n\\begin{aligned}\n    \\lambda I^n + 2X^T(XW-Y) = 0\\\\\n    \\Rightarrow 2X^TXW = 2X^TY - \\lambda I^n\\\\\n\n    \\lambda I^n - 2X^T(XW-Y) = 0\\\\\n    \\Rightarrow 2X^TXW = 2X^TY + \\lambda I^n \\\\\n\\end{aligned}\n$$\n可以发现，如果不是$\\lambda$为0，则两个导数不可能同时为0，所以希望通过导数为0得到损失函数极小值的办法行不通。\n\nLASSO回归可以考虑坐标下降法或者是转换为带约束条件的优化问题来解决。\n\n## 对LASSO回归的理解\n如果是基于梯度的方法来求解LASSO回归，则可以发现，如果$W$中某个元素当前的值为负数，则其梯度中包含$-\\lambda$这一项，向着梯度相反的方向更新的话，则需要加上$\\alpha \\lambda$这一项，其实是在让$W$中的元素趋近于0，即让参数变得稀疏，有助于模型的可解释性，同时$\\alpha \\lambda$这一项也倾向于使得这个元素的绝对值变小，有助于缓解过拟合。\n\n# 逻辑回归\n逻辑回归表面上是个回归方法，其实是用于二分类任务的，只不过需要回归的值变成了$ln\\frac{P(y=1)}{P(y=0)}$，其中$P(y=1)$表示样本类别为1的概率，$P(y=0)$表示样本类别为0的概率，因为是二分类：$P(y=1) = 1 - P(y=0)$，所以$\\frac{P(y=1)}{P(y=0)}$其实就是$\\frac{P(y=1)}{1 - P(y=1)}$即类别为1的对数几率，因此逻辑回归也叫对数几率回归。\n\n如果令$x \\in R^n$表示一个样本，$w \\in R^n,\\ b\\in R^n$表示逻辑回归的参数，其中$n$是数据特征维数，则逻辑回归可以表示为$ln\\frac{P(y=1)}{P(y=0)} = z,\\ z = w^Tx + b$，即使用$w^Tx + b$去回归类别为1的对数几率。\n\n因为$ln\\frac{P(y=1)}{1 - P(y=1)} = z,\\ P(y=1) = 1 - P(y=0)$，所以可以得出$P(y=1) = \\frac{1}{1 + e^{-z}},\\ P(y=0) = \\frac{1}{1+e^z}$。\n\n其对数似然函数可以写成$\\sum\\limits_X ylnP(y=1) + (1-y)lnP(y=0)$，这个表达式就是负的二值交叉熵，因此极大化其对数似然，就是极小化二值交叉熵损失。\n\n在分类CNN中线性回归的部分被替换为卷积神经网络，最后一层的激活函数使用Sigmoid或者Softmax，将回归输出转换为（多）类别概率，如果使用（二值）交叉熵损失函数，其实就是在最大化对数似然。\n\n\n\n\n\n\n","slug":"学习笔记/回归方法总结","published":1,"updated":"2020-08-31T06:39:20.766Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckjxpc3ru000v44mq83lddka9","content":"<p>在机器学习中，回归方法主要有线性回归和逻辑回归两种，其中线性回归是真的用于回归任务，而逻辑回归主要用于分类任务。</p>\r\n<h1 id=\"线性回归linear-regression-lr\">线性回归（Linear Regression, LR）</h1>\r\n<p>线性回归的表达方式很简单：<span class=\"math inline\">\\(\\hat{y} = x^Tw + b\\)</span>，其中<span class=\"math inline\">\\(x\\)</span>是一个表示为<span class=\"math inline\">\\(R^n\\)</span>的样本，<span class=\"math inline\">\\(\\hat{y}\\)</span>表示对样本<span class=\"math inline\">\\(x\\)</span>的回归值，<span class=\"math inline\">\\(w,\\ b\\)</span>是线性回归的参数。</p>\r\n<p>如果令<span class=\"math inline\">\\(X=\\begin{bmatrix}x_1 &amp; 1\\\\ x_2 &amp; 1\\\\ \\vdots &amp; \\vdots \\\\ x_m &amp; 1 \\end{bmatrix} \\in R^{m \\times (n+1)}\\)</span>表示所有数据的扩展矩阵，其中<span class=\"math inline\">\\(m\\)</span>是数据数量，<span class=\"math inline\">\\(Y=\\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_m\\end{bmatrix} \\in R^{m \\times 1}\\)</span>表示数据的标签，<span class=\"math inline\">\\(W \\in R^{(n+1) \\times 1}\\)</span>表示线性回归的参数，其中<span class=\"math inline\">\\(n\\)</span>是数据特征维数，则线性回归可以表示为<span class=\"math inline\">\\(\\hat{Y} = XW\\)</span>。</p>\r\n<p>如果首先对数据和标签进行了中心化，即<span class=\"math inline\">\\(\\sum_i x_i=0,\\ \\sum_i y_i=0\\)</span>，那么可以令<span class=\"math inline\">\\(X=\\begin{bmatrix}x_1\\\\ x_2\\\\ \\vdots\\\\ x_m \\end{bmatrix} \\in R^{m \\times n}\\)</span>表示所有数据的矩阵，其中<span class=\"math inline\">\\(m\\)</span>是数据数量，<span class=\"math inline\">\\(W \\in R^{n \\times 1}\\)</span>表示线性回归的参数。</p>\r\n<h2 id=\"线性回归的最小二乘法求解\">线性回归的最小二乘法求解</h2>\r\n<p>如果定义线性回归的损失函数为<span class=\"math inline\">\\(L = (\\hat{Y} - Y)^T(\\hat{Y} - Y)\\)</span>，虽小化损失函数即最小化均方误差。</p>\r\n<p>损失函数可以进一步表达为： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\nL(W) &amp;= (XW - Y)^T (XW - Y) \\\\\r\n&amp;= W^TX^TXW - YXW - XWY + Y^TY\\\\\r\n\\end{aligned}\r\n\\]</span> 求损失函数对<span class=\"math inline\">\\(w\\)</span>的偏导： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\ndL(W) &amp;= Z^TdZ + dZ^TZ,\\ Z = XW-Y\\\\\r\n&amp;=2Z^TdZ\\\\\r\n&amp;=2Z^Td(XW-Y)\\\\\r\n&amp;=2Z^TXdW\\\\\r\n\\frac{\\partial L(W)}{\\partial W} &amp;= X^TZ\\\\\r\n&amp;=X^T(XW-Y)\r\n\\end{aligned}\r\n\\]</span> 如果<span class=\"math inline\">\\(X^TX\\)</span>可逆，则令<span class=\"math inline\">\\(\\frac{\\partial L(W)}{\\partial W} = 0\\)</span>可得<span class=\"math inline\">\\(W = (X^TX)^{-1}X^TY\\)</span>，如果<span class=\"math inline\">\\(X^TX\\)</span>不可逆，则可以利用梯度下降法进行求解。</p>\r\n<h2 id=\"最小二乘法线性回归的理解\">最小二乘法线性回归的理解</h2>\r\n<p>如果假设数据标签服从高斯分布，则可以根据最大似然法推导得出最小二乘法，这里不再赘述。</p>\r\n<p>如果首先对数据进行了中心化，则<span class=\"math inline\">\\(X^TY\\)</span>是表示各个属性和回归值相关性的一个向量，<span class=\"math inline\">\\(X^TX\\)</span>表示的是数据的协方差矩阵。</p>\r\n<p>如果数据量小于特征个数（<span class=\"math inline\">\\(X\\)</span>中肯定有列相关的情况），那么<span class=\"math inline\">\\(X^TX\\)</span>不可逆，且容易导致过拟合。</p>\r\n<p>如果一些属性之间存在近似线性关系甚至线性关系，即<span class=\"math inline\">\\(X^TX\\)</span>是个病态矩阵甚至<span class=\"math inline\">\\(X^TX\\)</span>中存在相关（<span class=\"math inline\">\\(X^TX\\)</span>不可逆，可以看做病态矩阵的一种极端情况），根据<span class=\"math inline\">\\(X^TXW = X^TY\\)</span>，当<span class=\"math inline\">\\(X^TY\\)</span>稍作变动，所求得的<span class=\"math inline\">\\(W\\)</span>变化会非常大，一般情况下，没有做特征工程的数据，都会存在特征之间有近似线性关系的情况，因此线性回归的一个问题是对噪声非常敏感。</p>\r\n<p>如果<span class=\"math inline\">\\(X^TX\\)</span>不可逆，则<span class=\"math inline\">\\(X^TXW = X^TY\\)</span>的解不止一个，这种时候，为了防止过拟合，同时也防止对噪声数据过于敏感，可以在线性回归的损失函数上加一个正则化项，因此就有了岭回归和LASSO回归。</p>\r\n<h2 id=\"岭回归ridge-regression-rr\">岭回归（ridge regression, RR）</h2>\r\n<p>岭回归在线性回归的损失函数中加入了L2正则化，即<span class=\"math inline\">\\(L(W) = (XW - Y)^T (XW - Y) + \\lambda W^TW\\)</span>。 对该损失函数求梯度如下： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    dL(W) &amp;= 2(XW-Y)^TXdW + 2\\lambda W^TdW\\\\\r\n    &amp;=(2(XW-Y)^TX + 2\\lambda W^T)dW\\\\\r\n    \\frac{\\partial L(W)}{\\partial W} &amp;= 2X^T(XW-Y) + 2\\lambda W\r\n\\end{aligned}\r\n\\]</span> 令<span class=\"math inline\">\\(\\frac{\\partial L(W)}{\\partial W} = 0\\)</span>可得<span class=\"math inline\">\\((X^TX + \\lambda I)W = X^TY\\)</span></p>\r\n<h2 id=\"对岭回归的理解\">对岭回归的理解</h2>\r\n<p>这里和线性回归的区别在于<span class=\"math inline\">\\(\\lambda I\\)</span>这一项。</p>\r\n<p>由于<span class=\"math inline\">\\(X^TX\\)</span>是实对称矩阵，其特征分解可以表示为<span class=\"math inline\">\\(X^TX = Q\\Lambda Q^T\\)</span>，其中<span class=\"math inline\">\\(Q\\)</span>是由特征向量构成的正交矩阵。</p>\r\n<p>那么<span class=\"math inline\">\\(X^TX + \\lambda I = Q\\Lambda Q^T + \\lambda I = Q (\\Lambda + \\lambda I) Q^T\\)</span>，可以看出加入了L2正则化之后，可以理解为一定程度上可以防止特征值为0，也可以理解为减小了<span class=\"math inline\">\\(X^TX\\)</span>为病态矩阵的影响。</p>\r\n<h2 id=\"lasso回归least-absolute-shrinkage-and-selection-operator-regression\">LASSO回归（Least absolute shrinkage and selection operator Regression）</h2>\r\n<p>类似于岭回归，LASSO回归在线性回归的损失函数中加入了L1正则化，即<span class=\"math inline\">\\(L(W) = (XW - Y)^T (XW - Y) + \\lambda ||W||_1\\)</span>。</p>\r\n<p>如果将<span class=\"math inline\">\\(W\\)</span>分解成<span class=\"math inline\">\\(W^+,\\ W^-\\)</span>两部分，其中<span class=\"math inline\">\\(W^+\\)</span>只包含非负数，<span class=\"math inline\">\\(W^-\\)</span>只包含非正数，<span class=\"math inline\">\\(W = W^+ - W^-\\)</span>，那么<span class=\"math inline\">\\(L(W^+, W^-) = (XW - Y)^T (XW - Y) + \\lambda (I^n)^TW^+ + \\lambda (I^n)^TW^-\\)</span></p>\r\n<p>为了最小化<span class=\"math inline\">\\(L(W^+, W^-)\\)</span>，求导可得： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    dL(W^+, W^-) &amp;= 2Z^TdZ + \\lambda (I^n)^TdW^+ + \\lambda (I^n)^TdW^-,\\ Z = XW-Y\\\\\r\n    &amp;=2Z^TXdW + \\lambda (I^n)^TdW^+ + \\lambda (I^n)^TdW^-\\\\\r\n    &amp;=2Z^TXdW^+ - 2Z^TXdW^- + \\lambda (I^n)^TdW^+ + \\lambda (I^n)^TdW^-\\\\\r\n    &amp;=(\\lambda (I^n)^T + 2Z^TX)dW^+ + (\\lambda (I^n)^T - 2Z^TX)dW^-\\\\\r\n    \\frac{\\partial L(W^+, W^-)}{\\partial W^+} &amp;= \\lambda I^n + 2X^TZ\\\\\r\n    &amp;=\\lambda I^n + 2X^T(XW-Y)\\\\\r\n    \\frac{\\partial L(W^+, W^-)}{\\partial W^-} &amp;= \\lambda I^n - 2X^TZ\\\\\r\n    &amp;=\\lambda I^n - 2X^T(XW-Y)\\\\\r\n\\end{aligned}\r\n\\]</span></p>\r\n令导数为0： $$\r\n<span class=\"math display\">\\[\\begin{aligned}\r\n    \\lambda I^n + 2X^T(XW-Y) = 0\\\\\r\n    \\Rightarrow 2X^TXW = 2X^TY - \\lambda I^n\\\\\r\n\r\n    \\lambda I^n - 2X^T(XW-Y) = 0\\\\\r\n    \\Rightarrow 2X^TXW = 2X^TY + \\lambda I^n \\\\\r\n\\end{aligned}\\]</span>\r\n<p>$$ 可以发现，如果不是<span class=\"math inline\">\\(\\lambda\\)</span>为0，则两个导数不可能同时为0，所以希望通过导数为0得到损失函数极小值的办法行不通。</p>\r\n<p>LASSO回归可以考虑坐标下降法或者是转换为带约束条件的优化问题来解决。</p>\r\n<h2 id=\"对lasso回归的理解\">对LASSO回归的理解</h2>\r\n<p>如果是基于梯度的方法来求解LASSO回归，则可以发现，如果<span class=\"math inline\">\\(W\\)</span>中某个元素当前的值为负数，则其梯度中包含<span class=\"math inline\">\\(-\\lambda\\)</span>这一项，向着梯度相反的方向更新的话，则需要加上<span class=\"math inline\">\\(\\alpha \\lambda\\)</span>这一项，其实是在让<span class=\"math inline\">\\(W\\)</span>中的元素趋近于0，即让参数变得稀疏，有助于模型的可解释性，同时<span class=\"math inline\">\\(\\alpha \\lambda\\)</span>这一项也倾向于使得这个元素的绝对值变小，有助于缓解过拟合。</p>\r\n<h1 id=\"逻辑回归\">逻辑回归</h1>\r\n<p>逻辑回归表面上是个回归方法，其实是用于二分类任务的，只不过需要回归的值变成了<span class=\"math inline\">\\(ln\\frac{P(y=1)}{P(y=0)}\\)</span>，其中<span class=\"math inline\">\\(P(y=1)\\)</span>表示样本类别为1的概率，<span class=\"math inline\">\\(P(y=0)\\)</span>表示样本类别为0的概率，因为是二分类：<span class=\"math inline\">\\(P(y=1) = 1 - P(y=0)\\)</span>，所以<span class=\"math inline\">\\(\\frac{P(y=1)}{P(y=0)}\\)</span>其实就是<span class=\"math inline\">\\(\\frac{P(y=1)}{1 - P(y=1)}\\)</span>即类别为1的对数几率，因此逻辑回归也叫对数几率回归。</p>\r\n<p>如果令<span class=\"math inline\">\\(x \\in R^n\\)</span>表示一个样本，<span class=\"math inline\">\\(w \\in R^n,\\ b\\in R^n\\)</span>表示逻辑回归的参数，其中<span class=\"math inline\">\\(n\\)</span>是数据特征维数，则逻辑回归可以表示为<span class=\"math inline\">\\(ln\\frac{P(y=1)}{P(y=0)} = z,\\ z = w^Tx + b\\)</span>，即使用<span class=\"math inline\">\\(w^Tx + b\\)</span>去回归类别为1的对数几率。</p>\r\n<p>因为<span class=\"math inline\">\\(ln\\frac{P(y=1)}{1 - P(y=1)} = z,\\ P(y=1) = 1 - P(y=0)\\)</span>，所以可以得出<span class=\"math inline\">\\(P(y=1) = \\frac{1}{1 + e^{-z}},\\ P(y=0) = \\frac{1}{1+e^z}\\)</span>。</p>\r\n<p>其对数似然函数可以写成<span class=\"math inline\">\\(\\sum\\limits_X ylnP(y=1) + (1-y)lnP(y=0)\\)</span>，这个表达式就是负的二值交叉熵，因此极大化其对数似然，就是极小化二值交叉熵损失。</p>\r\n<p>在分类CNN中线性回归的部分被替换为卷积神经网络，最后一层的激活函数使用Sigmoid或者Softmax，将回归输出转换为（多）类别概率，如果使用（二值）交叉熵损失函数，其实就是在最大化对数似然。</p>\r\n","site":{"data":{}},"excerpt":"","more":"<p>在机器学习中，回归方法主要有线性回归和逻辑回归两种，其中线性回归是真的用于回归任务，而逻辑回归主要用于分类任务。</p>\r\n<h1 id=\"线性回归linear-regression-lr\">线性回归（Linear Regression, LR）</h1>\r\n<p>线性回归的表达方式很简单：<span class=\"math inline\">\\(\\hat{y} = x^Tw + b\\)</span>，其中<span class=\"math inline\">\\(x\\)</span>是一个表示为<span class=\"math inline\">\\(R^n\\)</span>的样本，<span class=\"math inline\">\\(\\hat{y}\\)</span>表示对样本<span class=\"math inline\">\\(x\\)</span>的回归值，<span class=\"math inline\">\\(w,\\ b\\)</span>是线性回归的参数。</p>\r\n<p>如果令<span class=\"math inline\">\\(X=\\begin{bmatrix}x_1 &amp; 1\\\\ x_2 &amp; 1\\\\ \\vdots &amp; \\vdots \\\\ x_m &amp; 1 \\end{bmatrix} \\in R^{m \\times (n+1)}\\)</span>表示所有数据的扩展矩阵，其中<span class=\"math inline\">\\(m\\)</span>是数据数量，<span class=\"math inline\">\\(Y=\\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_m\\end{bmatrix} \\in R^{m \\times 1}\\)</span>表示数据的标签，<span class=\"math inline\">\\(W \\in R^{(n+1) \\times 1}\\)</span>表示线性回归的参数，其中<span class=\"math inline\">\\(n\\)</span>是数据特征维数，则线性回归可以表示为<span class=\"math inline\">\\(\\hat{Y} = XW\\)</span>。</p>\r\n<p>如果首先对数据和标签进行了中心化，即<span class=\"math inline\">\\(\\sum_i x_i=0,\\ \\sum_i y_i=0\\)</span>，那么可以令<span class=\"math inline\">\\(X=\\begin{bmatrix}x_1\\\\ x_2\\\\ \\vdots\\\\ x_m \\end{bmatrix} \\in R^{m \\times n}\\)</span>表示所有数据的矩阵，其中<span class=\"math inline\">\\(m\\)</span>是数据数量，<span class=\"math inline\">\\(W \\in R^{n \\times 1}\\)</span>表示线性回归的参数。</p>\r\n<h2 id=\"线性回归的最小二乘法求解\">线性回归的最小二乘法求解</h2>\r\n<p>如果定义线性回归的损失函数为<span class=\"math inline\">\\(L = (\\hat{Y} - Y)^T(\\hat{Y} - Y)\\)</span>，虽小化损失函数即最小化均方误差。</p>\r\n<p>损失函数可以进一步表达为： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\nL(W) &amp;= (XW - Y)^T (XW - Y) \\\\\r\n&amp;= W^TX^TXW - YXW - XWY + Y^TY\\\\\r\n\\end{aligned}\r\n\\]</span> 求损失函数对<span class=\"math inline\">\\(w\\)</span>的偏导： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\ndL(W) &amp;= Z^TdZ + dZ^TZ,\\ Z = XW-Y\\\\\r\n&amp;=2Z^TdZ\\\\\r\n&amp;=2Z^Td(XW-Y)\\\\\r\n&amp;=2Z^TXdW\\\\\r\n\\frac{\\partial L(W)}{\\partial W} &amp;= X^TZ\\\\\r\n&amp;=X^T(XW-Y)\r\n\\end{aligned}\r\n\\]</span> 如果<span class=\"math inline\">\\(X^TX\\)</span>可逆，则令<span class=\"math inline\">\\(\\frac{\\partial L(W)}{\\partial W} = 0\\)</span>可得<span class=\"math inline\">\\(W = (X^TX)^{-1}X^TY\\)</span>，如果<span class=\"math inline\">\\(X^TX\\)</span>不可逆，则可以利用梯度下降法进行求解。</p>\r\n<h2 id=\"最小二乘法线性回归的理解\">最小二乘法线性回归的理解</h2>\r\n<p>如果假设数据标签服从高斯分布，则可以根据最大似然法推导得出最小二乘法，这里不再赘述。</p>\r\n<p>如果首先对数据进行了中心化，则<span class=\"math inline\">\\(X^TY\\)</span>是表示各个属性和回归值相关性的一个向量，<span class=\"math inline\">\\(X^TX\\)</span>表示的是数据的协方差矩阵。</p>\r\n<p>如果数据量小于特征个数（<span class=\"math inline\">\\(X\\)</span>中肯定有列相关的情况），那么<span class=\"math inline\">\\(X^TX\\)</span>不可逆，且容易导致过拟合。</p>\r\n<p>如果一些属性之间存在近似线性关系甚至线性关系，即<span class=\"math inline\">\\(X^TX\\)</span>是个病态矩阵甚至<span class=\"math inline\">\\(X^TX\\)</span>中存在相关（<span class=\"math inline\">\\(X^TX\\)</span>不可逆，可以看做病态矩阵的一种极端情况），根据<span class=\"math inline\">\\(X^TXW = X^TY\\)</span>，当<span class=\"math inline\">\\(X^TY\\)</span>稍作变动，所求得的<span class=\"math inline\">\\(W\\)</span>变化会非常大，一般情况下，没有做特征工程的数据，都会存在特征之间有近似线性关系的情况，因此线性回归的一个问题是对噪声非常敏感。</p>\r\n<p>如果<span class=\"math inline\">\\(X^TX\\)</span>不可逆，则<span class=\"math inline\">\\(X^TXW = X^TY\\)</span>的解不止一个，这种时候，为了防止过拟合，同时也防止对噪声数据过于敏感，可以在线性回归的损失函数上加一个正则化项，因此就有了岭回归和LASSO回归。</p>\r\n<h2 id=\"岭回归ridge-regression-rr\">岭回归（ridge regression, RR）</h2>\r\n<p>岭回归在线性回归的损失函数中加入了L2正则化，即<span class=\"math inline\">\\(L(W) = (XW - Y)^T (XW - Y) + \\lambda W^TW\\)</span>。 对该损失函数求梯度如下： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    dL(W) &amp;= 2(XW-Y)^TXdW + 2\\lambda W^TdW\\\\\r\n    &amp;=(2(XW-Y)^TX + 2\\lambda W^T)dW\\\\\r\n    \\frac{\\partial L(W)}{\\partial W} &amp;= 2X^T(XW-Y) + 2\\lambda W\r\n\\end{aligned}\r\n\\]</span> 令<span class=\"math inline\">\\(\\frac{\\partial L(W)}{\\partial W} = 0\\)</span>可得<span class=\"math inline\">\\((X^TX + \\lambda I)W = X^TY\\)</span></p>\r\n<h2 id=\"对岭回归的理解\">对岭回归的理解</h2>\r\n<p>这里和线性回归的区别在于<span class=\"math inline\">\\(\\lambda I\\)</span>这一项。</p>\r\n<p>由于<span class=\"math inline\">\\(X^TX\\)</span>是实对称矩阵，其特征分解可以表示为<span class=\"math inline\">\\(X^TX = Q\\Lambda Q^T\\)</span>，其中<span class=\"math inline\">\\(Q\\)</span>是由特征向量构成的正交矩阵。</p>\r\n<p>那么<span class=\"math inline\">\\(X^TX + \\lambda I = Q\\Lambda Q^T + \\lambda I = Q (\\Lambda + \\lambda I) Q^T\\)</span>，可以看出加入了L2正则化之后，可以理解为一定程度上可以防止特征值为0，也可以理解为减小了<span class=\"math inline\">\\(X^TX\\)</span>为病态矩阵的影响。</p>\r\n<h2 id=\"lasso回归least-absolute-shrinkage-and-selection-operator-regression\">LASSO回归（Least absolute shrinkage and selection operator Regression）</h2>\r\n<p>类似于岭回归，LASSO回归在线性回归的损失函数中加入了L1正则化，即<span class=\"math inline\">\\(L(W) = (XW - Y)^T (XW - Y) + \\lambda ||W||_1\\)</span>。</p>\r\n<p>如果将<span class=\"math inline\">\\(W\\)</span>分解成<span class=\"math inline\">\\(W^+,\\ W^-\\)</span>两部分，其中<span class=\"math inline\">\\(W^+\\)</span>只包含非负数，<span class=\"math inline\">\\(W^-\\)</span>只包含非正数，<span class=\"math inline\">\\(W = W^+ - W^-\\)</span>，那么<span class=\"math inline\">\\(L(W^+, W^-) = (XW - Y)^T (XW - Y) + \\lambda (I^n)^TW^+ + \\lambda (I^n)^TW^-\\)</span></p>\r\n<p>为了最小化<span class=\"math inline\">\\(L(W^+, W^-)\\)</span>，求导可得： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    dL(W^+, W^-) &amp;= 2Z^TdZ + \\lambda (I^n)^TdW^+ + \\lambda (I^n)^TdW^-,\\ Z = XW-Y\\\\\r\n    &amp;=2Z^TXdW + \\lambda (I^n)^TdW^+ + \\lambda (I^n)^TdW^-\\\\\r\n    &amp;=2Z^TXdW^+ - 2Z^TXdW^- + \\lambda (I^n)^TdW^+ + \\lambda (I^n)^TdW^-\\\\\r\n    &amp;=(\\lambda (I^n)^T + 2Z^TX)dW^+ + (\\lambda (I^n)^T - 2Z^TX)dW^-\\\\\r\n    \\frac{\\partial L(W^+, W^-)}{\\partial W^+} &amp;= \\lambda I^n + 2X^TZ\\\\\r\n    &amp;=\\lambda I^n + 2X^T(XW-Y)\\\\\r\n    \\frac{\\partial L(W^+, W^-)}{\\partial W^-} &amp;= \\lambda I^n - 2X^TZ\\\\\r\n    &amp;=\\lambda I^n - 2X^T(XW-Y)\\\\\r\n\\end{aligned}\r\n\\]</span></p>\r\n令导数为0： $$\r\n<span class=\"math display\">\\[\\begin{aligned}\r\n    \\lambda I^n + 2X^T(XW-Y) = 0\\\\\r\n    \\Rightarrow 2X^TXW = 2X^TY - \\lambda I^n\\\\\r\n\r\n    \\lambda I^n - 2X^T(XW-Y) = 0\\\\\r\n    \\Rightarrow 2X^TXW = 2X^TY + \\lambda I^n \\\\\r\n\\end{aligned}\\]</span>\r\n<p>$$ 可以发现，如果不是<span class=\"math inline\">\\(\\lambda\\)</span>为0，则两个导数不可能同时为0，所以希望通过导数为0得到损失函数极小值的办法行不通。</p>\r\n<p>LASSO回归可以考虑坐标下降法或者是转换为带约束条件的优化问题来解决。</p>\r\n<h2 id=\"对lasso回归的理解\">对LASSO回归的理解</h2>\r\n<p>如果是基于梯度的方法来求解LASSO回归，则可以发现，如果<span class=\"math inline\">\\(W\\)</span>中某个元素当前的值为负数，则其梯度中包含<span class=\"math inline\">\\(-\\lambda\\)</span>这一项，向着梯度相反的方向更新的话，则需要加上<span class=\"math inline\">\\(\\alpha \\lambda\\)</span>这一项，其实是在让<span class=\"math inline\">\\(W\\)</span>中的元素趋近于0，即让参数变得稀疏，有助于模型的可解释性，同时<span class=\"math inline\">\\(\\alpha \\lambda\\)</span>这一项也倾向于使得这个元素的绝对值变小，有助于缓解过拟合。</p>\r\n<h1 id=\"逻辑回归\">逻辑回归</h1>\r\n<p>逻辑回归表面上是个回归方法，其实是用于二分类任务的，只不过需要回归的值变成了<span class=\"math inline\">\\(ln\\frac{P(y=1)}{P(y=0)}\\)</span>，其中<span class=\"math inline\">\\(P(y=1)\\)</span>表示样本类别为1的概率，<span class=\"math inline\">\\(P(y=0)\\)</span>表示样本类别为0的概率，因为是二分类：<span class=\"math inline\">\\(P(y=1) = 1 - P(y=0)\\)</span>，所以<span class=\"math inline\">\\(\\frac{P(y=1)}{P(y=0)}\\)</span>其实就是<span class=\"math inline\">\\(\\frac{P(y=1)}{1 - P(y=1)}\\)</span>即类别为1的对数几率，因此逻辑回归也叫对数几率回归。</p>\r\n<p>如果令<span class=\"math inline\">\\(x \\in R^n\\)</span>表示一个样本，<span class=\"math inline\">\\(w \\in R^n,\\ b\\in R^n\\)</span>表示逻辑回归的参数，其中<span class=\"math inline\">\\(n\\)</span>是数据特征维数，则逻辑回归可以表示为<span class=\"math inline\">\\(ln\\frac{P(y=1)}{P(y=0)} = z,\\ z = w^Tx + b\\)</span>，即使用<span class=\"math inline\">\\(w^Tx + b\\)</span>去回归类别为1的对数几率。</p>\r\n<p>因为<span class=\"math inline\">\\(ln\\frac{P(y=1)}{1 - P(y=1)} = z,\\ P(y=1) = 1 - P(y=0)\\)</span>，所以可以得出<span class=\"math inline\">\\(P(y=1) = \\frac{1}{1 + e^{-z}},\\ P(y=0) = \\frac{1}{1+e^z}\\)</span>。</p>\r\n<p>其对数似然函数可以写成<span class=\"math inline\">\\(\\sum\\limits_X ylnP(y=1) + (1-y)lnP(y=0)\\)</span>，这个表达式就是负的二值交叉熵，因此极大化其对数似然，就是极小化二值交叉熵损失。</p>\r\n<p>在分类CNN中线性回归的部分被替换为卷积神经网络，最后一层的激活函数使用Sigmoid或者Softmax，将回归输出转换为（多）类别概率，如果使用（二值）交叉熵损失函数，其实就是在最大化对数似然。</p>\r\n"},{"title":"学习笔记-2019-03","date":"2019-03-06T00:50:08.000Z","mathjax":true,"_content":"\n# 2019-03-06\n\n## RetinaNet论文阅读\n\n论文中提出，One-Stage的检测方案，由于候选框类别极度不平衡，导致训练不好，损失精度，因此提出使用focal loss来平衡损失函数的方法。\n\nfocal loss 定义如下：\n$$\n\\begin{aligned}\n    p_t &= \\begin{cases}\n    p &if \\quad y = 1\\\\\n    \\\\\n    1 - p &otherwise \n\\end{cases}\\\\\n\\\\\nFL(p_t) &= -\\alpha_t(1 - p_t)^{\\gamma}\\log(p_t)\n\\end{aligned}\n$$\n其中$\\alpha_t$的值可以根据不同类别手动设置。\n\n论文中使用的五种基础大小的feature map进行anchor分解，每种大小又分别又三种比例，三种缩放。","source":"_posts/学习笔记/学习笔记-2019-03.md","raw":"---\ntitle: 学习笔记-2019-03\ndate: 2019-03-06 08:50:08\ntags: [学习笔记，杂项]\nmathjax: true\n---\n\n# 2019-03-06\n\n## RetinaNet论文阅读\n\n论文中提出，One-Stage的检测方案，由于候选框类别极度不平衡，导致训练不好，损失精度，因此提出使用focal loss来平衡损失函数的方法。\n\nfocal loss 定义如下：\n$$\n\\begin{aligned}\n    p_t &= \\begin{cases}\n    p &if \\quad y = 1\\\\\n    \\\\\n    1 - p &otherwise \n\\end{cases}\\\\\n\\\\\nFL(p_t) &= -\\alpha_t(1 - p_t)^{\\gamma}\\log(p_t)\n\\end{aligned}\n$$\n其中$\\alpha_t$的值可以根据不同类别手动设置。\n\n论文中使用的五种基础大小的feature map进行anchor分解，每种大小又分别又三种比例，三种缩放。","slug":"学习笔记/学习笔记-2019-03","published":1,"updated":"2019-03-13T06:49:17.183Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckjxpc3ru000x44mq2hraefy9","content":"<h1 id=\"section\">2019-03-06</h1>\r\n<h2 id=\"retinanet论文阅读\">RetinaNet论文阅读</h2>\r\n<p>论文中提出，One-Stage的检测方案，由于候选框类别极度不平衡，导致训练不好，损失精度，因此提出使用focal loss来平衡损失函数的方法。</p>\r\n<p>focal loss 定义如下： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    p_t &amp;= \\begin{cases}\r\n    p &amp;if \\quad y = 1\\\\\r\n    \\\\\r\n    1 - p &amp;otherwise \r\n\\end{cases}\\\\\r\n\\\\\r\nFL(p_t) &amp;= -\\alpha_t(1 - p_t)^{\\gamma}\\log(p_t)\r\n\\end{aligned}\r\n\\]</span> 其中<span class=\"math inline\">\\(\\alpha_t\\)</span>的值可以根据不同类别手动设置。</p>\r\n<p>论文中使用的五种基础大小的feature map进行anchor分解，每种大小又分别又三种比例，三种缩放。</p>\r\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"section\">2019-03-06</h1>\r\n<h2 id=\"retinanet论文阅读\">RetinaNet论文阅读</h2>\r\n<p>论文中提出，One-Stage的检测方案，由于候选框类别极度不平衡，导致训练不好，损失精度，因此提出使用focal loss来平衡损失函数的方法。</p>\r\n<p>focal loss 定义如下： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    p_t &amp;= \\begin{cases}\r\n    p &amp;if \\quad y = 1\\\\\r\n    \\\\\r\n    1 - p &amp;otherwise \r\n\\end{cases}\\\\\r\n\\\\\r\nFL(p_t) &amp;= -\\alpha_t(1 - p_t)^{\\gamma}\\log(p_t)\r\n\\end{aligned}\r\n\\]</span> 其中<span class=\"math inline\">\\(\\alpha_t\\)</span>的值可以根据不同类别手动设置。</p>\r\n<p>论文中使用的五种基础大小的feature map进行anchor分解，每种大小又分别又三种比例，三种缩放。</p>\r\n"},{"title":"学习笔记-2019-04","date":"2019-04-06T12:05:26.000Z","mathjax":true,"_content":"# 2019-04-06\n## 对于随机批梯度下降理解\n- 对于batch size：从n个训练数据对真实数据分布进行估计，其标准差可以写为$\\sigma/\\sqrt{n}$，因此加大batch size对模型的学习效果的提升不是线性的。\n- 对于数据：随机批梯度下降，可以减少计算的冗余度，因为很多数据其求出的梯度基本相同，最坏的情况就是训练集中每个数据都是相同的，如果使用原始的梯度下降算法，对于整个训练集进行梯度计算，最后得到的梯度和根据单个样本计算的梯度相同，存在大量计算冗余，随机批梯度下降对这种问题有一定缓解作用。\n- 小的batch size：小的batch size 可以起到正则化的作用，有可能是因为小的batch size在估计梯度的过程中，引入了一定地噪声，从而可以使得模型的泛化误差减小，但是小的batch size 需要更小的学习速率和更多的学习步数。\n- 对于优化算法：不同的优化算法对于batch size有不一样的要求，因为不同的优化算法对采样误差的敏感程度不同，可能一些优化算法需要从样本中提取的信息很难从少量的样本中估计出来，因此需要设置batch size为一个较大值才会有较好效果。仅仅直接使用梯度$g$的优化算法往往有较好的鲁棒性，并一般可以适应100这种小的batch size。\n\n## 病态矩阵与条件数理解\n对于矩阵方程如下：\n$$\n\\begin{aligned}\n    A x = b\n\\end{aligned}\n$$\n若对于一个较小的样本观察误差$\\Delta b$，使得$A(x+\\Delta x)=b + \\Delta b$，$\\Delta x$如果比$\\Delta b$大很多，则称矩阵A是一个病态矩阵，一个典型的病态矩阵如下：\n$$\n\\begin{bmatrix}\n    1000 &1000\\\\\n    0 &0.001\n\\end{bmatrix}\n$$\n其中两个列向量的相关性非常大，夹角非常小，表示的特征太过相似，若$b$是第一个列向量方向的单位向量,则求出的解为$[n, 0]^T$，若$b$稍微偏差一点，变到了第二个列向量的方向，则求出来的解变为$[0\\ 1]^T$，两次解的差距非常大，仅仅因为$b$上有非常小的偏移。\n\n对于某种矩阵范数$||A||$以及某种向量范数$||b||$，若:\n$$\nA(x+\\Delta x)=b + \\Delta b\n$$\n则\n$$\nA\\Delta x = \\Delta b\n$$\n即\n$$\n\\Delta x = A^{-1}\\Delta b\n$$ \n则有\n$$\n||\\Delta x|| \\le ||A^{-1}||\\cdot||\\Delta b||\n$$\n同时有\n$$\n||A|| \\cdot ||x|| \\ge ||b||\n$$\n因此\n$$\n\\frac{||\\Delta x||}{||A|| \\cdot ||x||} \\le \\frac{||A^{-1}||\\cdot||\\Delta b||}{||b||}\n$$\n$$\n\\frac{||\\Delta x||}{||x||} \\le (||A^{-1}||\\cdot||A||)\\frac{||\\Delta b||}{||b||}\n$$\n这里将$||A^{-1}||\\cdot||A||$称为矩阵A的条件数，对于不同的范数，条件数各有不同，但都反映了矩阵A的病态程度，条件数越大，矩阵呈现更加明显的病态特征。\n\n# 2019-04-11\n## SGD中的Momentum方法\nmomentum方法主要针对两个问题：\n- 参数空间存在条件数较大的病态Hessian矩阵。\n- 随机梯度下降对梯度的估计存在偏差。\n$$\nv = \\alpha v - \\epsilon \\nabla_\\theta(\\frac{1}{m}\\sum_{i=1}^{m}L(f(x^{(i)};\\theta), y^{(i)}))\\\\\n\\theta \\leftarrow \\theta + v\n$$\n如果每次梯度都是$g$，那么最终的$v$会趋近于$\\frac{\\epsilon g}{1 - \\alpha}$\n\n## SGD中的Nesterov Momentum方法\n$$\nv = \\alpha v - \\epsilon \\nabla_\\theta(\\frac{1}{m}\\sum_{i=1}^{m}L(f(x^{(i)};\\theta + \\alpha v), y^{(i)}))\\\\\n\\theta \\leftarrow \\theta + v\n$$\n\n## 网络权重随机初始化的原因\n如果两个计算单元具有相同的输入和激活函数，如果初始化相同，那么这两个计算单元在优化过程中很可能一直同步，最终出现冗余单元的情况，即两个计算单元计算的是相同的函数，重复计算没有意义。\n\n# 2019-04-12\n\n## AdaGrad优化算法\nGlobal learning rate $\\epsilon$\n\nInitial parameter $\\theta$\n\nSmall constant $\\delta$\n\nInitial variable $r = 0$\n\nbatch size $m$\n\n$A \\odot B$: hadamard product of $A$ and $B$\n\n每次计算迭代过程如下：\n$$\n\\begin{aligned}\ng &\\leftarrow \\frac{1}{m} \\nabla_\\theta\\sum_iL(f(x^{(i)}; \\theta), y^{(i)})\\\\\nr &\\leftarrow r + g \\odot g\\\\\n\\Delta \\theta &\\leftarrow -\\frac{\\epsilon}{\\delta + \\sqrt{r}} \\odot g\\\\\n\\theta &\\leftarrow \\theta + \\Delta \\theta    \n\\end{aligned}\n$$\n此算法主要用于凸函数的优化问题，在梯度较小的地方可以加大参数移动速度，但是我觉得这个算法不好，因为对梯度的方向进行了更改，而且$r$项一直增大，会使得学习过程很快趋于停滞。\n\n## RMSProp优化算法\nGlobal learning rate $\\epsilon$\n\ndecay rate $\\rho$\n\nInitial parameter $\\theta$\n\nSmall constant $\\delta$\n\nInitial variable $r = 0$\n\nbatch size $m$\n\n$A \\odot B$: hadamard product of $A$ and $B$\n\n每次计算迭代过程如下：\n$$\n\\begin{aligned}\ng &\\leftarrow \\frac{1}{m} \\nabla_\\theta\\sum_iL(f(x^{(i)}; \\theta), y^{(i)})\\\\\nr &\\leftarrow \\rho r + (1 - \\rho)g \\odot g\\\\\n\\Delta \\theta &\\leftarrow -\\frac{\\epsilon}{\\sqrt{\\delta + r}} \\odot g\\\\\n\\theta &\\leftarrow \\theta + \\Delta \\theta    \n\\end{aligned}\n$$\n在AdaGrad算法的基础上，通过衰减参数$\\rho$可以避免学习过程趋于停滞。\n\n## 带有Nesterov Momentum 的RMSProp优化算法\nGlobal learning rate $\\epsilon$\n\ndecay rate $\\rho$\n\nmomentum coefficent $\\alpha$\n\nInitial parameter $\\theta$\n\nSmall constant $\\delta$\n\ninitial velocity $v$\n\nInitial variable $r = 0$\n\nbatch size $m$\n\n$A \\odot B$: hadamard product of $A$ and $B$\n\n每次计算迭代过程如下：\n$$\n\\begin{aligned}\n\\tilde{\\theta} &\\leftarrow \\theta + \\alpha v\\\\\ng &\\leftarrow \\frac{1}{m} \\nabla_{\\tilde{\\theta}}\\sum_iL(f(x^{(i)}; \\tilde{\\theta}), y^{(i)})\\\\\nr &\\leftarrow \\rho r + (1 - \\rho)g \\odot g\\\\\nv &\\leftarrow \\alpha v -\\frac{\\epsilon}{\\sqrt{r}} \\odot g\\\\\n\\theta &\\leftarrow \\theta + v\n\\end{aligned}\n$$\n这里计算了Nesterov动量项以应对可能的梯度估计偏差和参数空间病态Hessian矩阵问题。\n\n## Adam优化算法\n\nStep size $\\epsilon$\n\nExponential decay rates for moment estimates $\\rho_1$ $\\rho_2$\n\nSmall constant $\\delta$\n\nInitial parameter $\\theta$\n\nInitialize 1st and 2nd moment variables $s=0$ $r=0$\n\nInitialize time step $t=0$\n\n$A \\odot B$: hadamard product of $A$ and $B$\n\n每次计算迭代过程如下:\n\n$$\n\\begin{aligned}\n    g &\\leftarrow \\frac{1}{m} \\nabla_\\theta\\sum_iL(f(x^{(i)}; \\theta), y^{(i)})\\\\\n    t &\\leftarrow t + 1\\\\\n    s &\\leftarrow \\rho_1 s + (1-\\rho_1) g\\\\\n    r &\\leftarrow \\rho_2 r + (1-\\rho_2) g \\odot g\\\\\n    \\hat{s} &\\leftarrow \\frac{s}{1-\\rho_1^t}\\\\\n    \\hat{r} &\\leftarrow \\frac{r}{1 - \\rho_2^t}\\\\\n    \\Delta_\\theta &\\leftarrow -\\epsilon \\frac{\\hat{s}}{\\sqrt{\\hat{r}} + \\delta}\\\\\n    \\theta &\\leftarrow \\theta + \\Delta_\\theta\n\\end{aligned}\n$$\n\n## 牛顿法\n首先通过二阶估计来表达损失函数：\n$$\n\\begin{aligned}\n    \\hat{J}(\\theta) &\\approx J(\\theta_0) + (\\theta - \\theta_0)^T\\nabla_\\theta J(\\theta_0) + \\frac{1}{2}(\\theta - \\theta_0)^TH(\\theta - \\theta_0)\\\\\n\\end{aligned}\n$$\n其一阶导数：\n$$\n\\begin{aligned}\n\\frac{\\partial \\hat{J}}{\\partial \\theta} &= (\\nabla_\\theta J(\\theta_0))^T + \\frac{1}{2}(H + H^T)(\\theta - \\theta_0)\\\\\n&= \\nabla_\\theta J(\\theta_0) + H(\\theta - \\theta_0)\n\\end{aligned}\n$$\n对于凸函数，当一阶导数为0时，有可以直接得到最优解：\n$$\n\\nabla_\\theta J(\\theta_0) + H(\\theta - \\theta_0) = 0\\\\\n\\theta = \\theta_0 - H^{-1}\\nabla_\\theta J(\\theta_0)\n$$\n对于非凸函数，牛顿法不能有效的收敛。\n\n# 2019-04-27\n## 论文：Feature Selective Anchor-Free Module for Single-Shot Object Detection\n这篇论文对retinanet进行了改进，在原始的anchor-based分支上，新增了一个anchor-free分支。\n\n目前一阶段的目标检测大部分都是基于先验框进行，就是对于所谓的anchor(锚点)，事先定义好anchor box的大小、比例、个数，模型的回归输出作为基于anchor box的偏移，训练时将目标检测框映射分配到anchor box上，指定回归输出的优化目标，而inference过程是将和先验anchor box一起计算得到最终的predict box。\n\nanchor-free分支不需要计算先验框，不需要定义anchor box的大小、比例、个数等超参数，而是直接将目标检测框映射到feature map上，并定义有效范围和忽略范围，feature map上在有效范围内的点作为正样本，在忽略范围外的点作为负样本进行训练，训练时，对于多尺度feature map的选择则根据每种大小的feature map的损失函数来计算，训练时只选择损失函数最小的feature map层进行训练。\n\n## 论文FCOS: Fully Convolutional One-Stage Object Detection\n这篇论文也致力于目标检测的模型去anchor化工作，将anchor-based的预测修改为anchor-free的方式，但是Feature Selective Anchor-Free Module for Single-Shot Object Detection论文中不同的是，这里的anchor-free分支不需要计算有效范围和忽略范围，而是直接将目标框在feature map上的投影覆盖的所有点作为正样本点，其余的直接作为负样本点，这样可以提升检测的召回率，但是必然提升检测的假阳性率，因此论文中针对这个问题，提出了center-ness层，在训练过程中，让这一层来预测当前像素点到gt中心的距离度量，这一层的训练目标被定义为如下：\n$$\n\\begin{aligned}\n    centerness^* = \\sqrt{\\frac{\\min(l^*, r^*)}{\\max(l^*, r^*)}\\frac{\\min(t^*, b^*)}{\\max(t^*, b^*)}}\n\\end{aligned}\n$$\n其中$t^*\\ b^*\\ l^*\\ r^*$四个值的定义如下图，这里截取的整张图都在同一个目标框的范围内，这里计算的就是两个轴交点的$centerness^*$值：\n\n{% asset_img center_ness.png center ness示意%}\n\n距离目标框中心越近，$centerness^*$值越大，在进行inference的时候，$centerness^*$值将作为feature map上每个点的预测权重，用于抑制和目标框偏移较大的点的预测值。\n","source":"_posts/学习笔记/学习笔记-2019-04.md","raw":"---\ntitle: 学习笔记-2019-04\ndate: 2019-04-06 20:05:26\ntags: [学习笔记，杂项]\nmathjax: true\n---\n# 2019-04-06\n## 对于随机批梯度下降理解\n- 对于batch size：从n个训练数据对真实数据分布进行估计，其标准差可以写为$\\sigma/\\sqrt{n}$，因此加大batch size对模型的学习效果的提升不是线性的。\n- 对于数据：随机批梯度下降，可以减少计算的冗余度，因为很多数据其求出的梯度基本相同，最坏的情况就是训练集中每个数据都是相同的，如果使用原始的梯度下降算法，对于整个训练集进行梯度计算，最后得到的梯度和根据单个样本计算的梯度相同，存在大量计算冗余，随机批梯度下降对这种问题有一定缓解作用。\n- 小的batch size：小的batch size 可以起到正则化的作用，有可能是因为小的batch size在估计梯度的过程中，引入了一定地噪声，从而可以使得模型的泛化误差减小，但是小的batch size 需要更小的学习速率和更多的学习步数。\n- 对于优化算法：不同的优化算法对于batch size有不一样的要求，因为不同的优化算法对采样误差的敏感程度不同，可能一些优化算法需要从样本中提取的信息很难从少量的样本中估计出来，因此需要设置batch size为一个较大值才会有较好效果。仅仅直接使用梯度$g$的优化算法往往有较好的鲁棒性，并一般可以适应100这种小的batch size。\n\n## 病态矩阵与条件数理解\n对于矩阵方程如下：\n$$\n\\begin{aligned}\n    A x = b\n\\end{aligned}\n$$\n若对于一个较小的样本观察误差$\\Delta b$，使得$A(x+\\Delta x)=b + \\Delta b$，$\\Delta x$如果比$\\Delta b$大很多，则称矩阵A是一个病态矩阵，一个典型的病态矩阵如下：\n$$\n\\begin{bmatrix}\n    1000 &1000\\\\\n    0 &0.001\n\\end{bmatrix}\n$$\n其中两个列向量的相关性非常大，夹角非常小，表示的特征太过相似，若$b$是第一个列向量方向的单位向量,则求出的解为$[n, 0]^T$，若$b$稍微偏差一点，变到了第二个列向量的方向，则求出来的解变为$[0\\ 1]^T$，两次解的差距非常大，仅仅因为$b$上有非常小的偏移。\n\n对于某种矩阵范数$||A||$以及某种向量范数$||b||$，若:\n$$\nA(x+\\Delta x)=b + \\Delta b\n$$\n则\n$$\nA\\Delta x = \\Delta b\n$$\n即\n$$\n\\Delta x = A^{-1}\\Delta b\n$$ \n则有\n$$\n||\\Delta x|| \\le ||A^{-1}||\\cdot||\\Delta b||\n$$\n同时有\n$$\n||A|| \\cdot ||x|| \\ge ||b||\n$$\n因此\n$$\n\\frac{||\\Delta x||}{||A|| \\cdot ||x||} \\le \\frac{||A^{-1}||\\cdot||\\Delta b||}{||b||}\n$$\n$$\n\\frac{||\\Delta x||}{||x||} \\le (||A^{-1}||\\cdot||A||)\\frac{||\\Delta b||}{||b||}\n$$\n这里将$||A^{-1}||\\cdot||A||$称为矩阵A的条件数，对于不同的范数，条件数各有不同，但都反映了矩阵A的病态程度，条件数越大，矩阵呈现更加明显的病态特征。\n\n# 2019-04-11\n## SGD中的Momentum方法\nmomentum方法主要针对两个问题：\n- 参数空间存在条件数较大的病态Hessian矩阵。\n- 随机梯度下降对梯度的估计存在偏差。\n$$\nv = \\alpha v - \\epsilon \\nabla_\\theta(\\frac{1}{m}\\sum_{i=1}^{m}L(f(x^{(i)};\\theta), y^{(i)}))\\\\\n\\theta \\leftarrow \\theta + v\n$$\n如果每次梯度都是$g$，那么最终的$v$会趋近于$\\frac{\\epsilon g}{1 - \\alpha}$\n\n## SGD中的Nesterov Momentum方法\n$$\nv = \\alpha v - \\epsilon \\nabla_\\theta(\\frac{1}{m}\\sum_{i=1}^{m}L(f(x^{(i)};\\theta + \\alpha v), y^{(i)}))\\\\\n\\theta \\leftarrow \\theta + v\n$$\n\n## 网络权重随机初始化的原因\n如果两个计算单元具有相同的输入和激活函数，如果初始化相同，那么这两个计算单元在优化过程中很可能一直同步，最终出现冗余单元的情况，即两个计算单元计算的是相同的函数，重复计算没有意义。\n\n# 2019-04-12\n\n## AdaGrad优化算法\nGlobal learning rate $\\epsilon$\n\nInitial parameter $\\theta$\n\nSmall constant $\\delta$\n\nInitial variable $r = 0$\n\nbatch size $m$\n\n$A \\odot B$: hadamard product of $A$ and $B$\n\n每次计算迭代过程如下：\n$$\n\\begin{aligned}\ng &\\leftarrow \\frac{1}{m} \\nabla_\\theta\\sum_iL(f(x^{(i)}; \\theta), y^{(i)})\\\\\nr &\\leftarrow r + g \\odot g\\\\\n\\Delta \\theta &\\leftarrow -\\frac{\\epsilon}{\\delta + \\sqrt{r}} \\odot g\\\\\n\\theta &\\leftarrow \\theta + \\Delta \\theta    \n\\end{aligned}\n$$\n此算法主要用于凸函数的优化问题，在梯度较小的地方可以加大参数移动速度，但是我觉得这个算法不好，因为对梯度的方向进行了更改，而且$r$项一直增大，会使得学习过程很快趋于停滞。\n\n## RMSProp优化算法\nGlobal learning rate $\\epsilon$\n\ndecay rate $\\rho$\n\nInitial parameter $\\theta$\n\nSmall constant $\\delta$\n\nInitial variable $r = 0$\n\nbatch size $m$\n\n$A \\odot B$: hadamard product of $A$ and $B$\n\n每次计算迭代过程如下：\n$$\n\\begin{aligned}\ng &\\leftarrow \\frac{1}{m} \\nabla_\\theta\\sum_iL(f(x^{(i)}; \\theta), y^{(i)})\\\\\nr &\\leftarrow \\rho r + (1 - \\rho)g \\odot g\\\\\n\\Delta \\theta &\\leftarrow -\\frac{\\epsilon}{\\sqrt{\\delta + r}} \\odot g\\\\\n\\theta &\\leftarrow \\theta + \\Delta \\theta    \n\\end{aligned}\n$$\n在AdaGrad算法的基础上，通过衰减参数$\\rho$可以避免学习过程趋于停滞。\n\n## 带有Nesterov Momentum 的RMSProp优化算法\nGlobal learning rate $\\epsilon$\n\ndecay rate $\\rho$\n\nmomentum coefficent $\\alpha$\n\nInitial parameter $\\theta$\n\nSmall constant $\\delta$\n\ninitial velocity $v$\n\nInitial variable $r = 0$\n\nbatch size $m$\n\n$A \\odot B$: hadamard product of $A$ and $B$\n\n每次计算迭代过程如下：\n$$\n\\begin{aligned}\n\\tilde{\\theta} &\\leftarrow \\theta + \\alpha v\\\\\ng &\\leftarrow \\frac{1}{m} \\nabla_{\\tilde{\\theta}}\\sum_iL(f(x^{(i)}; \\tilde{\\theta}), y^{(i)})\\\\\nr &\\leftarrow \\rho r + (1 - \\rho)g \\odot g\\\\\nv &\\leftarrow \\alpha v -\\frac{\\epsilon}{\\sqrt{r}} \\odot g\\\\\n\\theta &\\leftarrow \\theta + v\n\\end{aligned}\n$$\n这里计算了Nesterov动量项以应对可能的梯度估计偏差和参数空间病态Hessian矩阵问题。\n\n## Adam优化算法\n\nStep size $\\epsilon$\n\nExponential decay rates for moment estimates $\\rho_1$ $\\rho_2$\n\nSmall constant $\\delta$\n\nInitial parameter $\\theta$\n\nInitialize 1st and 2nd moment variables $s=0$ $r=0$\n\nInitialize time step $t=0$\n\n$A \\odot B$: hadamard product of $A$ and $B$\n\n每次计算迭代过程如下:\n\n$$\n\\begin{aligned}\n    g &\\leftarrow \\frac{1}{m} \\nabla_\\theta\\sum_iL(f(x^{(i)}; \\theta), y^{(i)})\\\\\n    t &\\leftarrow t + 1\\\\\n    s &\\leftarrow \\rho_1 s + (1-\\rho_1) g\\\\\n    r &\\leftarrow \\rho_2 r + (1-\\rho_2) g \\odot g\\\\\n    \\hat{s} &\\leftarrow \\frac{s}{1-\\rho_1^t}\\\\\n    \\hat{r} &\\leftarrow \\frac{r}{1 - \\rho_2^t}\\\\\n    \\Delta_\\theta &\\leftarrow -\\epsilon \\frac{\\hat{s}}{\\sqrt{\\hat{r}} + \\delta}\\\\\n    \\theta &\\leftarrow \\theta + \\Delta_\\theta\n\\end{aligned}\n$$\n\n## 牛顿法\n首先通过二阶估计来表达损失函数：\n$$\n\\begin{aligned}\n    \\hat{J}(\\theta) &\\approx J(\\theta_0) + (\\theta - \\theta_0)^T\\nabla_\\theta J(\\theta_0) + \\frac{1}{2}(\\theta - \\theta_0)^TH(\\theta - \\theta_0)\\\\\n\\end{aligned}\n$$\n其一阶导数：\n$$\n\\begin{aligned}\n\\frac{\\partial \\hat{J}}{\\partial \\theta} &= (\\nabla_\\theta J(\\theta_0))^T + \\frac{1}{2}(H + H^T)(\\theta - \\theta_0)\\\\\n&= \\nabla_\\theta J(\\theta_0) + H(\\theta - \\theta_0)\n\\end{aligned}\n$$\n对于凸函数，当一阶导数为0时，有可以直接得到最优解：\n$$\n\\nabla_\\theta J(\\theta_0) + H(\\theta - \\theta_0) = 0\\\\\n\\theta = \\theta_0 - H^{-1}\\nabla_\\theta J(\\theta_0)\n$$\n对于非凸函数，牛顿法不能有效的收敛。\n\n# 2019-04-27\n## 论文：Feature Selective Anchor-Free Module for Single-Shot Object Detection\n这篇论文对retinanet进行了改进，在原始的anchor-based分支上，新增了一个anchor-free分支。\n\n目前一阶段的目标检测大部分都是基于先验框进行，就是对于所谓的anchor(锚点)，事先定义好anchor box的大小、比例、个数，模型的回归输出作为基于anchor box的偏移，训练时将目标检测框映射分配到anchor box上，指定回归输出的优化目标，而inference过程是将和先验anchor box一起计算得到最终的predict box。\n\nanchor-free分支不需要计算先验框，不需要定义anchor box的大小、比例、个数等超参数，而是直接将目标检测框映射到feature map上，并定义有效范围和忽略范围，feature map上在有效范围内的点作为正样本，在忽略范围外的点作为负样本进行训练，训练时，对于多尺度feature map的选择则根据每种大小的feature map的损失函数来计算，训练时只选择损失函数最小的feature map层进行训练。\n\n## 论文FCOS: Fully Convolutional One-Stage Object Detection\n这篇论文也致力于目标检测的模型去anchor化工作，将anchor-based的预测修改为anchor-free的方式，但是Feature Selective Anchor-Free Module for Single-Shot Object Detection论文中不同的是，这里的anchor-free分支不需要计算有效范围和忽略范围，而是直接将目标框在feature map上的投影覆盖的所有点作为正样本点，其余的直接作为负样本点，这样可以提升检测的召回率，但是必然提升检测的假阳性率，因此论文中针对这个问题，提出了center-ness层，在训练过程中，让这一层来预测当前像素点到gt中心的距离度量，这一层的训练目标被定义为如下：\n$$\n\\begin{aligned}\n    centerness^* = \\sqrt{\\frac{\\min(l^*, r^*)}{\\max(l^*, r^*)}\\frac{\\min(t^*, b^*)}{\\max(t^*, b^*)}}\n\\end{aligned}\n$$\n其中$t^*\\ b^*\\ l^*\\ r^*$四个值的定义如下图，这里截取的整张图都在同一个目标框的范围内，这里计算的就是两个轴交点的$centerness^*$值：\n\n{% asset_img center_ness.png center ness示意%}\n\n距离目标框中心越近，$centerness^*$值越大，在进行inference的时候，$centerness^*$值将作为feature map上每个点的预测权重，用于抑制和目标框偏移较大的点的预测值。\n","slug":"学习笔记/学习笔记-2019-04","published":1,"updated":"2019-04-28T00:23:23.417Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckjxpc3rv001044mqaf3w7yqp","content":"<h1 id=\"section\">2019-04-06</h1>\r\n<h2 id=\"对于随机批梯度下降理解\">对于随机批梯度下降理解</h2>\r\n<ul>\r\n<li>对于batch size：从n个训练数据对真实数据分布进行估计，其标准差可以写为<span class=\"math inline\">\\(\\sigma/\\sqrt{n}\\)</span>，因此加大batch size对模型的学习效果的提升不是线性的。</li>\r\n<li>对于数据：随机批梯度下降，可以减少计算的冗余度，因为很多数据其求出的梯度基本相同，最坏的情况就是训练集中每个数据都是相同的，如果使用原始的梯度下降算法，对于整个训练集进行梯度计算，最后得到的梯度和根据单个样本计算的梯度相同，存在大量计算冗余，随机批梯度下降对这种问题有一定缓解作用。</li>\r\n<li>小的batch size：小的batch size 可以起到正则化的作用，有可能是因为小的batch size在估计梯度的过程中，引入了一定地噪声，从而可以使得模型的泛化误差减小，但是小的batch size 需要更小的学习速率和更多的学习步数。</li>\r\n<li>对于优化算法：不同的优化算法对于batch size有不一样的要求，因为不同的优化算法对采样误差的敏感程度不同，可能一些优化算法需要从样本中提取的信息很难从少量的样本中估计出来，因此需要设置batch size为一个较大值才会有较好效果。仅仅直接使用梯度<span class=\"math inline\">\\(g\\)</span>的优化算法往往有较好的鲁棒性，并一般可以适应100这种小的batch size。</li>\r\n</ul>\r\n<h2 id=\"病态矩阵与条件数理解\">病态矩阵与条件数理解</h2>\r\n<p>对于矩阵方程如下： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    A x = b\r\n\\end{aligned}\r\n\\]</span> 若对于一个较小的样本观察误差<span class=\"math inline\">\\(\\Delta b\\)</span>，使得<span class=\"math inline\">\\(A(x+\\Delta x)=b + \\Delta b\\)</span>，<span class=\"math inline\">\\(\\Delta x\\)</span>如果比<span class=\"math inline\">\\(\\Delta b\\)</span>大很多，则称矩阵A是一个病态矩阵，一个典型的病态矩阵如下： <span class=\"math display\">\\[\r\n\\begin{bmatrix}\r\n    1000 &amp;1000\\\\\r\n    0 &amp;0.001\r\n\\end{bmatrix}\r\n\\]</span> 其中两个列向量的相关性非常大，夹角非常小，表示的特征太过相似，若<span class=\"math inline\">\\(b\\)</span>是第一个列向量方向的单位向量,则求出的解为<span class=\"math inline\">\\([n, 0]^T\\)</span>，若<span class=\"math inline\">\\(b\\)</span>稍微偏差一点，变到了第二个列向量的方向，则求出来的解变为<span class=\"math inline\">\\([0\\ 1]^T\\)</span>，两次解的差距非常大，仅仅因为<span class=\"math inline\">\\(b\\)</span>上有非常小的偏移。</p>\r\n<p>对于某种矩阵范数<span class=\"math inline\">\\(||A||\\)</span>以及某种向量范数<span class=\"math inline\">\\(||b||\\)</span>，若: <span class=\"math display\">\\[\r\nA(x+\\Delta x)=b + \\Delta b\r\n\\]</span> 则 <span class=\"math display\">\\[\r\nA\\Delta x = \\Delta b\r\n\\]</span> 即 <span class=\"math display\">\\[\r\n\\Delta x = A^{-1}\\Delta b\r\n\\]</span> 则有 <span class=\"math display\">\\[\r\n||\\Delta x|| \\le ||A^{-1}||\\cdot||\\Delta b||\r\n\\]</span> 同时有 <span class=\"math display\">\\[\r\n||A|| \\cdot ||x|| \\ge ||b||\r\n\\]</span> 因此 <span class=\"math display\">\\[\r\n\\frac{||\\Delta x||}{||A|| \\cdot ||x||} \\le \\frac{||A^{-1}||\\cdot||\\Delta b||}{||b||}\r\n\\]</span> <span class=\"math display\">\\[\r\n\\frac{||\\Delta x||}{||x||} \\le (||A^{-1}||\\cdot||A||)\\frac{||\\Delta b||}{||b||}\r\n\\]</span> 这里将<span class=\"math inline\">\\(||A^{-1}||\\cdot||A||\\)</span>称为矩阵A的条件数，对于不同的范数，条件数各有不同，但都反映了矩阵A的病态程度，条件数越大，矩阵呈现更加明显的病态特征。</p>\r\n<h1 id=\"section-1\">2019-04-11</h1>\r\n<h2 id=\"sgd中的momentum方法\">SGD中的Momentum方法</h2>\r\n<p>momentum方法主要针对两个问题： - 参数空间存在条件数较大的病态Hessian矩阵。 - 随机梯度下降对梯度的估计存在偏差。 <span class=\"math display\">\\[\r\nv = \\alpha v - \\epsilon \\nabla_\\theta(\\frac{1}{m}\\sum_{i=1}^{m}L(f(x^{(i)};\\theta), y^{(i)}))\\\\\r\n\\theta \\leftarrow \\theta + v\r\n\\]</span> 如果每次梯度都是<span class=\"math inline\">\\(g\\)</span>，那么最终的<span class=\"math inline\">\\(v\\)</span>会趋近于<span class=\"math inline\">\\(\\frac{\\epsilon g}{1 - \\alpha}\\)</span></p>\r\n<h2 id=\"sgd中的nesterov-momentum方法\">SGD中的Nesterov Momentum方法</h2>\r\n<p><span class=\"math display\">\\[\r\nv = \\alpha v - \\epsilon \\nabla_\\theta(\\frac{1}{m}\\sum_{i=1}^{m}L(f(x^{(i)};\\theta + \\alpha v), y^{(i)}))\\\\\r\n\\theta \\leftarrow \\theta + v\r\n\\]</span></p>\r\n<h2 id=\"网络权重随机初始化的原因\">网络权重随机初始化的原因</h2>\r\n<p>如果两个计算单元具有相同的输入和激活函数，如果初始化相同，那么这两个计算单元在优化过程中很可能一直同步，最终出现冗余单元的情况，即两个计算单元计算的是相同的函数，重复计算没有意义。</p>\r\n<h1 id=\"section-2\">2019-04-12</h1>\r\n<h2 id=\"adagrad优化算法\">AdaGrad优化算法</h2>\r\n<p>Global learning rate <span class=\"math inline\">\\(\\epsilon\\)</span></p>\r\n<p>Initial parameter <span class=\"math inline\">\\(\\theta\\)</span></p>\r\n<p>Small constant <span class=\"math inline\">\\(\\delta\\)</span></p>\r\n<p>Initial variable <span class=\"math inline\">\\(r = 0\\)</span></p>\r\n<p>batch size <span class=\"math inline\">\\(m\\)</span></p>\r\n<p><span class=\"math inline\">\\(A \\odot B\\)</span>: hadamard product of <span class=\"math inline\">\\(A\\)</span> and <span class=\"math inline\">\\(B\\)</span></p>\r\n<p>每次计算迭代过程如下： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\ng &amp;\\leftarrow \\frac{1}{m} \\nabla_\\theta\\sum_iL(f(x^{(i)}; \\theta), y^{(i)})\\\\\r\nr &amp;\\leftarrow r + g \\odot g\\\\\r\n\\Delta \\theta &amp;\\leftarrow -\\frac{\\epsilon}{\\delta + \\sqrt{r}} \\odot g\\\\\r\n\\theta &amp;\\leftarrow \\theta + \\Delta \\theta    \r\n\\end{aligned}\r\n\\]</span> 此算法主要用于凸函数的优化问题，在梯度较小的地方可以加大参数移动速度，但是我觉得这个算法不好，因为对梯度的方向进行了更改，而且<span class=\"math inline\">\\(r\\)</span>项一直增大，会使得学习过程很快趋于停滞。</p>\r\n<h2 id=\"rmsprop优化算法\">RMSProp优化算法</h2>\r\n<p>Global learning rate <span class=\"math inline\">\\(\\epsilon\\)</span></p>\r\n<p>decay rate <span class=\"math inline\">\\(\\rho\\)</span></p>\r\n<p>Initial parameter <span class=\"math inline\">\\(\\theta\\)</span></p>\r\n<p>Small constant <span class=\"math inline\">\\(\\delta\\)</span></p>\r\n<p>Initial variable <span class=\"math inline\">\\(r = 0\\)</span></p>\r\n<p>batch size <span class=\"math inline\">\\(m\\)</span></p>\r\n<p><span class=\"math inline\">\\(A \\odot B\\)</span>: hadamard product of <span class=\"math inline\">\\(A\\)</span> and <span class=\"math inline\">\\(B\\)</span></p>\r\n<p>每次计算迭代过程如下： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\ng &amp;\\leftarrow \\frac{1}{m} \\nabla_\\theta\\sum_iL(f(x^{(i)}; \\theta), y^{(i)})\\\\\r\nr &amp;\\leftarrow \\rho r + (1 - \\rho)g \\odot g\\\\\r\n\\Delta \\theta &amp;\\leftarrow -\\frac{\\epsilon}{\\sqrt{\\delta + r}} \\odot g\\\\\r\n\\theta &amp;\\leftarrow \\theta + \\Delta \\theta    \r\n\\end{aligned}\r\n\\]</span> 在AdaGrad算法的基础上，通过衰减参数<span class=\"math inline\">\\(\\rho\\)</span>可以避免学习过程趋于停滞。</p>\r\n<h2 id=\"带有nesterov-momentum-的rmsprop优化算法\">带有Nesterov Momentum 的RMSProp优化算法</h2>\r\n<p>Global learning rate <span class=\"math inline\">\\(\\epsilon\\)</span></p>\r\n<p>decay rate <span class=\"math inline\">\\(\\rho\\)</span></p>\r\n<p>momentum coefficent <span class=\"math inline\">\\(\\alpha\\)</span></p>\r\n<p>Initial parameter <span class=\"math inline\">\\(\\theta\\)</span></p>\r\n<p>Small constant <span class=\"math inline\">\\(\\delta\\)</span></p>\r\n<p>initial velocity <span class=\"math inline\">\\(v\\)</span></p>\r\n<p>Initial variable <span class=\"math inline\">\\(r = 0\\)</span></p>\r\n<p>batch size <span class=\"math inline\">\\(m\\)</span></p>\r\n<p><span class=\"math inline\">\\(A \\odot B\\)</span>: hadamard product of <span class=\"math inline\">\\(A\\)</span> and <span class=\"math inline\">\\(B\\)</span></p>\r\n<p>每次计算迭代过程如下： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n\\tilde{\\theta} &amp;\\leftarrow \\theta + \\alpha v\\\\\r\ng &amp;\\leftarrow \\frac{1}{m} \\nabla_{\\tilde{\\theta}}\\sum_iL(f(x^{(i)}; \\tilde{\\theta}), y^{(i)})\\\\\r\nr &amp;\\leftarrow \\rho r + (1 - \\rho)g \\odot g\\\\\r\nv &amp;\\leftarrow \\alpha v -\\frac{\\epsilon}{\\sqrt{r}} \\odot g\\\\\r\n\\theta &amp;\\leftarrow \\theta + v\r\n\\end{aligned}\r\n\\]</span> 这里计算了Nesterov动量项以应对可能的梯度估计偏差和参数空间病态Hessian矩阵问题。</p>\r\n<h2 id=\"adam优化算法\">Adam优化算法</h2>\r\n<p>Step size <span class=\"math inline\">\\(\\epsilon\\)</span></p>\r\n<p>Exponential decay rates for moment estimates <span class=\"math inline\">\\(\\rho_1\\)</span> <span class=\"math inline\">\\(\\rho_2\\)</span></p>\r\n<p>Small constant <span class=\"math inline\">\\(\\delta\\)</span></p>\r\n<p>Initial parameter <span class=\"math inline\">\\(\\theta\\)</span></p>\r\n<p>Initialize 1st and 2nd moment variables <span class=\"math inline\">\\(s=0\\)</span> <span class=\"math inline\">\\(r=0\\)</span></p>\r\n<p>Initialize time step <span class=\"math inline\">\\(t=0\\)</span></p>\r\n<p><span class=\"math inline\">\\(A \\odot B\\)</span>: hadamard product of <span class=\"math inline\">\\(A\\)</span> and <span class=\"math inline\">\\(B\\)</span></p>\r\n<p>每次计算迭代过程如下:</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    g &amp;\\leftarrow \\frac{1}{m} \\nabla_\\theta\\sum_iL(f(x^{(i)}; \\theta), y^{(i)})\\\\\r\n    t &amp;\\leftarrow t + 1\\\\\r\n    s &amp;\\leftarrow \\rho_1 s + (1-\\rho_1) g\\\\\r\n    r &amp;\\leftarrow \\rho_2 r + (1-\\rho_2) g \\odot g\\\\\r\n    \\hat{s} &amp;\\leftarrow \\frac{s}{1-\\rho_1^t}\\\\\r\n    \\hat{r} &amp;\\leftarrow \\frac{r}{1 - \\rho_2^t}\\\\\r\n    \\Delta_\\theta &amp;\\leftarrow -\\epsilon \\frac{\\hat{s}}{\\sqrt{\\hat{r}} + \\delta}\\\\\r\n    \\theta &amp;\\leftarrow \\theta + \\Delta_\\theta\r\n\\end{aligned}\r\n\\]</span></p>\r\n<h2 id=\"牛顿法\">牛顿法</h2>\r\n<p>首先通过二阶估计来表达损失函数： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\hat{J}(\\theta) &amp;\\approx J(\\theta_0) + (\\theta - \\theta_0)^T\\nabla_\\theta J(\\theta_0) + \\frac{1}{2}(\\theta - \\theta_0)^TH(\\theta - \\theta_0)\\\\\r\n\\end{aligned}\r\n\\]</span> 其一阶导数： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n\\frac{\\partial \\hat{J}}{\\partial \\theta} &amp;= (\\nabla_\\theta J(\\theta_0))^T + \\frac{1}{2}(H + H^T)(\\theta - \\theta_0)\\\\\r\n&amp;= \\nabla_\\theta J(\\theta_0) + H(\\theta - \\theta_0)\r\n\\end{aligned}\r\n\\]</span> 对于凸函数，当一阶导数为0时，有可以直接得到最优解： <span class=\"math display\">\\[\r\n\\nabla_\\theta J(\\theta_0) + H(\\theta - \\theta_0) = 0\\\\\r\n\\theta = \\theta_0 - H^{-1}\\nabla_\\theta J(\\theta_0)\r\n\\]</span> 对于非凸函数，牛顿法不能有效的收敛。</p>\r\n<h1 id=\"section-3\">2019-04-27</h1>\r\n<h2 id=\"论文feature-selective-anchor-free-module-for-single-shot-object-detection\">论文：Feature Selective Anchor-Free Module for Single-Shot Object Detection</h2>\r\n<p>这篇论文对retinanet进行了改进，在原始的anchor-based分支上，新增了一个anchor-free分支。</p>\r\n<p>目前一阶段的目标检测大部分都是基于先验框进行，就是对于所谓的anchor(锚点)，事先定义好anchor box的大小、比例、个数，模型的回归输出作为基于anchor box的偏移，训练时将目标检测框映射分配到anchor box上，指定回归输出的优化目标，而inference过程是将和先验anchor box一起计算得到最终的predict box。</p>\r\n<p>anchor-free分支不需要计算先验框，不需要定义anchor box的大小、比例、个数等超参数，而是直接将目标检测框映射到feature map上，并定义有效范围和忽略范围，feature map上在有效范围内的点作为正样本，在忽略范围外的点作为负样本进行训练，训练时，对于多尺度feature map的选择则根据每种大小的feature map的损失函数来计算，训练时只选择损失函数最小的feature map层进行训练。</p>\r\n<h2 id=\"论文fcos-fully-convolutional-one-stage-object-detection\">论文FCOS: Fully Convolutional One-Stage Object Detection</h2>\r\n<p>这篇论文也致力于目标检测的模型去anchor化工作，将anchor-based的预测修改为anchor-free的方式，但是Feature Selective Anchor-Free Module for Single-Shot Object Detection论文中不同的是，这里的anchor-free分支不需要计算有效范围和忽略范围，而是直接将目标框在feature map上的投影覆盖的所有点作为正样本点，其余的直接作为负样本点，这样可以提升检测的召回率，但是必然提升检测的假阳性率，因此论文中针对这个问题，提出了center-ness层，在训练过程中，让这一层来预测当前像素点到gt中心的距离度量，这一层的训练目标被定义为如下： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    centerness^* = \\sqrt{\\frac{\\min(l^*, r^*)}{\\max(l^*, r^*)}\\frac{\\min(t^*, b^*)}{\\max(t^*, b^*)}}\r\n\\end{aligned}\r\n\\]</span> 其中<span class=\"math inline\">\\(t^*\\ b^*\\ l^*\\ r^*\\)</span>四个值的定义如下图，这里截取的整张图都在同一个目标框的范围内，这里计算的就是两个轴交点的<span class=\"math inline\">\\(centerness^*\\)</span>值：</p>\r\n<img src=\"/2019/04/06/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2019-04/center_ness.png\" class=\"\" title=\"center ness示意\">\r\n<p>距离目标框中心越近，<span class=\"math inline\">\\(centerness^*\\)</span>值越大，在进行inference的时候，<span class=\"math inline\">\\(centerness^*\\)</span>值将作为feature map上每个点的预测权重，用于抑制和目标框偏移较大的点的预测值。</p>\r\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"section\">2019-04-06</h1>\r\n<h2 id=\"对于随机批梯度下降理解\">对于随机批梯度下降理解</h2>\r\n<ul>\r\n<li>对于batch size：从n个训练数据对真实数据分布进行估计，其标准差可以写为<span class=\"math inline\">\\(\\sigma/\\sqrt{n}\\)</span>，因此加大batch size对模型的学习效果的提升不是线性的。</li>\r\n<li>对于数据：随机批梯度下降，可以减少计算的冗余度，因为很多数据其求出的梯度基本相同，最坏的情况就是训练集中每个数据都是相同的，如果使用原始的梯度下降算法，对于整个训练集进行梯度计算，最后得到的梯度和根据单个样本计算的梯度相同，存在大量计算冗余，随机批梯度下降对这种问题有一定缓解作用。</li>\r\n<li>小的batch size：小的batch size 可以起到正则化的作用，有可能是因为小的batch size在估计梯度的过程中，引入了一定地噪声，从而可以使得模型的泛化误差减小，但是小的batch size 需要更小的学习速率和更多的学习步数。</li>\r\n<li>对于优化算法：不同的优化算法对于batch size有不一样的要求，因为不同的优化算法对采样误差的敏感程度不同，可能一些优化算法需要从样本中提取的信息很难从少量的样本中估计出来，因此需要设置batch size为一个较大值才会有较好效果。仅仅直接使用梯度<span class=\"math inline\">\\(g\\)</span>的优化算法往往有较好的鲁棒性，并一般可以适应100这种小的batch size。</li>\r\n</ul>\r\n<h2 id=\"病态矩阵与条件数理解\">病态矩阵与条件数理解</h2>\r\n<p>对于矩阵方程如下： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    A x = b\r\n\\end{aligned}\r\n\\]</span> 若对于一个较小的样本观察误差<span class=\"math inline\">\\(\\Delta b\\)</span>，使得<span class=\"math inline\">\\(A(x+\\Delta x)=b + \\Delta b\\)</span>，<span class=\"math inline\">\\(\\Delta x\\)</span>如果比<span class=\"math inline\">\\(\\Delta b\\)</span>大很多，则称矩阵A是一个病态矩阵，一个典型的病态矩阵如下： <span class=\"math display\">\\[\r\n\\begin{bmatrix}\r\n    1000 &amp;1000\\\\\r\n    0 &amp;0.001\r\n\\end{bmatrix}\r\n\\]</span> 其中两个列向量的相关性非常大，夹角非常小，表示的特征太过相似，若<span class=\"math inline\">\\(b\\)</span>是第一个列向量方向的单位向量,则求出的解为<span class=\"math inline\">\\([n, 0]^T\\)</span>，若<span class=\"math inline\">\\(b\\)</span>稍微偏差一点，变到了第二个列向量的方向，则求出来的解变为<span class=\"math inline\">\\([0\\ 1]^T\\)</span>，两次解的差距非常大，仅仅因为<span class=\"math inline\">\\(b\\)</span>上有非常小的偏移。</p>\r\n<p>对于某种矩阵范数<span class=\"math inline\">\\(||A||\\)</span>以及某种向量范数<span class=\"math inline\">\\(||b||\\)</span>，若: <span class=\"math display\">\\[\r\nA(x+\\Delta x)=b + \\Delta b\r\n\\]</span> 则 <span class=\"math display\">\\[\r\nA\\Delta x = \\Delta b\r\n\\]</span> 即 <span class=\"math display\">\\[\r\n\\Delta x = A^{-1}\\Delta b\r\n\\]</span> 则有 <span class=\"math display\">\\[\r\n||\\Delta x|| \\le ||A^{-1}||\\cdot||\\Delta b||\r\n\\]</span> 同时有 <span class=\"math display\">\\[\r\n||A|| \\cdot ||x|| \\ge ||b||\r\n\\]</span> 因此 <span class=\"math display\">\\[\r\n\\frac{||\\Delta x||}{||A|| \\cdot ||x||} \\le \\frac{||A^{-1}||\\cdot||\\Delta b||}{||b||}\r\n\\]</span> <span class=\"math display\">\\[\r\n\\frac{||\\Delta x||}{||x||} \\le (||A^{-1}||\\cdot||A||)\\frac{||\\Delta b||}{||b||}\r\n\\]</span> 这里将<span class=\"math inline\">\\(||A^{-1}||\\cdot||A||\\)</span>称为矩阵A的条件数，对于不同的范数，条件数各有不同，但都反映了矩阵A的病态程度，条件数越大，矩阵呈现更加明显的病态特征。</p>\r\n<h1 id=\"section-1\">2019-04-11</h1>\r\n<h2 id=\"sgd中的momentum方法\">SGD中的Momentum方法</h2>\r\n<p>momentum方法主要针对两个问题： - 参数空间存在条件数较大的病态Hessian矩阵。 - 随机梯度下降对梯度的估计存在偏差。 <span class=\"math display\">\\[\r\nv = \\alpha v - \\epsilon \\nabla_\\theta(\\frac{1}{m}\\sum_{i=1}^{m}L(f(x^{(i)};\\theta), y^{(i)}))\\\\\r\n\\theta \\leftarrow \\theta + v\r\n\\]</span> 如果每次梯度都是<span class=\"math inline\">\\(g\\)</span>，那么最终的<span class=\"math inline\">\\(v\\)</span>会趋近于<span class=\"math inline\">\\(\\frac{\\epsilon g}{1 - \\alpha}\\)</span></p>\r\n<h2 id=\"sgd中的nesterov-momentum方法\">SGD中的Nesterov Momentum方法</h2>\r\n<p><span class=\"math display\">\\[\r\nv = \\alpha v - \\epsilon \\nabla_\\theta(\\frac{1}{m}\\sum_{i=1}^{m}L(f(x^{(i)};\\theta + \\alpha v), y^{(i)}))\\\\\r\n\\theta \\leftarrow \\theta + v\r\n\\]</span></p>\r\n<h2 id=\"网络权重随机初始化的原因\">网络权重随机初始化的原因</h2>\r\n<p>如果两个计算单元具有相同的输入和激活函数，如果初始化相同，那么这两个计算单元在优化过程中很可能一直同步，最终出现冗余单元的情况，即两个计算单元计算的是相同的函数，重复计算没有意义。</p>\r\n<h1 id=\"section-2\">2019-04-12</h1>\r\n<h2 id=\"adagrad优化算法\">AdaGrad优化算法</h2>\r\n<p>Global learning rate <span class=\"math inline\">\\(\\epsilon\\)</span></p>\r\n<p>Initial parameter <span class=\"math inline\">\\(\\theta\\)</span></p>\r\n<p>Small constant <span class=\"math inline\">\\(\\delta\\)</span></p>\r\n<p>Initial variable <span class=\"math inline\">\\(r = 0\\)</span></p>\r\n<p>batch size <span class=\"math inline\">\\(m\\)</span></p>\r\n<p><span class=\"math inline\">\\(A \\odot B\\)</span>: hadamard product of <span class=\"math inline\">\\(A\\)</span> and <span class=\"math inline\">\\(B\\)</span></p>\r\n<p>每次计算迭代过程如下： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\ng &amp;\\leftarrow \\frac{1}{m} \\nabla_\\theta\\sum_iL(f(x^{(i)}; \\theta), y^{(i)})\\\\\r\nr &amp;\\leftarrow r + g \\odot g\\\\\r\n\\Delta \\theta &amp;\\leftarrow -\\frac{\\epsilon}{\\delta + \\sqrt{r}} \\odot g\\\\\r\n\\theta &amp;\\leftarrow \\theta + \\Delta \\theta    \r\n\\end{aligned}\r\n\\]</span> 此算法主要用于凸函数的优化问题，在梯度较小的地方可以加大参数移动速度，但是我觉得这个算法不好，因为对梯度的方向进行了更改，而且<span class=\"math inline\">\\(r\\)</span>项一直增大，会使得学习过程很快趋于停滞。</p>\r\n<h2 id=\"rmsprop优化算法\">RMSProp优化算法</h2>\r\n<p>Global learning rate <span class=\"math inline\">\\(\\epsilon\\)</span></p>\r\n<p>decay rate <span class=\"math inline\">\\(\\rho\\)</span></p>\r\n<p>Initial parameter <span class=\"math inline\">\\(\\theta\\)</span></p>\r\n<p>Small constant <span class=\"math inline\">\\(\\delta\\)</span></p>\r\n<p>Initial variable <span class=\"math inline\">\\(r = 0\\)</span></p>\r\n<p>batch size <span class=\"math inline\">\\(m\\)</span></p>\r\n<p><span class=\"math inline\">\\(A \\odot B\\)</span>: hadamard product of <span class=\"math inline\">\\(A\\)</span> and <span class=\"math inline\">\\(B\\)</span></p>\r\n<p>每次计算迭代过程如下： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\ng &amp;\\leftarrow \\frac{1}{m} \\nabla_\\theta\\sum_iL(f(x^{(i)}; \\theta), y^{(i)})\\\\\r\nr &amp;\\leftarrow \\rho r + (1 - \\rho)g \\odot g\\\\\r\n\\Delta \\theta &amp;\\leftarrow -\\frac{\\epsilon}{\\sqrt{\\delta + r}} \\odot g\\\\\r\n\\theta &amp;\\leftarrow \\theta + \\Delta \\theta    \r\n\\end{aligned}\r\n\\]</span> 在AdaGrad算法的基础上，通过衰减参数<span class=\"math inline\">\\(\\rho\\)</span>可以避免学习过程趋于停滞。</p>\r\n<h2 id=\"带有nesterov-momentum-的rmsprop优化算法\">带有Nesterov Momentum 的RMSProp优化算法</h2>\r\n<p>Global learning rate <span class=\"math inline\">\\(\\epsilon\\)</span></p>\r\n<p>decay rate <span class=\"math inline\">\\(\\rho\\)</span></p>\r\n<p>momentum coefficent <span class=\"math inline\">\\(\\alpha\\)</span></p>\r\n<p>Initial parameter <span class=\"math inline\">\\(\\theta\\)</span></p>\r\n<p>Small constant <span class=\"math inline\">\\(\\delta\\)</span></p>\r\n<p>initial velocity <span class=\"math inline\">\\(v\\)</span></p>\r\n<p>Initial variable <span class=\"math inline\">\\(r = 0\\)</span></p>\r\n<p>batch size <span class=\"math inline\">\\(m\\)</span></p>\r\n<p><span class=\"math inline\">\\(A \\odot B\\)</span>: hadamard product of <span class=\"math inline\">\\(A\\)</span> and <span class=\"math inline\">\\(B\\)</span></p>\r\n<p>每次计算迭代过程如下： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n\\tilde{\\theta} &amp;\\leftarrow \\theta + \\alpha v\\\\\r\ng &amp;\\leftarrow \\frac{1}{m} \\nabla_{\\tilde{\\theta}}\\sum_iL(f(x^{(i)}; \\tilde{\\theta}), y^{(i)})\\\\\r\nr &amp;\\leftarrow \\rho r + (1 - \\rho)g \\odot g\\\\\r\nv &amp;\\leftarrow \\alpha v -\\frac{\\epsilon}{\\sqrt{r}} \\odot g\\\\\r\n\\theta &amp;\\leftarrow \\theta + v\r\n\\end{aligned}\r\n\\]</span> 这里计算了Nesterov动量项以应对可能的梯度估计偏差和参数空间病态Hessian矩阵问题。</p>\r\n<h2 id=\"adam优化算法\">Adam优化算法</h2>\r\n<p>Step size <span class=\"math inline\">\\(\\epsilon\\)</span></p>\r\n<p>Exponential decay rates for moment estimates <span class=\"math inline\">\\(\\rho_1\\)</span> <span class=\"math inline\">\\(\\rho_2\\)</span></p>\r\n<p>Small constant <span class=\"math inline\">\\(\\delta\\)</span></p>\r\n<p>Initial parameter <span class=\"math inline\">\\(\\theta\\)</span></p>\r\n<p>Initialize 1st and 2nd moment variables <span class=\"math inline\">\\(s=0\\)</span> <span class=\"math inline\">\\(r=0\\)</span></p>\r\n<p>Initialize time step <span class=\"math inline\">\\(t=0\\)</span></p>\r\n<p><span class=\"math inline\">\\(A \\odot B\\)</span>: hadamard product of <span class=\"math inline\">\\(A\\)</span> and <span class=\"math inline\">\\(B\\)</span></p>\r\n<p>每次计算迭代过程如下:</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    g &amp;\\leftarrow \\frac{1}{m} \\nabla_\\theta\\sum_iL(f(x^{(i)}; \\theta), y^{(i)})\\\\\r\n    t &amp;\\leftarrow t + 1\\\\\r\n    s &amp;\\leftarrow \\rho_1 s + (1-\\rho_1) g\\\\\r\n    r &amp;\\leftarrow \\rho_2 r + (1-\\rho_2) g \\odot g\\\\\r\n    \\hat{s} &amp;\\leftarrow \\frac{s}{1-\\rho_1^t}\\\\\r\n    \\hat{r} &amp;\\leftarrow \\frac{r}{1 - \\rho_2^t}\\\\\r\n    \\Delta_\\theta &amp;\\leftarrow -\\epsilon \\frac{\\hat{s}}{\\sqrt{\\hat{r}} + \\delta}\\\\\r\n    \\theta &amp;\\leftarrow \\theta + \\Delta_\\theta\r\n\\end{aligned}\r\n\\]</span></p>\r\n<h2 id=\"牛顿法\">牛顿法</h2>\r\n<p>首先通过二阶估计来表达损失函数： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\hat{J}(\\theta) &amp;\\approx J(\\theta_0) + (\\theta - \\theta_0)^T\\nabla_\\theta J(\\theta_0) + \\frac{1}{2}(\\theta - \\theta_0)^TH(\\theta - \\theta_0)\\\\\r\n\\end{aligned}\r\n\\]</span> 其一阶导数： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n\\frac{\\partial \\hat{J}}{\\partial \\theta} &amp;= (\\nabla_\\theta J(\\theta_0))^T + \\frac{1}{2}(H + H^T)(\\theta - \\theta_0)\\\\\r\n&amp;= \\nabla_\\theta J(\\theta_0) + H(\\theta - \\theta_0)\r\n\\end{aligned}\r\n\\]</span> 对于凸函数，当一阶导数为0时，有可以直接得到最优解： <span class=\"math display\">\\[\r\n\\nabla_\\theta J(\\theta_0) + H(\\theta - \\theta_0) = 0\\\\\r\n\\theta = \\theta_0 - H^{-1}\\nabla_\\theta J(\\theta_0)\r\n\\]</span> 对于非凸函数，牛顿法不能有效的收敛。</p>\r\n<h1 id=\"section-3\">2019-04-27</h1>\r\n<h2 id=\"论文feature-selective-anchor-free-module-for-single-shot-object-detection\">论文：Feature Selective Anchor-Free Module for Single-Shot Object Detection</h2>\r\n<p>这篇论文对retinanet进行了改进，在原始的anchor-based分支上，新增了一个anchor-free分支。</p>\r\n<p>目前一阶段的目标检测大部分都是基于先验框进行，就是对于所谓的anchor(锚点)，事先定义好anchor box的大小、比例、个数，模型的回归输出作为基于anchor box的偏移，训练时将目标检测框映射分配到anchor box上，指定回归输出的优化目标，而inference过程是将和先验anchor box一起计算得到最终的predict box。</p>\r\n<p>anchor-free分支不需要计算先验框，不需要定义anchor box的大小、比例、个数等超参数，而是直接将目标检测框映射到feature map上，并定义有效范围和忽略范围，feature map上在有效范围内的点作为正样本，在忽略范围外的点作为负样本进行训练，训练时，对于多尺度feature map的选择则根据每种大小的feature map的损失函数来计算，训练时只选择损失函数最小的feature map层进行训练。</p>\r\n<h2 id=\"论文fcos-fully-convolutional-one-stage-object-detection\">论文FCOS: Fully Convolutional One-Stage Object Detection</h2>\r\n<p>这篇论文也致力于目标检测的模型去anchor化工作，将anchor-based的预测修改为anchor-free的方式，但是Feature Selective Anchor-Free Module for Single-Shot Object Detection论文中不同的是，这里的anchor-free分支不需要计算有效范围和忽略范围，而是直接将目标框在feature map上的投影覆盖的所有点作为正样本点，其余的直接作为负样本点，这样可以提升检测的召回率，但是必然提升检测的假阳性率，因此论文中针对这个问题，提出了center-ness层，在训练过程中，让这一层来预测当前像素点到gt中心的距离度量，这一层的训练目标被定义为如下： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    centerness^* = \\sqrt{\\frac{\\min(l^*, r^*)}{\\max(l^*, r^*)}\\frac{\\min(t^*, b^*)}{\\max(t^*, b^*)}}\r\n\\end{aligned}\r\n\\]</span> 其中<span class=\"math inline\">\\(t^*\\ b^*\\ l^*\\ r^*\\)</span>四个值的定义如下图，这里截取的整张图都在同一个目标框的范围内，这里计算的就是两个轴交点的<span class=\"math inline\">\\(centerness^*\\)</span>值：</p>\r\n<img src=\"/2019/04/06/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2019-04/center_ness.png\" class=\"\" title=\"center ness示意\">\r\n<p>距离目标框中心越近，<span class=\"math inline\">\\(centerness^*\\)</span>值越大，在进行inference的时候，<span class=\"math inline\">\\(centerness^*\\)</span>值将作为feature map上每个点的预测权重，用于抑制和目标框偏移较大的点的预测值。</p>\r\n"},{"title":"学习笔记-2019-06","date":"2019-06-20T11:13:39.000Z","mathjax":true,"_content":"\n# 2019-06-20\n## 课程学习在弱监督学习中的应用\n在网上看到了关于CVPR2017 WebVision图片分类竞赛冠军的技术分享，大致整理其主要思路。\n\n这个比赛的数据非常多，但是标签可能存在问题，要求是在存在问题标签的数据中进行模型训练。\n\n这里主要使用了一种课程学习的思想，先让模型学习简单的样本，然后再让其学习较困难的样本，循序渐进，得到最终的训练模型。\n\n难点在于没有先验知识的情况下定义数据集或者图片的难易程度。\n\n其基本思想是在同一个类别的图像的特征空间中，越密集的地方的图像特征越相似，表示图片类别越可能正确，根据这个思想将图片分类成不同的难易程度，分步，使用不同权重对待不同数据，进行模型训练。\n\n这里先介绍一种样本分析方法density-distance：对于样本$P_i$，其特征$f(P_i)$，计算所有样本之间的特征距离矩阵：$D_{ij} = {||f(P_i) - f(P_j)||} ^ 2$，定义样本密度：$\\rho_i = \\sum_i X(D_{ij} - d_c) \\quad X(d) = \\begin{cases} 1 & d < 0 \\\\ 0 & other \\end{cases}$，其中$d_c$是一个超参数， 计算每个样本的密度距离值：$\\delta_i = \\begin{cases}min_{j:\\rho_j > \\rho_i}(D_{ij}) & if \\quad \\exists j s.t. \\rho_j > \\rho_i \\\\ max(D_{ij}) & otherwise \\end{cases}$，将$\\rho_i$和$\\delta_i$相乘，值越大的说明可能在聚类中心，数据标签越可能正确。\n\n对数据进行分类的具体实现步骤如下：\n\n- 设计课程\n  - 使用所有数据($P_i$)训练一个特征提取模型：$f$\n  - 对所有样本的进行特征提取：$f(P_i)$\n  - 对每个类别的图片分别进行density-distance分析\n  - 按照$\\rho_i \\times \\delta_i$结果划分出三个子类，分别代表难易度不同的图片，作为先后训练的数据\n- 课程训练\n  - 首先使用第一类最简单的数据进行训练\n  - 为第二类数据设置权重0.5，继续训练\n  - 为第三类数据设置权重0.5，继续训练\n\n# 2019-06-29\n## BCE、CE两个分类损失与激活函数的关系\n网上都说BCE用于二分类，CE用于多分类，我认为这里有些问题可以讨论，下面的例子基于tensorflow和keras。\n\nBCE和CE是常用的分类损失函数，计算公式如下。\n$$\n\\begin{aligned}\n    BCE(x)_i &= -(y_i log(f(x)_i) + (1 - y_i) log(1 - f(x)_i)) \\\\\n    BCE_{loss}(x) &= \\frac{\\sum_{i=0}^{C}BCE(x)_i}{C}\\\\\n    CE_{loss}(x) &= CE(x) = \\sum_{i=0}^{C}-y_i log(f(x)_i)\n\\end{aligned}\n$$\n从上面可以看出，CE是个标量，直接作为loss函数使用，而BCE计算出的是一个向量，在keras的实现中，BCE损失最终需要对每个类别求平均才能作为loss函数使用。\n\n而对于一个batch的数据，在keras实现中，BCE和CE都是直接对每个样本求平均。\n$$\n\\begin{aligned}\n    BCE_{final} &= \\frac{\\sum_{b=1}^{N} BCE_{loss}(x^{(b)}}{N}\\\\\n    CE_{final} &= \\frac{\\sum_{b=1}^{N} CE_{loss}(x^{(b)}}{N}\n\\end{aligned}\n$$\n\n对于一般的分类任务，最终都是一个全连接层变为输出向量$O$，之后再经过sigmoid或者softmax变为预测概率向量。\n\n$$\n\\begin{aligned}\n    O_{sigmoid} = [\\frac{1}{1 + e^{-O_1}} \\quad \\frac{1}{1 + e^{-O_2}} \\quad ... \\quad \\frac{1}{1 + e^{-O_c}}]\\\\\n    O_{softmax} = [\\frac{O_1}{\\sum_{j = 1}^c O_j} \\quad \\frac{O_2}{\\sum_{j = 1}^c O_j} \\quad ... \\quad \\frac{O_c}{\\sum_{j = 1}^c O_j}]\n\\end{aligned}\n$$\n\n这里要注意的是，如果输出的损失函数是用的sigmoid，那么输出向量中，每个元素是相互独立的，如果这个时候使用CE作为损失函数，CE只计算了$y_i$为1的地方的损失，对于$y_i$为0的地方不考虑，所以可能导致模型训练出现问题。\n\ntensorflow作为后端的keras定义的两种损失函数见下方代码，一般使用时要注意from_logits这个参数，如果为False，则binary_crossentropy默认模型的输出是$O_{sigmoid}$而categorical_crossentropy默认模型的输出的是$O_{softmax}$BCE会尝试将$O_{sigmoid}$还原成$O$，然后调用tf.nn.sigmoid_cross_entropy_with_logits这个方法，而对于CE,因为softmax无法还原，因此keras直接自己写了个损失函数，否则会直接调用softmax_cross_entropy_with_logits这个方法。\n\n```python\ndef binary_crossentropy(target, output, from_logits=False):\n    \"\"\"Binary crossentropy between an output tensor and a target tensor.\n\n    # Arguments\n        target: A tensor with the same shape as `output`.\n        output: A tensor.\n        from_logits: Whether `output` is expected to be a logits tensor.\n            By default, we consider that `output`\n            encodes a probability distribution.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    # Note: tf.nn.sigmoid_cross_entropy_with_logits\n    # expects logits, Keras expects probabilities.\n    if not from_logits:\n        # transform back to logits\n        _epsilon = _to_tensor(epsilon(), output.dtype.base_dtype)\n        output = tf.clip_by_value(output, _epsilon, 1 - _epsilon)\n        output = tf.log(output / (1 - output))\n\n    return tf.nn.sigmoid_cross_entropy_with_logits(labels=target,\n                                                   logits=output)\n\n\ndef categorical_crossentropy(target, output, from_logits=False, axis=-1):\n    \"\"\"Categorical crossentropy between an output tensor and a target tensor.\n\n    # Arguments\n        target: A tensor of the same shape as `output`.\n        output: A tensor resulting from a softmax\n            (unless `from_logits` is True, in which\n            case `output` is expected to be the logits).\n        from_logits: Boolean, whether `output` is the\n            result of a softmax, or is a tensor of logits.\n        axis: Int specifying the channels axis. `axis=-1`\n            corresponds to data format `channels_last`,\n            and `axis=1` corresponds to data format\n            `channels_first`.\n\n    # Returns\n        Output tensor.\n\n    # Raises\n        ValueError: if `axis` is neither -1 nor one of\n            the axes of `output`.\n    \"\"\"\n    output_dimensions = list(range(len(output.get_shape())))\n    if axis != -1 and axis not in output_dimensions:\n        raise ValueError(\n            '{}{}{}'.format(\n                'Unexpected channels axis {}. '.format(axis),\n                'Expected to be -1 or one of the axes of `output`, ',\n                'which has {} dimensions.'.format(len(output.get_shape()))))\n    # Note: tf.nn.softmax_cross_entropy_with_logits\n    # expects logits, Keras expects probabilities.\n    if not from_logits:\n        # scale preds so that the class probas of each sample sum to 1\n        output /= tf.reduce_sum(output, axis, True)\n        # manual computation of crossentropy\n        _epsilon = _to_tensor(epsilon(), output.dtype.base_dtype)\n        output = tf.clip_by_value(output, _epsilon, 1. - _epsilon)\n        return - tf.reduce_sum(target * tf.log(output), axis)\n    else:\n        return tf.nn.softmax_cross_entropy_with_logits(labels=target,\n                                                       logits=output)\n```\n\n最后，总结一下就是：**BCE loss不是不能用于多分类, 但CE loss不适合单输出的分类, BCE loss最好用sigmoid激活函数，而CE loss最好用softmax函数。**\n","source":"_posts/学习笔记/学习笔记-2019-06.md","raw":"---\ntitle: 学习笔记-2019-06\ndate: 2019-06-20 19:13:39\ntags: [学习笔记，杂项]\nmathjax: true\n---\n\n# 2019-06-20\n## 课程学习在弱监督学习中的应用\n在网上看到了关于CVPR2017 WebVision图片分类竞赛冠军的技术分享，大致整理其主要思路。\n\n这个比赛的数据非常多，但是标签可能存在问题，要求是在存在问题标签的数据中进行模型训练。\n\n这里主要使用了一种课程学习的思想，先让模型学习简单的样本，然后再让其学习较困难的样本，循序渐进，得到最终的训练模型。\n\n难点在于没有先验知识的情况下定义数据集或者图片的难易程度。\n\n其基本思想是在同一个类别的图像的特征空间中，越密集的地方的图像特征越相似，表示图片类别越可能正确，根据这个思想将图片分类成不同的难易程度，分步，使用不同权重对待不同数据，进行模型训练。\n\n这里先介绍一种样本分析方法density-distance：对于样本$P_i$，其特征$f(P_i)$，计算所有样本之间的特征距离矩阵：$D_{ij} = {||f(P_i) - f(P_j)||} ^ 2$，定义样本密度：$\\rho_i = \\sum_i X(D_{ij} - d_c) \\quad X(d) = \\begin{cases} 1 & d < 0 \\\\ 0 & other \\end{cases}$，其中$d_c$是一个超参数， 计算每个样本的密度距离值：$\\delta_i = \\begin{cases}min_{j:\\rho_j > \\rho_i}(D_{ij}) & if \\quad \\exists j s.t. \\rho_j > \\rho_i \\\\ max(D_{ij}) & otherwise \\end{cases}$，将$\\rho_i$和$\\delta_i$相乘，值越大的说明可能在聚类中心，数据标签越可能正确。\n\n对数据进行分类的具体实现步骤如下：\n\n- 设计课程\n  - 使用所有数据($P_i$)训练一个特征提取模型：$f$\n  - 对所有样本的进行特征提取：$f(P_i)$\n  - 对每个类别的图片分别进行density-distance分析\n  - 按照$\\rho_i \\times \\delta_i$结果划分出三个子类，分别代表难易度不同的图片，作为先后训练的数据\n- 课程训练\n  - 首先使用第一类最简单的数据进行训练\n  - 为第二类数据设置权重0.5，继续训练\n  - 为第三类数据设置权重0.5，继续训练\n\n# 2019-06-29\n## BCE、CE两个分类损失与激活函数的关系\n网上都说BCE用于二分类，CE用于多分类，我认为这里有些问题可以讨论，下面的例子基于tensorflow和keras。\n\nBCE和CE是常用的分类损失函数，计算公式如下。\n$$\n\\begin{aligned}\n    BCE(x)_i &= -(y_i log(f(x)_i) + (1 - y_i) log(1 - f(x)_i)) \\\\\n    BCE_{loss}(x) &= \\frac{\\sum_{i=0}^{C}BCE(x)_i}{C}\\\\\n    CE_{loss}(x) &= CE(x) = \\sum_{i=0}^{C}-y_i log(f(x)_i)\n\\end{aligned}\n$$\n从上面可以看出，CE是个标量，直接作为loss函数使用，而BCE计算出的是一个向量，在keras的实现中，BCE损失最终需要对每个类别求平均才能作为loss函数使用。\n\n而对于一个batch的数据，在keras实现中，BCE和CE都是直接对每个样本求平均。\n$$\n\\begin{aligned}\n    BCE_{final} &= \\frac{\\sum_{b=1}^{N} BCE_{loss}(x^{(b)}}{N}\\\\\n    CE_{final} &= \\frac{\\sum_{b=1}^{N} CE_{loss}(x^{(b)}}{N}\n\\end{aligned}\n$$\n\n对于一般的分类任务，最终都是一个全连接层变为输出向量$O$，之后再经过sigmoid或者softmax变为预测概率向量。\n\n$$\n\\begin{aligned}\n    O_{sigmoid} = [\\frac{1}{1 + e^{-O_1}} \\quad \\frac{1}{1 + e^{-O_2}} \\quad ... \\quad \\frac{1}{1 + e^{-O_c}}]\\\\\n    O_{softmax} = [\\frac{O_1}{\\sum_{j = 1}^c O_j} \\quad \\frac{O_2}{\\sum_{j = 1}^c O_j} \\quad ... \\quad \\frac{O_c}{\\sum_{j = 1}^c O_j}]\n\\end{aligned}\n$$\n\n这里要注意的是，如果输出的损失函数是用的sigmoid，那么输出向量中，每个元素是相互独立的，如果这个时候使用CE作为损失函数，CE只计算了$y_i$为1的地方的损失，对于$y_i$为0的地方不考虑，所以可能导致模型训练出现问题。\n\ntensorflow作为后端的keras定义的两种损失函数见下方代码，一般使用时要注意from_logits这个参数，如果为False，则binary_crossentropy默认模型的输出是$O_{sigmoid}$而categorical_crossentropy默认模型的输出的是$O_{softmax}$BCE会尝试将$O_{sigmoid}$还原成$O$，然后调用tf.nn.sigmoid_cross_entropy_with_logits这个方法，而对于CE,因为softmax无法还原，因此keras直接自己写了个损失函数，否则会直接调用softmax_cross_entropy_with_logits这个方法。\n\n```python\ndef binary_crossentropy(target, output, from_logits=False):\n    \"\"\"Binary crossentropy between an output tensor and a target tensor.\n\n    # Arguments\n        target: A tensor with the same shape as `output`.\n        output: A tensor.\n        from_logits: Whether `output` is expected to be a logits tensor.\n            By default, we consider that `output`\n            encodes a probability distribution.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    # Note: tf.nn.sigmoid_cross_entropy_with_logits\n    # expects logits, Keras expects probabilities.\n    if not from_logits:\n        # transform back to logits\n        _epsilon = _to_tensor(epsilon(), output.dtype.base_dtype)\n        output = tf.clip_by_value(output, _epsilon, 1 - _epsilon)\n        output = tf.log(output / (1 - output))\n\n    return tf.nn.sigmoid_cross_entropy_with_logits(labels=target,\n                                                   logits=output)\n\n\ndef categorical_crossentropy(target, output, from_logits=False, axis=-1):\n    \"\"\"Categorical crossentropy between an output tensor and a target tensor.\n\n    # Arguments\n        target: A tensor of the same shape as `output`.\n        output: A tensor resulting from a softmax\n            (unless `from_logits` is True, in which\n            case `output` is expected to be the logits).\n        from_logits: Boolean, whether `output` is the\n            result of a softmax, or is a tensor of logits.\n        axis: Int specifying the channels axis. `axis=-1`\n            corresponds to data format `channels_last`,\n            and `axis=1` corresponds to data format\n            `channels_first`.\n\n    # Returns\n        Output tensor.\n\n    # Raises\n        ValueError: if `axis` is neither -1 nor one of\n            the axes of `output`.\n    \"\"\"\n    output_dimensions = list(range(len(output.get_shape())))\n    if axis != -1 and axis not in output_dimensions:\n        raise ValueError(\n            '{}{}{}'.format(\n                'Unexpected channels axis {}. '.format(axis),\n                'Expected to be -1 or one of the axes of `output`, ',\n                'which has {} dimensions.'.format(len(output.get_shape()))))\n    # Note: tf.nn.softmax_cross_entropy_with_logits\n    # expects logits, Keras expects probabilities.\n    if not from_logits:\n        # scale preds so that the class probas of each sample sum to 1\n        output /= tf.reduce_sum(output, axis, True)\n        # manual computation of crossentropy\n        _epsilon = _to_tensor(epsilon(), output.dtype.base_dtype)\n        output = tf.clip_by_value(output, _epsilon, 1. - _epsilon)\n        return - tf.reduce_sum(target * tf.log(output), axis)\n    else:\n        return tf.nn.softmax_cross_entropy_with_logits(labels=target,\n                                                       logits=output)\n```\n\n最后，总结一下就是：**BCE loss不是不能用于多分类, 但CE loss不适合单输出的分类, BCE loss最好用sigmoid激活函数，而CE loss最好用softmax函数。**\n","slug":"学习笔记/学习笔记-2019-06","published":1,"updated":"2019-09-23T02:45:26.395Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckjxpc3rw001344mq1rdsghw2","content":"<h1 id=\"section\">2019-06-20</h1>\r\n<h2 id=\"课程学习在弱监督学习中的应用\">课程学习在弱监督学习中的应用</h2>\r\n<p>在网上看到了关于CVPR2017 WebVision图片分类竞赛冠军的技术分享，大致整理其主要思路。</p>\r\n<p>这个比赛的数据非常多，但是标签可能存在问题，要求是在存在问题标签的数据中进行模型训练。</p>\r\n<p>这里主要使用了一种课程学习的思想，先让模型学习简单的样本，然后再让其学习较困难的样本，循序渐进，得到最终的训练模型。</p>\r\n<p>难点在于没有先验知识的情况下定义数据集或者图片的难易程度。</p>\r\n<p>其基本思想是在同一个类别的图像的特征空间中，越密集的地方的图像特征越相似，表示图片类别越可能正确，根据这个思想将图片分类成不同的难易程度，分步，使用不同权重对待不同数据，进行模型训练。</p>\r\n<p>这里先介绍一种样本分析方法density-distance：对于样本<span class=\"math inline\">\\(P_i\\)</span>，其特征<span class=\"math inline\">\\(f(P_i)\\)</span>，计算所有样本之间的特征距离矩阵：<span class=\"math inline\">\\(D_{ij} = {||f(P_i) - f(P_j)||} ^ 2\\)</span>，定义样本密度：<span class=\"math inline\">\\(\\rho_i = \\sum_i X(D_{ij} - d_c) \\quad X(d) = \\begin{cases} 1 &amp; d &lt; 0 \\\\ 0 &amp; other \\end{cases}\\)</span>，其中<span class=\"math inline\">\\(d_c\\)</span>是一个超参数， 计算每个样本的密度距离值：<span class=\"math inline\">\\(\\delta_i = \\begin{cases}min_{j:\\rho_j &gt; \\rho_i}(D_{ij}) &amp; if \\quad \\exists j s.t. \\rho_j &gt; \\rho_i \\\\ max(D_{ij}) &amp; otherwise \\end{cases}\\)</span>，将<span class=\"math inline\">\\(\\rho_i\\)</span>和<span class=\"math inline\">\\(\\delta_i\\)</span>相乘，值越大的说明可能在聚类中心，数据标签越可能正确。</p>\r\n<p>对数据进行分类的具体实现步骤如下：</p>\r\n<ul>\r\n<li>设计课程\r\n<ul>\r\n<li>使用所有数据(<span class=\"math inline\">\\(P_i\\)</span>)训练一个特征提取模型：<span class=\"math inline\">\\(f\\)</span></li>\r\n<li>对所有样本的进行特征提取：<span class=\"math inline\">\\(f(P_i)\\)</span></li>\r\n<li>对每个类别的图片分别进行density-distance分析</li>\r\n<li>按照<span class=\"math inline\">\\(\\rho_i \\times \\delta_i\\)</span>结果划分出三个子类，分别代表难易度不同的图片，作为先后训练的数据</li>\r\n</ul></li>\r\n<li>课程训练\r\n<ul>\r\n<li>首先使用第一类最简单的数据进行训练</li>\r\n<li>为第二类数据设置权重0.5，继续训练</li>\r\n<li>为第三类数据设置权重0.5，继续训练</li>\r\n</ul></li>\r\n</ul>\r\n<h1 id=\"section-1\">2019-06-29</h1>\r\n<h2 id=\"bcece两个分类损失与激活函数的关系\">BCE、CE两个分类损失与激活函数的关系</h2>\r\n<p>网上都说BCE用于二分类，CE用于多分类，我认为这里有些问题可以讨论，下面的例子基于tensorflow和keras。</p>\r\n<p>BCE和CE是常用的分类损失函数，计算公式如下。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    BCE(x)_i &amp;= -(y_i log(f(x)_i) + (1 - y_i) log(1 - f(x)_i)) \\\\\r\n    BCE_{loss}(x) &amp;= \\frac{\\sum_{i=0}^{C}BCE(x)_i}{C}\\\\\r\n    CE_{loss}(x) &amp;= CE(x) = \\sum_{i=0}^{C}-y_i log(f(x)_i)\r\n\\end{aligned}\r\n\\]</span> 从上面可以看出，CE是个标量，直接作为loss函数使用，而BCE计算出的是一个向量，在keras的实现中，BCE损失最终需要对每个类别求平均才能作为loss函数使用。</p>\r\n<p>而对于一个batch的数据，在keras实现中，BCE和CE都是直接对每个样本求平均。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    BCE_{final} &amp;= \\frac{\\sum_{b=1}^{N} BCE_{loss}(x^{(b)}}{N}\\\\\r\n    CE_{final} &amp;= \\frac{\\sum_{b=1}^{N} CE_{loss}(x^{(b)}}{N}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>对于一般的分类任务，最终都是一个全连接层变为输出向量<span class=\"math inline\">\\(O\\)</span>，之后再经过sigmoid或者softmax变为预测概率向量。</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    O_{sigmoid} = [\\frac{1}{1 + e^{-O_1}} \\quad \\frac{1}{1 + e^{-O_2}} \\quad ... \\quad \\frac{1}{1 + e^{-O_c}}]\\\\\r\n    O_{softmax} = [\\frac{O_1}{\\sum_{j = 1}^c O_j} \\quad \\frac{O_2}{\\sum_{j = 1}^c O_j} \\quad ... \\quad \\frac{O_c}{\\sum_{j = 1}^c O_j}]\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>这里要注意的是，如果输出的损失函数是用的sigmoid，那么输出向量中，每个元素是相互独立的，如果这个时候使用CE作为损失函数，CE只计算了<span class=\"math inline\">\\(y_i\\)</span>为1的地方的损失，对于<span class=\"math inline\">\\(y_i\\)</span>为0的地方不考虑，所以可能导致模型训练出现问题。</p>\r\n<p>tensorflow作为后端的keras定义的两种损失函数见下方代码，一般使用时要注意from_logits这个参数，如果为False，则binary_crossentropy默认模型的输出是<span class=\"math inline\">\\(O_{sigmoid}\\)</span>而categorical_crossentropy默认模型的输出的是<span class=\"math inline\">\\(O_{softmax}\\)</span>BCE会尝试将<span class=\"math inline\">\\(O_{sigmoid}\\)</span>还原成<span class=\"math inline\">\\(O\\)</span>，然后调用tf.nn.sigmoid_cross_entropy_with_logits这个方法，而对于CE,因为softmax无法还原，因此keras直接自己写了个损失函数，否则会直接调用softmax_cross_entropy_with_logits这个方法。</p>\r\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">binary_crossentropy</span>(<span class=\"params\">target, output, from_logits=<span class=\"literal\">False</span></span>):</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;Binary crossentropy between an output tensor and a target tensor.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    # Arguments</span></span><br><span class=\"line\"><span class=\"string\">        target: A tensor with the same shape as `output`.</span></span><br><span class=\"line\"><span class=\"string\">        output: A tensor.</span></span><br><span class=\"line\"><span class=\"string\">        from_logits: Whether `output` is expected to be a logits tensor.</span></span><br><span class=\"line\"><span class=\"string\">            By default, we consider that `output`</span></span><br><span class=\"line\"><span class=\"string\">            encodes a probability distribution.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    # Returns</span></span><br><span class=\"line\"><span class=\"string\">        A tensor.</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"comment\"># Note: tf.nn.sigmoid_cross_entropy_with_logits</span></span><br><span class=\"line\">    <span class=\"comment\"># expects logits, Keras expects probabilities.</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> from_logits:</span><br><span class=\"line\">        <span class=\"comment\"># transform back to logits</span></span><br><span class=\"line\">        _epsilon = _to_tensor(epsilon(), output.dtype.base_dtype)</span><br><span class=\"line\">        output = tf.clip_by_value(output, _epsilon, <span class=\"number\">1</span> - _epsilon)</span><br><span class=\"line\">        output = tf.log(output / (<span class=\"number\">1</span> - output))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> tf.nn.sigmoid_cross_entropy_with_logits(labels=target,</span><br><span class=\"line\">                                                   logits=output)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">categorical_crossentropy</span>(<span class=\"params\">target, output, from_logits=<span class=\"literal\">False</span>, axis=-<span class=\"number\">1</span></span>):</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;Categorical crossentropy between an output tensor and a target tensor.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    # Arguments</span></span><br><span class=\"line\"><span class=\"string\">        target: A tensor of the same shape as `output`.</span></span><br><span class=\"line\"><span class=\"string\">        output: A tensor resulting from a softmax</span></span><br><span class=\"line\"><span class=\"string\">            (unless `from_logits` is True, in which</span></span><br><span class=\"line\"><span class=\"string\">            case `output` is expected to be the logits).</span></span><br><span class=\"line\"><span class=\"string\">        from_logits: Boolean, whether `output` is the</span></span><br><span class=\"line\"><span class=\"string\">            result of a softmax, or is a tensor of logits.</span></span><br><span class=\"line\"><span class=\"string\">        axis: Int specifying the channels axis. `axis=-1`</span></span><br><span class=\"line\"><span class=\"string\">            corresponds to data format `channels_last`,</span></span><br><span class=\"line\"><span class=\"string\">            and `axis=1` corresponds to data format</span></span><br><span class=\"line\"><span class=\"string\">            `channels_first`.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    # Returns</span></span><br><span class=\"line\"><span class=\"string\">        Output tensor.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    # Raises</span></span><br><span class=\"line\"><span class=\"string\">        ValueError: if `axis` is neither -1 nor one of</span></span><br><span class=\"line\"><span class=\"string\">            the axes of `output`.</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    output_dimensions = <span class=\"built_in\">list</span>(<span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(output.get_shape())))</span><br><span class=\"line\">    <span class=\"keyword\">if</span> axis != -<span class=\"number\">1</span> <span class=\"keyword\">and</span> axis <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> output_dimensions:</span><br><span class=\"line\">        <span class=\"keyword\">raise</span> ValueError(</span><br><span class=\"line\">            <span class=\"string\">&#x27;&#123;&#125;&#123;&#125;&#123;&#125;&#x27;</span>.<span class=\"built_in\">format</span>(</span><br><span class=\"line\">                <span class=\"string\">&#x27;Unexpected channels axis &#123;&#125;. &#x27;</span>.<span class=\"built_in\">format</span>(axis),</span><br><span class=\"line\">                <span class=\"string\">&#x27;Expected to be -1 or one of the axes of `output`, &#x27;</span>,</span><br><span class=\"line\">                <span class=\"string\">&#x27;which has &#123;&#125; dimensions.&#x27;</span>.<span class=\"built_in\">format</span>(<span class=\"built_in\">len</span>(output.get_shape()))))</span><br><span class=\"line\">    <span class=\"comment\"># Note: tf.nn.softmax_cross_entropy_with_logits</span></span><br><span class=\"line\">    <span class=\"comment\"># expects logits, Keras expects probabilities.</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> from_logits:</span><br><span class=\"line\">        <span class=\"comment\"># scale preds so that the class probas of each sample sum to 1</span></span><br><span class=\"line\">        output /= tf.reduce_sum(output, axis, <span class=\"literal\">True</span>)</span><br><span class=\"line\">        <span class=\"comment\"># manual computation of crossentropy</span></span><br><span class=\"line\">        _epsilon = _to_tensor(epsilon(), output.dtype.base_dtype)</span><br><span class=\"line\">        output = tf.clip_by_value(output, _epsilon, <span class=\"number\">1.</span> - _epsilon)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> - tf.reduce_sum(target * tf.log(output), axis)</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> tf.nn.softmax_cross_entropy_with_logits(labels=target,</span><br><span class=\"line\">                                                       logits=output)</span><br></pre></td></tr></table></figure>\r\n<p>最后，总结一下就是：<strong>BCE loss不是不能用于多分类, 但CE loss不适合单输出的分类, BCE loss最好用sigmoid激活函数，而CE loss最好用softmax函数。</strong></p>\r\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"section\">2019-06-20</h1>\r\n<h2 id=\"课程学习在弱监督学习中的应用\">课程学习在弱监督学习中的应用</h2>\r\n<p>在网上看到了关于CVPR2017 WebVision图片分类竞赛冠军的技术分享，大致整理其主要思路。</p>\r\n<p>这个比赛的数据非常多，但是标签可能存在问题，要求是在存在问题标签的数据中进行模型训练。</p>\r\n<p>这里主要使用了一种课程学习的思想，先让模型学习简单的样本，然后再让其学习较困难的样本，循序渐进，得到最终的训练模型。</p>\r\n<p>难点在于没有先验知识的情况下定义数据集或者图片的难易程度。</p>\r\n<p>其基本思想是在同一个类别的图像的特征空间中，越密集的地方的图像特征越相似，表示图片类别越可能正确，根据这个思想将图片分类成不同的难易程度，分步，使用不同权重对待不同数据，进行模型训练。</p>\r\n<p>这里先介绍一种样本分析方法density-distance：对于样本<span class=\"math inline\">\\(P_i\\)</span>，其特征<span class=\"math inline\">\\(f(P_i)\\)</span>，计算所有样本之间的特征距离矩阵：<span class=\"math inline\">\\(D_{ij} = {||f(P_i) - f(P_j)||} ^ 2\\)</span>，定义样本密度：<span class=\"math inline\">\\(\\rho_i = \\sum_i X(D_{ij} - d_c) \\quad X(d) = \\begin{cases} 1 &amp; d &lt; 0 \\\\ 0 &amp; other \\end{cases}\\)</span>，其中<span class=\"math inline\">\\(d_c\\)</span>是一个超参数， 计算每个样本的密度距离值：<span class=\"math inline\">\\(\\delta_i = \\begin{cases}min_{j:\\rho_j &gt; \\rho_i}(D_{ij}) &amp; if \\quad \\exists j s.t. \\rho_j &gt; \\rho_i \\\\ max(D_{ij}) &amp; otherwise \\end{cases}\\)</span>，将<span class=\"math inline\">\\(\\rho_i\\)</span>和<span class=\"math inline\">\\(\\delta_i\\)</span>相乘，值越大的说明可能在聚类中心，数据标签越可能正确。</p>\r\n<p>对数据进行分类的具体实现步骤如下：</p>\r\n<ul>\r\n<li>设计课程\r\n<ul>\r\n<li>使用所有数据(<span class=\"math inline\">\\(P_i\\)</span>)训练一个特征提取模型：<span class=\"math inline\">\\(f\\)</span></li>\r\n<li>对所有样本的进行特征提取：<span class=\"math inline\">\\(f(P_i)\\)</span></li>\r\n<li>对每个类别的图片分别进行density-distance分析</li>\r\n<li>按照<span class=\"math inline\">\\(\\rho_i \\times \\delta_i\\)</span>结果划分出三个子类，分别代表难易度不同的图片，作为先后训练的数据</li>\r\n</ul></li>\r\n<li>课程训练\r\n<ul>\r\n<li>首先使用第一类最简单的数据进行训练</li>\r\n<li>为第二类数据设置权重0.5，继续训练</li>\r\n<li>为第三类数据设置权重0.5，继续训练</li>\r\n</ul></li>\r\n</ul>\r\n<h1 id=\"section-1\">2019-06-29</h1>\r\n<h2 id=\"bcece两个分类损失与激活函数的关系\">BCE、CE两个分类损失与激活函数的关系</h2>\r\n<p>网上都说BCE用于二分类，CE用于多分类，我认为这里有些问题可以讨论，下面的例子基于tensorflow和keras。</p>\r\n<p>BCE和CE是常用的分类损失函数，计算公式如下。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    BCE(x)_i &amp;= -(y_i log(f(x)_i) + (1 - y_i) log(1 - f(x)_i)) \\\\\r\n    BCE_{loss}(x) &amp;= \\frac{\\sum_{i=0}^{C}BCE(x)_i}{C}\\\\\r\n    CE_{loss}(x) &amp;= CE(x) = \\sum_{i=0}^{C}-y_i log(f(x)_i)\r\n\\end{aligned}\r\n\\]</span> 从上面可以看出，CE是个标量，直接作为loss函数使用，而BCE计算出的是一个向量，在keras的实现中，BCE损失最终需要对每个类别求平均才能作为loss函数使用。</p>\r\n<p>而对于一个batch的数据，在keras实现中，BCE和CE都是直接对每个样本求平均。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    BCE_{final} &amp;= \\frac{\\sum_{b=1}^{N} BCE_{loss}(x^{(b)}}{N}\\\\\r\n    CE_{final} &amp;= \\frac{\\sum_{b=1}^{N} CE_{loss}(x^{(b)}}{N}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>对于一般的分类任务，最终都是一个全连接层变为输出向量<span class=\"math inline\">\\(O\\)</span>，之后再经过sigmoid或者softmax变为预测概率向量。</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    O_{sigmoid} = [\\frac{1}{1 + e^{-O_1}} \\quad \\frac{1}{1 + e^{-O_2}} \\quad ... \\quad \\frac{1}{1 + e^{-O_c}}]\\\\\r\n    O_{softmax} = [\\frac{O_1}{\\sum_{j = 1}^c O_j} \\quad \\frac{O_2}{\\sum_{j = 1}^c O_j} \\quad ... \\quad \\frac{O_c}{\\sum_{j = 1}^c O_j}]\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>这里要注意的是，如果输出的损失函数是用的sigmoid，那么输出向量中，每个元素是相互独立的，如果这个时候使用CE作为损失函数，CE只计算了<span class=\"math inline\">\\(y_i\\)</span>为1的地方的损失，对于<span class=\"math inline\">\\(y_i\\)</span>为0的地方不考虑，所以可能导致模型训练出现问题。</p>\r\n<p>tensorflow作为后端的keras定义的两种损失函数见下方代码，一般使用时要注意from_logits这个参数，如果为False，则binary_crossentropy默认模型的输出是<span class=\"math inline\">\\(O_{sigmoid}\\)</span>而categorical_crossentropy默认模型的输出的是<span class=\"math inline\">\\(O_{softmax}\\)</span>BCE会尝试将<span class=\"math inline\">\\(O_{sigmoid}\\)</span>还原成<span class=\"math inline\">\\(O\\)</span>，然后调用tf.nn.sigmoid_cross_entropy_with_logits这个方法，而对于CE,因为softmax无法还原，因此keras直接自己写了个损失函数，否则会直接调用softmax_cross_entropy_with_logits这个方法。</p>\r\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">binary_crossentropy</span>(<span class=\"params\">target, output, from_logits=<span class=\"literal\">False</span></span>):</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;Binary crossentropy between an output tensor and a target tensor.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    # Arguments</span></span><br><span class=\"line\"><span class=\"string\">        target: A tensor with the same shape as `output`.</span></span><br><span class=\"line\"><span class=\"string\">        output: A tensor.</span></span><br><span class=\"line\"><span class=\"string\">        from_logits: Whether `output` is expected to be a logits tensor.</span></span><br><span class=\"line\"><span class=\"string\">            By default, we consider that `output`</span></span><br><span class=\"line\"><span class=\"string\">            encodes a probability distribution.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    # Returns</span></span><br><span class=\"line\"><span class=\"string\">        A tensor.</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"comment\"># Note: tf.nn.sigmoid_cross_entropy_with_logits</span></span><br><span class=\"line\">    <span class=\"comment\"># expects logits, Keras expects probabilities.</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> from_logits:</span><br><span class=\"line\">        <span class=\"comment\"># transform back to logits</span></span><br><span class=\"line\">        _epsilon = _to_tensor(epsilon(), output.dtype.base_dtype)</span><br><span class=\"line\">        output = tf.clip_by_value(output, _epsilon, <span class=\"number\">1</span> - _epsilon)</span><br><span class=\"line\">        output = tf.log(output / (<span class=\"number\">1</span> - output))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> tf.nn.sigmoid_cross_entropy_with_logits(labels=target,</span><br><span class=\"line\">                                                   logits=output)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">categorical_crossentropy</span>(<span class=\"params\">target, output, from_logits=<span class=\"literal\">False</span>, axis=-<span class=\"number\">1</span></span>):</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;Categorical crossentropy between an output tensor and a target tensor.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    # Arguments</span></span><br><span class=\"line\"><span class=\"string\">        target: A tensor of the same shape as `output`.</span></span><br><span class=\"line\"><span class=\"string\">        output: A tensor resulting from a softmax</span></span><br><span class=\"line\"><span class=\"string\">            (unless `from_logits` is True, in which</span></span><br><span class=\"line\"><span class=\"string\">            case `output` is expected to be the logits).</span></span><br><span class=\"line\"><span class=\"string\">        from_logits: Boolean, whether `output` is the</span></span><br><span class=\"line\"><span class=\"string\">            result of a softmax, or is a tensor of logits.</span></span><br><span class=\"line\"><span class=\"string\">        axis: Int specifying the channels axis. `axis=-1`</span></span><br><span class=\"line\"><span class=\"string\">            corresponds to data format `channels_last`,</span></span><br><span class=\"line\"><span class=\"string\">            and `axis=1` corresponds to data format</span></span><br><span class=\"line\"><span class=\"string\">            `channels_first`.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    # Returns</span></span><br><span class=\"line\"><span class=\"string\">        Output tensor.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    # Raises</span></span><br><span class=\"line\"><span class=\"string\">        ValueError: if `axis` is neither -1 nor one of</span></span><br><span class=\"line\"><span class=\"string\">            the axes of `output`.</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    output_dimensions = <span class=\"built_in\">list</span>(<span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(output.get_shape())))</span><br><span class=\"line\">    <span class=\"keyword\">if</span> axis != -<span class=\"number\">1</span> <span class=\"keyword\">and</span> axis <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> output_dimensions:</span><br><span class=\"line\">        <span class=\"keyword\">raise</span> ValueError(</span><br><span class=\"line\">            <span class=\"string\">&#x27;&#123;&#125;&#123;&#125;&#123;&#125;&#x27;</span>.<span class=\"built_in\">format</span>(</span><br><span class=\"line\">                <span class=\"string\">&#x27;Unexpected channels axis &#123;&#125;. &#x27;</span>.<span class=\"built_in\">format</span>(axis),</span><br><span class=\"line\">                <span class=\"string\">&#x27;Expected to be -1 or one of the axes of `output`, &#x27;</span>,</span><br><span class=\"line\">                <span class=\"string\">&#x27;which has &#123;&#125; dimensions.&#x27;</span>.<span class=\"built_in\">format</span>(<span class=\"built_in\">len</span>(output.get_shape()))))</span><br><span class=\"line\">    <span class=\"comment\"># Note: tf.nn.softmax_cross_entropy_with_logits</span></span><br><span class=\"line\">    <span class=\"comment\"># expects logits, Keras expects probabilities.</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> from_logits:</span><br><span class=\"line\">        <span class=\"comment\"># scale preds so that the class probas of each sample sum to 1</span></span><br><span class=\"line\">        output /= tf.reduce_sum(output, axis, <span class=\"literal\">True</span>)</span><br><span class=\"line\">        <span class=\"comment\"># manual computation of crossentropy</span></span><br><span class=\"line\">        _epsilon = _to_tensor(epsilon(), output.dtype.base_dtype)</span><br><span class=\"line\">        output = tf.clip_by_value(output, _epsilon, <span class=\"number\">1.</span> - _epsilon)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> - tf.reduce_sum(target * tf.log(output), axis)</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> tf.nn.softmax_cross_entropy_with_logits(labels=target,</span><br><span class=\"line\">                                                       logits=output)</span><br></pre></td></tr></table></figure>\r\n<p>最后，总结一下就是：<strong>BCE loss不是不能用于多分类, 但CE loss不适合单输出的分类, BCE loss最好用sigmoid激活函数，而CE loss最好用softmax函数。</strong></p>\r\n"},{"title":"学习笔记-2019-07","date":"2019-07-04T12:16:54.000Z","mathjax":true,"_content":"# 2019-07-04\n## 几种回归的概念\n线性回归很简单，就是$y=W^Tx + b$，而广义线性回归则可以用一个单调可微的函数$g(\\centerdot)$写成$y=g^{-1}(W^Tx + b)$或者$g(y) = W^Tx + b$,对数回归$ln(y) = W^Tx + b$就是广义线性回归的一个例子。\n\n如果要使用回归来解决分类问题，理想的情况是使用单位阶跃函数将回归值映射为类别标签，但单位阶跃函数不连续，因此使用对数几率函数来完成这个映射，对数几率函数即$\\frac{1}{1 + e^{-z}}$，是sigmoid函数的一种。\n\n这里顺便介绍下几率的概念：若一个事件发生的概率为$y$，则这个事件不发生的概率为$1-y$，两者的比值$\\frac{y}{1-y}$被称为几率，反应的是事件发生的相对可能性，对几率取对数得到对数几率$log\\frac{y}{1-y}$即为logit，逻辑回归(logistic regression)也可称为对数几率回归(logit regressoion)，就是用线性回归去逼近对数几率，如下：\n$$\n\\begin{aligned}\n    y &= \\frac{1}{1 + e^{-z}}\\\\\n    代入线&性回归得到的z：\\\\\n    y &= \\frac{1}{1 + e^{-(W^Tx + b)}}\\\\\n    log\\frac{y}{1-y} &= W^Tx + b\n\\end{aligned}\n$$\n\n## 线性判别分析(Linear Discriminant Analysis, LDA)\n线性判别分析也叫“Fisher判别分析”，其思想非常朴素：试图找到一条过原点的直线，可以表示为$y=w^Tx$(这个表达式中的$y$表示$x$投影到这条直线后和原点的距离)，使得当样本投影到该直线时不同类别样本的距离尽可能远，而相同类别的数据尽可能接近，预测时按照样本的投影位置对其进行分类。\n\n以两类数据$x_1, x_2$为例，设$\\mu_1,\\mu_2,\\Sigma_1,\\Sigma_2$分别表示两类数据的均值和方差，则投影之后的均值和方差为$w\\mu_1,w\\mu_2,w^T\\Sigma_1w,w^T\\Sigma_2w$，因为直线是一维空间，所以这些均为实数，投影之后的类内距离可以用方差来衡量，这里使用$w^T\\Sigma_1w + w^T\\Sigma_2w$来度量投影之后的类内距离，而类间距离可以写成$||w\\mu_2 - w\\mu_1||_2^2$，同时考虑两种距离，给出希望最大化的目标函数如下。\n$$\n\\begin{aligned}\nJ &= \\frac{||w^T\\mu_2 - w^T\\mu_1||_2^2}{w^T\\Sigma_1w + w^T\\Sigma_2w}\\\\\n&= \\frac{w^T(\\mu_2 - \\mu_1)(\\mu_2 - \\mu_1)^Tw}{w^T(\\Sigma_1 + \\Sigma_2)w}\n\\end{aligned}\n$$\n定义类内散度矩阵$S_w = \\Sigma_1 + \\Sigma_2$，类间散度矩阵$S_b = (\\mu_2 - \\mu_1)(\\mu_2 - \\mu_1)^T$，上面的优化目标可以简写为如下。\n$$\n\\begin{aligned}\n    J = \\frac{w^TS_bw}{w^TS_ww}\n\\end{aligned}\n$$\n这个优化目标又称为$S_b$和$S_w$的广义瑞利商，注意到分子分母中都有$w$的二次项，因此和$w$大小无关，只和w方向有关，所以优化问题可以写成下式。\n$$\n\\begin{aligned}\n\\min_w& \\quad-w^TS_bw\\\\\ns.t.& \\quad w^TS_ww = 1\n\\end{aligned}\n$$\n用拉格朗日乘子法进行优化，求解$S_bw = \\lambda S_ww$，因$S_bw$方向和$\\mu_2 - \\mu_1$相同，因此令$S_bw = \\lambda(\\mu_2 - \\mu_1)$，代入求解，可以得到$w = S_w^{-1}(\\mu_2 - \\mu_1)$。\n","source":"_posts/学习笔记/学习笔记-2019-07.md","raw":"---\ntitle: 学习笔记-2019-07\ndate: 2019-07-04 20:16:54\ntags: [学习笔记，杂项]\nmathjax: true\n---\n# 2019-07-04\n## 几种回归的概念\n线性回归很简单，就是$y=W^Tx + b$，而广义线性回归则可以用一个单调可微的函数$g(\\centerdot)$写成$y=g^{-1}(W^Tx + b)$或者$g(y) = W^Tx + b$,对数回归$ln(y) = W^Tx + b$就是广义线性回归的一个例子。\n\n如果要使用回归来解决分类问题，理想的情况是使用单位阶跃函数将回归值映射为类别标签，但单位阶跃函数不连续，因此使用对数几率函数来完成这个映射，对数几率函数即$\\frac{1}{1 + e^{-z}}$，是sigmoid函数的一种。\n\n这里顺便介绍下几率的概念：若一个事件发生的概率为$y$，则这个事件不发生的概率为$1-y$，两者的比值$\\frac{y}{1-y}$被称为几率，反应的是事件发生的相对可能性，对几率取对数得到对数几率$log\\frac{y}{1-y}$即为logit，逻辑回归(logistic regression)也可称为对数几率回归(logit regressoion)，就是用线性回归去逼近对数几率，如下：\n$$\n\\begin{aligned}\n    y &= \\frac{1}{1 + e^{-z}}\\\\\n    代入线&性回归得到的z：\\\\\n    y &= \\frac{1}{1 + e^{-(W^Tx + b)}}\\\\\n    log\\frac{y}{1-y} &= W^Tx + b\n\\end{aligned}\n$$\n\n## 线性判别分析(Linear Discriminant Analysis, LDA)\n线性判别分析也叫“Fisher判别分析”，其思想非常朴素：试图找到一条过原点的直线，可以表示为$y=w^Tx$(这个表达式中的$y$表示$x$投影到这条直线后和原点的距离)，使得当样本投影到该直线时不同类别样本的距离尽可能远，而相同类别的数据尽可能接近，预测时按照样本的投影位置对其进行分类。\n\n以两类数据$x_1, x_2$为例，设$\\mu_1,\\mu_2,\\Sigma_1,\\Sigma_2$分别表示两类数据的均值和方差，则投影之后的均值和方差为$w\\mu_1,w\\mu_2,w^T\\Sigma_1w,w^T\\Sigma_2w$，因为直线是一维空间，所以这些均为实数，投影之后的类内距离可以用方差来衡量，这里使用$w^T\\Sigma_1w + w^T\\Sigma_2w$来度量投影之后的类内距离，而类间距离可以写成$||w\\mu_2 - w\\mu_1||_2^2$，同时考虑两种距离，给出希望最大化的目标函数如下。\n$$\n\\begin{aligned}\nJ &= \\frac{||w^T\\mu_2 - w^T\\mu_1||_2^2}{w^T\\Sigma_1w + w^T\\Sigma_2w}\\\\\n&= \\frac{w^T(\\mu_2 - \\mu_1)(\\mu_2 - \\mu_1)^Tw}{w^T(\\Sigma_1 + \\Sigma_2)w}\n\\end{aligned}\n$$\n定义类内散度矩阵$S_w = \\Sigma_1 + \\Sigma_2$，类间散度矩阵$S_b = (\\mu_2 - \\mu_1)(\\mu_2 - \\mu_1)^T$，上面的优化目标可以简写为如下。\n$$\n\\begin{aligned}\n    J = \\frac{w^TS_bw}{w^TS_ww}\n\\end{aligned}\n$$\n这个优化目标又称为$S_b$和$S_w$的广义瑞利商，注意到分子分母中都有$w$的二次项，因此和$w$大小无关，只和w方向有关，所以优化问题可以写成下式。\n$$\n\\begin{aligned}\n\\min_w& \\quad-w^TS_bw\\\\\ns.t.& \\quad w^TS_ww = 1\n\\end{aligned}\n$$\n用拉格朗日乘子法进行优化，求解$S_bw = \\lambda S_ww$，因$S_bw$方向和$\\mu_2 - \\mu_1$相同，因此令$S_bw = \\lambda(\\mu_2 - \\mu_1)$，代入求解，可以得到$w = S_w^{-1}(\\mu_2 - \\mu_1)$。\n","slug":"学习笔记/学习笔记-2019-07","published":1,"updated":"2020-08-31T06:39:20.768Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckjxpc3rx001644mq8ni57qil","content":"<h1 id=\"section\">2019-07-04</h1>\r\n<h2 id=\"几种回归的概念\">几种回归的概念</h2>\r\n<p>线性回归很简单，就是<span class=\"math inline\">\\(y=W^Tx + b\\)</span>，而广义线性回归则可以用一个单调可微的函数<span class=\"math inline\">\\(g(\\centerdot)\\)</span>写成<span class=\"math inline\">\\(y=g^{-1}(W^Tx + b)\\)</span>或者<span class=\"math inline\">\\(g(y) = W^Tx + b\\)</span>,对数回归<span class=\"math inline\">\\(ln(y) = W^Tx + b\\)</span>就是广义线性回归的一个例子。</p>\r\n<p>如果要使用回归来解决分类问题，理想的情况是使用单位阶跃函数将回归值映射为类别标签，但单位阶跃函数不连续，因此使用对数几率函数来完成这个映射，对数几率函数即<span class=\"math inline\">\\(\\frac{1}{1 + e^{-z}}\\)</span>，是sigmoid函数的一种。</p>\r\n<p>这里顺便介绍下几率的概念：若一个事件发生的概率为<span class=\"math inline\">\\(y\\)</span>，则这个事件不发生的概率为<span class=\"math inline\">\\(1-y\\)</span>，两者的比值<span class=\"math inline\">\\(\\frac{y}{1-y}\\)</span>被称为几率，反应的是事件发生的相对可能性，对几率取对数得到对数几率<span class=\"math inline\">\\(log\\frac{y}{1-y}\\)</span>即为logit，逻辑回归(logistic regression)也可称为对数几率回归(logit regressoion)，就是用线性回归去逼近对数几率，如下： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    y &amp;= \\frac{1}{1 + e^{-z}}\\\\\r\n    代入线&amp;性回归得到的z：\\\\\r\n    y &amp;= \\frac{1}{1 + e^{-(W^Tx + b)}}\\\\\r\n    log\\frac{y}{1-y} &amp;= W^Tx + b\r\n\\end{aligned}\r\n\\]</span></p>\r\n<h2 id=\"线性判别分析linear-discriminant-analysis-lda\">线性判别分析(Linear Discriminant Analysis, LDA)</h2>\r\n<p>线性判别分析也叫“Fisher判别分析”，其思想非常朴素：试图找到一条过原点的直线，可以表示为<span class=\"math inline\">\\(y=w^Tx\\)</span>(这个表达式中的<span class=\"math inline\">\\(y\\)</span>表示<span class=\"math inline\">\\(x\\)</span>投影到这条直线后和原点的距离)，使得当样本投影到该直线时不同类别样本的距离尽可能远，而相同类别的数据尽可能接近，预测时按照样本的投影位置对其进行分类。</p>\r\n<p>以两类数据<span class=\"math inline\">\\(x_1, x_2\\)</span>为例，设<span class=\"math inline\">\\(\\mu_1,\\mu_2,\\Sigma_1,\\Sigma_2\\)</span>分别表示两类数据的均值和方差，则投影之后的均值和方差为<span class=\"math inline\">\\(w\\mu_1,w\\mu_2,w^T\\Sigma_1w,w^T\\Sigma_2w\\)</span>，因为直线是一维空间，所以这些均为实数，投影之后的类内距离可以用方差来衡量，这里使用<span class=\"math inline\">\\(w^T\\Sigma_1w + w^T\\Sigma_2w\\)</span>来度量投影之后的类内距离，而类间距离可以写成<span class=\"math inline\">\\(||w\\mu_2 - w\\mu_1||_2^2\\)</span>，同时考虑两种距离，给出希望最大化的目标函数如下。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\nJ &amp;= \\frac{||w^T\\mu_2 - w^T\\mu_1||_2^2}{w^T\\Sigma_1w + w^T\\Sigma_2w}\\\\\r\n&amp;= \\frac{w^T(\\mu_2 - \\mu_1)(\\mu_2 - \\mu_1)^Tw}{w^T(\\Sigma_1 + \\Sigma_2)w}\r\n\\end{aligned}\r\n\\]</span> 定义类内散度矩阵<span class=\"math inline\">\\(S_w = \\Sigma_1 + \\Sigma_2\\)</span>，类间散度矩阵<span class=\"math inline\">\\(S_b = (\\mu_2 - \\mu_1)(\\mu_2 - \\mu_1)^T\\)</span>，上面的优化目标可以简写为如下。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    J = \\frac{w^TS_bw}{w^TS_ww}\r\n\\end{aligned}\r\n\\]</span> 这个优化目标又称为<span class=\"math inline\">\\(S_b\\)</span>和<span class=\"math inline\">\\(S_w\\)</span>的广义瑞利商，注意到分子分母中都有<span class=\"math inline\">\\(w\\)</span>的二次项，因此和<span class=\"math inline\">\\(w\\)</span>大小无关，只和w方向有关，所以优化问题可以写成下式。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n\\min_w&amp; \\quad-w^TS_bw\\\\\r\ns.t.&amp; \\quad w^TS_ww = 1\r\n\\end{aligned}\r\n\\]</span> 用拉格朗日乘子法进行优化，求解<span class=\"math inline\">\\(S_bw = \\lambda S_ww\\)</span>，因<span class=\"math inline\">\\(S_bw\\)</span>方向和<span class=\"math inline\">\\(\\mu_2 - \\mu_1\\)</span>相同，因此令<span class=\"math inline\">\\(S_bw = \\lambda(\\mu_2 - \\mu_1)\\)</span>，代入求解，可以得到<span class=\"math inline\">\\(w = S_w^{-1}(\\mu_2 - \\mu_1)\\)</span>。</p>\r\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"section\">2019-07-04</h1>\r\n<h2 id=\"几种回归的概念\">几种回归的概念</h2>\r\n<p>线性回归很简单，就是<span class=\"math inline\">\\(y=W^Tx + b\\)</span>，而广义线性回归则可以用一个单调可微的函数<span class=\"math inline\">\\(g(\\centerdot)\\)</span>写成<span class=\"math inline\">\\(y=g^{-1}(W^Tx + b)\\)</span>或者<span class=\"math inline\">\\(g(y) = W^Tx + b\\)</span>,对数回归<span class=\"math inline\">\\(ln(y) = W^Tx + b\\)</span>就是广义线性回归的一个例子。</p>\r\n<p>如果要使用回归来解决分类问题，理想的情况是使用单位阶跃函数将回归值映射为类别标签，但单位阶跃函数不连续，因此使用对数几率函数来完成这个映射，对数几率函数即<span class=\"math inline\">\\(\\frac{1}{1 + e^{-z}}\\)</span>，是sigmoid函数的一种。</p>\r\n<p>这里顺便介绍下几率的概念：若一个事件发生的概率为<span class=\"math inline\">\\(y\\)</span>，则这个事件不发生的概率为<span class=\"math inline\">\\(1-y\\)</span>，两者的比值<span class=\"math inline\">\\(\\frac{y}{1-y}\\)</span>被称为几率，反应的是事件发生的相对可能性，对几率取对数得到对数几率<span class=\"math inline\">\\(log\\frac{y}{1-y}\\)</span>即为logit，逻辑回归(logistic regression)也可称为对数几率回归(logit regressoion)，就是用线性回归去逼近对数几率，如下： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    y &amp;= \\frac{1}{1 + e^{-z}}\\\\\r\n    代入线&amp;性回归得到的z：\\\\\r\n    y &amp;= \\frac{1}{1 + e^{-(W^Tx + b)}}\\\\\r\n    log\\frac{y}{1-y} &amp;= W^Tx + b\r\n\\end{aligned}\r\n\\]</span></p>\r\n<h2 id=\"线性判别分析linear-discriminant-analysis-lda\">线性判别分析(Linear Discriminant Analysis, LDA)</h2>\r\n<p>线性判别分析也叫“Fisher判别分析”，其思想非常朴素：试图找到一条过原点的直线，可以表示为<span class=\"math inline\">\\(y=w^Tx\\)</span>(这个表达式中的<span class=\"math inline\">\\(y\\)</span>表示<span class=\"math inline\">\\(x\\)</span>投影到这条直线后和原点的距离)，使得当样本投影到该直线时不同类别样本的距离尽可能远，而相同类别的数据尽可能接近，预测时按照样本的投影位置对其进行分类。</p>\r\n<p>以两类数据<span class=\"math inline\">\\(x_1, x_2\\)</span>为例，设<span class=\"math inline\">\\(\\mu_1,\\mu_2,\\Sigma_1,\\Sigma_2\\)</span>分别表示两类数据的均值和方差，则投影之后的均值和方差为<span class=\"math inline\">\\(w\\mu_1,w\\mu_2,w^T\\Sigma_1w,w^T\\Sigma_2w\\)</span>，因为直线是一维空间，所以这些均为实数，投影之后的类内距离可以用方差来衡量，这里使用<span class=\"math inline\">\\(w^T\\Sigma_1w + w^T\\Sigma_2w\\)</span>来度量投影之后的类内距离，而类间距离可以写成<span class=\"math inline\">\\(||w\\mu_2 - w\\mu_1||_2^2\\)</span>，同时考虑两种距离，给出希望最大化的目标函数如下。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\nJ &amp;= \\frac{||w^T\\mu_2 - w^T\\mu_1||_2^2}{w^T\\Sigma_1w + w^T\\Sigma_2w}\\\\\r\n&amp;= \\frac{w^T(\\mu_2 - \\mu_1)(\\mu_2 - \\mu_1)^Tw}{w^T(\\Sigma_1 + \\Sigma_2)w}\r\n\\end{aligned}\r\n\\]</span> 定义类内散度矩阵<span class=\"math inline\">\\(S_w = \\Sigma_1 + \\Sigma_2\\)</span>，类间散度矩阵<span class=\"math inline\">\\(S_b = (\\mu_2 - \\mu_1)(\\mu_2 - \\mu_1)^T\\)</span>，上面的优化目标可以简写为如下。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    J = \\frac{w^TS_bw}{w^TS_ww}\r\n\\end{aligned}\r\n\\]</span> 这个优化目标又称为<span class=\"math inline\">\\(S_b\\)</span>和<span class=\"math inline\">\\(S_w\\)</span>的广义瑞利商，注意到分子分母中都有<span class=\"math inline\">\\(w\\)</span>的二次项，因此和<span class=\"math inline\">\\(w\\)</span>大小无关，只和w方向有关，所以优化问题可以写成下式。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n\\min_w&amp; \\quad-w^TS_bw\\\\\r\ns.t.&amp; \\quad w^TS_ww = 1\r\n\\end{aligned}\r\n\\]</span> 用拉格朗日乘子法进行优化，求解<span class=\"math inline\">\\(S_bw = \\lambda S_ww\\)</span>，因<span class=\"math inline\">\\(S_bw\\)</span>方向和<span class=\"math inline\">\\(\\mu_2 - \\mu_1\\)</span>相同，因此令<span class=\"math inline\">\\(S_bw = \\lambda(\\mu_2 - \\mu_1)\\)</span>，代入求解，可以得到<span class=\"math inline\">\\(w = S_w^{-1}(\\mu_2 - \\mu_1)\\)</span>。</p>\r\n"},{"title":"常见概率分布","date":"2020-08-11T02:28:09.000Z","mathjax":true,"_content":"\n# 概率相关的一些概念简介\n## 随机变量\n随机变量表示值不确定的变量，对于一个随机变量，可以从两个方面描述，第一是其取值范围，可以是连续区间或者离散的集合（连续型随机变量和离散型随机变量），第二是其概率函数，即用函数来表示不同取值的概率。\n\n## 离散型随机变量的概率函数（概率质量函数）\n对于离散型随机变量，其概率函数将变量的取值映射为对应的概率，取值为$x$的概率表示为$p(x)$\n\n## 连续型随机变量的概率函数（概率密度函数）\n对于连续型随机变量，其概率函数被称为概率密度函数，表示的不是概率，而是概率在某个点位置的密集程度，不等于概率，因此其值可能大于1。在$x$处的概率密度表示为$p(x)$\n\n## 概率分布\n对于离散型变量，其概率分布可以表示为“离散型随机变量的值和概率的分布列表”，即表示为一个表格，也简称为“概率分布列”，需要列出所有情况和所有情况对应的概率。\n\n对于连续型随机变量，其没有概率分布的概念，因为无法用一个概率分布列写出来，只能用概率密度函数对其进行描述。\n\n## 概率分布函数\n不同于概率函数，概率分布函数描述的是一个范围内的概率，例如$F(x_k) = P(x \\le x_k)$，对于离散型随机变量，其概率分布函数可以进一步写为$F(x_k) = P(x \\le x_k) = \\sum\\limits_{x \\le x_k} p(x)$，对于连续型随机变量，其概率分布函数可以进一步写为$F(x_k) = p(x \\le x_k) = \\int_{-\\infty}^{x_k} p(x)dx$\n\n关于概率的一些概念简介到此结束，下面来看一些具体的概率分布。\n\n# 连续型随机变量的分布\n## 均匀分布（Uniform Distribution）\n取值范围在连续区间$[a, b]$上的连续变量，每个值的概率都相等，其参数为$a, b$，均匀分布用$U$表示。\n$$\n\\begin{aligned}\n    x &\\sim U(a, b)\\\\\n    p(x|a,b) &= \\frac{1}{b - a}\\\\\n    E(x) &= \\frac{b + a}{2}\\\\\n    Var(x) &= \\frac{(b - a)^2}{12}\n\\end{aligned}\n$$\n\n## 高斯分布（Gaussian Dsitribution）\n\n中心极限定理说明：大量独立同分布的随机变量，其均值适当标准化之后会收敛到高斯分布。\n\n服从高斯分布的随机变量取值范围为$(-\\infty, \\infty)$。\n\n对于一元高斯分布，可以用两个参数来描述：$\\mu$、$\\sigma^2$，分别表示高斯分布的均值和方差。\n$$\n\\begin{aligned}\n    x &\\sim N(\\mu, \\sigma^2)\\\\\n    p(x) &= \\frac{1}{\\sqrt{2\\pi} \\sigma} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\\\\n    E(x) &= \\mu\\\\\n    Var(x) &= \\sigma^2\\\\\n\\end{aligned}\n$$\n\n对于多元高斯分布，如果其随机变量是$m$维向量，那么可以用均值向量$\\mu$和协方差矩阵$\\Sigma$来描述。\n$$\n\\begin{aligned}\n    x &\\sim N(\\mu, \\Sigma)\\\\\n    p(x) &= \\frac{1}{(2\\pi)^\\frac{m}{2} |\\Sigma|^\\frac{1}{2}} e^{-\\frac{(x-\\mu)^T \\Sigma^{-1}(x-\\mu)}{2}}\\\\\n    E(x) &= \\mu\\\\\n    Var(x) &= \\Sigma = E((x - \\mu) (x - \\mu)^T)\\\\\n\\end{aligned}\n$$\n\n# 离散型随机变量的分布\n## 伯努利分布（Bernoulli Distribution）\n取值范围在离散集合$\\{0, 1\\}$上的离散型随机变量，取值1的概率为$\\mu$，取值0的概率为$1 - \\mu$。\n$$\n\\begin{aligned}\n    x &\\sim Bernoulli(\\mu)\\\\\n    p(x | \\mu) &= \\mu^x(1-\\mu)^{1-x}\\\\\n    E(x) &= \\mu\\\\\n    Var(x) &= \\mu(1 - \\mu)\n\\end{aligned}\n$$\n\n## 分类分布（Categorical Distribution）\n也叫范畴分布或者Multinoulli Distribution，这个分布是伯努利分布的一个推广，伯努利分布表示一次伯努利实验，得到两种状态，而分类分布表示一次实验，可以得到$C$种不同的状态，其变量可以表示为一个$C$维向量$x \\in \\{0, 1\\}^C$其中每个维度的取值只可能是0或者1，并且满足有且只有一个位置取得1，即$\\mathbf{1}^T x = \\sum\\limits_{c = 1}^C x_c = 1$，其参数包括不同状态的概率，表示为$\\mu_c$。\n\n在分类分布中，我们一般不关心其均值和方差。\n$$\n\\begin{aligned}\n    x \\in \\{0, 1\\}^C,\\ \\mathbf{1}^T x &= 1,\\ x \\sim multinoulli(\\mu)\\\\\n    p(x | \\mu) &= x^T \\mu \\\\\n    E(x_c) &= \\mu_c\\\\\n    Var(x_c) &= \\mu_c(1 - \\mu_c)\n\\end{aligned}\n$$\n\n## 二项分布（Binomial Distribution）\n用于表示N次对$Bernoulli(\\mu)$进行独立同分布采样之后，有m次得到1的概率分布，取值范围为$\\{0, 1, 2, ... , m\\}$，如果$N = 1$，那么二项分布就是伯努利分布，二项分布用$Binomial(N, \\mu)$表示。\n$$\n\\begin{aligned}\n    x &\\sim Binomial(N, \\mu)\\\\\n    p(x = m) &= C_N^m \\mu^m(1-\\mu)^{N-m}\\\\\n    E(x) &= N\\mu\\\\\n    Var(x) &= N\\mu(1 - \\mu)\n\\end{aligned}\n$$\n\n## 多项分布（Nultinomial Distribution）\n类似于分类分布在伯努利分布上的扩展，如果在二项分布上将状态扩展至C个，那么N次采样之后，每个状态分别出现$m_c$次的概率分布即为多项分布，$\\mathbf{1}^T m = N$，其参数包括$N$和$\\mu$，表示采样次数和每次采样时，不同状态出现的概率，$\\mathbf{1}^T \\mu = 1$\n$$\n\\begin{aligned}\n    m &\\sim Nultinomial(N, \\mu)\\\\\n    p(m) &= \\frac{N!}{\\prod\\limits_{c=1}^C (m_c!)}\\prod\\limits_{c=1}^C\\mu_c^{m_c}\\\\\n    E(m_c) &= N\\mu_c\\\\\n    Var(m_c) &= N\\mu_c(1 - \\mu_c)\n\\end{aligned}\n$$\n\n## 泊松分布（Poisson Distribution）\n一个随机事件发生的概率不随时间变化，而且单位时间内发生的平均次数记为$\\lambda$，那么在单位时间内该事件发生的次数（取值可以为所有非负整数）服从参数为$\\lambda$的泊松分布。\n\n当二项分布中的$N$较大（例如大于20），而$\\mu$较小，例如小于0.05时，可以用Poisson分布来近似二项分布。\n\n$$\n\\begin{aligned}\n    x &\\sim Poisson(\\lambda)\\\\\n    p(x) &= \\frac{\\lambda^x e^{-\\lambda}}{x!}\\\\\n    E(x) &= \\lambda\\\\\n    Var(x) &= \\lambda\n\\end{aligned}\n$$\n\n## 几何分布\n每一次实验，事件发生概率为$\\mu$，那么重复实验直到事件第一次发生，需要重复的实验次数服从几何分布。\n\n$$\n\\begin{aligned}\n    p(x) &= \\mu (1 - \\mu)^{x-1}\\\\\n    E(x) &= \\frac{1}{\\mu}\\\\\n    Var(x) &= \\frac{1-\\mu}{\\mu^2}\n\\end{aligned}\n$$","source":"_posts/学习笔记/常见概率分布.md","raw":"---\ntitle: 常见概率分布\ndate: 2020-08-11 10:28:09\ntags: [机器学习]\nmathjax: true\n---\n\n# 概率相关的一些概念简介\n## 随机变量\n随机变量表示值不确定的变量，对于一个随机变量，可以从两个方面描述，第一是其取值范围，可以是连续区间或者离散的集合（连续型随机变量和离散型随机变量），第二是其概率函数，即用函数来表示不同取值的概率。\n\n## 离散型随机变量的概率函数（概率质量函数）\n对于离散型随机变量，其概率函数将变量的取值映射为对应的概率，取值为$x$的概率表示为$p(x)$\n\n## 连续型随机变量的概率函数（概率密度函数）\n对于连续型随机变量，其概率函数被称为概率密度函数，表示的不是概率，而是概率在某个点位置的密集程度，不等于概率，因此其值可能大于1。在$x$处的概率密度表示为$p(x)$\n\n## 概率分布\n对于离散型变量，其概率分布可以表示为“离散型随机变量的值和概率的分布列表”，即表示为一个表格，也简称为“概率分布列”，需要列出所有情况和所有情况对应的概率。\n\n对于连续型随机变量，其没有概率分布的概念，因为无法用一个概率分布列写出来，只能用概率密度函数对其进行描述。\n\n## 概率分布函数\n不同于概率函数，概率分布函数描述的是一个范围内的概率，例如$F(x_k) = P(x \\le x_k)$，对于离散型随机变量，其概率分布函数可以进一步写为$F(x_k) = P(x \\le x_k) = \\sum\\limits_{x \\le x_k} p(x)$，对于连续型随机变量，其概率分布函数可以进一步写为$F(x_k) = p(x \\le x_k) = \\int_{-\\infty}^{x_k} p(x)dx$\n\n关于概率的一些概念简介到此结束，下面来看一些具体的概率分布。\n\n# 连续型随机变量的分布\n## 均匀分布（Uniform Distribution）\n取值范围在连续区间$[a, b]$上的连续变量，每个值的概率都相等，其参数为$a, b$，均匀分布用$U$表示。\n$$\n\\begin{aligned}\n    x &\\sim U(a, b)\\\\\n    p(x|a,b) &= \\frac{1}{b - a}\\\\\n    E(x) &= \\frac{b + a}{2}\\\\\n    Var(x) &= \\frac{(b - a)^2}{12}\n\\end{aligned}\n$$\n\n## 高斯分布（Gaussian Dsitribution）\n\n中心极限定理说明：大量独立同分布的随机变量，其均值适当标准化之后会收敛到高斯分布。\n\n服从高斯分布的随机变量取值范围为$(-\\infty, \\infty)$。\n\n对于一元高斯分布，可以用两个参数来描述：$\\mu$、$\\sigma^2$，分别表示高斯分布的均值和方差。\n$$\n\\begin{aligned}\n    x &\\sim N(\\mu, \\sigma^2)\\\\\n    p(x) &= \\frac{1}{\\sqrt{2\\pi} \\sigma} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\\\\n    E(x) &= \\mu\\\\\n    Var(x) &= \\sigma^2\\\\\n\\end{aligned}\n$$\n\n对于多元高斯分布，如果其随机变量是$m$维向量，那么可以用均值向量$\\mu$和协方差矩阵$\\Sigma$来描述。\n$$\n\\begin{aligned}\n    x &\\sim N(\\mu, \\Sigma)\\\\\n    p(x) &= \\frac{1}{(2\\pi)^\\frac{m}{2} |\\Sigma|^\\frac{1}{2}} e^{-\\frac{(x-\\mu)^T \\Sigma^{-1}(x-\\mu)}{2}}\\\\\n    E(x) &= \\mu\\\\\n    Var(x) &= \\Sigma = E((x - \\mu) (x - \\mu)^T)\\\\\n\\end{aligned}\n$$\n\n# 离散型随机变量的分布\n## 伯努利分布（Bernoulli Distribution）\n取值范围在离散集合$\\{0, 1\\}$上的离散型随机变量，取值1的概率为$\\mu$，取值0的概率为$1 - \\mu$。\n$$\n\\begin{aligned}\n    x &\\sim Bernoulli(\\mu)\\\\\n    p(x | \\mu) &= \\mu^x(1-\\mu)^{1-x}\\\\\n    E(x) &= \\mu\\\\\n    Var(x) &= \\mu(1 - \\mu)\n\\end{aligned}\n$$\n\n## 分类分布（Categorical Distribution）\n也叫范畴分布或者Multinoulli Distribution，这个分布是伯努利分布的一个推广，伯努利分布表示一次伯努利实验，得到两种状态，而分类分布表示一次实验，可以得到$C$种不同的状态，其变量可以表示为一个$C$维向量$x \\in \\{0, 1\\}^C$其中每个维度的取值只可能是0或者1，并且满足有且只有一个位置取得1，即$\\mathbf{1}^T x = \\sum\\limits_{c = 1}^C x_c = 1$，其参数包括不同状态的概率，表示为$\\mu_c$。\n\n在分类分布中，我们一般不关心其均值和方差。\n$$\n\\begin{aligned}\n    x \\in \\{0, 1\\}^C,\\ \\mathbf{1}^T x &= 1,\\ x \\sim multinoulli(\\mu)\\\\\n    p(x | \\mu) &= x^T \\mu \\\\\n    E(x_c) &= \\mu_c\\\\\n    Var(x_c) &= \\mu_c(1 - \\mu_c)\n\\end{aligned}\n$$\n\n## 二项分布（Binomial Distribution）\n用于表示N次对$Bernoulli(\\mu)$进行独立同分布采样之后，有m次得到1的概率分布，取值范围为$\\{0, 1, 2, ... , m\\}$，如果$N = 1$，那么二项分布就是伯努利分布，二项分布用$Binomial(N, \\mu)$表示。\n$$\n\\begin{aligned}\n    x &\\sim Binomial(N, \\mu)\\\\\n    p(x = m) &= C_N^m \\mu^m(1-\\mu)^{N-m}\\\\\n    E(x) &= N\\mu\\\\\n    Var(x) &= N\\mu(1 - \\mu)\n\\end{aligned}\n$$\n\n## 多项分布（Nultinomial Distribution）\n类似于分类分布在伯努利分布上的扩展，如果在二项分布上将状态扩展至C个，那么N次采样之后，每个状态分别出现$m_c$次的概率分布即为多项分布，$\\mathbf{1}^T m = N$，其参数包括$N$和$\\mu$，表示采样次数和每次采样时，不同状态出现的概率，$\\mathbf{1}^T \\mu = 1$\n$$\n\\begin{aligned}\n    m &\\sim Nultinomial(N, \\mu)\\\\\n    p(m) &= \\frac{N!}{\\prod\\limits_{c=1}^C (m_c!)}\\prod\\limits_{c=1}^C\\mu_c^{m_c}\\\\\n    E(m_c) &= N\\mu_c\\\\\n    Var(m_c) &= N\\mu_c(1 - \\mu_c)\n\\end{aligned}\n$$\n\n## 泊松分布（Poisson Distribution）\n一个随机事件发生的概率不随时间变化，而且单位时间内发生的平均次数记为$\\lambda$，那么在单位时间内该事件发生的次数（取值可以为所有非负整数）服从参数为$\\lambda$的泊松分布。\n\n当二项分布中的$N$较大（例如大于20），而$\\mu$较小，例如小于0.05时，可以用Poisson分布来近似二项分布。\n\n$$\n\\begin{aligned}\n    x &\\sim Poisson(\\lambda)\\\\\n    p(x) &= \\frac{\\lambda^x e^{-\\lambda}}{x!}\\\\\n    E(x) &= \\lambda\\\\\n    Var(x) &= \\lambda\n\\end{aligned}\n$$\n\n## 几何分布\n每一次实验，事件发生概率为$\\mu$，那么重复实验直到事件第一次发生，需要重复的实验次数服从几何分布。\n\n$$\n\\begin{aligned}\n    p(x) &= \\mu (1 - \\mu)^{x-1}\\\\\n    E(x) &= \\frac{1}{\\mu}\\\\\n    Var(x) &= \\frac{1-\\mu}{\\mu^2}\n\\end{aligned}\n$$","slug":"学习笔记/常见概率分布","published":1,"updated":"2020-08-31T06:39:20.769Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckjxpc3s0001844mq0q329zm6","content":"<h1 id=\"概率相关的一些概念简介\">概率相关的一些概念简介</h1>\r\n<h2 id=\"随机变量\">随机变量</h2>\r\n<p>随机变量表示值不确定的变量，对于一个随机变量，可以从两个方面描述，第一是其取值范围，可以是连续区间或者离散的集合（连续型随机变量和离散型随机变量），第二是其概率函数，即用函数来表示不同取值的概率。</p>\r\n<h2 id=\"离散型随机变量的概率函数概率质量函数\">离散型随机变量的概率函数（概率质量函数）</h2>\r\n<p>对于离散型随机变量，其概率函数将变量的取值映射为对应的概率，取值为<span class=\"math inline\">\\(x\\)</span>的概率表示为<span class=\"math inline\">\\(p(x)\\)</span></p>\r\n<h2 id=\"连续型随机变量的概率函数概率密度函数\">连续型随机变量的概率函数（概率密度函数）</h2>\r\n<p>对于连续型随机变量，其概率函数被称为概率密度函数，表示的不是概率，而是概率在某个点位置的密集程度，不等于概率，因此其值可能大于1。在<span class=\"math inline\">\\(x\\)</span>处的概率密度表示为<span class=\"math inline\">\\(p(x)\\)</span></p>\r\n<h2 id=\"概率分布\">概率分布</h2>\r\n<p>对于离散型变量，其概率分布可以表示为“离散型随机变量的值和概率的分布列表”，即表示为一个表格，也简称为“概率分布列”，需要列出所有情况和所有情况对应的概率。</p>\r\n<p>对于连续型随机变量，其没有概率分布的概念，因为无法用一个概率分布列写出来，只能用概率密度函数对其进行描述。</p>\r\n<h2 id=\"概率分布函数\">概率分布函数</h2>\r\n<p>不同于概率函数，概率分布函数描述的是一个范围内的概率，例如<span class=\"math inline\">\\(F(x_k) = P(x \\le x_k)\\)</span>，对于离散型随机变量，其概率分布函数可以进一步写为<span class=\"math inline\">\\(F(x_k) = P(x \\le x_k) = \\sum\\limits_{x \\le x_k} p(x)\\)</span>，对于连续型随机变量，其概率分布函数可以进一步写为<span class=\"math inline\">\\(F(x_k) = p(x \\le x_k) = \\int_{-\\infty}^{x_k} p(x)dx\\)</span></p>\r\n<p>关于概率的一些概念简介到此结束，下面来看一些具体的概率分布。</p>\r\n<h1 id=\"连续型随机变量的分布\">连续型随机变量的分布</h1>\r\n<h2 id=\"均匀分布uniform-distribution\">均匀分布（Uniform Distribution）</h2>\r\n<p>取值范围在连续区间<span class=\"math inline\">\\([a, b]\\)</span>上的连续变量，每个值的概率都相等，其参数为<span class=\"math inline\">\\(a, b\\)</span>，均匀分布用<span class=\"math inline\">\\(U\\)</span>表示。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    x &amp;\\sim U(a, b)\\\\\r\n    p(x|a,b) &amp;= \\frac{1}{b - a}\\\\\r\n    E(x) &amp;= \\frac{b + a}{2}\\\\\r\n    Var(x) &amp;= \\frac{(b - a)^2}{12}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<h2 id=\"高斯分布gaussian-dsitribution\">高斯分布（Gaussian Dsitribution）</h2>\r\n<p>中心极限定理说明：大量独立同分布的随机变量，其均值适当标准化之后会收敛到高斯分布。</p>\r\n<p>服从高斯分布的随机变量取值范围为<span class=\"math inline\">\\((-\\infty, \\infty)\\)</span>。</p>\r\n<p>对于一元高斯分布，可以用两个参数来描述：<span class=\"math inline\">\\(\\mu\\)</span>、<span class=\"math inline\">\\(\\sigma^2\\)</span>，分别表示高斯分布的均值和方差。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    x &amp;\\sim N(\\mu, \\sigma^2)\\\\\r\n    p(x) &amp;= \\frac{1}{\\sqrt{2\\pi} \\sigma} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\\\\r\n    E(x) &amp;= \\mu\\\\\r\n    Var(x) &amp;= \\sigma^2\\\\\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>对于多元高斯分布，如果其随机变量是<span class=\"math inline\">\\(m\\)</span>维向量，那么可以用均值向量<span class=\"math inline\">\\(\\mu\\)</span>和协方差矩阵<span class=\"math inline\">\\(\\Sigma\\)</span>来描述。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    x &amp;\\sim N(\\mu, \\Sigma)\\\\\r\n    p(x) &amp;= \\frac{1}{(2\\pi)^\\frac{m}{2} |\\Sigma|^\\frac{1}{2}} e^{-\\frac{(x-\\mu)^T \\Sigma^{-1}(x-\\mu)}{2}}\\\\\r\n    E(x) &amp;= \\mu\\\\\r\n    Var(x) &amp;= \\Sigma = E((x - \\mu) (x - \\mu)^T)\\\\\r\n\\end{aligned}\r\n\\]</span></p>\r\n<h1 id=\"离散型随机变量的分布\">离散型随机变量的分布</h1>\r\n<h2 id=\"伯努利分布bernoulli-distribution\">伯努利分布（Bernoulli Distribution）</h2>\r\n<p>取值范围在离散集合<span class=\"math inline\">\\(\\{0, 1\\}\\)</span>上的离散型随机变量，取值1的概率为<span class=\"math inline\">\\(\\mu\\)</span>，取值0的概率为<span class=\"math inline\">\\(1 - \\mu\\)</span>。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    x &amp;\\sim Bernoulli(\\mu)\\\\\r\n    p(x | \\mu) &amp;= \\mu^x(1-\\mu)^{1-x}\\\\\r\n    E(x) &amp;= \\mu\\\\\r\n    Var(x) &amp;= \\mu(1 - \\mu)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<h2 id=\"分类分布categorical-distribution\">分类分布（Categorical Distribution）</h2>\r\n<p>也叫范畴分布或者Multinoulli Distribution，这个分布是伯努利分布的一个推广，伯努利分布表示一次伯努利实验，得到两种状态，而分类分布表示一次实验，可以得到<span class=\"math inline\">\\(C\\)</span>种不同的状态，其变量可以表示为一个<span class=\"math inline\">\\(C\\)</span>维向量<span class=\"math inline\">\\(x \\in \\{0, 1\\}^C\\)</span>其中每个维度的取值只可能是0或者1，并且满足有且只有一个位置取得1，即<span class=\"math inline\">\\(\\mathbf{1}^T x = \\sum\\limits_{c = 1}^C x_c = 1\\)</span>，其参数包括不同状态的概率，表示为<span class=\"math inline\">\\(\\mu_c\\)</span>。</p>\r\n<p>在分类分布中，我们一般不关心其均值和方差。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    x \\in \\{0, 1\\}^C,\\ \\mathbf{1}^T x &amp;= 1,\\ x \\sim multinoulli(\\mu)\\\\\r\n    p(x | \\mu) &amp;= x^T \\mu \\\\\r\n    E(x_c) &amp;= \\mu_c\\\\\r\n    Var(x_c) &amp;= \\mu_c(1 - \\mu_c)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<h2 id=\"二项分布binomial-distribution\">二项分布（Binomial Distribution）</h2>\r\n<p>用于表示N次对<span class=\"math inline\">\\(Bernoulli(\\mu)\\)</span>进行独立同分布采样之后，有m次得到1的概率分布，取值范围为<span class=\"math inline\">\\(\\{0, 1, 2, ... , m\\}\\)</span>，如果<span class=\"math inline\">\\(N = 1\\)</span>，那么二项分布就是伯努利分布，二项分布用<span class=\"math inline\">\\(Binomial(N, \\mu)\\)</span>表示。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    x &amp;\\sim Binomial(N, \\mu)\\\\\r\n    p(x = m) &amp;= C_N^m \\mu^m(1-\\mu)^{N-m}\\\\\r\n    E(x) &amp;= N\\mu\\\\\r\n    Var(x) &amp;= N\\mu(1 - \\mu)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<h2 id=\"多项分布nultinomial-distribution\">多项分布（Nultinomial Distribution）</h2>\r\n<p>类似于分类分布在伯努利分布上的扩展，如果在二项分布上将状态扩展至C个，那么N次采样之后，每个状态分别出现<span class=\"math inline\">\\(m_c\\)</span>次的概率分布即为多项分布，<span class=\"math inline\">\\(\\mathbf{1}^T m = N\\)</span>，其参数包括<span class=\"math inline\">\\(N\\)</span>和<span class=\"math inline\">\\(\\mu\\)</span>，表示采样次数和每次采样时，不同状态出现的概率，<span class=\"math inline\">\\(\\mathbf{1}^T \\mu = 1\\)</span> <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    m &amp;\\sim Nultinomial(N, \\mu)\\\\\r\n    p(m) &amp;= \\frac{N!}{\\prod\\limits_{c=1}^C (m_c!)}\\prod\\limits_{c=1}^C\\mu_c^{m_c}\\\\\r\n    E(m_c) &amp;= N\\mu_c\\\\\r\n    Var(m_c) &amp;= N\\mu_c(1 - \\mu_c)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<h2 id=\"泊松分布poisson-distribution\">泊松分布（Poisson Distribution）</h2>\r\n<p>一个随机事件发生的概率不随时间变化，而且单位时间内发生的平均次数记为<span class=\"math inline\">\\(\\lambda\\)</span>，那么在单位时间内该事件发生的次数（取值可以为所有非负整数）服从参数为<span class=\"math inline\">\\(\\lambda\\)</span>的泊松分布。</p>\r\n<p>当二项分布中的<span class=\"math inline\">\\(N\\)</span>较大（例如大于20），而<span class=\"math inline\">\\(\\mu\\)</span>较小，例如小于0.05时，可以用Poisson分布来近似二项分布。</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    x &amp;\\sim Poisson(\\lambda)\\\\\r\n    p(x) &amp;= \\frac{\\lambda^x e^{-\\lambda}}{x!}\\\\\r\n    E(x) &amp;= \\lambda\\\\\r\n    Var(x) &amp;= \\lambda\r\n\\end{aligned}\r\n\\]</span></p>\r\n<h2 id=\"几何分布\">几何分布</h2>\r\n<p>每一次实验，事件发生概率为<span class=\"math inline\">\\(\\mu\\)</span>，那么重复实验直到事件第一次发生，需要重复的实验次数服从几何分布。</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    p(x) &amp;= \\mu (1 - \\mu)^{x-1}\\\\\r\n    E(x) &amp;= \\frac{1}{\\mu}\\\\\r\n    Var(x) &amp;= \\frac{1-\\mu}{\\mu^2}\r\n\\end{aligned}\r\n\\]</span></p>\r\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"概率相关的一些概念简介\">概率相关的一些概念简介</h1>\r\n<h2 id=\"随机变量\">随机变量</h2>\r\n<p>随机变量表示值不确定的变量，对于一个随机变量，可以从两个方面描述，第一是其取值范围，可以是连续区间或者离散的集合（连续型随机变量和离散型随机变量），第二是其概率函数，即用函数来表示不同取值的概率。</p>\r\n<h2 id=\"离散型随机变量的概率函数概率质量函数\">离散型随机变量的概率函数（概率质量函数）</h2>\r\n<p>对于离散型随机变量，其概率函数将变量的取值映射为对应的概率，取值为<span class=\"math inline\">\\(x\\)</span>的概率表示为<span class=\"math inline\">\\(p(x)\\)</span></p>\r\n<h2 id=\"连续型随机变量的概率函数概率密度函数\">连续型随机变量的概率函数（概率密度函数）</h2>\r\n<p>对于连续型随机变量，其概率函数被称为概率密度函数，表示的不是概率，而是概率在某个点位置的密集程度，不等于概率，因此其值可能大于1。在<span class=\"math inline\">\\(x\\)</span>处的概率密度表示为<span class=\"math inline\">\\(p(x)\\)</span></p>\r\n<h2 id=\"概率分布\">概率分布</h2>\r\n<p>对于离散型变量，其概率分布可以表示为“离散型随机变量的值和概率的分布列表”，即表示为一个表格，也简称为“概率分布列”，需要列出所有情况和所有情况对应的概率。</p>\r\n<p>对于连续型随机变量，其没有概率分布的概念，因为无法用一个概率分布列写出来，只能用概率密度函数对其进行描述。</p>\r\n<h2 id=\"概率分布函数\">概率分布函数</h2>\r\n<p>不同于概率函数，概率分布函数描述的是一个范围内的概率，例如<span class=\"math inline\">\\(F(x_k) = P(x \\le x_k)\\)</span>，对于离散型随机变量，其概率分布函数可以进一步写为<span class=\"math inline\">\\(F(x_k) = P(x \\le x_k) = \\sum\\limits_{x \\le x_k} p(x)\\)</span>，对于连续型随机变量，其概率分布函数可以进一步写为<span class=\"math inline\">\\(F(x_k) = p(x \\le x_k) = \\int_{-\\infty}^{x_k} p(x)dx\\)</span></p>\r\n<p>关于概率的一些概念简介到此结束，下面来看一些具体的概率分布。</p>\r\n<h1 id=\"连续型随机变量的分布\">连续型随机变量的分布</h1>\r\n<h2 id=\"均匀分布uniform-distribution\">均匀分布（Uniform Distribution）</h2>\r\n<p>取值范围在连续区间<span class=\"math inline\">\\([a, b]\\)</span>上的连续变量，每个值的概率都相等，其参数为<span class=\"math inline\">\\(a, b\\)</span>，均匀分布用<span class=\"math inline\">\\(U\\)</span>表示。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    x &amp;\\sim U(a, b)\\\\\r\n    p(x|a,b) &amp;= \\frac{1}{b - a}\\\\\r\n    E(x) &amp;= \\frac{b + a}{2}\\\\\r\n    Var(x) &amp;= \\frac{(b - a)^2}{12}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<h2 id=\"高斯分布gaussian-dsitribution\">高斯分布（Gaussian Dsitribution）</h2>\r\n<p>中心极限定理说明：大量独立同分布的随机变量，其均值适当标准化之后会收敛到高斯分布。</p>\r\n<p>服从高斯分布的随机变量取值范围为<span class=\"math inline\">\\((-\\infty, \\infty)\\)</span>。</p>\r\n<p>对于一元高斯分布，可以用两个参数来描述：<span class=\"math inline\">\\(\\mu\\)</span>、<span class=\"math inline\">\\(\\sigma^2\\)</span>，分别表示高斯分布的均值和方差。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    x &amp;\\sim N(\\mu, \\sigma^2)\\\\\r\n    p(x) &amp;= \\frac{1}{\\sqrt{2\\pi} \\sigma} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\\\\r\n    E(x) &amp;= \\mu\\\\\r\n    Var(x) &amp;= \\sigma^2\\\\\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>对于多元高斯分布，如果其随机变量是<span class=\"math inline\">\\(m\\)</span>维向量，那么可以用均值向量<span class=\"math inline\">\\(\\mu\\)</span>和协方差矩阵<span class=\"math inline\">\\(\\Sigma\\)</span>来描述。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    x &amp;\\sim N(\\mu, \\Sigma)\\\\\r\n    p(x) &amp;= \\frac{1}{(2\\pi)^\\frac{m}{2} |\\Sigma|^\\frac{1}{2}} e^{-\\frac{(x-\\mu)^T \\Sigma^{-1}(x-\\mu)}{2}}\\\\\r\n    E(x) &amp;= \\mu\\\\\r\n    Var(x) &amp;= \\Sigma = E((x - \\mu) (x - \\mu)^T)\\\\\r\n\\end{aligned}\r\n\\]</span></p>\r\n<h1 id=\"离散型随机变量的分布\">离散型随机变量的分布</h1>\r\n<h2 id=\"伯努利分布bernoulli-distribution\">伯努利分布（Bernoulli Distribution）</h2>\r\n<p>取值范围在离散集合<span class=\"math inline\">\\(\\{0, 1\\}\\)</span>上的离散型随机变量，取值1的概率为<span class=\"math inline\">\\(\\mu\\)</span>，取值0的概率为<span class=\"math inline\">\\(1 - \\mu\\)</span>。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    x &amp;\\sim Bernoulli(\\mu)\\\\\r\n    p(x | \\mu) &amp;= \\mu^x(1-\\mu)^{1-x}\\\\\r\n    E(x) &amp;= \\mu\\\\\r\n    Var(x) &amp;= \\mu(1 - \\mu)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<h2 id=\"分类分布categorical-distribution\">分类分布（Categorical Distribution）</h2>\r\n<p>也叫范畴分布或者Multinoulli Distribution，这个分布是伯努利分布的一个推广，伯努利分布表示一次伯努利实验，得到两种状态，而分类分布表示一次实验，可以得到<span class=\"math inline\">\\(C\\)</span>种不同的状态，其变量可以表示为一个<span class=\"math inline\">\\(C\\)</span>维向量<span class=\"math inline\">\\(x \\in \\{0, 1\\}^C\\)</span>其中每个维度的取值只可能是0或者1，并且满足有且只有一个位置取得1，即<span class=\"math inline\">\\(\\mathbf{1}^T x = \\sum\\limits_{c = 1}^C x_c = 1\\)</span>，其参数包括不同状态的概率，表示为<span class=\"math inline\">\\(\\mu_c\\)</span>。</p>\r\n<p>在分类分布中，我们一般不关心其均值和方差。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    x \\in \\{0, 1\\}^C,\\ \\mathbf{1}^T x &amp;= 1,\\ x \\sim multinoulli(\\mu)\\\\\r\n    p(x | \\mu) &amp;= x^T \\mu \\\\\r\n    E(x_c) &amp;= \\mu_c\\\\\r\n    Var(x_c) &amp;= \\mu_c(1 - \\mu_c)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<h2 id=\"二项分布binomial-distribution\">二项分布（Binomial Distribution）</h2>\r\n<p>用于表示N次对<span class=\"math inline\">\\(Bernoulli(\\mu)\\)</span>进行独立同分布采样之后，有m次得到1的概率分布，取值范围为<span class=\"math inline\">\\(\\{0, 1, 2, ... , m\\}\\)</span>，如果<span class=\"math inline\">\\(N = 1\\)</span>，那么二项分布就是伯努利分布，二项分布用<span class=\"math inline\">\\(Binomial(N, \\mu)\\)</span>表示。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    x &amp;\\sim Binomial(N, \\mu)\\\\\r\n    p(x = m) &amp;= C_N^m \\mu^m(1-\\mu)^{N-m}\\\\\r\n    E(x) &amp;= N\\mu\\\\\r\n    Var(x) &amp;= N\\mu(1 - \\mu)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<h2 id=\"多项分布nultinomial-distribution\">多项分布（Nultinomial Distribution）</h2>\r\n<p>类似于分类分布在伯努利分布上的扩展，如果在二项分布上将状态扩展至C个，那么N次采样之后，每个状态分别出现<span class=\"math inline\">\\(m_c\\)</span>次的概率分布即为多项分布，<span class=\"math inline\">\\(\\mathbf{1}^T m = N\\)</span>，其参数包括<span class=\"math inline\">\\(N\\)</span>和<span class=\"math inline\">\\(\\mu\\)</span>，表示采样次数和每次采样时，不同状态出现的概率，<span class=\"math inline\">\\(\\mathbf{1}^T \\mu = 1\\)</span> <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    m &amp;\\sim Nultinomial(N, \\mu)\\\\\r\n    p(m) &amp;= \\frac{N!}{\\prod\\limits_{c=1}^C (m_c!)}\\prod\\limits_{c=1}^C\\mu_c^{m_c}\\\\\r\n    E(m_c) &amp;= N\\mu_c\\\\\r\n    Var(m_c) &amp;= N\\mu_c(1 - \\mu_c)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<h2 id=\"泊松分布poisson-distribution\">泊松分布（Poisson Distribution）</h2>\r\n<p>一个随机事件发生的概率不随时间变化，而且单位时间内发生的平均次数记为<span class=\"math inline\">\\(\\lambda\\)</span>，那么在单位时间内该事件发生的次数（取值可以为所有非负整数）服从参数为<span class=\"math inline\">\\(\\lambda\\)</span>的泊松分布。</p>\r\n<p>当二项分布中的<span class=\"math inline\">\\(N\\)</span>较大（例如大于20），而<span class=\"math inline\">\\(\\mu\\)</span>较小，例如小于0.05时，可以用Poisson分布来近似二项分布。</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    x &amp;\\sim Poisson(\\lambda)\\\\\r\n    p(x) &amp;= \\frac{\\lambda^x e^{-\\lambda}}{x!}\\\\\r\n    E(x) &amp;= \\lambda\\\\\r\n    Var(x) &amp;= \\lambda\r\n\\end{aligned}\r\n\\]</span></p>\r\n<h2 id=\"几何分布\">几何分布</h2>\r\n<p>每一次实验，事件发生概率为<span class=\"math inline\">\\(\\mu\\)</span>，那么重复实验直到事件第一次发生，需要重复的实验次数服从几何分布。</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    p(x) &amp;= \\mu (1 - \\mu)^{x-1}\\\\\r\n    E(x) &amp;= \\frac{1}{\\mu}\\\\\r\n    Var(x) &amp;= \\frac{1-\\mu}{\\mu^2}\r\n\\end{aligned}\r\n\\]</span></p>\r\n"},{"title":"推荐系统学习笔记","date":"2020-08-11T13:21:06.000Z","_content":"\n# 推荐系统简介\n## 基于用户的推荐系统\n计算用户信息之间的相似性，对于和用户A相似的用户B，将用户A喜欢的物品推荐给用户B。\n## 基于内容的推荐系统\n计算物品之间的相似性，与某个物品A相似的物品B，将被推荐给喜欢物品A的用户。\n## 基于协同过滤（Collaborative Filtering）的推荐系统\n协同过滤是指用户行为，需要的数据表现为一个用户对物品的评分矩阵（或者点击、喜爱程度矩阵），协同过滤推荐有两种方式，一种是User-CF，对于喜欢的物品相似的用户A和用户B,将他们喜欢的其他东西推荐给对方。一种是Item-CF，如果一系列的物品都被同一个用户喜欢，则这些物品判定为相似，可以推荐给喜欢这类物品的其他用户。\n## 混合推荐\n实际系统中，一般是多种推荐方式一起使用，但是混合方式很多，例如加权混合、切换混合（在不同情况下，使用不同的推荐系统）、分区混合（不同方式推荐的物品放在不同位置显示）、分层混合（不同的推荐方式串行）。\n\n# 推荐系统的评测方式\n## 推荐系统的实验方法\n- 离线实验，将已有数据作为划分为训练数据和测试数据，在训练数据上训练推荐算法，在测试数据上对测试算法进行测评。\n- 用户调查，记录用户在推荐系统上的行为。\n- 在线实验，AB测试：将用户随机分成两部分，分别使用不同的推荐系统，查看两部分用户的行为。\n\n## 评测指标\n### 预测准确度\n- 评分预测的准确度，均方根误差（RMSE），或者平均绝对值误差（MAE）。\n- Top-N推荐，精确率（precision=TP/(FP + TP)），召回率（Recall = TP / (TP+FN)）\n### 用户满意度\n### 覆盖率\n关注冷门物品的推荐。\n### 多样性\n每次推荐的物品的多样性。\n### 惊喜度\n### 信任度\n### 实时性\n### 健壮性\n### 商业目标\n\n\n## 基于UGC(用户生成标签)的推荐\n用户对物品的标签可以用三元组表示，（用户，物品，标签），表示为（u, i, b），用户u对物品i的兴趣公式可以表达为$p(u, i) = \\sum\\limits_b n_{u,b}n_{b,i}$，其中，$n_{u,b}$表示用户$u$打过$b$标签的次数，$n_{b, i}$表示物品$i$被打标签$b$的次数。\n\n## TF-IDF\n在UGC中，如果有热门标签，或者热门物品，那么UGC总是倾向于推荐热门物品或者有热门标签的物品，因此需要对热门程度做一个惩罚，例如在文章的关键字提取中，不能仅仅用词语出现的频率大小来决定关键字，因此提出了TF-IDF方法，通过计算TFIDF值的大小来决定文章的关键字，计算方式如下。\n\n$TFIDF = TF \\times IDF$，这个值越大，那么当前词就越可能是当前文档的关键词。\n\n### 词频(Term Frenquency, TF)\n表示某个词语在改文件中的出现频率，$TF_{i,j} = \\frac{n_{i, j}}{n_j}$，$n_{i, j}$是在文档$j$中，词语$i$出现的次数，$n_j$表示在文档$j$中的总词数。\n\n### 逆向文件频率（Inverse Document Frequency, IDF）\n表示一个词语的重要性，可以用文档数目除以包含该文档的总数目，然后取对数，$IDF_i = \\log(\\frac{N+1}{N_i + 1})$，其中$N$表示所有文档的数量，$N_i$表示包括词语i的文档数量。\n\n## TF-IDF对UGC推荐的改进\n在原始的UGC推荐中，用户$u$对于物品$i$的感兴趣程度由$p(u, i) = \\sum\\limits_b n_{u,b}n_{b,i}$计算。为了避免热门标签和热门物品的影响，我们将$n_{u,b}$替换为$\\frac{n_{u,b}}{\\log(1 + n^{u}_b)}$，将$n_{b,i}$替换为$\\frac{n_{b,i}}{\\log(1 + n^{u}_i)}$，其中$n^{u}_b$表示有多少个用户打过$b$标签，$n^{u}_i$表示有多少个用户对物品$i$打过标签。\n$$\n\\begin{aligned}\n    p(u, i) = \\sum\\limits_b\\frac{n_{u,b}}{\\log(1 + n^{u}_b)}\\frac{n_{b,i}}{\\log(1 + n^{u}_i)}\n\\end{aligned}\n$$\n\n## 隐语义模型(LFM，Latent Factor Model)\n对于行代表用户，列代表物品的一个喜爱程度矩阵$M \\in R^{m \\times n}$，可以通过矩阵分解的方式，分解为$M = P \\times Q$，其中$P \\in R^{m \\times k}$，$Q \\in R^{k \\times n}$，$k$表示我们提取出来的隐语义特征的个数。\n\n由于$M$将会是一个稀疏矩阵，很多位置的信息我们并没有收集到（而且这些位置正是我们要预测的），因此这里的矩阵分解不是求一个解析解，而是可以通过梯度下降的方式，损失函数定义如下：\n\n$$\nL = \\lambda ||P_i.^T||^2 + \\lambda ||Q._j||^2 + \\sum\\limits_{i,j} I_{i,j}(M_{i,j} - (P_i.\\ Q._j))^2\n$$\n\n这里$I_{i,j}$是个指示函数，表示矩阵$M$中第$i$行第$j$列是否是已收集到的数据，训练时只对已收集的数据做约束，其他地方是需要预测的，$P_i.$表示矩阵$P$的第$i$行，$Q._j$表示矩阵$Q$的第$j$列，这两个用于模型的l2正则化。通过这样的方式将$P$、$Q$学习好之后，就可以用$PQ$来获取完整的$M$矩阵，即可知道每个用户对每个物品喜爱程度的预测值，可用于推荐。\n\n## 未完待续。。。","source":"_posts/学习笔记/推荐系统学习笔记.md","raw":"---\ntitle: 推荐系统学习笔记\ndate: 2020-08-11 21:21:06\ntags:\n---\n\n# 推荐系统简介\n## 基于用户的推荐系统\n计算用户信息之间的相似性，对于和用户A相似的用户B，将用户A喜欢的物品推荐给用户B。\n## 基于内容的推荐系统\n计算物品之间的相似性，与某个物品A相似的物品B，将被推荐给喜欢物品A的用户。\n## 基于协同过滤（Collaborative Filtering）的推荐系统\n协同过滤是指用户行为，需要的数据表现为一个用户对物品的评分矩阵（或者点击、喜爱程度矩阵），协同过滤推荐有两种方式，一种是User-CF，对于喜欢的物品相似的用户A和用户B,将他们喜欢的其他东西推荐给对方。一种是Item-CF，如果一系列的物品都被同一个用户喜欢，则这些物品判定为相似，可以推荐给喜欢这类物品的其他用户。\n## 混合推荐\n实际系统中，一般是多种推荐方式一起使用，但是混合方式很多，例如加权混合、切换混合（在不同情况下，使用不同的推荐系统）、分区混合（不同方式推荐的物品放在不同位置显示）、分层混合（不同的推荐方式串行）。\n\n# 推荐系统的评测方式\n## 推荐系统的实验方法\n- 离线实验，将已有数据作为划分为训练数据和测试数据，在训练数据上训练推荐算法，在测试数据上对测试算法进行测评。\n- 用户调查，记录用户在推荐系统上的行为。\n- 在线实验，AB测试：将用户随机分成两部分，分别使用不同的推荐系统，查看两部分用户的行为。\n\n## 评测指标\n### 预测准确度\n- 评分预测的准确度，均方根误差（RMSE），或者平均绝对值误差（MAE）。\n- Top-N推荐，精确率（precision=TP/(FP + TP)），召回率（Recall = TP / (TP+FN)）\n### 用户满意度\n### 覆盖率\n关注冷门物品的推荐。\n### 多样性\n每次推荐的物品的多样性。\n### 惊喜度\n### 信任度\n### 实时性\n### 健壮性\n### 商业目标\n\n\n## 基于UGC(用户生成标签)的推荐\n用户对物品的标签可以用三元组表示，（用户，物品，标签），表示为（u, i, b），用户u对物品i的兴趣公式可以表达为$p(u, i) = \\sum\\limits_b n_{u,b}n_{b,i}$，其中，$n_{u,b}$表示用户$u$打过$b$标签的次数，$n_{b, i}$表示物品$i$被打标签$b$的次数。\n\n## TF-IDF\n在UGC中，如果有热门标签，或者热门物品，那么UGC总是倾向于推荐热门物品或者有热门标签的物品，因此需要对热门程度做一个惩罚，例如在文章的关键字提取中，不能仅仅用词语出现的频率大小来决定关键字，因此提出了TF-IDF方法，通过计算TFIDF值的大小来决定文章的关键字，计算方式如下。\n\n$TFIDF = TF \\times IDF$，这个值越大，那么当前词就越可能是当前文档的关键词。\n\n### 词频(Term Frenquency, TF)\n表示某个词语在改文件中的出现频率，$TF_{i,j} = \\frac{n_{i, j}}{n_j}$，$n_{i, j}$是在文档$j$中，词语$i$出现的次数，$n_j$表示在文档$j$中的总词数。\n\n### 逆向文件频率（Inverse Document Frequency, IDF）\n表示一个词语的重要性，可以用文档数目除以包含该文档的总数目，然后取对数，$IDF_i = \\log(\\frac{N+1}{N_i + 1})$，其中$N$表示所有文档的数量，$N_i$表示包括词语i的文档数量。\n\n## TF-IDF对UGC推荐的改进\n在原始的UGC推荐中，用户$u$对于物品$i$的感兴趣程度由$p(u, i) = \\sum\\limits_b n_{u,b}n_{b,i}$计算。为了避免热门标签和热门物品的影响，我们将$n_{u,b}$替换为$\\frac{n_{u,b}}{\\log(1 + n^{u}_b)}$，将$n_{b,i}$替换为$\\frac{n_{b,i}}{\\log(1 + n^{u}_i)}$，其中$n^{u}_b$表示有多少个用户打过$b$标签，$n^{u}_i$表示有多少个用户对物品$i$打过标签。\n$$\n\\begin{aligned}\n    p(u, i) = \\sum\\limits_b\\frac{n_{u,b}}{\\log(1 + n^{u}_b)}\\frac{n_{b,i}}{\\log(1 + n^{u}_i)}\n\\end{aligned}\n$$\n\n## 隐语义模型(LFM，Latent Factor Model)\n对于行代表用户，列代表物品的一个喜爱程度矩阵$M \\in R^{m \\times n}$，可以通过矩阵分解的方式，分解为$M = P \\times Q$，其中$P \\in R^{m \\times k}$，$Q \\in R^{k \\times n}$，$k$表示我们提取出来的隐语义特征的个数。\n\n由于$M$将会是一个稀疏矩阵，很多位置的信息我们并没有收集到（而且这些位置正是我们要预测的），因此这里的矩阵分解不是求一个解析解，而是可以通过梯度下降的方式，损失函数定义如下：\n\n$$\nL = \\lambda ||P_i.^T||^2 + \\lambda ||Q._j||^2 + \\sum\\limits_{i,j} I_{i,j}(M_{i,j} - (P_i.\\ Q._j))^2\n$$\n\n这里$I_{i,j}$是个指示函数，表示矩阵$M$中第$i$行第$j$列是否是已收集到的数据，训练时只对已收集的数据做约束，其他地方是需要预测的，$P_i.$表示矩阵$P$的第$i$行，$Q._j$表示矩阵$Q$的第$j$列，这两个用于模型的l2正则化。通过这样的方式将$P$、$Q$学习好之后，就可以用$PQ$来获取完整的$M$矩阵，即可知道每个用户对每个物品喜爱程度的预测值，可用于推荐。\n\n## 未完待续。。。","slug":"学习笔记/推荐系统学习笔记","published":1,"updated":"2020-08-31T06:39:20.769Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckjxpc3s1001b44mq0vamcz97","content":"<h1 id=\"推荐系统简介\">推荐系统简介</h1>\r\n<h2 id=\"基于用户的推荐系统\">基于用户的推荐系统</h2>\r\n<p>计算用户信息之间的相似性，对于和用户A相似的用户B，将用户A喜欢的物品推荐给用户B。 ## 基于内容的推荐系统 计算物品之间的相似性，与某个物品A相似的物品B，将被推荐给喜欢物品A的用户。 ## 基于协同过滤（Collaborative Filtering）的推荐系统 协同过滤是指用户行为，需要的数据表现为一个用户对物品的评分矩阵（或者点击、喜爱程度矩阵），协同过滤推荐有两种方式，一种是User-CF，对于喜欢的物品相似的用户A和用户B,将他们喜欢的其他东西推荐给对方。一种是Item-CF，如果一系列的物品都被同一个用户喜欢，则这些物品判定为相似，可以推荐给喜欢这类物品的其他用户。 ## 混合推荐 实际系统中，一般是多种推荐方式一起使用，但是混合方式很多，例如加权混合、切换混合（在不同情况下，使用不同的推荐系统）、分区混合（不同方式推荐的物品放在不同位置显示）、分层混合（不同的推荐方式串行）。</p>\r\n<h1 id=\"推荐系统的评测方式\">推荐系统的评测方式</h1>\r\n<h2 id=\"推荐系统的实验方法\">推荐系统的实验方法</h2>\r\n<ul>\r\n<li>离线实验，将已有数据作为划分为训练数据和测试数据，在训练数据上训练推荐算法，在测试数据上对测试算法进行测评。</li>\r\n<li>用户调查，记录用户在推荐系统上的行为。</li>\r\n<li>在线实验，AB测试：将用户随机分成两部分，分别使用不同的推荐系统，查看两部分用户的行为。</li>\r\n</ul>\r\n<h2 id=\"评测指标\">评测指标</h2>\r\n<h3 id=\"预测准确度\">预测准确度</h3>\r\n<ul>\r\n<li>评分预测的准确度，均方根误差（RMSE），或者平均绝对值误差（MAE）。</li>\r\n<li>Top-N推荐，精确率（precision=TP/(FP + TP)），召回率（Recall = TP / (TP+FN)） ### 用户满意度 ### 覆盖率 关注冷门物品的推荐。 ### 多样性 每次推荐的物品的多样性。 ### 惊喜度 ### 信任度 ### 实时性 ### 健壮性 ### 商业目标</li>\r\n</ul>\r\n<h2 id=\"基于ugc用户生成标签的推荐\">基于UGC(用户生成标签)的推荐</h2>\r\n<p>用户对物品的标签可以用三元组表示，（用户，物品，标签），表示为（u, i, b），用户u对物品i的兴趣公式可以表达为<span class=\"math inline\">\\(p(u, i) = \\sum\\limits_b n_{u,b}n_{b,i}\\)</span>，其中，<span class=\"math inline\">\\(n_{u,b}\\)</span>表示用户<span class=\"math inline\">\\(u\\)</span>打过<span class=\"math inline\">\\(b\\)</span>标签的次数，<span class=\"math inline\">\\(n_{b, i}\\)</span>表示物品<span class=\"math inline\">\\(i\\)</span>被打标签<span class=\"math inline\">\\(b\\)</span>的次数。</p>\r\n<h2 id=\"tf-idf\">TF-IDF</h2>\r\n<p>在UGC中，如果有热门标签，或者热门物品，那么UGC总是倾向于推荐热门物品或者有热门标签的物品，因此需要对热门程度做一个惩罚，例如在文章的关键字提取中，不能仅仅用词语出现的频率大小来决定关键字，因此提出了TF-IDF方法，通过计算TFIDF值的大小来决定文章的关键字，计算方式如下。</p>\r\n<p><span class=\"math inline\">\\(TFIDF = TF \\times IDF\\)</span>，这个值越大，那么当前词就越可能是当前文档的关键词。</p>\r\n<h3 id=\"词频term-frenquency-tf\">词频(Term Frenquency, TF)</h3>\r\n<p>表示某个词语在改文件中的出现频率，<span class=\"math inline\">\\(TF_{i,j} = \\frac{n_{i, j}}{n_j}\\)</span>，<span class=\"math inline\">\\(n_{i, j}\\)</span>是在文档<span class=\"math inline\">\\(j\\)</span>中，词语<span class=\"math inline\">\\(i\\)</span>出现的次数，<span class=\"math inline\">\\(n_j\\)</span>表示在文档<span class=\"math inline\">\\(j\\)</span>中的总词数。</p>\r\n<h3 id=\"逆向文件频率inverse-document-frequency-idf\">逆向文件频率（Inverse Document Frequency, IDF）</h3>\r\n<p>表示一个词语的重要性，可以用文档数目除以包含该文档的总数目，然后取对数，<span class=\"math inline\">\\(IDF_i = \\log(\\frac{N+1}{N_i + 1})\\)</span>，其中<span class=\"math inline\">\\(N\\)</span>表示所有文档的数量，<span class=\"math inline\">\\(N_i\\)</span>表示包括词语i的文档数量。</p>\r\n<h2 id=\"tf-idf对ugc推荐的改进\">TF-IDF对UGC推荐的改进</h2>\r\n<p>在原始的UGC推荐中，用户<span class=\"math inline\">\\(u\\)</span>对于物品<span class=\"math inline\">\\(i\\)</span>的感兴趣程度由<span class=\"math inline\">\\(p(u, i) = \\sum\\limits_b n_{u,b}n_{b,i}\\)</span>计算。为了避免热门标签和热门物品的影响，我们将<span class=\"math inline\">\\(n_{u,b}\\)</span>替换为<span class=\"math inline\">\\(\\frac{n_{u,b}}{\\log(1 + n^{u}_b)}\\)</span>，将<span class=\"math inline\">\\(n_{b,i}\\)</span>替换为<span class=\"math inline\">\\(\\frac{n_{b,i}}{\\log(1 + n^{u}_i)}\\)</span>，其中<span class=\"math inline\">\\(n^{u}_b\\)</span>表示有多少个用户打过<span class=\"math inline\">\\(b\\)</span>标签，<span class=\"math inline\">\\(n^{u}_i\\)</span>表示有多少个用户对物品<span class=\"math inline\">\\(i\\)</span>打过标签。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    p(u, i) = \\sum\\limits_b\\frac{n_{u,b}}{\\log(1 + n^{u}_b)}\\frac{n_{b,i}}{\\log(1 + n^{u}_i)}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<h2 id=\"隐语义模型lfmlatent-factor-model\">隐语义模型(LFM，Latent Factor Model)</h2>\r\n<p>对于行代表用户，列代表物品的一个喜爱程度矩阵<span class=\"math inline\">\\(M \\in R^{m \\times n}\\)</span>，可以通过矩阵分解的方式，分解为<span class=\"math inline\">\\(M = P \\times Q\\)</span>，其中<span class=\"math inline\">\\(P \\in R^{m \\times k}\\)</span>，<span class=\"math inline\">\\(Q \\in R^{k \\times n}\\)</span>，<span class=\"math inline\">\\(k\\)</span>表示我们提取出来的隐语义特征的个数。</p>\r\n<p>由于<span class=\"math inline\">\\(M\\)</span>将会是一个稀疏矩阵，很多位置的信息我们并没有收集到（而且这些位置正是我们要预测的），因此这里的矩阵分解不是求一个解析解，而是可以通过梯度下降的方式，损失函数定义如下：</p>\r\n<p><span class=\"math display\">\\[\r\nL = \\lambda ||P_i.^T||^2 + \\lambda ||Q._j||^2 + \\sum\\limits_{i,j} I_{i,j}(M_{i,j} - (P_i.\\ Q._j))^2\r\n\\]</span></p>\r\n<p>这里<span class=\"math inline\">\\(I_{i,j}\\)</span>是个指示函数，表示矩阵<span class=\"math inline\">\\(M\\)</span>中第<span class=\"math inline\">\\(i\\)</span>行第<span class=\"math inline\">\\(j\\)</span>列是否是已收集到的数据，训练时只对已收集的数据做约束，其他地方是需要预测的，<span class=\"math inline\">\\(P_i.\\)</span>表示矩阵<span class=\"math inline\">\\(P\\)</span>的第<span class=\"math inline\">\\(i\\)</span>行，<span class=\"math inline\">\\(Q._j\\)</span>表示矩阵<span class=\"math inline\">\\(Q\\)</span>的第<span class=\"math inline\">\\(j\\)</span>列，这两个用于模型的l2正则化。通过这样的方式将<span class=\"math inline\">\\(P\\)</span>、<span class=\"math inline\">\\(Q\\)</span>学习好之后，就可以用<span class=\"math inline\">\\(PQ\\)</span>来获取完整的<span class=\"math inline\">\\(M\\)</span>矩阵，即可知道每个用户对每个物品喜爱程度的预测值，可用于推荐。</p>\r\n<h2 id=\"未完待续\">未完待续。。。</h2>\r\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"推荐系统简介\">推荐系统简介</h1>\r\n<h2 id=\"基于用户的推荐系统\">基于用户的推荐系统</h2>\r\n<p>计算用户信息之间的相似性，对于和用户A相似的用户B，将用户A喜欢的物品推荐给用户B。 ## 基于内容的推荐系统 计算物品之间的相似性，与某个物品A相似的物品B，将被推荐给喜欢物品A的用户。 ## 基于协同过滤（Collaborative Filtering）的推荐系统 协同过滤是指用户行为，需要的数据表现为一个用户对物品的评分矩阵（或者点击、喜爱程度矩阵），协同过滤推荐有两种方式，一种是User-CF，对于喜欢的物品相似的用户A和用户B,将他们喜欢的其他东西推荐给对方。一种是Item-CF，如果一系列的物品都被同一个用户喜欢，则这些物品判定为相似，可以推荐给喜欢这类物品的其他用户。 ## 混合推荐 实际系统中，一般是多种推荐方式一起使用，但是混合方式很多，例如加权混合、切换混合（在不同情况下，使用不同的推荐系统）、分区混合（不同方式推荐的物品放在不同位置显示）、分层混合（不同的推荐方式串行）。</p>\r\n<h1 id=\"推荐系统的评测方式\">推荐系统的评测方式</h1>\r\n<h2 id=\"推荐系统的实验方法\">推荐系统的实验方法</h2>\r\n<ul>\r\n<li>离线实验，将已有数据作为划分为训练数据和测试数据，在训练数据上训练推荐算法，在测试数据上对测试算法进行测评。</li>\r\n<li>用户调查，记录用户在推荐系统上的行为。</li>\r\n<li>在线实验，AB测试：将用户随机分成两部分，分别使用不同的推荐系统，查看两部分用户的行为。</li>\r\n</ul>\r\n<h2 id=\"评测指标\">评测指标</h2>\r\n<h3 id=\"预测准确度\">预测准确度</h3>\r\n<ul>\r\n<li>评分预测的准确度，均方根误差（RMSE），或者平均绝对值误差（MAE）。</li>\r\n<li>Top-N推荐，精确率（precision=TP/(FP + TP)），召回率（Recall = TP / (TP+FN)） ### 用户满意度 ### 覆盖率 关注冷门物品的推荐。 ### 多样性 每次推荐的物品的多样性。 ### 惊喜度 ### 信任度 ### 实时性 ### 健壮性 ### 商业目标</li>\r\n</ul>\r\n<h2 id=\"基于ugc用户生成标签的推荐\">基于UGC(用户生成标签)的推荐</h2>\r\n<p>用户对物品的标签可以用三元组表示，（用户，物品，标签），表示为（u, i, b），用户u对物品i的兴趣公式可以表达为<span class=\"math inline\">\\(p(u, i) = \\sum\\limits_b n_{u,b}n_{b,i}\\)</span>，其中，<span class=\"math inline\">\\(n_{u,b}\\)</span>表示用户<span class=\"math inline\">\\(u\\)</span>打过<span class=\"math inline\">\\(b\\)</span>标签的次数，<span class=\"math inline\">\\(n_{b, i}\\)</span>表示物品<span class=\"math inline\">\\(i\\)</span>被打标签<span class=\"math inline\">\\(b\\)</span>的次数。</p>\r\n<h2 id=\"tf-idf\">TF-IDF</h2>\r\n<p>在UGC中，如果有热门标签，或者热门物品，那么UGC总是倾向于推荐热门物品或者有热门标签的物品，因此需要对热门程度做一个惩罚，例如在文章的关键字提取中，不能仅仅用词语出现的频率大小来决定关键字，因此提出了TF-IDF方法，通过计算TFIDF值的大小来决定文章的关键字，计算方式如下。</p>\r\n<p><span class=\"math inline\">\\(TFIDF = TF \\times IDF\\)</span>，这个值越大，那么当前词就越可能是当前文档的关键词。</p>\r\n<h3 id=\"词频term-frenquency-tf\">词频(Term Frenquency, TF)</h3>\r\n<p>表示某个词语在改文件中的出现频率，<span class=\"math inline\">\\(TF_{i,j} = \\frac{n_{i, j}}{n_j}\\)</span>，<span class=\"math inline\">\\(n_{i, j}\\)</span>是在文档<span class=\"math inline\">\\(j\\)</span>中，词语<span class=\"math inline\">\\(i\\)</span>出现的次数，<span class=\"math inline\">\\(n_j\\)</span>表示在文档<span class=\"math inline\">\\(j\\)</span>中的总词数。</p>\r\n<h3 id=\"逆向文件频率inverse-document-frequency-idf\">逆向文件频率（Inverse Document Frequency, IDF）</h3>\r\n<p>表示一个词语的重要性，可以用文档数目除以包含该文档的总数目，然后取对数，<span class=\"math inline\">\\(IDF_i = \\log(\\frac{N+1}{N_i + 1})\\)</span>，其中<span class=\"math inline\">\\(N\\)</span>表示所有文档的数量，<span class=\"math inline\">\\(N_i\\)</span>表示包括词语i的文档数量。</p>\r\n<h2 id=\"tf-idf对ugc推荐的改进\">TF-IDF对UGC推荐的改进</h2>\r\n<p>在原始的UGC推荐中，用户<span class=\"math inline\">\\(u\\)</span>对于物品<span class=\"math inline\">\\(i\\)</span>的感兴趣程度由<span class=\"math inline\">\\(p(u, i) = \\sum\\limits_b n_{u,b}n_{b,i}\\)</span>计算。为了避免热门标签和热门物品的影响，我们将<span class=\"math inline\">\\(n_{u,b}\\)</span>替换为<span class=\"math inline\">\\(\\frac{n_{u,b}}{\\log(1 + n^{u}_b)}\\)</span>，将<span class=\"math inline\">\\(n_{b,i}\\)</span>替换为<span class=\"math inline\">\\(\\frac{n_{b,i}}{\\log(1 + n^{u}_i)}\\)</span>，其中<span class=\"math inline\">\\(n^{u}_b\\)</span>表示有多少个用户打过<span class=\"math inline\">\\(b\\)</span>标签，<span class=\"math inline\">\\(n^{u}_i\\)</span>表示有多少个用户对物品<span class=\"math inline\">\\(i\\)</span>打过标签。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    p(u, i) = \\sum\\limits_b\\frac{n_{u,b}}{\\log(1 + n^{u}_b)}\\frac{n_{b,i}}{\\log(1 + n^{u}_i)}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<h2 id=\"隐语义模型lfmlatent-factor-model\">隐语义模型(LFM，Latent Factor Model)</h2>\r\n<p>对于行代表用户，列代表物品的一个喜爱程度矩阵<span class=\"math inline\">\\(M \\in R^{m \\times n}\\)</span>，可以通过矩阵分解的方式，分解为<span class=\"math inline\">\\(M = P \\times Q\\)</span>，其中<span class=\"math inline\">\\(P \\in R^{m \\times k}\\)</span>，<span class=\"math inline\">\\(Q \\in R^{k \\times n}\\)</span>，<span class=\"math inline\">\\(k\\)</span>表示我们提取出来的隐语义特征的个数。</p>\r\n<p>由于<span class=\"math inline\">\\(M\\)</span>将会是一个稀疏矩阵，很多位置的信息我们并没有收集到（而且这些位置正是我们要预测的），因此这里的矩阵分解不是求一个解析解，而是可以通过梯度下降的方式，损失函数定义如下：</p>\r\n<p><span class=\"math display\">\\[\r\nL = \\lambda ||P_i.^T||^2 + \\lambda ||Q._j||^2 + \\sum\\limits_{i,j} I_{i,j}(M_{i,j} - (P_i.\\ Q._j))^2\r\n\\]</span></p>\r\n<p>这里<span class=\"math inline\">\\(I_{i,j}\\)</span>是个指示函数，表示矩阵<span class=\"math inline\">\\(M\\)</span>中第<span class=\"math inline\">\\(i\\)</span>行第<span class=\"math inline\">\\(j\\)</span>列是否是已收集到的数据，训练时只对已收集的数据做约束，其他地方是需要预测的，<span class=\"math inline\">\\(P_i.\\)</span>表示矩阵<span class=\"math inline\">\\(P\\)</span>的第<span class=\"math inline\">\\(i\\)</span>行，<span class=\"math inline\">\\(Q._j\\)</span>表示矩阵<span class=\"math inline\">\\(Q\\)</span>的第<span class=\"math inline\">\\(j\\)</span>列，这两个用于模型的l2正则化。通过这样的方式将<span class=\"math inline\">\\(P\\)</span>、<span class=\"math inline\">\\(Q\\)</span>学习好之后，就可以用<span class=\"math inline\">\\(PQ\\)</span>来获取完整的<span class=\"math inline\">\\(M\\)</span>矩阵，即可知道每个用户对每个物品喜爱程度的预测值，可用于推荐。</p>\r\n<h2 id=\"未完待续\">未完待续。。。</h2>\r\n"},{"title":"标签噪声的处理","date":"2020-10-08T01:41:36.000Z","_content":"\n# 论文《UNDERSTANDING DEEP LEARNING REQUIRES RETHINKING GENERALIZATION》\n\n该论文中首先做了两组实验：随机标签实验和随机噪声实验，数据集使用CIFAR10和ImageNet，在随机标签实验中，将所有标签替换为随机生成的，然后来训练CNN，发现即使是随机标签，CNN也可以完美拟合训练数据（训练误差为0），但是测试集误差基本上和瞎猜差不多，该实验表明模型可以强行记住所有数据，另外一个实验是随机噪声实验，在数据上添加随机噪声（例如高斯噪声），实验结果表明，随着噪声水平的增加，模型仍然可以拟合好训练集，但是泛化误差稳定上升。\n\n根据上面的实验，论文中主要突出了一个问题：模型明明可以暴力记住所有数据，为什么会得到泛化能力？论文中以一个线性模型为例，认为SGD也包含一定的正则化能力，从而导致模型可以有一定的泛化性能。\n\n# 论文《TRAINING DEEP NEURAL-NETWORKS USING A NOISEADAPTATION LAYER》\n\n论文中首先定义了噪声标签的一种概率框架，例如一个多类别的神经网络软分类器可以表达为$p(y=i|x;w)$，其中$x$表示特征向量，$w$表示模型权重，$y$表示没有包含噪声的真实标签，而当前获取到的标签$z$包含噪声，假设噪声标签$z$的分布和特征$x$独立，因此可以简单的通过一个参数矩阵来描述噪声标签和真实标签的关系：$\\theta (i,j)=p(z=j|y=i)$，如下图所示。\n\n![噪声标签和真实标签的模型示意图](噪声标签和真实标签的关系.png)\n\n由于在当前模型中存在隐变量，因此自然使用EM算法来进行优化，在E-step，使用目前的参数估计第$t$个样本真实标签为$i$的概率$c_{ti} = p(y_t = i | x_t, z_t; w_0, \\theta_0), i=1,2,...,k, t=1,2,...,n$，其中$k$是类别个数，$n$是样本个数。而在M-step，则需要同时更新参数$\\theta$和参数$w$，参数$\\theta$有闭式解：$\\theta(i,j) = \\frac{\\sum_t c_{ti} \\mathbb{1}(z_t=j)}{\\sum_t c_{ti}}, i,j \\in \\{1, 2, ..., k\\}$，而参数$w$则需要使用梯度下降的方式来更新，其目标定义为最大化$S(w) = \\sum\\limits_{t=1}^n\\sum\\limits_{i=1}^k c_{ti} log(p(y_t=i|x_i;w))$，这个就是一个软化版本的似然函数。\n\n但是，在上面的EM算法过程中，存在一些问题，第一个是EM算法很可能收敛到局部最优点，第二个是在上面的EM框架中，每次在M-step更新的时候，都需要训练$w$直至收敛，这个对于稍微大型的模型或者数据集来说都是非常费时间的，第三是关于上述讨论的基本假设：\"噪声标签$z$的分布和特征$x$独立\"，这个假设太强，而且一般是不成立的，因此论文针对这些问题进一步提出了一些解决方案。\n\n将上面的模型顺序做个调换，得到如下图所示的模型示意图，其中上面的示意图是面模型训练的结构，下面的示意图是模型测试时的结构，在训练结构中，这里的$x$表示样本，$h$表示神经网络提取到的特征，$y$和$z$的定义和前面一样，这里有两个softmax层(这里的softmax是指以softmax为激活函数的全连接层)，这里的意思是在原始模型的基础上，增加一个softmax来对噪声进行建模，然后使用带有标签的噪声$z$进行训练。预测时输出中间结果$y$作为对隐藏的正确标签的预测。\n\n![论文提出使用一个新加的层对噪声进行建模](sModel.png)\n\n这个模型非常简单，但是如果没有一些特殊方法的话，肯定不能保证最终$y$会收敛成为真实标签的预测，因此还需要仔细的设计该模型的初始化方法。\n\n首先softmax层可以表达如下，这里的$u^T$和$b$就是softmax层的参数。\n\n$$\n\\begin{aligned}\n    p(z=j|y=i,x) = \\frac{exp(u^T_{ij}h + b_{ij})}{\\sum_l exp(u^T_{il}h + b_{il})}\n\\end{aligned}\n$$\n论文中提出，首先使用标签$z$去训练图中的non-linear function和第一个softmax层，就当成正常的训练，得到$p(y|x;w)$，训练好后这两个模块的权重不变，而且将当前模型预测的$y$就当做真实标签，去计算和噪声标签$z$的混淆矩阵，使用这个混淆矩阵去初始化$b$，另外$u$初始化为0，$b_{ij}$的初始化方法如下，注意这里的$z$是表示含噪声的标签。\n\n$$\n\\begin{aligned}\n    b_{ij} = log(\\frac{\\sum_t\\mathbf{1}_{\\{z_t=j\\}}p(y_t=i|x_t)}{\\sum_tp(y_t=i|x_t)})\n\\end{aligned}\n$$\n\n\n# 论文《MentorNet: Learning Data-Driven Curriculumfor Very Deep Neural Networks on Corrupted Labels》\n该论文提出了一种通过学习数据驱动的课程来在包含噪声的数据上达到更好效果的方法，论文中提出的MentorNet要不就是去学习一个预先定义好的课程，要不就在一个干净的数据上先进行训练（随机构造noisy label，然后让MentorNet来区分），如果直接去学习一个预定义好的课程，那么MentorNet其实没啥作用，和直接使用定义好的课程效果差不多，但是如果有一个干净的数据集可以让MentorNet去训练，那么效果会好很多。\n\n\n# 论文《Joint Optimization Framework for Learning with Noisy Labels》\n该论文关注的是分类任务上的噪声标签数据，并使用迭代的标签修正方法来构造伪标签，最后使用伪标签来重头训练分类器，以达到更好的分类效果，整体方法框架如下图所示，其中$x_i$表示的是训练样本，$y^{(t)}_i$表示的是第$t$次迭代所使用的标签，先使用原始的含噪声标签$y^{(0)}_i$来计算损失函数$L$以训练模型，然后使用模型的预测概率$s$来对标签进行更新，有两种方式，一种是one-hot的更新标签（即对模型预测概率求argmax，这种方法在论文中每次不会更新所有标签，而是只更新预测结果和当前标签差距最大的top500个样本的标签），另外一种是使用概率来构成软标签（即直接使用模型预测概率作为新的标签，这种方法每次都更新所有标签。实验表明，软标签更新的效果更好），更新后的标签又继续训练模型，经过这样的迭代过程之后，可以得到经过修复的标签，这个时候就可以使用得到的标签来重头训练一个分类器以得到更好的效果。\n\n![迭代的标签修正](标签修正框架.png)\n\n上面的框架非常简单，其实中心思想就是一个EM算法的过程，最终得到伪标签，然后用伪标签来训练新的模型，该论文重要的部分在于其其学习速率的选择，以及损失函数的设计。\n\n首先该论文对文章《A closer look at memorization in deep networks》中的提到的一种现象进行了实验验证：**在大学习速率的情况下，可以有效的抑制模型对无规则数据的记忆（即可以有效的抵抗标签噪声的影响）**。因此在学习速率的选择上，论文在训练上述的标签迭代修正过程时。使用了较高的学习速率（例如论文中对于CIFAR-10数据，使用batch size 128, SGD优化器, lr 0.1，其他数据集的学习速率则不同，可能需要调参）\n\n第二比较重要的是在迭代标签修正过程中的损失函数$L$的设计，其总体的损失函数定义如下，其中$L_c$可以是任何分类损失函数，例如交叉熵，而$L_p$和$L_e$分别是两个不同的约束项，$L_p$中的$p_j$表示类$j$的先验分布，这一项希望模型预测的类别分布（$\\bar{s}_j(\\theta,X)$从一个Batch的数据统计得到，所以这里的batch size不宜太小）趋近于训练数据的类别先验分布，这是为了防止在标签迭代更新的过程中，$y$和$s$全都变成同一个类别，另外一项$L_e$计算的是模型预测的熵，最小化模型预测的熵，可以使模型的预测更加自信，避免模型的预测变成每个类均匀分布，这里比较难的就是超参数$\\alpha$和$\\beta$的设置，从论文中来看，作者在不同的数据集上使用的超参数不太相同，可能需要经过大量实验调参。\n$$\n\\begin{aligned}\n    L(\\theta,Y|X) &= L_c(\\theta,Y|X) + \\alpha L_p(\\theta|X) + \\beta L_e(\\theta|X)\\\\\n    L_p(\\theta|X) &= \\sum\\limits_{j=1}^c p_j \\log\\frac{p_j}{\\bar{s}_j(\\theta,X)}\\\\\n    \\bar{s}_j(\\theta,X) &= \\frac{1}{n}\\sum\\limits_{i=1}^n s(\\theta, x_i) \\approx \\frac{1}{|\\mathcal{B}|}\\sum\\limits_{x \\in \\mathcal{B}} s(\\theta, x)\\\\\n    L_e(\\theta|X) &= -\\frac{1}{n}\\sum\\limits_{i=1}^n \\sum\\limits_{j=1}^c s_j(\\theta,x_i) \\log s_j(\\theta,x_i)\\\\\n\\end{aligned}\n$$\n\n# 论文《Confident Learning: Estimating Uncertainty in Dataset Labels》\n该论文主要关注在多分类任务上的噪声标签数据，使用置信学习的方法来估计噪声标签分布和未知的真实样本分布之间的联合分布，并据此修复数据中的噪声。\n\n首先，$x_k \\in \\mathbf{X}, k=1,2,3,...,N$代表所有数据，包含噪声的数据标签表示为$\\tilde{y} \\in \\{1, 2, 3, ..., m\\}$，未知的真实标签表示为$y^\\star \\in \\{1, 2, 3, ..., m\\}$，论文中假设噪声标签是由一个CNP(class-conditional classification noise process)产生的，即这个CNP构造了映射$y^\\star \\rightarrow \\tilde{y}$，这个假设算是个比较强的假设，因为其蕴含的意思就是$p(\\tilde{y} = i|y^\\star=j) = p(\\tilde{y} = i|y^\\star=j,x)$，即标签是否错误和样本无关，但是事实上很多时候标签错误都是和样本相关的。\n\n论文中的目标就是直接估计联合分布$p(\\tilde{y}, y^\\star)$，为此，论文首先去估计一个矩阵$C_{\\tilde{y}, y^\\star}$，其中$C_{\\tilde{y}, y^\\star}[i][j]$表示真实标签是$j$的样本被误标注为$i$的数量，其估计方法如下，其中绝对值符号是计算数量的意思。\n$$\n\\begin{aligned}\n    C_{\\tilde{y}, y^\\star} &:= |\\hat{\\mathbf{X}}_{\\tilde{y} = i,y^\\star=j}|\\\\\n    \\hat{\\mathbf{X}}_{\\tilde{y} = i,y^\\star=j} &:= \\{x | x \\in X_{\\tilde{y} = i}, \\hat{p}(\\tilde{y}=j|x,\\theta) \\ge t_j, j=\\mathop{\\arg\\max}\\limits_{l \\in [m]} \\hat{p}(\\tilde{y} = l|x, \\theta)\\}\\\\\n    t_j &= \\frac{1}{|\\mathbf{X}_{\\tilde{y} = j}|}\\sum\\limits_{x \\in \\mathbf{X}_{\\tilde{y} = j}}\\hat{p}(\\tilde{y} = j| x, \\theta)\n\\end{aligned}\n$$\n\n得到$C_{\\tilde{y}, y^\\star}$之后，即可用$C_{\\tilde{y}, y^\\star}$来估计$\\tilde{y}$和$y^\\star$的联合分布矩阵$Q_{\\tilde{y}, y^\\star}$，计算方法如下：\n$$\n\\begin{aligned}\n    \\hat{Q}_{\\tilde{y}=i, y^\\star=j} = \\frac{\\frac{C_{\\tilde{y}=i, y^\\star=j}}{\\sum\\limits_{j\\in{1, 2, ..., m}}C_{\\tilde{y}=i, y^\\star=j}} |\\mathbf{X}_{\\tilde{y}=i}|}{\\sum\\limits_{i\\in{1, 2, ..., m}, j\\in{1, 2, ..., m}}(\\frac{C_{\\tilde{y}=i, y^\\star=j}}{\\sum\\limits_{j\\in{1, 2, ..., m}}C_{\\tilde{y}=i, y^\\star=j}} |\\mathbf{X}_{\\tilde{y}=i}|)}\n\\end{aligned}\n$$\n\n$\\hat{Q}_{\\tilde{y}, y^\\star}$是一个概率矩阵，得到Q之后，就可以知道标签为$i$的样本中，有$\\hat{Q}_{\\tilde{y}=i, y^\\star=j} \\times n$个样本是从$j$错误标注过来的，因此后面可以按照预测结果中的自信程度，在$i$类别的样本中，去掉$n\\times\\sum\\limits_{j}\\hat{Q}_{\\tilde{y}=i, y^\\star=j}$个最不自信的样本就行了，这种方法在论文中称为Prune by Class（PBC）。论文中也提出了一些其他处理方法，比如Prune by Noise Rate（PBNR），在置信度排序时，不是按照预测概率排序，而是按照间隔：$\\hat{p}_{x,\\tilde{y}=j} - \\hat{p}_{x,\\tilde{y}=i}$来排序，去掉间隔最大的那些样本。\n\n该论文所提出的方法在CIFAR-10分类问题上表现较好，在Accuracy上远超INCV、Mixup等方法，且在python下的CleanLab库中有官方实现，可直接调用。\n","source":"_posts/学习笔记/标签噪声的处理.md","raw":"---\ntitle: 标签噪声的处理\ndate: 2020-10-08 09:41:36\ntags: [深度学习]\n---\n\n# 论文《UNDERSTANDING DEEP LEARNING REQUIRES RETHINKING GENERALIZATION》\n\n该论文中首先做了两组实验：随机标签实验和随机噪声实验，数据集使用CIFAR10和ImageNet，在随机标签实验中，将所有标签替换为随机生成的，然后来训练CNN，发现即使是随机标签，CNN也可以完美拟合训练数据（训练误差为0），但是测试集误差基本上和瞎猜差不多，该实验表明模型可以强行记住所有数据，另外一个实验是随机噪声实验，在数据上添加随机噪声（例如高斯噪声），实验结果表明，随着噪声水平的增加，模型仍然可以拟合好训练集，但是泛化误差稳定上升。\n\n根据上面的实验，论文中主要突出了一个问题：模型明明可以暴力记住所有数据，为什么会得到泛化能力？论文中以一个线性模型为例，认为SGD也包含一定的正则化能力，从而导致模型可以有一定的泛化性能。\n\n# 论文《TRAINING DEEP NEURAL-NETWORKS USING A NOISEADAPTATION LAYER》\n\n论文中首先定义了噪声标签的一种概率框架，例如一个多类别的神经网络软分类器可以表达为$p(y=i|x;w)$，其中$x$表示特征向量，$w$表示模型权重，$y$表示没有包含噪声的真实标签，而当前获取到的标签$z$包含噪声，假设噪声标签$z$的分布和特征$x$独立，因此可以简单的通过一个参数矩阵来描述噪声标签和真实标签的关系：$\\theta (i,j)=p(z=j|y=i)$，如下图所示。\n\n![噪声标签和真实标签的模型示意图](噪声标签和真实标签的关系.png)\n\n由于在当前模型中存在隐变量，因此自然使用EM算法来进行优化，在E-step，使用目前的参数估计第$t$个样本真实标签为$i$的概率$c_{ti} = p(y_t = i | x_t, z_t; w_0, \\theta_0), i=1,2,...,k, t=1,2,...,n$，其中$k$是类别个数，$n$是样本个数。而在M-step，则需要同时更新参数$\\theta$和参数$w$，参数$\\theta$有闭式解：$\\theta(i,j) = \\frac{\\sum_t c_{ti} \\mathbb{1}(z_t=j)}{\\sum_t c_{ti}}, i,j \\in \\{1, 2, ..., k\\}$，而参数$w$则需要使用梯度下降的方式来更新，其目标定义为最大化$S(w) = \\sum\\limits_{t=1}^n\\sum\\limits_{i=1}^k c_{ti} log(p(y_t=i|x_i;w))$，这个就是一个软化版本的似然函数。\n\n但是，在上面的EM算法过程中，存在一些问题，第一个是EM算法很可能收敛到局部最优点，第二个是在上面的EM框架中，每次在M-step更新的时候，都需要训练$w$直至收敛，这个对于稍微大型的模型或者数据集来说都是非常费时间的，第三是关于上述讨论的基本假设：\"噪声标签$z$的分布和特征$x$独立\"，这个假设太强，而且一般是不成立的，因此论文针对这些问题进一步提出了一些解决方案。\n\n将上面的模型顺序做个调换，得到如下图所示的模型示意图，其中上面的示意图是面模型训练的结构，下面的示意图是模型测试时的结构，在训练结构中，这里的$x$表示样本，$h$表示神经网络提取到的特征，$y$和$z$的定义和前面一样，这里有两个softmax层(这里的softmax是指以softmax为激活函数的全连接层)，这里的意思是在原始模型的基础上，增加一个softmax来对噪声进行建模，然后使用带有标签的噪声$z$进行训练。预测时输出中间结果$y$作为对隐藏的正确标签的预测。\n\n![论文提出使用一个新加的层对噪声进行建模](sModel.png)\n\n这个模型非常简单，但是如果没有一些特殊方法的话，肯定不能保证最终$y$会收敛成为真实标签的预测，因此还需要仔细的设计该模型的初始化方法。\n\n首先softmax层可以表达如下，这里的$u^T$和$b$就是softmax层的参数。\n\n$$\n\\begin{aligned}\n    p(z=j|y=i,x) = \\frac{exp(u^T_{ij}h + b_{ij})}{\\sum_l exp(u^T_{il}h + b_{il})}\n\\end{aligned}\n$$\n论文中提出，首先使用标签$z$去训练图中的non-linear function和第一个softmax层，就当成正常的训练，得到$p(y|x;w)$，训练好后这两个模块的权重不变，而且将当前模型预测的$y$就当做真实标签，去计算和噪声标签$z$的混淆矩阵，使用这个混淆矩阵去初始化$b$，另外$u$初始化为0，$b_{ij}$的初始化方法如下，注意这里的$z$是表示含噪声的标签。\n\n$$\n\\begin{aligned}\n    b_{ij} = log(\\frac{\\sum_t\\mathbf{1}_{\\{z_t=j\\}}p(y_t=i|x_t)}{\\sum_tp(y_t=i|x_t)})\n\\end{aligned}\n$$\n\n\n# 论文《MentorNet: Learning Data-Driven Curriculumfor Very Deep Neural Networks on Corrupted Labels》\n该论文提出了一种通过学习数据驱动的课程来在包含噪声的数据上达到更好效果的方法，论文中提出的MentorNet要不就是去学习一个预先定义好的课程，要不就在一个干净的数据上先进行训练（随机构造noisy label，然后让MentorNet来区分），如果直接去学习一个预定义好的课程，那么MentorNet其实没啥作用，和直接使用定义好的课程效果差不多，但是如果有一个干净的数据集可以让MentorNet去训练，那么效果会好很多。\n\n\n# 论文《Joint Optimization Framework for Learning with Noisy Labels》\n该论文关注的是分类任务上的噪声标签数据，并使用迭代的标签修正方法来构造伪标签，最后使用伪标签来重头训练分类器，以达到更好的分类效果，整体方法框架如下图所示，其中$x_i$表示的是训练样本，$y^{(t)}_i$表示的是第$t$次迭代所使用的标签，先使用原始的含噪声标签$y^{(0)}_i$来计算损失函数$L$以训练模型，然后使用模型的预测概率$s$来对标签进行更新，有两种方式，一种是one-hot的更新标签（即对模型预测概率求argmax，这种方法在论文中每次不会更新所有标签，而是只更新预测结果和当前标签差距最大的top500个样本的标签），另外一种是使用概率来构成软标签（即直接使用模型预测概率作为新的标签，这种方法每次都更新所有标签。实验表明，软标签更新的效果更好），更新后的标签又继续训练模型，经过这样的迭代过程之后，可以得到经过修复的标签，这个时候就可以使用得到的标签来重头训练一个分类器以得到更好的效果。\n\n![迭代的标签修正](标签修正框架.png)\n\n上面的框架非常简单，其实中心思想就是一个EM算法的过程，最终得到伪标签，然后用伪标签来训练新的模型，该论文重要的部分在于其其学习速率的选择，以及损失函数的设计。\n\n首先该论文对文章《A closer look at memorization in deep networks》中的提到的一种现象进行了实验验证：**在大学习速率的情况下，可以有效的抑制模型对无规则数据的记忆（即可以有效的抵抗标签噪声的影响）**。因此在学习速率的选择上，论文在训练上述的标签迭代修正过程时。使用了较高的学习速率（例如论文中对于CIFAR-10数据，使用batch size 128, SGD优化器, lr 0.1，其他数据集的学习速率则不同，可能需要调参）\n\n第二比较重要的是在迭代标签修正过程中的损失函数$L$的设计，其总体的损失函数定义如下，其中$L_c$可以是任何分类损失函数，例如交叉熵，而$L_p$和$L_e$分别是两个不同的约束项，$L_p$中的$p_j$表示类$j$的先验分布，这一项希望模型预测的类别分布（$\\bar{s}_j(\\theta,X)$从一个Batch的数据统计得到，所以这里的batch size不宜太小）趋近于训练数据的类别先验分布，这是为了防止在标签迭代更新的过程中，$y$和$s$全都变成同一个类别，另外一项$L_e$计算的是模型预测的熵，最小化模型预测的熵，可以使模型的预测更加自信，避免模型的预测变成每个类均匀分布，这里比较难的就是超参数$\\alpha$和$\\beta$的设置，从论文中来看，作者在不同的数据集上使用的超参数不太相同，可能需要经过大量实验调参。\n$$\n\\begin{aligned}\n    L(\\theta,Y|X) &= L_c(\\theta,Y|X) + \\alpha L_p(\\theta|X) + \\beta L_e(\\theta|X)\\\\\n    L_p(\\theta|X) &= \\sum\\limits_{j=1}^c p_j \\log\\frac{p_j}{\\bar{s}_j(\\theta,X)}\\\\\n    \\bar{s}_j(\\theta,X) &= \\frac{1}{n}\\sum\\limits_{i=1}^n s(\\theta, x_i) \\approx \\frac{1}{|\\mathcal{B}|}\\sum\\limits_{x \\in \\mathcal{B}} s(\\theta, x)\\\\\n    L_e(\\theta|X) &= -\\frac{1}{n}\\sum\\limits_{i=1}^n \\sum\\limits_{j=1}^c s_j(\\theta,x_i) \\log s_j(\\theta,x_i)\\\\\n\\end{aligned}\n$$\n\n# 论文《Confident Learning: Estimating Uncertainty in Dataset Labels》\n该论文主要关注在多分类任务上的噪声标签数据，使用置信学习的方法来估计噪声标签分布和未知的真实样本分布之间的联合分布，并据此修复数据中的噪声。\n\n首先，$x_k \\in \\mathbf{X}, k=1,2,3,...,N$代表所有数据，包含噪声的数据标签表示为$\\tilde{y} \\in \\{1, 2, 3, ..., m\\}$，未知的真实标签表示为$y^\\star \\in \\{1, 2, 3, ..., m\\}$，论文中假设噪声标签是由一个CNP(class-conditional classification noise process)产生的，即这个CNP构造了映射$y^\\star \\rightarrow \\tilde{y}$，这个假设算是个比较强的假设，因为其蕴含的意思就是$p(\\tilde{y} = i|y^\\star=j) = p(\\tilde{y} = i|y^\\star=j,x)$，即标签是否错误和样本无关，但是事实上很多时候标签错误都是和样本相关的。\n\n论文中的目标就是直接估计联合分布$p(\\tilde{y}, y^\\star)$，为此，论文首先去估计一个矩阵$C_{\\tilde{y}, y^\\star}$，其中$C_{\\tilde{y}, y^\\star}[i][j]$表示真实标签是$j$的样本被误标注为$i$的数量，其估计方法如下，其中绝对值符号是计算数量的意思。\n$$\n\\begin{aligned}\n    C_{\\tilde{y}, y^\\star} &:= |\\hat{\\mathbf{X}}_{\\tilde{y} = i,y^\\star=j}|\\\\\n    \\hat{\\mathbf{X}}_{\\tilde{y} = i,y^\\star=j} &:= \\{x | x \\in X_{\\tilde{y} = i}, \\hat{p}(\\tilde{y}=j|x,\\theta) \\ge t_j, j=\\mathop{\\arg\\max}\\limits_{l \\in [m]} \\hat{p}(\\tilde{y} = l|x, \\theta)\\}\\\\\n    t_j &= \\frac{1}{|\\mathbf{X}_{\\tilde{y} = j}|}\\sum\\limits_{x \\in \\mathbf{X}_{\\tilde{y} = j}}\\hat{p}(\\tilde{y} = j| x, \\theta)\n\\end{aligned}\n$$\n\n得到$C_{\\tilde{y}, y^\\star}$之后，即可用$C_{\\tilde{y}, y^\\star}$来估计$\\tilde{y}$和$y^\\star$的联合分布矩阵$Q_{\\tilde{y}, y^\\star}$，计算方法如下：\n$$\n\\begin{aligned}\n    \\hat{Q}_{\\tilde{y}=i, y^\\star=j} = \\frac{\\frac{C_{\\tilde{y}=i, y^\\star=j}}{\\sum\\limits_{j\\in{1, 2, ..., m}}C_{\\tilde{y}=i, y^\\star=j}} |\\mathbf{X}_{\\tilde{y}=i}|}{\\sum\\limits_{i\\in{1, 2, ..., m}, j\\in{1, 2, ..., m}}(\\frac{C_{\\tilde{y}=i, y^\\star=j}}{\\sum\\limits_{j\\in{1, 2, ..., m}}C_{\\tilde{y}=i, y^\\star=j}} |\\mathbf{X}_{\\tilde{y}=i}|)}\n\\end{aligned}\n$$\n\n$\\hat{Q}_{\\tilde{y}, y^\\star}$是一个概率矩阵，得到Q之后，就可以知道标签为$i$的样本中，有$\\hat{Q}_{\\tilde{y}=i, y^\\star=j} \\times n$个样本是从$j$错误标注过来的，因此后面可以按照预测结果中的自信程度，在$i$类别的样本中，去掉$n\\times\\sum\\limits_{j}\\hat{Q}_{\\tilde{y}=i, y^\\star=j}$个最不自信的样本就行了，这种方法在论文中称为Prune by Class（PBC）。论文中也提出了一些其他处理方法，比如Prune by Noise Rate（PBNR），在置信度排序时，不是按照预测概率排序，而是按照间隔：$\\hat{p}_{x,\\tilde{y}=j} - \\hat{p}_{x,\\tilde{y}=i}$来排序，去掉间隔最大的那些样本。\n\n该论文所提出的方法在CIFAR-10分类问题上表现较好，在Accuracy上远超INCV、Mixup等方法，且在python下的CleanLab库中有官方实现，可直接调用。\n","slug":"学习笔记/标签噪声的处理","published":1,"updated":"2021-01-15T02:58:24.402Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckjxpc3s1001d44mq6twsc1ix","content":"<h1 id=\"论文understanding-deep-learning-requires-rethinking-generalization\">论文《UNDERSTANDING DEEP LEARNING REQUIRES RETHINKING GENERALIZATION》</h1>\r\n<p>该论文中首先做了两组实验：随机标签实验和随机噪声实验，数据集使用CIFAR10和ImageNet，在随机标签实验中，将所有标签替换为随机生成的，然后来训练CNN，发现即使是随机标签，CNN也可以完美拟合训练数据（训练误差为0），但是测试集误差基本上和瞎猜差不多，该实验表明模型可以强行记住所有数据，另外一个实验是随机噪声实验，在数据上添加随机噪声（例如高斯噪声），实验结果表明，随着噪声水平的增加，模型仍然可以拟合好训练集，但是泛化误差稳定上升。</p>\r\n<p>根据上面的实验，论文中主要突出了一个问题：模型明明可以暴力记住所有数据，为什么会得到泛化能力？论文中以一个线性模型为例，认为SGD也包含一定的正则化能力，从而导致模型可以有一定的泛化性能。</p>\r\n<h1 id=\"论文training-deep-neural-networks-using-a-noiseadaptation-layer\">论文《TRAINING DEEP NEURAL-NETWORKS USING A NOISEADAPTATION LAYER》</h1>\r\n<p>论文中首先定义了噪声标签的一种概率框架，例如一个多类别的神经网络软分类器可以表达为<span class=\"math inline\">\\(p(y=i|x;w)\\)</span>，其中<span class=\"math inline\">\\(x\\)</span>表示特征向量，<span class=\"math inline\">\\(w\\)</span>表示模型权重，<span class=\"math inline\">\\(y\\)</span>表示没有包含噪声的真实标签，而当前获取到的标签<span class=\"math inline\">\\(z\\)</span>包含噪声，假设噪声标签<span class=\"math inline\">\\(z\\)</span>的分布和特征<span class=\"math inline\">\\(x\\)</span>独立，因此可以简单的通过一个参数矩阵来描述噪声标签和真实标签的关系：<span class=\"math inline\">\\(\\theta (i,j)=p(z=j|y=i)\\)</span>，如下图所示。</p>\r\n<figure>\r\n<img src=\"噪声标签和真实标签的关系.png\" alt=\"噪声标签和真实标签的模型示意图\" /><figcaption aria-hidden=\"true\">噪声标签和真实标签的模型示意图</figcaption>\r\n</figure>\r\n<p>由于在当前模型中存在隐变量，因此自然使用EM算法来进行优化，在E-step，使用目前的参数估计第<span class=\"math inline\">\\(t\\)</span>个样本真实标签为<span class=\"math inline\">\\(i\\)</span>的概率<span class=\"math inline\">\\(c_{ti} = p(y_t = i | x_t, z_t; w_0, \\theta_0), i=1,2,...,k, t=1,2,...,n\\)</span>，其中<span class=\"math inline\">\\(k\\)</span>是类别个数，<span class=\"math inline\">\\(n\\)</span>是样本个数。而在M-step，则需要同时更新参数<span class=\"math inline\">\\(\\theta\\)</span>和参数<span class=\"math inline\">\\(w\\)</span>，参数<span class=\"math inline\">\\(\\theta\\)</span>有闭式解：<span class=\"math inline\">\\(\\theta(i,j) = \\frac{\\sum_t c_{ti} \\mathbb{1}(z_t=j)}{\\sum_t c_{ti}}, i,j \\in \\{1, 2, ..., k\\}\\)</span>，而参数<span class=\"math inline\">\\(w\\)</span>则需要使用梯度下降的方式来更新，其目标定义为最大化<span class=\"math inline\">\\(S(w) = \\sum\\limits_{t=1}^n\\sum\\limits_{i=1}^k c_{ti} log(p(y_t=i|x_i;w))\\)</span>，这个就是一个软化版本的似然函数。</p>\r\n<p>但是，在上面的EM算法过程中，存在一些问题，第一个是EM算法很可能收敛到局部最优点，第二个是在上面的EM框架中，每次在M-step更新的时候，都需要训练<span class=\"math inline\">\\(w\\)</span>直至收敛，这个对于稍微大型的模型或者数据集来说都是非常费时间的，第三是关于上述讨论的基本假设：\"噪声标签<span class=\"math inline\">\\(z\\)</span>的分布和特征<span class=\"math inline\">\\(x\\)</span>独立\"，这个假设太强，而且一般是不成立的，因此论文针对这些问题进一步提出了一些解决方案。</p>\r\n<p>将上面的模型顺序做个调换，得到如下图所示的模型示意图，其中上面的示意图是面模型训练的结构，下面的示意图是模型测试时的结构，在训练结构中，这里的<span class=\"math inline\">\\(x\\)</span>表示样本，<span class=\"math inline\">\\(h\\)</span>表示神经网络提取到的特征，<span class=\"math inline\">\\(y\\)</span>和<span class=\"math inline\">\\(z\\)</span>的定义和前面一样，这里有两个softmax层(这里的softmax是指以softmax为激活函数的全连接层)，这里的意思是在原始模型的基础上，增加一个softmax来对噪声进行建模，然后使用带有标签的噪声<span class=\"math inline\">\\(z\\)</span>进行训练。预测时输出中间结果<span class=\"math inline\">\\(y\\)</span>作为对隐藏的正确标签的预测。</p>\r\n<figure>\r\n<img src=\"sModel.png\" alt=\"论文提出使用一个新加的层对噪声进行建模\" /><figcaption aria-hidden=\"true\">论文提出使用一个新加的层对噪声进行建模</figcaption>\r\n</figure>\r\n<p>这个模型非常简单，但是如果没有一些特殊方法的话，肯定不能保证最终<span class=\"math inline\">\\(y\\)</span>会收敛成为真实标签的预测，因此还需要仔细的设计该模型的初始化方法。</p>\r\n<p>首先softmax层可以表达如下，这里的<span class=\"math inline\">\\(u^T\\)</span>和<span class=\"math inline\">\\(b\\)</span>就是softmax层的参数。</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    p(z=j|y=i,x) = \\frac{exp(u^T_{ij}h + b_{ij})}{\\sum_l exp(u^T_{il}h + b_{il})}\r\n\\end{aligned}\r\n\\]</span> 论文中提出，首先使用标签<span class=\"math inline\">\\(z\\)</span>去训练图中的non-linear function和第一个softmax层，就当成正常的训练，得到<span class=\"math inline\">\\(p(y|x;w)\\)</span>，训练好后这两个模块的权重不变，而且将当前模型预测的<span class=\"math inline\">\\(y\\)</span>就当做真实标签，去计算和噪声标签<span class=\"math inline\">\\(z\\)</span>的混淆矩阵，使用这个混淆矩阵去初始化<span class=\"math inline\">\\(b\\)</span>，另外<span class=\"math inline\">\\(u\\)</span>初始化为0，<span class=\"math inline\">\\(b_{ij}\\)</span>的初始化方法如下，注意这里的<span class=\"math inline\">\\(z\\)</span>是表示含噪声的标签。</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    b_{ij} = log(\\frac{\\sum_t\\mathbf{1}_{\\{z_t=j\\}}p(y_t=i|x_t)}{\\sum_tp(y_t=i|x_t)})\r\n\\end{aligned}\r\n\\]</span></p>\r\n<h1 id=\"论文mentornet-learning-data-driven-curriculumfor-very-deep-neural-networks-on-corrupted-labels\">论文《MentorNet: Learning Data-Driven Curriculumfor Very Deep Neural Networks on Corrupted Labels》</h1>\r\n<p>该论文提出了一种通过学习数据驱动的课程来在包含噪声的数据上达到更好效果的方法，论文中提出的MentorNet要不就是去学习一个预先定义好的课程，要不就在一个干净的数据上先进行训练（随机构造noisy label，然后让MentorNet来区分），如果直接去学习一个预定义好的课程，那么MentorNet其实没啥作用，和直接使用定义好的课程效果差不多，但是如果有一个干净的数据集可以让MentorNet去训练，那么效果会好很多。</p>\r\n<h1 id=\"论文joint-optimization-framework-for-learning-with-noisy-labels\">论文《Joint Optimization Framework for Learning with Noisy Labels》</h1>\r\n<p>该论文关注的是分类任务上的噪声标签数据，并使用迭代的标签修正方法来构造伪标签，最后使用伪标签来重头训练分类器，以达到更好的分类效果，整体方法框架如下图所示，其中<span class=\"math inline\">\\(x_i\\)</span>表示的是训练样本，<span class=\"math inline\">\\(y^{(t)}_i\\)</span>表示的是第<span class=\"math inline\">\\(t\\)</span>次迭代所使用的标签，先使用原始的含噪声标签<span class=\"math inline\">\\(y^{(0)}_i\\)</span>来计算损失函数<span class=\"math inline\">\\(L\\)</span>以训练模型，然后使用模型的预测概率<span class=\"math inline\">\\(s\\)</span>来对标签进行更新，有两种方式，一种是one-hot的更新标签（即对模型预测概率求argmax，这种方法在论文中每次不会更新所有标签，而是只更新预测结果和当前标签差距最大的top500个样本的标签），另外一种是使用概率来构成软标签（即直接使用模型预测概率作为新的标签，这种方法每次都更新所有标签。实验表明，软标签更新的效果更好），更新后的标签又继续训练模型，经过这样的迭代过程之后，可以得到经过修复的标签，这个时候就可以使用得到的标签来重头训练一个分类器以得到更好的效果。</p>\r\n<figure>\r\n<img src=\"标签修正框架.png\" alt=\"迭代的标签修正\" /><figcaption aria-hidden=\"true\">迭代的标签修正</figcaption>\r\n</figure>\r\n<p>上面的框架非常简单，其实中心思想就是一个EM算法的过程，最终得到伪标签，然后用伪标签来训练新的模型，该论文重要的部分在于其其学习速率的选择，以及损失函数的设计。</p>\r\n<p>首先该论文对文章《A closer look at memorization in deep networks》中的提到的一种现象进行了实验验证：<strong>在大学习速率的情况下，可以有效的抑制模型对无规则数据的记忆（即可以有效的抵抗标签噪声的影响）</strong>。因此在学习速率的选择上，论文在训练上述的标签迭代修正过程时。使用了较高的学习速率（例如论文中对于CIFAR-10数据，使用batch size 128, SGD优化器, lr 0.1，其他数据集的学习速率则不同，可能需要调参）</p>\r\n<p>第二比较重要的是在迭代标签修正过程中的损失函数<span class=\"math inline\">\\(L\\)</span>的设计，其总体的损失函数定义如下，其中<span class=\"math inline\">\\(L_c\\)</span>可以是任何分类损失函数，例如交叉熵，而<span class=\"math inline\">\\(L_p\\)</span>和<span class=\"math inline\">\\(L_e\\)</span>分别是两个不同的约束项，<span class=\"math inline\">\\(L_p\\)</span>中的<span class=\"math inline\">\\(p_j\\)</span>表示类<span class=\"math inline\">\\(j\\)</span>的先验分布，这一项希望模型预测的类别分布（<span class=\"math inline\">\\(\\bar{s}_j(\\theta,X)\\)</span>从一个Batch的数据统计得到，所以这里的batch size不宜太小）趋近于训练数据的类别先验分布，这是为了防止在标签迭代更新的过程中，<span class=\"math inline\">\\(y\\)</span>和<span class=\"math inline\">\\(s\\)</span>全都变成同一个类别，另外一项<span class=\"math inline\">\\(L_e\\)</span>计算的是模型预测的熵，最小化模型预测的熵，可以使模型的预测更加自信，避免模型的预测变成每个类均匀分布，这里比较难的就是超参数<span class=\"math inline\">\\(\\alpha\\)</span>和<span class=\"math inline\">\\(\\beta\\)</span>的设置，从论文中来看，作者在不同的数据集上使用的超参数不太相同，可能需要经过大量实验调参。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    L(\\theta,Y|X) &amp;= L_c(\\theta,Y|X) + \\alpha L_p(\\theta|X) + \\beta L_e(\\theta|X)\\\\\r\n    L_p(\\theta|X) &amp;= \\sum\\limits_{j=1}^c p_j \\log\\frac{p_j}{\\bar{s}_j(\\theta,X)}\\\\\r\n    \\bar{s}_j(\\theta,X) &amp;= \\frac{1}{n}\\sum\\limits_{i=1}^n s(\\theta, x_i) \\approx \\frac{1}{|\\mathcal{B}|}\\sum\\limits_{x \\in \\mathcal{B}} s(\\theta, x)\\\\\r\n    L_e(\\theta|X) &amp;= -\\frac{1}{n}\\sum\\limits_{i=1}^n \\sum\\limits_{j=1}^c s_j(\\theta,x_i) \\log s_j(\\theta,x_i)\\\\\r\n\\end{aligned}\r\n\\]</span></p>\r\n<h1 id=\"论文confident-learning-estimating-uncertainty-in-dataset-labels\">论文《Confident Learning: Estimating Uncertainty in Dataset Labels》</h1>\r\n<p>该论文主要关注在多分类任务上的噪声标签数据，使用置信学习的方法来估计噪声标签分布和未知的真实样本分布之间的联合分布，并据此修复数据中的噪声。</p>\r\n<p>首先，<span class=\"math inline\">\\(x_k \\in \\mathbf{X}, k=1,2,3,...,N\\)</span>代表所有数据，包含噪声的数据标签表示为<span class=\"math inline\">\\(\\tilde{y} \\in \\{1, 2, 3, ..., m\\}\\)</span>，未知的真实标签表示为<span class=\"math inline\">\\(y^\\star \\in \\{1, 2, 3, ..., m\\}\\)</span>，论文中假设噪声标签是由一个CNP(class-conditional classification noise process)产生的，即这个CNP构造了映射<span class=\"math inline\">\\(y^\\star \\rightarrow \\tilde{y}\\)</span>，这个假设算是个比较强的假设，因为其蕴含的意思就是<span class=\"math inline\">\\(p(\\tilde{y} = i|y^\\star=j) = p(\\tilde{y} = i|y^\\star=j,x)\\)</span>，即标签是否错误和样本无关，但是事实上很多时候标签错误都是和样本相关的。</p>\r\n<p>论文中的目标就是直接估计联合分布<span class=\"math inline\">\\(p(\\tilde{y}, y^\\star)\\)</span>，为此，论文首先去估计一个矩阵<span class=\"math inline\">\\(C_{\\tilde{y}, y^\\star}\\)</span>，其中<span class=\"math inline\">\\(C_{\\tilde{y}, y^\\star}[i][j]\\)</span>表示真实标签是<span class=\"math inline\">\\(j\\)</span>的样本被误标注为<span class=\"math inline\">\\(i\\)</span>的数量，其估计方法如下，其中绝对值符号是计算数量的意思。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    C_{\\tilde{y}, y^\\star} &amp;:= |\\hat{\\mathbf{X}}_{\\tilde{y} = i,y^\\star=j}|\\\\\r\n    \\hat{\\mathbf{X}}_{\\tilde{y} = i,y^\\star=j} &amp;:= \\{x | x \\in X_{\\tilde{y} = i}, \\hat{p}(\\tilde{y}=j|x,\\theta) \\ge t_j, j=\\mathop{\\arg\\max}\\limits_{l \\in [m]} \\hat{p}(\\tilde{y} = l|x, \\theta)\\}\\\\\r\n    t_j &amp;= \\frac{1}{|\\mathbf{X}_{\\tilde{y} = j}|}\\sum\\limits_{x \\in \\mathbf{X}_{\\tilde{y} = j}}\\hat{p}(\\tilde{y} = j| x, \\theta)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>得到<span class=\"math inline\">\\(C_{\\tilde{y}, y^\\star}\\)</span>之后，即可用<span class=\"math inline\">\\(C_{\\tilde{y}, y^\\star}\\)</span>来估计<span class=\"math inline\">\\(\\tilde{y}\\)</span>和<span class=\"math inline\">\\(y^\\star\\)</span>的联合分布矩阵<span class=\"math inline\">\\(Q_{\\tilde{y}, y^\\star}\\)</span>，计算方法如下： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\hat{Q}_{\\tilde{y}=i, y^\\star=j} = \\frac{\\frac{C_{\\tilde{y}=i, y^\\star=j}}{\\sum\\limits_{j\\in{1, 2, ..., m}}C_{\\tilde{y}=i, y^\\star=j}} |\\mathbf{X}_{\\tilde{y}=i}|}{\\sum\\limits_{i\\in{1, 2, ..., m}, j\\in{1, 2, ..., m}}(\\frac{C_{\\tilde{y}=i, y^\\star=j}}{\\sum\\limits_{j\\in{1, 2, ..., m}}C_{\\tilde{y}=i, y^\\star=j}} |\\mathbf{X}_{\\tilde{y}=i}|)}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p><span class=\"math inline\">\\(\\hat{Q}_{\\tilde{y}, y^\\star}\\)</span>是一个概率矩阵，得到Q之后，就可以知道标签为<span class=\"math inline\">\\(i\\)</span>的样本中，有<span class=\"math inline\">\\(\\hat{Q}_{\\tilde{y}=i, y^\\star=j} \\times n\\)</span>个样本是从<span class=\"math inline\">\\(j\\)</span>错误标注过来的，因此后面可以按照预测结果中的自信程度，在<span class=\"math inline\">\\(i\\)</span>类别的样本中，去掉<span class=\"math inline\">\\(n\\times\\sum\\limits_{j}\\hat{Q}_{\\tilde{y}=i, y^\\star=j}\\)</span>个最不自信的样本就行了，这种方法在论文中称为Prune by Class（PBC）。论文中也提出了一些其他处理方法，比如Prune by Noise Rate（PBNR），在置信度排序时，不是按照预测概率排序，而是按照间隔：<span class=\"math inline\">\\(\\hat{p}_{x,\\tilde{y}=j} - \\hat{p}_{x,\\tilde{y}=i}\\)</span>来排序，去掉间隔最大的那些样本。</p>\r\n<p>该论文所提出的方法在CIFAR-10分类问题上表现较好，在Accuracy上远超INCV、Mixup等方法，且在python下的CleanLab库中有官方实现，可直接调用。</p>\r\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"论文understanding-deep-learning-requires-rethinking-generalization\">论文《UNDERSTANDING DEEP LEARNING REQUIRES RETHINKING GENERALIZATION》</h1>\r\n<p>该论文中首先做了两组实验：随机标签实验和随机噪声实验，数据集使用CIFAR10和ImageNet，在随机标签实验中，将所有标签替换为随机生成的，然后来训练CNN，发现即使是随机标签，CNN也可以完美拟合训练数据（训练误差为0），但是测试集误差基本上和瞎猜差不多，该实验表明模型可以强行记住所有数据，另外一个实验是随机噪声实验，在数据上添加随机噪声（例如高斯噪声），实验结果表明，随着噪声水平的增加，模型仍然可以拟合好训练集，但是泛化误差稳定上升。</p>\r\n<p>根据上面的实验，论文中主要突出了一个问题：模型明明可以暴力记住所有数据，为什么会得到泛化能力？论文中以一个线性模型为例，认为SGD也包含一定的正则化能力，从而导致模型可以有一定的泛化性能。</p>\r\n<h1 id=\"论文training-deep-neural-networks-using-a-noiseadaptation-layer\">论文《TRAINING DEEP NEURAL-NETWORKS USING A NOISEADAPTATION LAYER》</h1>\r\n<p>论文中首先定义了噪声标签的一种概率框架，例如一个多类别的神经网络软分类器可以表达为<span class=\"math inline\">\\(p(y=i|x;w)\\)</span>，其中<span class=\"math inline\">\\(x\\)</span>表示特征向量，<span class=\"math inline\">\\(w\\)</span>表示模型权重，<span class=\"math inline\">\\(y\\)</span>表示没有包含噪声的真实标签，而当前获取到的标签<span class=\"math inline\">\\(z\\)</span>包含噪声，假设噪声标签<span class=\"math inline\">\\(z\\)</span>的分布和特征<span class=\"math inline\">\\(x\\)</span>独立，因此可以简单的通过一个参数矩阵来描述噪声标签和真实标签的关系：<span class=\"math inline\">\\(\\theta (i,j)=p(z=j|y=i)\\)</span>，如下图所示。</p>\r\n<figure>\r\n<img src=\"噪声标签和真实标签的关系.png\" alt=\"噪声标签和真实标签的模型示意图\" /><figcaption aria-hidden=\"true\">噪声标签和真实标签的模型示意图</figcaption>\r\n</figure>\r\n<p>由于在当前模型中存在隐变量，因此自然使用EM算法来进行优化，在E-step，使用目前的参数估计第<span class=\"math inline\">\\(t\\)</span>个样本真实标签为<span class=\"math inline\">\\(i\\)</span>的概率<span class=\"math inline\">\\(c_{ti} = p(y_t = i | x_t, z_t; w_0, \\theta_0), i=1,2,...,k, t=1,2,...,n\\)</span>，其中<span class=\"math inline\">\\(k\\)</span>是类别个数，<span class=\"math inline\">\\(n\\)</span>是样本个数。而在M-step，则需要同时更新参数<span class=\"math inline\">\\(\\theta\\)</span>和参数<span class=\"math inline\">\\(w\\)</span>，参数<span class=\"math inline\">\\(\\theta\\)</span>有闭式解：<span class=\"math inline\">\\(\\theta(i,j) = \\frac{\\sum_t c_{ti} \\mathbb{1}(z_t=j)}{\\sum_t c_{ti}}, i,j \\in \\{1, 2, ..., k\\}\\)</span>，而参数<span class=\"math inline\">\\(w\\)</span>则需要使用梯度下降的方式来更新，其目标定义为最大化<span class=\"math inline\">\\(S(w) = \\sum\\limits_{t=1}^n\\sum\\limits_{i=1}^k c_{ti} log(p(y_t=i|x_i;w))\\)</span>，这个就是一个软化版本的似然函数。</p>\r\n<p>但是，在上面的EM算法过程中，存在一些问题，第一个是EM算法很可能收敛到局部最优点，第二个是在上面的EM框架中，每次在M-step更新的时候，都需要训练<span class=\"math inline\">\\(w\\)</span>直至收敛，这个对于稍微大型的模型或者数据集来说都是非常费时间的，第三是关于上述讨论的基本假设：\"噪声标签<span class=\"math inline\">\\(z\\)</span>的分布和特征<span class=\"math inline\">\\(x\\)</span>独立\"，这个假设太强，而且一般是不成立的，因此论文针对这些问题进一步提出了一些解决方案。</p>\r\n<p>将上面的模型顺序做个调换，得到如下图所示的模型示意图，其中上面的示意图是面模型训练的结构，下面的示意图是模型测试时的结构，在训练结构中，这里的<span class=\"math inline\">\\(x\\)</span>表示样本，<span class=\"math inline\">\\(h\\)</span>表示神经网络提取到的特征，<span class=\"math inline\">\\(y\\)</span>和<span class=\"math inline\">\\(z\\)</span>的定义和前面一样，这里有两个softmax层(这里的softmax是指以softmax为激活函数的全连接层)，这里的意思是在原始模型的基础上，增加一个softmax来对噪声进行建模，然后使用带有标签的噪声<span class=\"math inline\">\\(z\\)</span>进行训练。预测时输出中间结果<span class=\"math inline\">\\(y\\)</span>作为对隐藏的正确标签的预测。</p>\r\n<figure>\r\n<img src=\"sModel.png\" alt=\"论文提出使用一个新加的层对噪声进行建模\" /><figcaption aria-hidden=\"true\">论文提出使用一个新加的层对噪声进行建模</figcaption>\r\n</figure>\r\n<p>这个模型非常简单，但是如果没有一些特殊方法的话，肯定不能保证最终<span class=\"math inline\">\\(y\\)</span>会收敛成为真实标签的预测，因此还需要仔细的设计该模型的初始化方法。</p>\r\n<p>首先softmax层可以表达如下，这里的<span class=\"math inline\">\\(u^T\\)</span>和<span class=\"math inline\">\\(b\\)</span>就是softmax层的参数。</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    p(z=j|y=i,x) = \\frac{exp(u^T_{ij}h + b_{ij})}{\\sum_l exp(u^T_{il}h + b_{il})}\r\n\\end{aligned}\r\n\\]</span> 论文中提出，首先使用标签<span class=\"math inline\">\\(z\\)</span>去训练图中的non-linear function和第一个softmax层，就当成正常的训练，得到<span class=\"math inline\">\\(p(y|x;w)\\)</span>，训练好后这两个模块的权重不变，而且将当前模型预测的<span class=\"math inline\">\\(y\\)</span>就当做真实标签，去计算和噪声标签<span class=\"math inline\">\\(z\\)</span>的混淆矩阵，使用这个混淆矩阵去初始化<span class=\"math inline\">\\(b\\)</span>，另外<span class=\"math inline\">\\(u\\)</span>初始化为0，<span class=\"math inline\">\\(b_{ij}\\)</span>的初始化方法如下，注意这里的<span class=\"math inline\">\\(z\\)</span>是表示含噪声的标签。</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    b_{ij} = log(\\frac{\\sum_t\\mathbf{1}_{\\{z_t=j\\}}p(y_t=i|x_t)}{\\sum_tp(y_t=i|x_t)})\r\n\\end{aligned}\r\n\\]</span></p>\r\n<h1 id=\"论文mentornet-learning-data-driven-curriculumfor-very-deep-neural-networks-on-corrupted-labels\">论文《MentorNet: Learning Data-Driven Curriculumfor Very Deep Neural Networks on Corrupted Labels》</h1>\r\n<p>该论文提出了一种通过学习数据驱动的课程来在包含噪声的数据上达到更好效果的方法，论文中提出的MentorNet要不就是去学习一个预先定义好的课程，要不就在一个干净的数据上先进行训练（随机构造noisy label，然后让MentorNet来区分），如果直接去学习一个预定义好的课程，那么MentorNet其实没啥作用，和直接使用定义好的课程效果差不多，但是如果有一个干净的数据集可以让MentorNet去训练，那么效果会好很多。</p>\r\n<h1 id=\"论文joint-optimization-framework-for-learning-with-noisy-labels\">论文《Joint Optimization Framework for Learning with Noisy Labels》</h1>\r\n<p>该论文关注的是分类任务上的噪声标签数据，并使用迭代的标签修正方法来构造伪标签，最后使用伪标签来重头训练分类器，以达到更好的分类效果，整体方法框架如下图所示，其中<span class=\"math inline\">\\(x_i\\)</span>表示的是训练样本，<span class=\"math inline\">\\(y^{(t)}_i\\)</span>表示的是第<span class=\"math inline\">\\(t\\)</span>次迭代所使用的标签，先使用原始的含噪声标签<span class=\"math inline\">\\(y^{(0)}_i\\)</span>来计算损失函数<span class=\"math inline\">\\(L\\)</span>以训练模型，然后使用模型的预测概率<span class=\"math inline\">\\(s\\)</span>来对标签进行更新，有两种方式，一种是one-hot的更新标签（即对模型预测概率求argmax，这种方法在论文中每次不会更新所有标签，而是只更新预测结果和当前标签差距最大的top500个样本的标签），另外一种是使用概率来构成软标签（即直接使用模型预测概率作为新的标签，这种方法每次都更新所有标签。实验表明，软标签更新的效果更好），更新后的标签又继续训练模型，经过这样的迭代过程之后，可以得到经过修复的标签，这个时候就可以使用得到的标签来重头训练一个分类器以得到更好的效果。</p>\r\n<figure>\r\n<img src=\"标签修正框架.png\" alt=\"迭代的标签修正\" /><figcaption aria-hidden=\"true\">迭代的标签修正</figcaption>\r\n</figure>\r\n<p>上面的框架非常简单，其实中心思想就是一个EM算法的过程，最终得到伪标签，然后用伪标签来训练新的模型，该论文重要的部分在于其其学习速率的选择，以及损失函数的设计。</p>\r\n<p>首先该论文对文章《A closer look at memorization in deep networks》中的提到的一种现象进行了实验验证：<strong>在大学习速率的情况下，可以有效的抑制模型对无规则数据的记忆（即可以有效的抵抗标签噪声的影响）</strong>。因此在学习速率的选择上，论文在训练上述的标签迭代修正过程时。使用了较高的学习速率（例如论文中对于CIFAR-10数据，使用batch size 128, SGD优化器, lr 0.1，其他数据集的学习速率则不同，可能需要调参）</p>\r\n<p>第二比较重要的是在迭代标签修正过程中的损失函数<span class=\"math inline\">\\(L\\)</span>的设计，其总体的损失函数定义如下，其中<span class=\"math inline\">\\(L_c\\)</span>可以是任何分类损失函数，例如交叉熵，而<span class=\"math inline\">\\(L_p\\)</span>和<span class=\"math inline\">\\(L_e\\)</span>分别是两个不同的约束项，<span class=\"math inline\">\\(L_p\\)</span>中的<span class=\"math inline\">\\(p_j\\)</span>表示类<span class=\"math inline\">\\(j\\)</span>的先验分布，这一项希望模型预测的类别分布（<span class=\"math inline\">\\(\\bar{s}_j(\\theta,X)\\)</span>从一个Batch的数据统计得到，所以这里的batch size不宜太小）趋近于训练数据的类别先验分布，这是为了防止在标签迭代更新的过程中，<span class=\"math inline\">\\(y\\)</span>和<span class=\"math inline\">\\(s\\)</span>全都变成同一个类别，另外一项<span class=\"math inline\">\\(L_e\\)</span>计算的是模型预测的熵，最小化模型预测的熵，可以使模型的预测更加自信，避免模型的预测变成每个类均匀分布，这里比较难的就是超参数<span class=\"math inline\">\\(\\alpha\\)</span>和<span class=\"math inline\">\\(\\beta\\)</span>的设置，从论文中来看，作者在不同的数据集上使用的超参数不太相同，可能需要经过大量实验调参。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    L(\\theta,Y|X) &amp;= L_c(\\theta,Y|X) + \\alpha L_p(\\theta|X) + \\beta L_e(\\theta|X)\\\\\r\n    L_p(\\theta|X) &amp;= \\sum\\limits_{j=1}^c p_j \\log\\frac{p_j}{\\bar{s}_j(\\theta,X)}\\\\\r\n    \\bar{s}_j(\\theta,X) &amp;= \\frac{1}{n}\\sum\\limits_{i=1}^n s(\\theta, x_i) \\approx \\frac{1}{|\\mathcal{B}|}\\sum\\limits_{x \\in \\mathcal{B}} s(\\theta, x)\\\\\r\n    L_e(\\theta|X) &amp;= -\\frac{1}{n}\\sum\\limits_{i=1}^n \\sum\\limits_{j=1}^c s_j(\\theta,x_i) \\log s_j(\\theta,x_i)\\\\\r\n\\end{aligned}\r\n\\]</span></p>\r\n<h1 id=\"论文confident-learning-estimating-uncertainty-in-dataset-labels\">论文《Confident Learning: Estimating Uncertainty in Dataset Labels》</h1>\r\n<p>该论文主要关注在多分类任务上的噪声标签数据，使用置信学习的方法来估计噪声标签分布和未知的真实样本分布之间的联合分布，并据此修复数据中的噪声。</p>\r\n<p>首先，<span class=\"math inline\">\\(x_k \\in \\mathbf{X}, k=1,2,3,...,N\\)</span>代表所有数据，包含噪声的数据标签表示为<span class=\"math inline\">\\(\\tilde{y} \\in \\{1, 2, 3, ..., m\\}\\)</span>，未知的真实标签表示为<span class=\"math inline\">\\(y^\\star \\in \\{1, 2, 3, ..., m\\}\\)</span>，论文中假设噪声标签是由一个CNP(class-conditional classification noise process)产生的，即这个CNP构造了映射<span class=\"math inline\">\\(y^\\star \\rightarrow \\tilde{y}\\)</span>，这个假设算是个比较强的假设，因为其蕴含的意思就是<span class=\"math inline\">\\(p(\\tilde{y} = i|y^\\star=j) = p(\\tilde{y} = i|y^\\star=j,x)\\)</span>，即标签是否错误和样本无关，但是事实上很多时候标签错误都是和样本相关的。</p>\r\n<p>论文中的目标就是直接估计联合分布<span class=\"math inline\">\\(p(\\tilde{y}, y^\\star)\\)</span>，为此，论文首先去估计一个矩阵<span class=\"math inline\">\\(C_{\\tilde{y}, y^\\star}\\)</span>，其中<span class=\"math inline\">\\(C_{\\tilde{y}, y^\\star}[i][j]\\)</span>表示真实标签是<span class=\"math inline\">\\(j\\)</span>的样本被误标注为<span class=\"math inline\">\\(i\\)</span>的数量，其估计方法如下，其中绝对值符号是计算数量的意思。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    C_{\\tilde{y}, y^\\star} &amp;:= |\\hat{\\mathbf{X}}_{\\tilde{y} = i,y^\\star=j}|\\\\\r\n    \\hat{\\mathbf{X}}_{\\tilde{y} = i,y^\\star=j} &amp;:= \\{x | x \\in X_{\\tilde{y} = i}, \\hat{p}(\\tilde{y}=j|x,\\theta) \\ge t_j, j=\\mathop{\\arg\\max}\\limits_{l \\in [m]} \\hat{p}(\\tilde{y} = l|x, \\theta)\\}\\\\\r\n    t_j &amp;= \\frac{1}{|\\mathbf{X}_{\\tilde{y} = j}|}\\sum\\limits_{x \\in \\mathbf{X}_{\\tilde{y} = j}}\\hat{p}(\\tilde{y} = j| x, \\theta)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>得到<span class=\"math inline\">\\(C_{\\tilde{y}, y^\\star}\\)</span>之后，即可用<span class=\"math inline\">\\(C_{\\tilde{y}, y^\\star}\\)</span>来估计<span class=\"math inline\">\\(\\tilde{y}\\)</span>和<span class=\"math inline\">\\(y^\\star\\)</span>的联合分布矩阵<span class=\"math inline\">\\(Q_{\\tilde{y}, y^\\star}\\)</span>，计算方法如下： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\hat{Q}_{\\tilde{y}=i, y^\\star=j} = \\frac{\\frac{C_{\\tilde{y}=i, y^\\star=j}}{\\sum\\limits_{j\\in{1, 2, ..., m}}C_{\\tilde{y}=i, y^\\star=j}} |\\mathbf{X}_{\\tilde{y}=i}|}{\\sum\\limits_{i\\in{1, 2, ..., m}, j\\in{1, 2, ..., m}}(\\frac{C_{\\tilde{y}=i, y^\\star=j}}{\\sum\\limits_{j\\in{1, 2, ..., m}}C_{\\tilde{y}=i, y^\\star=j}} |\\mathbf{X}_{\\tilde{y}=i}|)}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p><span class=\"math inline\">\\(\\hat{Q}_{\\tilde{y}, y^\\star}\\)</span>是一个概率矩阵，得到Q之后，就可以知道标签为<span class=\"math inline\">\\(i\\)</span>的样本中，有<span class=\"math inline\">\\(\\hat{Q}_{\\tilde{y}=i, y^\\star=j} \\times n\\)</span>个样本是从<span class=\"math inline\">\\(j\\)</span>错误标注过来的，因此后面可以按照预测结果中的自信程度，在<span class=\"math inline\">\\(i\\)</span>类别的样本中，去掉<span class=\"math inline\">\\(n\\times\\sum\\limits_{j}\\hat{Q}_{\\tilde{y}=i, y^\\star=j}\\)</span>个最不自信的样本就行了，这种方法在论文中称为Prune by Class（PBC）。论文中也提出了一些其他处理方法，比如Prune by Noise Rate（PBNR），在置信度排序时，不是按照预测概率排序，而是按照间隔：<span class=\"math inline\">\\(\\hat{p}_{x,\\tilde{y}=j} - \\hat{p}_{x,\\tilde{y}=i}\\)</span>来排序，去掉间隔最大的那些样本。</p>\r\n<p>该论文所提出的方法在CIFAR-10分类问题上表现较好，在Accuracy上远超INCV、Mixup等方法，且在python下的CleanLab库中有官方实现，可直接调用。</p>\r\n"},{"title":"正则化方法","date":"2020-05-12T02:40:41.000Z","mathjax":true,"_content":"\n# 偏差和方差\n在监督学习中，模型的泛化误差（测试集误差）被视作偏差、方差和噪声之和。\n\n其中，偏差表示学习算法的期望预测与真实结果的偏离程度，刻画的是算法的拟合能力。\n\n方差则表示一定大小的训练数据扰动所导致的学习性能的变化大小，刻画的是输入扰动所造成的影响\n\n噪声则表示在当前任务上任何学习算法所能达到的期望泛化误差的下界，刻画的是问题本身的难度。\n\n偏差主要是因为模型的复杂度不够而引起，可以从训练集误差上体现出来。\n\n方差主要是因为模型复杂度过高而引起，表现为测试集误差和训练集误差的差值。\n\n模型复杂度高，容易导致方差变大，模型复杂度低，又容易导致偏差变大，因此需要平衡模型的方差与偏差，使得模型的泛化误差最小。\n\n# 正则化方法\n正则化方法是向原始模型引入额外信息的一种方法的统称，其目的是通过为复杂模型添加正则化的方式，尽量在不影响偏差的同时降低方差。\n\n# 参数范数正则化\n通过在损失函数中加入参数范数项来降低模型容量，从而达到正则化的效果，形如下式，其中$J$表示原始的损失函数，$\\Omega(w)$表示参数的一种范数，$\\alpha$参数用于控制正则化程度，$\\tilde{J}(w;X,y)$表示添加了正则化之后的损失函数。\n$$\n\\tilde{J}(w;X,y) = J(w;X,y) + \\alpha\\Omega(w)\n$$\n\n## L2正则化（参数范数正则化的特例，范数使用2范数）\n普通的损失函数可以写成如下：\n$$\nJ(w;X,y)\n$$\n\n在权重参数上加上l2正则化项之后，损失函数如下，为方便分析，这里将$\\alpha$写成$\\frac{\\alpha}{2}$：\n\n$$\n\\begin{aligned}\n    &\\tilde{J} = \\frac{\\alpha}{2}w^Tw + J(w;X,y)\\\\\n    &对w的求导：\\\\\n    &\\nabla_w\\tilde{J} = \\alpha w + \\nabla_w J(w;X,y)\n\\end{aligned}\n$$\n\n每次的参数更新可以写成如下，其中$\\epsilon$表示学习速率：\n\n$$\n\\begin{aligned}\n    &w \\leftarrow w - \\epsilon(\\alpha w + \\nabla_w J(w;X,y))\\\\\n    &等价于：\\\\\n    &w \\leftarrow (1 - \\epsilon\\alpha) w + \\epsilon\\nabla_w J(w;X,y)\n\\end{aligned}\n$$\n从这里可以看到，学习规则发生了变化，在每次执行普通的梯度更新之前，会首先将原来的参数乘以一个常数项$(1 - \\epsilon\\alpha)$。\n\n对于原来的损失函数中存在的鞍点（梯度$\\nabla_w J(w;X,y) = 0$），则L2正则化将使得参数在这些点任然会得到更新，从而有助于模型逃离鞍点。（但是这个说法我认为不太对，因为加入$\\alpha w$这一项，只是将鞍点的位置进行了偏移，并不是去掉了鞍点。）\n\n为了分析l2正则化的作用，假定$J(w ^ \\ast ; X , y)$是损失函数的极小值。\n$w^\\ast$是使损失函数取得极小值的参数，先简单的对损失函数进行二次近似如下，这里没有一次项是因为在函数极小值的地方，一次导数应该为0。\n\n$$\n\\begin{aligned}\n    \\hat{J}(w;X,y) = J(w^\\ast;X,y) + \\frac{1}{2}(w-w^\\ast)^TH(w-w^\\ast)\n\\end{aligned}\n$$\n\n对加上l2正则化项的损失函数近似形式如下：\n\n$$\n\\begin{aligned}\n    \\hat{J}(w;X,y) = J(w^\\ast;X,y) + \\frac{1}{2}(w-w^\\ast)^TH(w-w^\\ast) + \\frac{\\alpha}{2} w^Tw\n\\end{aligned}\n$$\n\n对损失函数进行求导如下。\n\n$$\n\\begin{aligned}\n    \\nabla_w \\hat{J}(w;X,y) = \\alpha w + H(w - w^\\ast)\n\\end{aligned}\n$$\n\n当$\\nabla_w\\hat{J}(w;X,y) = 0$时，有如下推导：\n\n$$\n\\begin{aligned}\n    \\alpha w + H(w - w^\\ast) = 0\\\\\n    (H + \\alpha I)w = Hw^\\ast\\\\\n    w = (H + \\alpha I)^{-1}Hw^\\ast\n\\end{aligned}\n$$\n\n可见当$\\alpha \\rightarrow 0$时，$w \\rightarrow w^\\ast$，进一步，因为$H$是实对称矩阵，因此必定可以正交对角化，$H=Q \\Lambda Q^T$，因此进一步推导如下：\n\n$$\n\\begin{aligned}\n    w &= (Q \\Lambda Q^T+ \\alpha I)^{-1}Q \\Lambda Q^Tw^\\ast\\\\\n    &=[Q (\\Lambda+ \\alpha I) Q^T]^{-1}Q \\Lambda Q^Tw^\\ast\\\\\n    &=Q (\\Lambda+ \\alpha I)^{-1}\\Lambda Q^Tw^\\ast\n\\end{aligned}\n$$\n\n其中，假设$\\Lambda$如下：\n\n$$\n\\begin{bmatrix}\n    &\\lambda_1 &0 &0 &\\cdots &0\\\\\n    &0 &\\lambda_2 &0 &\\cdots &0\\\\\n    &0 &0 &\\lambda_3 &\\cdots &0\\\\\n    &\\vdots &\\vdots &\\vdots &\\ddots &\\vdots\\\\\n    &0 &0 &0 &\\cdots &\\lambda_n\\\\\n\\end{bmatrix}\n$$\n\n则$(\\Lambda+ \\alpha I)^{-1}\\Lambda$如下：\n$$\n\\begin{bmatrix}\n    &\\frac{\\lambda_1}{\\lambda_1 + \\alpha} &0 &0 &\\cdots &0\\\\\n    &0 &\\frac{\\lambda_2}{\\lambda_2 + \\alpha} &0 &\\cdots &0\\\\\n    &0 &0 &\\frac{\\lambda_3}{\\lambda_3 + \\alpha} &\\cdots &0\\\\\n    &\\vdots &\\vdots &\\vdots &\\ddots &\\vdots\\\\\n    &0 &0 &0 &\\cdots &\\frac{\\lambda_n}{\\lambda_n + \\alpha}\\\\\n\\end{bmatrix}\n$$\n这就相当于在原损失函数极小值点的Hession矩阵$H$的特征向量方向上，将$w^\\ast$进行了缩放，而且特征值$\\lambda_i$越小的方向，$\\alpha$对其影响越大，缩小得越大，即加入l2正则化项之后，模型参数倾向于对损失函数影响显著的方向（Hession矩阵特征值大的方向）偏移。\n\n以最小二乘线性回归为例，其损失函数如下：\n\n$$\n(Xw-y)^T(Xw-y)\n$$\n\n如果加上l2正则化，则损失函数变成如下形式：\n\n$$\n(Xw-y)^T(Xw-y) + \\frac{1}{2}\\alpha w^Tw\n$$\n\n那么线性回归的解就从：\n\n$$\nw = (X^TX)^{-1}X^Ty\n$$\n\n变成了：\n$$\nw = (X^TX + \\alpha I)^{-1}X^Ty\n$$\n\n其中$X$可以写成如下,其中$x_{ij}$表示第i个样本$x_i$的第j维：\n$$\nX=\\begin{bmatrix}\n    &x_{11} &x_{12} &\\cdots &x_{1m}\\\\\n    &x_{21} &x_{22} &\\cdots &x_{2m}\\\\\n    &\\vdots &\\vdots &\\ddots &\\vdots\\\\\n    &x_{n1} &x_{n1} &\\cdots &x_{nm}\\\\\n\\end{bmatrix}\n$$\n\n则$X^TX$可以表示如下：\n$$\nX^TX=\\begin{bmatrix}\n    &\\sum_i^nx_{i1}x_{i1} &\\sum_i^nx_{i1}x_{i2} &\\cdots &\\sum_i^nx_{i1}x_{im}\\\\\n    &\\sum_i^nx_{i2}x_{i1} &\\sum_i^nx_{i2}x_{i2} &\\cdots &\\sum_i^nx_{i2}x_{im}\\\\\n    &\\vdots &\\vdots &\\ddots &\\vdots\\\\\n    &\\sum_i^nx_{im}x_{i1} &\\sum_i^nx_{im}x_{i2} &\\cdots &\\sum_i^nx_{im}x_{im}\\\\\n\\end{bmatrix}\n$$\n\n$X^TX$同样可以正交对角化，$X^TX = Q \\Lambda Q^T$，这里的$\\Lambda$对角线上的值是$X$奇异值的平方，之后的推导和上面相同，可见，加入l2正则化项之后，模型参数倾向于对损失函数影响显著的方向偏移，从而忽略掉数据中的一些干扰，增强模型泛化能力。\n\n## L1正则化（参数范数正则化的特例，范数使用1范数）\n进行l1正则化之后的损失函数如下：\n\n$$\n\\tilde{J} = \\alpha ||w||_1 + J(w;X,y)\n$$\n\n对其进行求导：\n$$\n\\nabla_w\\tilde{J} = \\alpha\\ sign(w) + \\nabla_wJ(w;X,y)\n$$\n从这里可以看出l1正则化项对导数的影响是一个固定值，和L2有很大区别（L2是固定的缩放）。\n\n使用在L2正则化分析中的损失函数近似方法，将原本的损失函数二次近似为$\\hat{J}(w;X,y) = J(w^\\ast;X,y) + \\frac{1}{2}(w-w^\\ast)^TH(w-w^\\ast)$，其导数如下：\n$$\n\\nabla_w\\hat{J} = H(w - w^\\ast)\n$$\n\n加上正则化项之后，其损失函数的二次近似可以表示为：\n\n$$\n\\hat{J}(w;X,y) = J(w^\\ast;X,y) + \\frac{1}{2}(w-w^\\ast)^T H(w-w^\\ast) + \\alpha||w||_1\n$$\n\n如果将Hessian矩阵简化成了对角阵（这个类似对数据进行PCA之类的操作，将数据之间的相关性去掉了，因此Hessian矩阵变成对角阵，这样分析要简单一些）。\n\n则：\n$$\n\\hat{J}(w;X,y) = J(w^\\ast;X,y) + \\sum_i[\\frac{1}{2}H_{i,i}(w_i-w_i^\\ast)^2 + \\alpha|w_i|]\n$$\n\n则对$w_i$进行求导，如下：\n$$\n\\nabla_{w_i}\\hat{J}(w;X,y) = H_{i,i}(w_i - w_i^\\ast) + \\alpha\\ sign(w_i)\n$$\n\n可以看出，加了L1正则化之后梯度为0的点变成了如下：\n$$\nw_i = sign(w_i^\\ast)max\\{|w_i^\\ast| - \\frac{\\alpha}{H_{i,i}}, 0\\}\n$$\n\n描述成图像大概如下：\n\n{%asset_img l1正则化效果示意.png L1正则化效果示意%}\n\n可以看出，l1使得权重变得更加稀疏，这在特征选择方面非常有用。\n\n# 数据集扩充\n数据集扩充的本质是要求模型对于相似的输入（例如图片的平移、缩放、添加噪声、旋转等不改变图片语义的操作），必须有相似的输出。\n\n# 对噪声的鲁棒性\n除了在原始输入上添加噪声之外，还有两种在隐层添加噪声的方法：\n- dropout在隐层的输出上添加噪声。\n- 直接对学习到的权重添加噪声，可以视作模拟贝叶斯方法，认为模型权重有一定的随机性，同时也鼓励模型进入到参数空间中比较稳定的区域。\n\n# 输出目标噪声\n数据标签很可能包含一定的错误，而且在分类任务重，由于softmax\\sigmoid函数的特点，模型不能拟合绝对的0和1标签，因此标签平滑技术通过把标签进行平滑，可以缓解这样的问题。\n\n# 稀疏表示\n类似L1正则化产生的参数稀疏效果，在隐层的输出上，也可以尝试使用L1正则化，让隐层的输出倾向于变得稀疏。\n\n# 参数绑定和参数共享\n- 某些足够相似的任务，其参数也应该相似，因此可以将参数之间相减的范数添加到损失函数中进行训练。\n- CNN中的卷积层通过“提取类似特征所需权重相同”的思想，大量的权重共享，显著降低了模型的参数量，同时使得模型更加容易训练。\n\n# 训练早停\n验证误差不再下降之后，及时停止训练模型，不需要等到训练集误差最小，这样有助于防止模型过拟合。\n\n# 多任务学习\n一些任务需要相同或者相似的特征，让共享大部分参数的多个模型来同时进行多个任务的拟合，可以起到使共享部分的参数学习到更优值的效果。","source":"_posts/学习笔记/正则化方法.md","raw":"---\ntitle: 正则化方法\ndate: 2020-05-12 10:40:41\ntags: [深度学习]\nmathjax: true\n---\n\n# 偏差和方差\n在监督学习中，模型的泛化误差（测试集误差）被视作偏差、方差和噪声之和。\n\n其中，偏差表示学习算法的期望预测与真实结果的偏离程度，刻画的是算法的拟合能力。\n\n方差则表示一定大小的训练数据扰动所导致的学习性能的变化大小，刻画的是输入扰动所造成的影响\n\n噪声则表示在当前任务上任何学习算法所能达到的期望泛化误差的下界，刻画的是问题本身的难度。\n\n偏差主要是因为模型的复杂度不够而引起，可以从训练集误差上体现出来。\n\n方差主要是因为模型复杂度过高而引起，表现为测试集误差和训练集误差的差值。\n\n模型复杂度高，容易导致方差变大，模型复杂度低，又容易导致偏差变大，因此需要平衡模型的方差与偏差，使得模型的泛化误差最小。\n\n# 正则化方法\n正则化方法是向原始模型引入额外信息的一种方法的统称，其目的是通过为复杂模型添加正则化的方式，尽量在不影响偏差的同时降低方差。\n\n# 参数范数正则化\n通过在损失函数中加入参数范数项来降低模型容量，从而达到正则化的效果，形如下式，其中$J$表示原始的损失函数，$\\Omega(w)$表示参数的一种范数，$\\alpha$参数用于控制正则化程度，$\\tilde{J}(w;X,y)$表示添加了正则化之后的损失函数。\n$$\n\\tilde{J}(w;X,y) = J(w;X,y) + \\alpha\\Omega(w)\n$$\n\n## L2正则化（参数范数正则化的特例，范数使用2范数）\n普通的损失函数可以写成如下：\n$$\nJ(w;X,y)\n$$\n\n在权重参数上加上l2正则化项之后，损失函数如下，为方便分析，这里将$\\alpha$写成$\\frac{\\alpha}{2}$：\n\n$$\n\\begin{aligned}\n    &\\tilde{J} = \\frac{\\alpha}{2}w^Tw + J(w;X,y)\\\\\n    &对w的求导：\\\\\n    &\\nabla_w\\tilde{J} = \\alpha w + \\nabla_w J(w;X,y)\n\\end{aligned}\n$$\n\n每次的参数更新可以写成如下，其中$\\epsilon$表示学习速率：\n\n$$\n\\begin{aligned}\n    &w \\leftarrow w - \\epsilon(\\alpha w + \\nabla_w J(w;X,y))\\\\\n    &等价于：\\\\\n    &w \\leftarrow (1 - \\epsilon\\alpha) w + \\epsilon\\nabla_w J(w;X,y)\n\\end{aligned}\n$$\n从这里可以看到，学习规则发生了变化，在每次执行普通的梯度更新之前，会首先将原来的参数乘以一个常数项$(1 - \\epsilon\\alpha)$。\n\n对于原来的损失函数中存在的鞍点（梯度$\\nabla_w J(w;X,y) = 0$），则L2正则化将使得参数在这些点任然会得到更新，从而有助于模型逃离鞍点。（但是这个说法我认为不太对，因为加入$\\alpha w$这一项，只是将鞍点的位置进行了偏移，并不是去掉了鞍点。）\n\n为了分析l2正则化的作用，假定$J(w ^ \\ast ; X , y)$是损失函数的极小值。\n$w^\\ast$是使损失函数取得极小值的参数，先简单的对损失函数进行二次近似如下，这里没有一次项是因为在函数极小值的地方，一次导数应该为0。\n\n$$\n\\begin{aligned}\n    \\hat{J}(w;X,y) = J(w^\\ast;X,y) + \\frac{1}{2}(w-w^\\ast)^TH(w-w^\\ast)\n\\end{aligned}\n$$\n\n对加上l2正则化项的损失函数近似形式如下：\n\n$$\n\\begin{aligned}\n    \\hat{J}(w;X,y) = J(w^\\ast;X,y) + \\frac{1}{2}(w-w^\\ast)^TH(w-w^\\ast) + \\frac{\\alpha}{2} w^Tw\n\\end{aligned}\n$$\n\n对损失函数进行求导如下。\n\n$$\n\\begin{aligned}\n    \\nabla_w \\hat{J}(w;X,y) = \\alpha w + H(w - w^\\ast)\n\\end{aligned}\n$$\n\n当$\\nabla_w\\hat{J}(w;X,y) = 0$时，有如下推导：\n\n$$\n\\begin{aligned}\n    \\alpha w + H(w - w^\\ast) = 0\\\\\n    (H + \\alpha I)w = Hw^\\ast\\\\\n    w = (H + \\alpha I)^{-1}Hw^\\ast\n\\end{aligned}\n$$\n\n可见当$\\alpha \\rightarrow 0$时，$w \\rightarrow w^\\ast$，进一步，因为$H$是实对称矩阵，因此必定可以正交对角化，$H=Q \\Lambda Q^T$，因此进一步推导如下：\n\n$$\n\\begin{aligned}\n    w &= (Q \\Lambda Q^T+ \\alpha I)^{-1}Q \\Lambda Q^Tw^\\ast\\\\\n    &=[Q (\\Lambda+ \\alpha I) Q^T]^{-1}Q \\Lambda Q^Tw^\\ast\\\\\n    &=Q (\\Lambda+ \\alpha I)^{-1}\\Lambda Q^Tw^\\ast\n\\end{aligned}\n$$\n\n其中，假设$\\Lambda$如下：\n\n$$\n\\begin{bmatrix}\n    &\\lambda_1 &0 &0 &\\cdots &0\\\\\n    &0 &\\lambda_2 &0 &\\cdots &0\\\\\n    &0 &0 &\\lambda_3 &\\cdots &0\\\\\n    &\\vdots &\\vdots &\\vdots &\\ddots &\\vdots\\\\\n    &0 &0 &0 &\\cdots &\\lambda_n\\\\\n\\end{bmatrix}\n$$\n\n则$(\\Lambda+ \\alpha I)^{-1}\\Lambda$如下：\n$$\n\\begin{bmatrix}\n    &\\frac{\\lambda_1}{\\lambda_1 + \\alpha} &0 &0 &\\cdots &0\\\\\n    &0 &\\frac{\\lambda_2}{\\lambda_2 + \\alpha} &0 &\\cdots &0\\\\\n    &0 &0 &\\frac{\\lambda_3}{\\lambda_3 + \\alpha} &\\cdots &0\\\\\n    &\\vdots &\\vdots &\\vdots &\\ddots &\\vdots\\\\\n    &0 &0 &0 &\\cdots &\\frac{\\lambda_n}{\\lambda_n + \\alpha}\\\\\n\\end{bmatrix}\n$$\n这就相当于在原损失函数极小值点的Hession矩阵$H$的特征向量方向上，将$w^\\ast$进行了缩放，而且特征值$\\lambda_i$越小的方向，$\\alpha$对其影响越大，缩小得越大，即加入l2正则化项之后，模型参数倾向于对损失函数影响显著的方向（Hession矩阵特征值大的方向）偏移。\n\n以最小二乘线性回归为例，其损失函数如下：\n\n$$\n(Xw-y)^T(Xw-y)\n$$\n\n如果加上l2正则化，则损失函数变成如下形式：\n\n$$\n(Xw-y)^T(Xw-y) + \\frac{1}{2}\\alpha w^Tw\n$$\n\n那么线性回归的解就从：\n\n$$\nw = (X^TX)^{-1}X^Ty\n$$\n\n变成了：\n$$\nw = (X^TX + \\alpha I)^{-1}X^Ty\n$$\n\n其中$X$可以写成如下,其中$x_{ij}$表示第i个样本$x_i$的第j维：\n$$\nX=\\begin{bmatrix}\n    &x_{11} &x_{12} &\\cdots &x_{1m}\\\\\n    &x_{21} &x_{22} &\\cdots &x_{2m}\\\\\n    &\\vdots &\\vdots &\\ddots &\\vdots\\\\\n    &x_{n1} &x_{n1} &\\cdots &x_{nm}\\\\\n\\end{bmatrix}\n$$\n\n则$X^TX$可以表示如下：\n$$\nX^TX=\\begin{bmatrix}\n    &\\sum_i^nx_{i1}x_{i1} &\\sum_i^nx_{i1}x_{i2} &\\cdots &\\sum_i^nx_{i1}x_{im}\\\\\n    &\\sum_i^nx_{i2}x_{i1} &\\sum_i^nx_{i2}x_{i2} &\\cdots &\\sum_i^nx_{i2}x_{im}\\\\\n    &\\vdots &\\vdots &\\ddots &\\vdots\\\\\n    &\\sum_i^nx_{im}x_{i1} &\\sum_i^nx_{im}x_{i2} &\\cdots &\\sum_i^nx_{im}x_{im}\\\\\n\\end{bmatrix}\n$$\n\n$X^TX$同样可以正交对角化，$X^TX = Q \\Lambda Q^T$，这里的$\\Lambda$对角线上的值是$X$奇异值的平方，之后的推导和上面相同，可见，加入l2正则化项之后，模型参数倾向于对损失函数影响显著的方向偏移，从而忽略掉数据中的一些干扰，增强模型泛化能力。\n\n## L1正则化（参数范数正则化的特例，范数使用1范数）\n进行l1正则化之后的损失函数如下：\n\n$$\n\\tilde{J} = \\alpha ||w||_1 + J(w;X,y)\n$$\n\n对其进行求导：\n$$\n\\nabla_w\\tilde{J} = \\alpha\\ sign(w) + \\nabla_wJ(w;X,y)\n$$\n从这里可以看出l1正则化项对导数的影响是一个固定值，和L2有很大区别（L2是固定的缩放）。\n\n使用在L2正则化分析中的损失函数近似方法，将原本的损失函数二次近似为$\\hat{J}(w;X,y) = J(w^\\ast;X,y) + \\frac{1}{2}(w-w^\\ast)^TH(w-w^\\ast)$，其导数如下：\n$$\n\\nabla_w\\hat{J} = H(w - w^\\ast)\n$$\n\n加上正则化项之后，其损失函数的二次近似可以表示为：\n\n$$\n\\hat{J}(w;X,y) = J(w^\\ast;X,y) + \\frac{1}{2}(w-w^\\ast)^T H(w-w^\\ast) + \\alpha||w||_1\n$$\n\n如果将Hessian矩阵简化成了对角阵（这个类似对数据进行PCA之类的操作，将数据之间的相关性去掉了，因此Hessian矩阵变成对角阵，这样分析要简单一些）。\n\n则：\n$$\n\\hat{J}(w;X,y) = J(w^\\ast;X,y) + \\sum_i[\\frac{1}{2}H_{i,i}(w_i-w_i^\\ast)^2 + \\alpha|w_i|]\n$$\n\n则对$w_i$进行求导，如下：\n$$\n\\nabla_{w_i}\\hat{J}(w;X,y) = H_{i,i}(w_i - w_i^\\ast) + \\alpha\\ sign(w_i)\n$$\n\n可以看出，加了L1正则化之后梯度为0的点变成了如下：\n$$\nw_i = sign(w_i^\\ast)max\\{|w_i^\\ast| - \\frac{\\alpha}{H_{i,i}}, 0\\}\n$$\n\n描述成图像大概如下：\n\n{%asset_img l1正则化效果示意.png L1正则化效果示意%}\n\n可以看出，l1使得权重变得更加稀疏，这在特征选择方面非常有用。\n\n# 数据集扩充\n数据集扩充的本质是要求模型对于相似的输入（例如图片的平移、缩放、添加噪声、旋转等不改变图片语义的操作），必须有相似的输出。\n\n# 对噪声的鲁棒性\n除了在原始输入上添加噪声之外，还有两种在隐层添加噪声的方法：\n- dropout在隐层的输出上添加噪声。\n- 直接对学习到的权重添加噪声，可以视作模拟贝叶斯方法，认为模型权重有一定的随机性，同时也鼓励模型进入到参数空间中比较稳定的区域。\n\n# 输出目标噪声\n数据标签很可能包含一定的错误，而且在分类任务重，由于softmax\\sigmoid函数的特点，模型不能拟合绝对的0和1标签，因此标签平滑技术通过把标签进行平滑，可以缓解这样的问题。\n\n# 稀疏表示\n类似L1正则化产生的参数稀疏效果，在隐层的输出上，也可以尝试使用L1正则化，让隐层的输出倾向于变得稀疏。\n\n# 参数绑定和参数共享\n- 某些足够相似的任务，其参数也应该相似，因此可以将参数之间相减的范数添加到损失函数中进行训练。\n- CNN中的卷积层通过“提取类似特征所需权重相同”的思想，大量的权重共享，显著降低了模型的参数量，同时使得模型更加容易训练。\n\n# 训练早停\n验证误差不再下降之后，及时停止训练模型，不需要等到训练集误差最小，这样有助于防止模型过拟合。\n\n# 多任务学习\n一些任务需要相同或者相似的特征，让共享大部分参数的多个模型来同时进行多个任务的拟合，可以起到使共享部分的参数学习到更优值的效果。","slug":"学习笔记/正则化方法","published":1,"updated":"2020-08-31T06:39:20.770Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckjxpc3s2001g44mqdgrma3xn","content":"<h1 id=\"偏差和方差\">偏差和方差</h1>\r\n<p>在监督学习中，模型的泛化误差（测试集误差）被视作偏差、方差和噪声之和。</p>\r\n<p>其中，偏差表示学习算法的期望预测与真实结果的偏离程度，刻画的是算法的拟合能力。</p>\r\n<p>方差则表示一定大小的训练数据扰动所导致的学习性能的变化大小，刻画的是输入扰动所造成的影响</p>\r\n<p>噪声则表示在当前任务上任何学习算法所能达到的期望泛化误差的下界，刻画的是问题本身的难度。</p>\r\n<p>偏差主要是因为模型的复杂度不够而引起，可以从训练集误差上体现出来。</p>\r\n<p>方差主要是因为模型复杂度过高而引起，表现为测试集误差和训练集误差的差值。</p>\r\n<p>模型复杂度高，容易导致方差变大，模型复杂度低，又容易导致偏差变大，因此需要平衡模型的方差与偏差，使得模型的泛化误差最小。</p>\r\n<h1 id=\"正则化方法\">正则化方法</h1>\r\n<p>正则化方法是向原始模型引入额外信息的一种方法的统称，其目的是通过为复杂模型添加正则化的方式，尽量在不影响偏差的同时降低方差。</p>\r\n<h1 id=\"参数范数正则化\">参数范数正则化</h1>\r\n<p>通过在损失函数中加入参数范数项来降低模型容量，从而达到正则化的效果，形如下式，其中<span class=\"math inline\">\\(J\\)</span>表示原始的损失函数，<span class=\"math inline\">\\(\\Omega(w)\\)</span>表示参数的一种范数，<span class=\"math inline\">\\(\\alpha\\)</span>参数用于控制正则化程度，<span class=\"math inline\">\\(\\tilde{J}(w;X,y)\\)</span>表示添加了正则化之后的损失函数。 <span class=\"math display\">\\[\r\n\\tilde{J}(w;X,y) = J(w;X,y) + \\alpha\\Omega(w)\r\n\\]</span></p>\r\n<h2 id=\"l2正则化参数范数正则化的特例范数使用2范数\">L2正则化（参数范数正则化的特例，范数使用2范数）</h2>\r\n<p>普通的损失函数可以写成如下： <span class=\"math display\">\\[\r\nJ(w;X,y)\r\n\\]</span></p>\r\n<p>在权重参数上加上l2正则化项之后，损失函数如下，为方便分析，这里将<span class=\"math inline\">\\(\\alpha\\)</span>写成<span class=\"math inline\">\\(\\frac{\\alpha}{2}\\)</span>：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    &amp;\\tilde{J} = \\frac{\\alpha}{2}w^Tw + J(w;X,y)\\\\\r\n    &amp;对w的求导：\\\\\r\n    &amp;\\nabla_w\\tilde{J} = \\alpha w + \\nabla_w J(w;X,y)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>每次的参数更新可以写成如下，其中<span class=\"math inline\">\\(\\epsilon\\)</span>表示学习速率：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    &amp;w \\leftarrow w - \\epsilon(\\alpha w + \\nabla_w J(w;X,y))\\\\\r\n    &amp;等价于：\\\\\r\n    &amp;w \\leftarrow (1 - \\epsilon\\alpha) w + \\epsilon\\nabla_w J(w;X,y)\r\n\\end{aligned}\r\n\\]</span> 从这里可以看到，学习规则发生了变化，在每次执行普通的梯度更新之前，会首先将原来的参数乘以一个常数项<span class=\"math inline\">\\((1 - \\epsilon\\alpha)\\)</span>。</p>\r\n<p>对于原来的损失函数中存在的鞍点（梯度<span class=\"math inline\">\\(\\nabla_w J(w;X,y) = 0\\)</span>），则L2正则化将使得参数在这些点任然会得到更新，从而有助于模型逃离鞍点。（但是这个说法我认为不太对，因为加入<span class=\"math inline\">\\(\\alpha w\\)</span>这一项，只是将鞍点的位置进行了偏移，并不是去掉了鞍点。）</p>\r\n<p>为了分析l2正则化的作用，假定<span class=\"math inline\">\\(J(w ^ \\ast ; X , y)\\)</span>是损失函数的极小值。 <span class=\"math inline\">\\(w^\\ast\\)</span>是使损失函数取得极小值的参数，先简单的对损失函数进行二次近似如下，这里没有一次项是因为在函数极小值的地方，一次导数应该为0。</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\hat{J}(w;X,y) = J(w^\\ast;X,y) + \\frac{1}{2}(w-w^\\ast)^TH(w-w^\\ast)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>对加上l2正则化项的损失函数近似形式如下：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\hat{J}(w;X,y) = J(w^\\ast;X,y) + \\frac{1}{2}(w-w^\\ast)^TH(w-w^\\ast) + \\frac{\\alpha}{2} w^Tw\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>对损失函数进行求导如下。</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\nabla_w \\hat{J}(w;X,y) = \\alpha w + H(w - w^\\ast)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>当<span class=\"math inline\">\\(\\nabla_w\\hat{J}(w;X,y) = 0\\)</span>时，有如下推导：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\alpha w + H(w - w^\\ast) = 0\\\\\r\n    (H + \\alpha I)w = Hw^\\ast\\\\\r\n    w = (H + \\alpha I)^{-1}Hw^\\ast\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>可见当<span class=\"math inline\">\\(\\alpha \\rightarrow 0\\)</span>时，<span class=\"math inline\">\\(w \\rightarrow w^\\ast\\)</span>，进一步，因为<span class=\"math inline\">\\(H\\)</span>是实对称矩阵，因此必定可以正交对角化，<span class=\"math inline\">\\(H=Q \\Lambda Q^T\\)</span>，因此进一步推导如下：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    w &amp;= (Q \\Lambda Q^T+ \\alpha I)^{-1}Q \\Lambda Q^Tw^\\ast\\\\\r\n    &amp;=[Q (\\Lambda+ \\alpha I) Q^T]^{-1}Q \\Lambda Q^Tw^\\ast\\\\\r\n    &amp;=Q (\\Lambda+ \\alpha I)^{-1}\\Lambda Q^Tw^\\ast\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>其中，假设<span class=\"math inline\">\\(\\Lambda\\)</span>如下：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{bmatrix}\r\n    &amp;\\lambda_1 &amp;0 &amp;0 &amp;\\cdots &amp;0\\\\\r\n    &amp;0 &amp;\\lambda_2 &amp;0 &amp;\\cdots &amp;0\\\\\r\n    &amp;0 &amp;0 &amp;\\lambda_3 &amp;\\cdots &amp;0\\\\\r\n    &amp;\\vdots &amp;\\vdots &amp;\\vdots &amp;\\ddots &amp;\\vdots\\\\\r\n    &amp;0 &amp;0 &amp;0 &amp;\\cdots &amp;\\lambda_n\\\\\r\n\\end{bmatrix}\r\n\\]</span></p>\r\n<p>则<span class=\"math inline\">\\((\\Lambda+ \\alpha I)^{-1}\\Lambda\\)</span>如下： <span class=\"math display\">\\[\r\n\\begin{bmatrix}\r\n    &amp;\\frac{\\lambda_1}{\\lambda_1 + \\alpha} &amp;0 &amp;0 &amp;\\cdots &amp;0\\\\\r\n    &amp;0 &amp;\\frac{\\lambda_2}{\\lambda_2 + \\alpha} &amp;0 &amp;\\cdots &amp;0\\\\\r\n    &amp;0 &amp;0 &amp;\\frac{\\lambda_3}{\\lambda_3 + \\alpha} &amp;\\cdots &amp;0\\\\\r\n    &amp;\\vdots &amp;\\vdots &amp;\\vdots &amp;\\ddots &amp;\\vdots\\\\\r\n    &amp;0 &amp;0 &amp;0 &amp;\\cdots &amp;\\frac{\\lambda_n}{\\lambda_n + \\alpha}\\\\\r\n\\end{bmatrix}\r\n\\]</span> 这就相当于在原损失函数极小值点的Hession矩阵<span class=\"math inline\">\\(H\\)</span>的特征向量方向上，将<span class=\"math inline\">\\(w^\\ast\\)</span>进行了缩放，而且特征值<span class=\"math inline\">\\(\\lambda_i\\)</span>越小的方向，<span class=\"math inline\">\\(\\alpha\\)</span>对其影响越大，缩小得越大，即加入l2正则化项之后，模型参数倾向于对损失函数影响显著的方向（Hession矩阵特征值大的方向）偏移。</p>\r\n<p>以最小二乘线性回归为例，其损失函数如下：</p>\r\n<p><span class=\"math display\">\\[\r\n(Xw-y)^T(Xw-y)\r\n\\]</span></p>\r\n<p>如果加上l2正则化，则损失函数变成如下形式：</p>\r\n<p><span class=\"math display\">\\[\r\n(Xw-y)^T(Xw-y) + \\frac{1}{2}\\alpha w^Tw\r\n\\]</span></p>\r\n<p>那么线性回归的解就从：</p>\r\n<p><span class=\"math display\">\\[\r\nw = (X^TX)^{-1}X^Ty\r\n\\]</span></p>\r\n<p>变成了： <span class=\"math display\">\\[\r\nw = (X^TX + \\alpha I)^{-1}X^Ty\r\n\\]</span></p>\r\n<p>其中<span class=\"math inline\">\\(X\\)</span>可以写成如下,其中<span class=\"math inline\">\\(x_{ij}\\)</span>表示第i个样本<span class=\"math inline\">\\(x_i\\)</span>的第j维： <span class=\"math display\">\\[\r\nX=\\begin{bmatrix}\r\n    &amp;x_{11} &amp;x_{12} &amp;\\cdots &amp;x_{1m}\\\\\r\n    &amp;x_{21} &amp;x_{22} &amp;\\cdots &amp;x_{2m}\\\\\r\n    &amp;\\vdots &amp;\\vdots &amp;\\ddots &amp;\\vdots\\\\\r\n    &amp;x_{n1} &amp;x_{n1} &amp;\\cdots &amp;x_{nm}\\\\\r\n\\end{bmatrix}\r\n\\]</span></p>\r\n<p>则<span class=\"math inline\">\\(X^TX\\)</span>可以表示如下： <span class=\"math display\">\\[\r\nX^TX=\\begin{bmatrix}\r\n    &amp;\\sum_i^nx_{i1}x_{i1} &amp;\\sum_i^nx_{i1}x_{i2} &amp;\\cdots &amp;\\sum_i^nx_{i1}x_{im}\\\\\r\n    &amp;\\sum_i^nx_{i2}x_{i1} &amp;\\sum_i^nx_{i2}x_{i2} &amp;\\cdots &amp;\\sum_i^nx_{i2}x_{im}\\\\\r\n    &amp;\\vdots &amp;\\vdots &amp;\\ddots &amp;\\vdots\\\\\r\n    &amp;\\sum_i^nx_{im}x_{i1} &amp;\\sum_i^nx_{im}x_{i2} &amp;\\cdots &amp;\\sum_i^nx_{im}x_{im}\\\\\r\n\\end{bmatrix}\r\n\\]</span></p>\r\n<p><span class=\"math inline\">\\(X^TX\\)</span>同样可以正交对角化，<span class=\"math inline\">\\(X^TX = Q \\Lambda Q^T\\)</span>，这里的<span class=\"math inline\">\\(\\Lambda\\)</span>对角线上的值是<span class=\"math inline\">\\(X\\)</span>奇异值的平方，之后的推导和上面相同，可见，加入l2正则化项之后，模型参数倾向于对损失函数影响显著的方向偏移，从而忽略掉数据中的一些干扰，增强模型泛化能力。</p>\r\n<h2 id=\"l1正则化参数范数正则化的特例范数使用1范数\">L1正则化（参数范数正则化的特例，范数使用1范数）</h2>\r\n<p>进行l1正则化之后的损失函数如下：</p>\r\n<p><span class=\"math display\">\\[\r\n\\tilde{J} = \\alpha ||w||_1 + J(w;X,y)\r\n\\]</span></p>\r\n<p>对其进行求导： <span class=\"math display\">\\[\r\n\\nabla_w\\tilde{J} = \\alpha\\ sign(w) + \\nabla_wJ(w;X,y)\r\n\\]</span> 从这里可以看出l1正则化项对导数的影响是一个固定值，和L2有很大区别（L2是固定的缩放）。</p>\r\n<p>使用在L2正则化分析中的损失函数近似方法，将原本的损失函数二次近似为<span class=\"math inline\">\\(\\hat{J}(w;X,y) = J(w^\\ast;X,y) + \\frac{1}{2}(w-w^\\ast)^TH(w-w^\\ast)\\)</span>，其导数如下： <span class=\"math display\">\\[\r\n\\nabla_w\\hat{J} = H(w - w^\\ast)\r\n\\]</span></p>\r\n<p>加上正则化项之后，其损失函数的二次近似可以表示为：</p>\r\n<p><span class=\"math display\">\\[\r\n\\hat{J}(w;X,y) = J(w^\\ast;X,y) + \\frac{1}{2}(w-w^\\ast)^T H(w-w^\\ast) + \\alpha||w||_1\r\n\\]</span></p>\r\n<p>如果将Hessian矩阵简化成了对角阵（这个类似对数据进行PCA之类的操作，将数据之间的相关性去掉了，因此Hessian矩阵变成对角阵，这样分析要简单一些）。</p>\r\n<p>则： <span class=\"math display\">\\[\r\n\\hat{J}(w;X,y) = J(w^\\ast;X,y) + \\sum_i[\\frac{1}{2}H_{i,i}(w_i-w_i^\\ast)^2 + \\alpha|w_i|]\r\n\\]</span></p>\r\n<p>则对<span class=\"math inline\">\\(w_i\\)</span>进行求导，如下： <span class=\"math display\">\\[\r\n\\nabla_{w_i}\\hat{J}(w;X,y) = H_{i,i}(w_i - w_i^\\ast) + \\alpha\\ sign(w_i)\r\n\\]</span></p>\r\n<p>可以看出，加了L1正则化之后梯度为0的点变成了如下： <span class=\"math display\">\\[\r\nw_i = sign(w_i^\\ast)max\\{|w_i^\\ast| - \\frac{\\alpha}{H_{i,i}}, 0\\}\r\n\\]</span></p>\r\n<p>描述成图像大概如下：</p>\r\n<img src=\"/2020/05/12/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/l1%E6%AD%A3%E5%88%99%E5%8C%96%E6%95%88%E6%9E%9C%E7%A4%BA%E6%84%8F.png\" class=\"\" title=\"L1正则化效果示意\">\r\n<p>可以看出，l1使得权重变得更加稀疏，这在特征选择方面非常有用。</p>\r\n<h1 id=\"数据集扩充\">数据集扩充</h1>\r\n<p>数据集扩充的本质是要求模型对于相似的输入（例如图片的平移、缩放、添加噪声、旋转等不改变图片语义的操作），必须有相似的输出。</p>\r\n<h1 id=\"对噪声的鲁棒性\">对噪声的鲁棒性</h1>\r\n<p>除了在原始输入上添加噪声之外，还有两种在隐层添加噪声的方法： - dropout在隐层的输出上添加噪声。 - 直接对学习到的权重添加噪声，可以视作模拟贝叶斯方法，认为模型权重有一定的随机性，同时也鼓励模型进入到参数空间中比较稳定的区域。</p>\r\n<h1 id=\"输出目标噪声\">输出目标噪声</h1>\r\n<p>数据标签很可能包含一定的错误，而且在分类任务重，由于softmax，模型不能拟合绝对的0和1标签，因此标签平滑技术通过把标签进行平滑，可以缓解这样的问题。</p>\r\n<h1 id=\"稀疏表示\">稀疏表示</h1>\r\n<p>类似L1正则化产生的参数稀疏效果，在隐层的输出上，也可以尝试使用L1正则化，让隐层的输出倾向于变得稀疏。</p>\r\n<h1 id=\"参数绑定和参数共享\">参数绑定和参数共享</h1>\r\n<ul>\r\n<li>某些足够相似的任务，其参数也应该相似，因此可以将参数之间相减的范数添加到损失函数中进行训练。</li>\r\n<li>CNN中的卷积层通过“提取类似特征所需权重相同”的思想，大量的权重共享，显著降低了模型的参数量，同时使得模型更加容易训练。</li>\r\n</ul>\r\n<h1 id=\"训练早停\">训练早停</h1>\r\n<p>验证误差不再下降之后，及时停止训练模型，不需要等到训练集误差最小，这样有助于防止模型过拟合。</p>\r\n<h1 id=\"多任务学习\">多任务学习</h1>\r\n<p>一些任务需要相同或者相似的特征，让共享大部分参数的多个模型来同时进行多个任务的拟合，可以起到使共享部分的参数学习到更优值的效果。</p>\r\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"偏差和方差\">偏差和方差</h1>\r\n<p>在监督学习中，模型的泛化误差（测试集误差）被视作偏差、方差和噪声之和。</p>\r\n<p>其中，偏差表示学习算法的期望预测与真实结果的偏离程度，刻画的是算法的拟合能力。</p>\r\n<p>方差则表示一定大小的训练数据扰动所导致的学习性能的变化大小，刻画的是输入扰动所造成的影响</p>\r\n<p>噪声则表示在当前任务上任何学习算法所能达到的期望泛化误差的下界，刻画的是问题本身的难度。</p>\r\n<p>偏差主要是因为模型的复杂度不够而引起，可以从训练集误差上体现出来。</p>\r\n<p>方差主要是因为模型复杂度过高而引起，表现为测试集误差和训练集误差的差值。</p>\r\n<p>模型复杂度高，容易导致方差变大，模型复杂度低，又容易导致偏差变大，因此需要平衡模型的方差与偏差，使得模型的泛化误差最小。</p>\r\n<h1 id=\"正则化方法\">正则化方法</h1>\r\n<p>正则化方法是向原始模型引入额外信息的一种方法的统称，其目的是通过为复杂模型添加正则化的方式，尽量在不影响偏差的同时降低方差。</p>\r\n<h1 id=\"参数范数正则化\">参数范数正则化</h1>\r\n<p>通过在损失函数中加入参数范数项来降低模型容量，从而达到正则化的效果，形如下式，其中<span class=\"math inline\">\\(J\\)</span>表示原始的损失函数，<span class=\"math inline\">\\(\\Omega(w)\\)</span>表示参数的一种范数，<span class=\"math inline\">\\(\\alpha\\)</span>参数用于控制正则化程度，<span class=\"math inline\">\\(\\tilde{J}(w;X,y)\\)</span>表示添加了正则化之后的损失函数。 <span class=\"math display\">\\[\r\n\\tilde{J}(w;X,y) = J(w;X,y) + \\alpha\\Omega(w)\r\n\\]</span></p>\r\n<h2 id=\"l2正则化参数范数正则化的特例范数使用2范数\">L2正则化（参数范数正则化的特例，范数使用2范数）</h2>\r\n<p>普通的损失函数可以写成如下： <span class=\"math display\">\\[\r\nJ(w;X,y)\r\n\\]</span></p>\r\n<p>在权重参数上加上l2正则化项之后，损失函数如下，为方便分析，这里将<span class=\"math inline\">\\(\\alpha\\)</span>写成<span class=\"math inline\">\\(\\frac{\\alpha}{2}\\)</span>：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    &amp;\\tilde{J} = \\frac{\\alpha}{2}w^Tw + J(w;X,y)\\\\\r\n    &amp;对w的求导：\\\\\r\n    &amp;\\nabla_w\\tilde{J} = \\alpha w + \\nabla_w J(w;X,y)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>每次的参数更新可以写成如下，其中<span class=\"math inline\">\\(\\epsilon\\)</span>表示学习速率：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    &amp;w \\leftarrow w - \\epsilon(\\alpha w + \\nabla_w J(w;X,y))\\\\\r\n    &amp;等价于：\\\\\r\n    &amp;w \\leftarrow (1 - \\epsilon\\alpha) w + \\epsilon\\nabla_w J(w;X,y)\r\n\\end{aligned}\r\n\\]</span> 从这里可以看到，学习规则发生了变化，在每次执行普通的梯度更新之前，会首先将原来的参数乘以一个常数项<span class=\"math inline\">\\((1 - \\epsilon\\alpha)\\)</span>。</p>\r\n<p>对于原来的损失函数中存在的鞍点（梯度<span class=\"math inline\">\\(\\nabla_w J(w;X,y) = 0\\)</span>），则L2正则化将使得参数在这些点任然会得到更新，从而有助于模型逃离鞍点。（但是这个说法我认为不太对，因为加入<span class=\"math inline\">\\(\\alpha w\\)</span>这一项，只是将鞍点的位置进行了偏移，并不是去掉了鞍点。）</p>\r\n<p>为了分析l2正则化的作用，假定<span class=\"math inline\">\\(J(w ^ \\ast ; X , y)\\)</span>是损失函数的极小值。 <span class=\"math inline\">\\(w^\\ast\\)</span>是使损失函数取得极小值的参数，先简单的对损失函数进行二次近似如下，这里没有一次项是因为在函数极小值的地方，一次导数应该为0。</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\hat{J}(w;X,y) = J(w^\\ast;X,y) + \\frac{1}{2}(w-w^\\ast)^TH(w-w^\\ast)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>对加上l2正则化项的损失函数近似形式如下：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\hat{J}(w;X,y) = J(w^\\ast;X,y) + \\frac{1}{2}(w-w^\\ast)^TH(w-w^\\ast) + \\frac{\\alpha}{2} w^Tw\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>对损失函数进行求导如下。</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\nabla_w \\hat{J}(w;X,y) = \\alpha w + H(w - w^\\ast)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>当<span class=\"math inline\">\\(\\nabla_w\\hat{J}(w;X,y) = 0\\)</span>时，有如下推导：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\alpha w + H(w - w^\\ast) = 0\\\\\r\n    (H + \\alpha I)w = Hw^\\ast\\\\\r\n    w = (H + \\alpha I)^{-1}Hw^\\ast\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>可见当<span class=\"math inline\">\\(\\alpha \\rightarrow 0\\)</span>时，<span class=\"math inline\">\\(w \\rightarrow w^\\ast\\)</span>，进一步，因为<span class=\"math inline\">\\(H\\)</span>是实对称矩阵，因此必定可以正交对角化，<span class=\"math inline\">\\(H=Q \\Lambda Q^T\\)</span>，因此进一步推导如下：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    w &amp;= (Q \\Lambda Q^T+ \\alpha I)^{-1}Q \\Lambda Q^Tw^\\ast\\\\\r\n    &amp;=[Q (\\Lambda+ \\alpha I) Q^T]^{-1}Q \\Lambda Q^Tw^\\ast\\\\\r\n    &amp;=Q (\\Lambda+ \\alpha I)^{-1}\\Lambda Q^Tw^\\ast\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>其中，假设<span class=\"math inline\">\\(\\Lambda\\)</span>如下：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{bmatrix}\r\n    &amp;\\lambda_1 &amp;0 &amp;0 &amp;\\cdots &amp;0\\\\\r\n    &amp;0 &amp;\\lambda_2 &amp;0 &amp;\\cdots &amp;0\\\\\r\n    &amp;0 &amp;0 &amp;\\lambda_3 &amp;\\cdots &amp;0\\\\\r\n    &amp;\\vdots &amp;\\vdots &amp;\\vdots &amp;\\ddots &amp;\\vdots\\\\\r\n    &amp;0 &amp;0 &amp;0 &amp;\\cdots &amp;\\lambda_n\\\\\r\n\\end{bmatrix}\r\n\\]</span></p>\r\n<p>则<span class=\"math inline\">\\((\\Lambda+ \\alpha I)^{-1}\\Lambda\\)</span>如下： <span class=\"math display\">\\[\r\n\\begin{bmatrix}\r\n    &amp;\\frac{\\lambda_1}{\\lambda_1 + \\alpha} &amp;0 &amp;0 &amp;\\cdots &amp;0\\\\\r\n    &amp;0 &amp;\\frac{\\lambda_2}{\\lambda_2 + \\alpha} &amp;0 &amp;\\cdots &amp;0\\\\\r\n    &amp;0 &amp;0 &amp;\\frac{\\lambda_3}{\\lambda_3 + \\alpha} &amp;\\cdots &amp;0\\\\\r\n    &amp;\\vdots &amp;\\vdots &amp;\\vdots &amp;\\ddots &amp;\\vdots\\\\\r\n    &amp;0 &amp;0 &amp;0 &amp;\\cdots &amp;\\frac{\\lambda_n}{\\lambda_n + \\alpha}\\\\\r\n\\end{bmatrix}\r\n\\]</span> 这就相当于在原损失函数极小值点的Hession矩阵<span class=\"math inline\">\\(H\\)</span>的特征向量方向上，将<span class=\"math inline\">\\(w^\\ast\\)</span>进行了缩放，而且特征值<span class=\"math inline\">\\(\\lambda_i\\)</span>越小的方向，<span class=\"math inline\">\\(\\alpha\\)</span>对其影响越大，缩小得越大，即加入l2正则化项之后，模型参数倾向于对损失函数影响显著的方向（Hession矩阵特征值大的方向）偏移。</p>\r\n<p>以最小二乘线性回归为例，其损失函数如下：</p>\r\n<p><span class=\"math display\">\\[\r\n(Xw-y)^T(Xw-y)\r\n\\]</span></p>\r\n<p>如果加上l2正则化，则损失函数变成如下形式：</p>\r\n<p><span class=\"math display\">\\[\r\n(Xw-y)^T(Xw-y) + \\frac{1}{2}\\alpha w^Tw\r\n\\]</span></p>\r\n<p>那么线性回归的解就从：</p>\r\n<p><span class=\"math display\">\\[\r\nw = (X^TX)^{-1}X^Ty\r\n\\]</span></p>\r\n<p>变成了： <span class=\"math display\">\\[\r\nw = (X^TX + \\alpha I)^{-1}X^Ty\r\n\\]</span></p>\r\n<p>其中<span class=\"math inline\">\\(X\\)</span>可以写成如下,其中<span class=\"math inline\">\\(x_{ij}\\)</span>表示第i个样本<span class=\"math inline\">\\(x_i\\)</span>的第j维： <span class=\"math display\">\\[\r\nX=\\begin{bmatrix}\r\n    &amp;x_{11} &amp;x_{12} &amp;\\cdots &amp;x_{1m}\\\\\r\n    &amp;x_{21} &amp;x_{22} &amp;\\cdots &amp;x_{2m}\\\\\r\n    &amp;\\vdots &amp;\\vdots &amp;\\ddots &amp;\\vdots\\\\\r\n    &amp;x_{n1} &amp;x_{n1} &amp;\\cdots &amp;x_{nm}\\\\\r\n\\end{bmatrix}\r\n\\]</span></p>\r\n<p>则<span class=\"math inline\">\\(X^TX\\)</span>可以表示如下： <span class=\"math display\">\\[\r\nX^TX=\\begin{bmatrix}\r\n    &amp;\\sum_i^nx_{i1}x_{i1} &amp;\\sum_i^nx_{i1}x_{i2} &amp;\\cdots &amp;\\sum_i^nx_{i1}x_{im}\\\\\r\n    &amp;\\sum_i^nx_{i2}x_{i1} &amp;\\sum_i^nx_{i2}x_{i2} &amp;\\cdots &amp;\\sum_i^nx_{i2}x_{im}\\\\\r\n    &amp;\\vdots &amp;\\vdots &amp;\\ddots &amp;\\vdots\\\\\r\n    &amp;\\sum_i^nx_{im}x_{i1} &amp;\\sum_i^nx_{im}x_{i2} &amp;\\cdots &amp;\\sum_i^nx_{im}x_{im}\\\\\r\n\\end{bmatrix}\r\n\\]</span></p>\r\n<p><span class=\"math inline\">\\(X^TX\\)</span>同样可以正交对角化，<span class=\"math inline\">\\(X^TX = Q \\Lambda Q^T\\)</span>，这里的<span class=\"math inline\">\\(\\Lambda\\)</span>对角线上的值是<span class=\"math inline\">\\(X\\)</span>奇异值的平方，之后的推导和上面相同，可见，加入l2正则化项之后，模型参数倾向于对损失函数影响显著的方向偏移，从而忽略掉数据中的一些干扰，增强模型泛化能力。</p>\r\n<h2 id=\"l1正则化参数范数正则化的特例范数使用1范数\">L1正则化（参数范数正则化的特例，范数使用1范数）</h2>\r\n<p>进行l1正则化之后的损失函数如下：</p>\r\n<p><span class=\"math display\">\\[\r\n\\tilde{J} = \\alpha ||w||_1 + J(w;X,y)\r\n\\]</span></p>\r\n<p>对其进行求导： <span class=\"math display\">\\[\r\n\\nabla_w\\tilde{J} = \\alpha\\ sign(w) + \\nabla_wJ(w;X,y)\r\n\\]</span> 从这里可以看出l1正则化项对导数的影响是一个固定值，和L2有很大区别（L2是固定的缩放）。</p>\r\n<p>使用在L2正则化分析中的损失函数近似方法，将原本的损失函数二次近似为<span class=\"math inline\">\\(\\hat{J}(w;X,y) = J(w^\\ast;X,y) + \\frac{1}{2}(w-w^\\ast)^TH(w-w^\\ast)\\)</span>，其导数如下： <span class=\"math display\">\\[\r\n\\nabla_w\\hat{J} = H(w - w^\\ast)\r\n\\]</span></p>\r\n<p>加上正则化项之后，其损失函数的二次近似可以表示为：</p>\r\n<p><span class=\"math display\">\\[\r\n\\hat{J}(w;X,y) = J(w^\\ast;X,y) + \\frac{1}{2}(w-w^\\ast)^T H(w-w^\\ast) + \\alpha||w||_1\r\n\\]</span></p>\r\n<p>如果将Hessian矩阵简化成了对角阵（这个类似对数据进行PCA之类的操作，将数据之间的相关性去掉了，因此Hessian矩阵变成对角阵，这样分析要简单一些）。</p>\r\n<p>则： <span class=\"math display\">\\[\r\n\\hat{J}(w;X,y) = J(w^\\ast;X,y) + \\sum_i[\\frac{1}{2}H_{i,i}(w_i-w_i^\\ast)^2 + \\alpha|w_i|]\r\n\\]</span></p>\r\n<p>则对<span class=\"math inline\">\\(w_i\\)</span>进行求导，如下： <span class=\"math display\">\\[\r\n\\nabla_{w_i}\\hat{J}(w;X,y) = H_{i,i}(w_i - w_i^\\ast) + \\alpha\\ sign(w_i)\r\n\\]</span></p>\r\n<p>可以看出，加了L1正则化之后梯度为0的点变成了如下： <span class=\"math display\">\\[\r\nw_i = sign(w_i^\\ast)max\\{|w_i^\\ast| - \\frac{\\alpha}{H_{i,i}}, 0\\}\r\n\\]</span></p>\r\n<p>描述成图像大概如下：</p>\r\n<img src=\"/2020/05/12/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/l1%E6%AD%A3%E5%88%99%E5%8C%96%E6%95%88%E6%9E%9C%E7%A4%BA%E6%84%8F.png\" class=\"\" title=\"L1正则化效果示意\">\r\n<p>可以看出，l1使得权重变得更加稀疏，这在特征选择方面非常有用。</p>\r\n<h1 id=\"数据集扩充\">数据集扩充</h1>\r\n<p>数据集扩充的本质是要求模型对于相似的输入（例如图片的平移、缩放、添加噪声、旋转等不改变图片语义的操作），必须有相似的输出。</p>\r\n<h1 id=\"对噪声的鲁棒性\">对噪声的鲁棒性</h1>\r\n<p>除了在原始输入上添加噪声之外，还有两种在隐层添加噪声的方法： - dropout在隐层的输出上添加噪声。 - 直接对学习到的权重添加噪声，可以视作模拟贝叶斯方法，认为模型权重有一定的随机性，同时也鼓励模型进入到参数空间中比较稳定的区域。</p>\r\n<h1 id=\"输出目标噪声\">输出目标噪声</h1>\r\n<p>数据标签很可能包含一定的错误，而且在分类任务重，由于softmax，模型不能拟合绝对的0和1标签，因此标签平滑技术通过把标签进行平滑，可以缓解这样的问题。</p>\r\n<h1 id=\"稀疏表示\">稀疏表示</h1>\r\n<p>类似L1正则化产生的参数稀疏效果，在隐层的输出上，也可以尝试使用L1正则化，让隐层的输出倾向于变得稀疏。</p>\r\n<h1 id=\"参数绑定和参数共享\">参数绑定和参数共享</h1>\r\n<ul>\r\n<li>某些足够相似的任务，其参数也应该相似，因此可以将参数之间相减的范数添加到损失函数中进行训练。</li>\r\n<li>CNN中的卷积层通过“提取类似特征所需权重相同”的思想，大量的权重共享，显著降低了模型的参数量，同时使得模型更加容易训练。</li>\r\n</ul>\r\n<h1 id=\"训练早停\">训练早停</h1>\r\n<p>验证误差不再下降之后，及时停止训练模型，不需要等到训练集误差最小，这样有助于防止模型过拟合。</p>\r\n<h1 id=\"多任务学习\">多任务学习</h1>\r\n<p>一些任务需要相同或者相似的特征，让共享大部分参数的多个模型来同时进行多个任务的拟合，可以起到使共享部分的参数学习到更优值的效果。</p>\r\n"},{"title":"线性代数杂项","date":"2020-04-23T08:01:16.000Z","mathjax":true,"_content":"\n# 方阵的一些理解\n\n## 方阵的特征分解和方阵的相似关系\n\n特征分解可以表示为$A = Pdiag(\\lambda)P^{-1}$，其中$\\lambda$是A的特征值组成的对角矩阵，$P$是$A$的特征向量作为列向量组成的矩阵。\n\n方阵和向量的乘法，例如$Ax$，其中$x$在标准正交基下的坐标可以表示为$(x_1\\ x_2\\ x_3\\ ...\\ x_n)$，可以看做一个对$x$的映射，而$Ax = Pdiag(\\lambda)P^{-1}x$，则可以理解为首先将x从标准正交基下的坐标转换到以$A$的特征向量作为基的坐标（$P^{-1}x$），然后在此坐标下进行缩放（$diag(\\lambda)P^{-1}x$），最后再将坐标转换回标准正交基的坐标（$Pdiag(\\lambda)P^{-1}x$）。\n\n方阵$A$相似与$B$即表示$A=PBP^{-1}$，也可以和方阵的特征分解类似理解，$A$可以看做在标准正交基下做的变换，$B$则是对应的在$P$的列向量作为基的情况下的相同变换，即同一个变换，在不同的基下的不同表示。","source":"_posts/学习笔记/线性代数杂项.md","raw":"---\ntitle: 线性代数杂项\ndate: 2020-04-23 16:01:16\ntags: [线性代数]\nmathjax: true\n---\n\n# 方阵的一些理解\n\n## 方阵的特征分解和方阵的相似关系\n\n特征分解可以表示为$A = Pdiag(\\lambda)P^{-1}$，其中$\\lambda$是A的特征值组成的对角矩阵，$P$是$A$的特征向量作为列向量组成的矩阵。\n\n方阵和向量的乘法，例如$Ax$，其中$x$在标准正交基下的坐标可以表示为$(x_1\\ x_2\\ x_3\\ ...\\ x_n)$，可以看做一个对$x$的映射，而$Ax = Pdiag(\\lambda)P^{-1}x$，则可以理解为首先将x从标准正交基下的坐标转换到以$A$的特征向量作为基的坐标（$P^{-1}x$），然后在此坐标下进行缩放（$diag(\\lambda)P^{-1}x$），最后再将坐标转换回标准正交基的坐标（$Pdiag(\\lambda)P^{-1}x$）。\n\n方阵$A$相似与$B$即表示$A=PBP^{-1}$，也可以和方阵的特征分解类似理解，$A$可以看做在标准正交基下做的变换，$B$则是对应的在$P$的列向量作为基的情况下的相同变换，即同一个变换，在不同的基下的不同表示。","slug":"学习笔记/线性代数杂项","published":1,"updated":"2020-08-31T06:39:20.784Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckjxpc3s3001i44mq670t6leq","content":"<h1 id=\"方阵的一些理解\">方阵的一些理解</h1>\r\n<h2 id=\"方阵的特征分解和方阵的相似关系\">方阵的特征分解和方阵的相似关系</h2>\r\n<p>特征分解可以表示为<span class=\"math inline\">\\(A = Pdiag(\\lambda)P^{-1}\\)</span>，其中<span class=\"math inline\">\\(\\lambda\\)</span>是A的特征值组成的对角矩阵，<span class=\"math inline\">\\(P\\)</span>是<span class=\"math inline\">\\(A\\)</span>的特征向量作为列向量组成的矩阵。</p>\r\n<p>方阵和向量的乘法，例如<span class=\"math inline\">\\(Ax\\)</span>，其中<span class=\"math inline\">\\(x\\)</span>在标准正交基下的坐标可以表示为<span class=\"math inline\">\\((x_1\\ x_2\\ x_3\\ ...\\ x_n)\\)</span>，可以看做一个对<span class=\"math inline\">\\(x\\)</span>的映射，而<span class=\"math inline\">\\(Ax = Pdiag(\\lambda)P^{-1}x\\)</span>，则可以理解为首先将x从标准正交基下的坐标转换到以<span class=\"math inline\">\\(A\\)</span>的特征向量作为基的坐标（<span class=\"math inline\">\\(P^{-1}x\\)</span>），然后在此坐标下进行缩放（<span class=\"math inline\">\\(diag(\\lambda)P^{-1}x\\)</span>），最后再将坐标转换回标准正交基的坐标（<span class=\"math inline\">\\(Pdiag(\\lambda)P^{-1}x\\)</span>）。</p>\r\n<p>方阵<span class=\"math inline\">\\(A\\)</span>相似与<span class=\"math inline\">\\(B\\)</span>即表示<span class=\"math inline\">\\(A=PBP^{-1}\\)</span>，也可以和方阵的特征分解类似理解，<span class=\"math inline\">\\(A\\)</span>可以看做在标准正交基下做的变换，<span class=\"math inline\">\\(B\\)</span>则是对应的在<span class=\"math inline\">\\(P\\)</span>的列向量作为基的情况下的相同变换，即同一个变换，在不同的基下的不同表示。</p>\r\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"方阵的一些理解\">方阵的一些理解</h1>\r\n<h2 id=\"方阵的特征分解和方阵的相似关系\">方阵的特征分解和方阵的相似关系</h2>\r\n<p>特征分解可以表示为<span class=\"math inline\">\\(A = Pdiag(\\lambda)P^{-1}\\)</span>，其中<span class=\"math inline\">\\(\\lambda\\)</span>是A的特征值组成的对角矩阵，<span class=\"math inline\">\\(P\\)</span>是<span class=\"math inline\">\\(A\\)</span>的特征向量作为列向量组成的矩阵。</p>\r\n<p>方阵和向量的乘法，例如<span class=\"math inline\">\\(Ax\\)</span>，其中<span class=\"math inline\">\\(x\\)</span>在标准正交基下的坐标可以表示为<span class=\"math inline\">\\((x_1\\ x_2\\ x_3\\ ...\\ x_n)\\)</span>，可以看做一个对<span class=\"math inline\">\\(x\\)</span>的映射，而<span class=\"math inline\">\\(Ax = Pdiag(\\lambda)P^{-1}x\\)</span>，则可以理解为首先将x从标准正交基下的坐标转换到以<span class=\"math inline\">\\(A\\)</span>的特征向量作为基的坐标（<span class=\"math inline\">\\(P^{-1}x\\)</span>），然后在此坐标下进行缩放（<span class=\"math inline\">\\(diag(\\lambda)P^{-1}x\\)</span>），最后再将坐标转换回标准正交基的坐标（<span class=\"math inline\">\\(Pdiag(\\lambda)P^{-1}x\\)</span>）。</p>\r\n<p>方阵<span class=\"math inline\">\\(A\\)</span>相似与<span class=\"math inline\">\\(B\\)</span>即表示<span class=\"math inline\">\\(A=PBP^{-1}\\)</span>，也可以和方阵的特征分解类似理解，<span class=\"math inline\">\\(A\\)</span>可以看做在标准正交基下做的变换，<span class=\"math inline\">\\(B\\)</span>则是对应的在<span class=\"math inline\">\\(P\\)</span>的列向量作为基的情况下的相同变换，即同一个变换，在不同的基下的不同表示。</p>\r\n"},{"layout":"w","title":"编译安装emacs记录","date":"2019-05-08T03:02:55.000Z","_content":"\n最近需要实现一些tensorflow OP，所以先找一个ubuntu下的C++ IDE，决定用emacs，下面是安装过程。\n\n## 文件下载\n使用镜像站: http://mirrors.kernel.org/gnu/emacs/，下载26.2版本\n\n## 安装流程\n按照安装说明，首先解包之后运行\n\n     ./configure --prefix=/usr/local\n\n遭遇无情报错：\n\n{% asset_img configure_error.png configure提示环境问题%}\n\n三个package没有，因此尝试安装\n     sudo apt-get install libxpm-dev\n     sudo apt-get install libdif-dev\n     sudo apt-get install gnutls-bin\n     sudo apt-get install libgnutls-dev\n\n安装完后，configure就没有问题了,configure执行完后，按照常规操作，开始make，但是这里又遇到无情报错：\n\n{% asset_img make_error.png make提示环境问题%}\n\nlocate 找了一下libpcre，发现就在Anaconda目录下面就有，因此make之前设置一下动态链接库搜索路径\n\n     ecport LD_LIBRARY_PATH=/home/lty/env/Anaconda/lib:$LD_LIBRARY_PATH\n\n重新make就成功了，make完成之后先检查编译出来的可执行文件是否可用，使用命令：\n\n     src/emacs -Q\n\n成功打开emacs，常规操作，没有什么问题，之后开始安装依旧是常规的 make install,","source":"_posts/学习笔记/编译安装emacs记录.md","raw":"---\nlayout: w\ntitle: 编译安装emacs记录\ndate: 2019-05-08 11:02:55\ntags: 环境搭建\n---\n\n最近需要实现一些tensorflow OP，所以先找一个ubuntu下的C++ IDE，决定用emacs，下面是安装过程。\n\n## 文件下载\n使用镜像站: http://mirrors.kernel.org/gnu/emacs/，下载26.2版本\n\n## 安装流程\n按照安装说明，首先解包之后运行\n\n     ./configure --prefix=/usr/local\n\n遭遇无情报错：\n\n{% asset_img configure_error.png configure提示环境问题%}\n\n三个package没有，因此尝试安装\n     sudo apt-get install libxpm-dev\n     sudo apt-get install libdif-dev\n     sudo apt-get install gnutls-bin\n     sudo apt-get install libgnutls-dev\n\n安装完后，configure就没有问题了,configure执行完后，按照常规操作，开始make，但是这里又遇到无情报错：\n\n{% asset_img make_error.png make提示环境问题%}\n\nlocate 找了一下libpcre，发现就在Anaconda目录下面就有，因此make之前设置一下动态链接库搜索路径\n\n     ecport LD_LIBRARY_PATH=/home/lty/env/Anaconda/lib:$LD_LIBRARY_PATH\n\n重新make就成功了，make完成之后先检查编译出来的可执行文件是否可用，使用命令：\n\n     src/emacs -Q\n\n成功打开emacs，常规操作，没有什么问题，之后开始安装依旧是常规的 make install,","slug":"学习笔记/编译安装emacs记录","published":1,"updated":"2019-05-08T07:01:34.620Z","comments":1,"photos":[],"link":"","_id":"ckjxpc3s4001k44mqfmo0h5cp","content":"<p>最近需要实现一些tensorflow OP，所以先找一个ubuntu下的C++ IDE，决定用emacs，下面是安装过程。</p>\r\n<h2 id=\"文件下载\">文件下载</h2>\r\n<p>使用镜像站: http://mirrors.kernel.org/gnu/emacs/，下载26.2版本</p>\r\n<h2 id=\"安装流程\">安装流程</h2>\r\n<p>按照安装说明，首先解包之后运行</p>\r\n<pre><code> ./configure --prefix=/usr/local</code></pre>\r\n<p>遭遇无情报错：</p>\r\n\r\n<p>三个package没有，因此尝试安装 sudo apt-get install libxpm-dev sudo apt-get install libdif-dev sudo apt-get install gnutls-bin sudo apt-get install libgnutls-dev</p>\r\n<p>安装完后，configure就没有问题了,configure执行完后，按照常规操作，开始make，但是这里又遇到无情报错：</p>\r\n\r\n<p>locate 找了一下libpcre，发现就在Anaconda目录下面就有，因此make之前设置一下动态链接库搜索路径</p>\r\n<pre><code> ecport LD_LIBRARY_PATH=/home/lty/env/Anaconda/lib:$LD_LIBRARY_PATH</code></pre>\r\n<p>重新make就成功了，make完成之后先检查编译出来的可执行文件是否可用，使用命令：</p>\r\n<pre><code> src/emacs -Q</code></pre>\r\n<p>成功打开emacs，常规操作，没有什么问题，之后开始安装依旧是常规的 make install,</p>\r\n","site":{"data":{}},"excerpt":"","more":"<p>最近需要实现一些tensorflow OP，所以先找一个ubuntu下的C++ IDE，决定用emacs，下面是安装过程。</p>\r\n<h2 id=\"文件下载\">文件下载</h2>\r\n<p>使用镜像站: http://mirrors.kernel.org/gnu/emacs/，下载26.2版本</p>\r\n<h2 id=\"安装流程\">安装流程</h2>\r\n<p>按照安装说明，首先解包之后运行</p>\r\n<pre><code> ./configure --prefix=/usr/local</code></pre>\r\n<p>遭遇无情报错：</p>\r\n\r\n<p>三个package没有，因此尝试安装 sudo apt-get install libxpm-dev sudo apt-get install libdif-dev sudo apt-get install gnutls-bin sudo apt-get install libgnutls-dev</p>\r\n<p>安装完后，configure就没有问题了,configure执行完后，按照常规操作，开始make，但是这里又遇到无情报错：</p>\r\n\r\n<p>locate 找了一下libpcre，发现就在Anaconda目录下面就有，因此make之前设置一下动态链接库搜索路径</p>\r\n<pre><code> ecport LD_LIBRARY_PATH=/home/lty/env/Anaconda/lib:$LD_LIBRARY_PATH</code></pre>\r\n<p>重新make就成功了，make完成之后先检查编译出来的可执行文件是否可用，使用命令：</p>\r\n<pre><code> src/emacs -Q</code></pre>\r\n<p>成功打开emacs，常规操作，没有什么问题，之后开始安装依旧是常规的 make install,</p>\r\n"},{"title":"非标量求导术","date":"2020-04-23T07:46:44.000Z","mathjax":true,"_content":"\n# 标量对标量求导\n$f(x)：R \\rightarrow R, x \\in R, df = f'(x)dx, f'(x) = \\frac{\\partial f}{\\partial x} \\in R$\n\n# 标量对向量求导\n$f(x)：R^n \\rightarrow R, x \\in R^n, df = (\\frac{\\partial f}{\\partial x}) ^ T dx, \\frac{\\partial f}{\\partial x} \\in R^n$\n\n# 向量对向量求导\n$f(x)：R^m \\rightarrow R^n, x \\in R^m, df = (\\frac{\\partial f}{\\partial x}) ^ T dx, \\frac{\\partial f}{\\partial x} \\in R^{m \\times n}$，这样得到的结果是个矩阵，$f(x)$的每个元素对应一列，$x$的每个元素对应一行，即：\n$$\n\\frac{\\partial f}{\\partial x} = \n\\begin{bmatrix}\n    \\frac{\\partial f(x)_1}{\\partial x_1}&\\cdots&\\frac{\\partial f(x)_n}{\\partial x_1}\\\\\n    \\vdots&\\ddots&\\vdots\\\\\n    \\frac{\\partial f(x)_1}{\\partial x_m}&\\cdots&\\frac{\\partial f(x)_n}{\\partial x_m}\\\\\n\\end{bmatrix}\n$$\n\n# 标量对矩阵求导\n$f(X)：R^{m \\times n} \\rightarrow R, X \\in R^{m \\times n}, d f = tr((\\frac{\\partial f}{\\partial X}) ^ T dX)$\n\n## 矩阵微分操作：\n- $d(X \\pm Y) = dX \\pm dY$\n- $d(XY) = XdY + YdX$\n- $d(X^T) = (dX)^T$\n- $dtr(X)=tr(dX)$\n- $tr(AdX) = A^TdX$\n- $d(X^{-1})=-X^{-1}dXX^{-1}$，由$XX^{-1}=I$两侧微分可得。\n- $d|X| = tr(X ^ \\# d X)$, $X^\\#$为$X$的伴随矩阵，当$X$可逆时，$d|X| = tr(|X|X ^ {-1} d X)$\n- $d(X \\odot Y) = X \\odot dY + dX \\odot Y$，其中$\\odot$为逐元素乘法\n- $d\\sigma(x)=\\sigma'(X) \\odot dX$，其中$\\sigma$为逐元素函数\n\n## 迹技巧\n- $a = tr(a)$，$a$为标量\n- $tr(A^T) = tr(A)$\n- $tr(A \\pm B) = tr(A) \\pm tr(B)$\n- $tr(A B) = tr(B A)$，其中$A$和$B^T$大小相同\n- $tr(A^TB \\odot C) = tr((A \\odot B)^T C)$，其中$A$、$B$、$C$尺寸相同\n\n## 逐元素乘法的转换技巧\n- $X \\odot Y = Y \\odot X$\n- $X \\odot Y = diag(X)Y, X,Y \\in R^n$\n- $X \\odot Y = vec^{-1}(diag(vec(X))vec(Y)), X,Y \\in R^{m \\times n}$\n\n# 矩阵对矩阵求导\n矩阵函数$F(X): R^{m \\times n} \\rightarrow R^{p \\times q}$\n\n定义矩阵的向量化：$vec(X) = \\begin{bmatrix}X_{1,1}&X_{2,1}&X_{3,1}& \\dots &X_{m,n}\\end{bmatrix}$，长度为$m \\times n$。\n\n矩阵对矩阵求导可以作为向量对向量的求导来处理：\n\n$vec(F)：R^{mn} \\rightarrow R^{pq}, x \\in R^{m \\times n}, dvec(F) = (\\frac{\\partial vec(F)}{\\partial vec(x)}) ^ T dvec(x), \\frac{\\partial vec(F)}{\\partial vec(x)} \\in R^{mn \\times pq}$\n\n其实还有其他矩阵对矩阵的导数定义方式，这里选择这种定义，可以兼容微分运算，在求导时比较方便。\n\n## 向量化的一些性质：\n- $vec(A \\pm B) = vec(A) \\pm vec(B)$\n- $vec(AXB) = (B^T \\bigotimes A)vec(X)$，其中$\\bigotimes$表示kronecker积\n- $vec(A^T) = K_{mn} vec(A)$，其中$K_{mn} \\in R^{mn \\times mn}$是交换矩阵，作用是将按列优先的向量化变为按行优先的向量化，满足性质$K_{mn} = K_{mn}^T, K_{mn}K_{mn} = I$\n- $vec(A \\odot X) = diag(vec(A))vec(X)$，其中$diag(vec(A)) \\in R^{mn \\times mn}$\n- $vec(ab^T) = b \\bigotimes a$\n\n## kronecker积的性质\n- $(A \\bigotimes B)^T = A^T \\bigotimes B^T$\n- $(A \\bigotimes B)(C \\bigotimes D) = AC \\bigotimes BD$\n- $K_{pm}(A \\bigotimes B)K_{nq} = B \\bigotimes A$","source":"_posts/学习笔记/非标量求导术.md","raw":"---\ntitle: 非标量求导术\ndate: 2020-04-23 15:46:44\ntags: [求导，矩阵，向量]\nmathjax: true\n---\n\n# 标量对标量求导\n$f(x)：R \\rightarrow R, x \\in R, df = f'(x)dx, f'(x) = \\frac{\\partial f}{\\partial x} \\in R$\n\n# 标量对向量求导\n$f(x)：R^n \\rightarrow R, x \\in R^n, df = (\\frac{\\partial f}{\\partial x}) ^ T dx, \\frac{\\partial f}{\\partial x} \\in R^n$\n\n# 向量对向量求导\n$f(x)：R^m \\rightarrow R^n, x \\in R^m, df = (\\frac{\\partial f}{\\partial x}) ^ T dx, \\frac{\\partial f}{\\partial x} \\in R^{m \\times n}$，这样得到的结果是个矩阵，$f(x)$的每个元素对应一列，$x$的每个元素对应一行，即：\n$$\n\\frac{\\partial f}{\\partial x} = \n\\begin{bmatrix}\n    \\frac{\\partial f(x)_1}{\\partial x_1}&\\cdots&\\frac{\\partial f(x)_n}{\\partial x_1}\\\\\n    \\vdots&\\ddots&\\vdots\\\\\n    \\frac{\\partial f(x)_1}{\\partial x_m}&\\cdots&\\frac{\\partial f(x)_n}{\\partial x_m}\\\\\n\\end{bmatrix}\n$$\n\n# 标量对矩阵求导\n$f(X)：R^{m \\times n} \\rightarrow R, X \\in R^{m \\times n}, d f = tr((\\frac{\\partial f}{\\partial X}) ^ T dX)$\n\n## 矩阵微分操作：\n- $d(X \\pm Y) = dX \\pm dY$\n- $d(XY) = XdY + YdX$\n- $d(X^T) = (dX)^T$\n- $dtr(X)=tr(dX)$\n- $tr(AdX) = A^TdX$\n- $d(X^{-1})=-X^{-1}dXX^{-1}$，由$XX^{-1}=I$两侧微分可得。\n- $d|X| = tr(X ^ \\# d X)$, $X^\\#$为$X$的伴随矩阵，当$X$可逆时，$d|X| = tr(|X|X ^ {-1} d X)$\n- $d(X \\odot Y) = X \\odot dY + dX \\odot Y$，其中$\\odot$为逐元素乘法\n- $d\\sigma(x)=\\sigma'(X) \\odot dX$，其中$\\sigma$为逐元素函数\n\n## 迹技巧\n- $a = tr(a)$，$a$为标量\n- $tr(A^T) = tr(A)$\n- $tr(A \\pm B) = tr(A) \\pm tr(B)$\n- $tr(A B) = tr(B A)$，其中$A$和$B^T$大小相同\n- $tr(A^TB \\odot C) = tr((A \\odot B)^T C)$，其中$A$、$B$、$C$尺寸相同\n\n## 逐元素乘法的转换技巧\n- $X \\odot Y = Y \\odot X$\n- $X \\odot Y = diag(X)Y, X,Y \\in R^n$\n- $X \\odot Y = vec^{-1}(diag(vec(X))vec(Y)), X,Y \\in R^{m \\times n}$\n\n# 矩阵对矩阵求导\n矩阵函数$F(X): R^{m \\times n} \\rightarrow R^{p \\times q}$\n\n定义矩阵的向量化：$vec(X) = \\begin{bmatrix}X_{1,1}&X_{2,1}&X_{3,1}& \\dots &X_{m,n}\\end{bmatrix}$，长度为$m \\times n$。\n\n矩阵对矩阵求导可以作为向量对向量的求导来处理：\n\n$vec(F)：R^{mn} \\rightarrow R^{pq}, x \\in R^{m \\times n}, dvec(F) = (\\frac{\\partial vec(F)}{\\partial vec(x)}) ^ T dvec(x), \\frac{\\partial vec(F)}{\\partial vec(x)} \\in R^{mn \\times pq}$\n\n其实还有其他矩阵对矩阵的导数定义方式，这里选择这种定义，可以兼容微分运算，在求导时比较方便。\n\n## 向量化的一些性质：\n- $vec(A \\pm B) = vec(A) \\pm vec(B)$\n- $vec(AXB) = (B^T \\bigotimes A)vec(X)$，其中$\\bigotimes$表示kronecker积\n- $vec(A^T) = K_{mn} vec(A)$，其中$K_{mn} \\in R^{mn \\times mn}$是交换矩阵，作用是将按列优先的向量化变为按行优先的向量化，满足性质$K_{mn} = K_{mn}^T, K_{mn}K_{mn} = I$\n- $vec(A \\odot X) = diag(vec(A))vec(X)$，其中$diag(vec(A)) \\in R^{mn \\times mn}$\n- $vec(ab^T) = b \\bigotimes a$\n\n## kronecker积的性质\n- $(A \\bigotimes B)^T = A^T \\bigotimes B^T$\n- $(A \\bigotimes B)(C \\bigotimes D) = AC \\bigotimes BD$\n- $K_{pm}(A \\bigotimes B)K_{nq} = B \\bigotimes A$","slug":"学习笔记/非标量求导术","published":1,"updated":"2020-08-31T06:39:20.791Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckjxpc3s5001n44mqec742c9a","content":"<h1 id=\"标量对标量求导\">标量对标量求导</h1>\r\n<p><span class=\"math inline\">\\(f(x)：R \\rightarrow R, x \\in R, df = f&#39;(x)dx, f&#39;(x) = \\frac{\\partial f}{\\partial x} \\in R\\)</span></p>\r\n<h1 id=\"标量对向量求导\">标量对向量求导</h1>\r\n<p><span class=\"math inline\">\\(f(x)：R^n \\rightarrow R, x \\in R^n, df = (\\frac{\\partial f}{\\partial x}) ^ T dx, \\frac{\\partial f}{\\partial x} \\in R^n\\)</span></p>\r\n<h1 id=\"向量对向量求导\">向量对向量求导</h1>\r\n<p><span class=\"math inline\">\\(f(x)：R^m \\rightarrow R^n, x \\in R^m, df = (\\frac{\\partial f}{\\partial x}) ^ T dx, \\frac{\\partial f}{\\partial x} \\in R^{m \\times n}\\)</span>，这样得到的结果是个矩阵，<span class=\"math inline\">\\(f(x)\\)</span>的每个元素对应一列，<span class=\"math inline\">\\(x\\)</span>的每个元素对应一行，即： <span class=\"math display\">\\[\r\n\\frac{\\partial f}{\\partial x} = \r\n\\begin{bmatrix}\r\n    \\frac{\\partial f(x)_1}{\\partial x_1}&amp;\\cdots&amp;\\frac{\\partial f(x)_n}{\\partial x_1}\\\\\r\n    \\vdots&amp;\\ddots&amp;\\vdots\\\\\r\n    \\frac{\\partial f(x)_1}{\\partial x_m}&amp;\\cdots&amp;\\frac{\\partial f(x)_n}{\\partial x_m}\\\\\r\n\\end{bmatrix}\r\n\\]</span></p>\r\n<h1 id=\"标量对矩阵求导\">标量对矩阵求导</h1>\r\n<p><span class=\"math inline\">\\(f(X)：R^{m \\times n} \\rightarrow R, X \\in R^{m \\times n}, d f = tr((\\frac{\\partial f}{\\partial X}) ^ T dX)\\)</span></p>\r\n<h2 id=\"矩阵微分操作\">矩阵微分操作：</h2>\r\n<ul>\r\n<li><span class=\"math inline\">\\(d(X \\pm Y) = dX \\pm dY\\)</span></li>\r\n<li><span class=\"math inline\">\\(d(XY) = XdY + YdX\\)</span></li>\r\n<li><span class=\"math inline\">\\(d(X^T) = (dX)^T\\)</span></li>\r\n<li><span class=\"math inline\">\\(dtr(X)=tr(dX)\\)</span></li>\r\n<li><span class=\"math inline\">\\(tr(AdX) = A^TdX\\)</span></li>\r\n<li><span class=\"math inline\">\\(d(X^{-1})=-X^{-1}dXX^{-1}\\)</span>，由<span class=\"math inline\">\\(XX^{-1}=I\\)</span>两侧微分可得。</li>\r\n<li><span class=\"math inline\">\\(d|X| = tr(X ^ \\# d X)\\)</span>, <span class=\"math inline\">\\(X^\\#\\)</span>为<span class=\"math inline\">\\(X\\)</span>的伴随矩阵，当<span class=\"math inline\">\\(X\\)</span>可逆时，<span class=\"math inline\">\\(d|X| = tr(|X|X ^ {-1} d X)\\)</span></li>\r\n<li><span class=\"math inline\">\\(d(X \\odot Y) = X \\odot dY + dX \\odot Y\\)</span>，其中<span class=\"math inline\">\\(\\odot\\)</span>为逐元素乘法</li>\r\n<li><span class=\"math inline\">\\(d\\sigma(x)=\\sigma&#39;(X) \\odot dX\\)</span>，其中<span class=\"math inline\">\\(\\sigma\\)</span>为逐元素函数</li>\r\n</ul>\r\n<h2 id=\"迹技巧\">迹技巧</h2>\r\n<ul>\r\n<li><span class=\"math inline\">\\(a = tr(a)\\)</span>，<span class=\"math inline\">\\(a\\)</span>为标量</li>\r\n<li><span class=\"math inline\">\\(tr(A^T) = tr(A)\\)</span></li>\r\n<li><span class=\"math inline\">\\(tr(A \\pm B) = tr(A) \\pm tr(B)\\)</span></li>\r\n<li><span class=\"math inline\">\\(tr(A B) = tr(B A)\\)</span>，其中<span class=\"math inline\">\\(A\\)</span>和<span class=\"math inline\">\\(B^T\\)</span>大小相同</li>\r\n<li><span class=\"math inline\">\\(tr(A^TB \\odot C) = tr((A \\odot B)^T C)\\)</span>，其中<span class=\"math inline\">\\(A\\)</span>、<span class=\"math inline\">\\(B\\)</span>、<span class=\"math inline\">\\(C\\)</span>尺寸相同</li>\r\n</ul>\r\n<h2 id=\"逐元素乘法的转换技巧\">逐元素乘法的转换技巧</h2>\r\n<ul>\r\n<li><span class=\"math inline\">\\(X \\odot Y = Y \\odot X\\)</span></li>\r\n<li><span class=\"math inline\">\\(X \\odot Y = diag(X)Y, X,Y \\in R^n\\)</span></li>\r\n<li><span class=\"math inline\">\\(X \\odot Y = vec^{-1}(diag(vec(X))vec(Y)), X,Y \\in R^{m \\times n}\\)</span></li>\r\n</ul>\r\n<h1 id=\"矩阵对矩阵求导\">矩阵对矩阵求导</h1>\r\n<p>矩阵函数<span class=\"math inline\">\\(F(X): R^{m \\times n} \\rightarrow R^{p \\times q}\\)</span></p>\r\n<p>定义矩阵的向量化：<span class=\"math inline\">\\(vec(X) = \\begin{bmatrix}X_{1,1}&amp;X_{2,1}&amp;X_{3,1}&amp; \\dots &amp;X_{m,n}\\end{bmatrix}\\)</span>，长度为<span class=\"math inline\">\\(m \\times n\\)</span>。</p>\r\n<p>矩阵对矩阵求导可以作为向量对向量的求导来处理：</p>\r\n<p><span class=\"math inline\">\\(vec(F)：R^{mn} \\rightarrow R^{pq}, x \\in R^{m \\times n}, dvec(F) = (\\frac{\\partial vec(F)}{\\partial vec(x)}) ^ T dvec(x), \\frac{\\partial vec(F)}{\\partial vec(x)} \\in R^{mn \\times pq}\\)</span></p>\r\n<p>其实还有其他矩阵对矩阵的导数定义方式，这里选择这种定义，可以兼容微分运算，在求导时比较方便。</p>\r\n<h2 id=\"向量化的一些性质\">向量化的一些性质：</h2>\r\n<ul>\r\n<li><span class=\"math inline\">\\(vec(A \\pm B) = vec(A) \\pm vec(B)\\)</span></li>\r\n<li><span class=\"math inline\">\\(vec(AXB) = (B^T \\bigotimes A)vec(X)\\)</span>，其中<span class=\"math inline\">\\(\\bigotimes\\)</span>表示kronecker积</li>\r\n<li><span class=\"math inline\">\\(vec(A^T) = K_{mn} vec(A)\\)</span>，其中<span class=\"math inline\">\\(K_{mn} \\in R^{mn \\times mn}\\)</span>是交换矩阵，作用是将按列优先的向量化变为按行优先的向量化，满足性质<span class=\"math inline\">\\(K_{mn} = K_{mn}^T, K_{mn}K_{mn} = I\\)</span></li>\r\n<li><span class=\"math inline\">\\(vec(A \\odot X) = diag(vec(A))vec(X)\\)</span>，其中<span class=\"math inline\">\\(diag(vec(A)) \\in R^{mn \\times mn}\\)</span></li>\r\n<li><span class=\"math inline\">\\(vec(ab^T) = b \\bigotimes a\\)</span></li>\r\n</ul>\r\n<h2 id=\"kronecker积的性质\">kronecker积的性质</h2>\r\n<ul>\r\n<li><span class=\"math inline\">\\((A \\bigotimes B)^T = A^T \\bigotimes B^T\\)</span></li>\r\n<li><span class=\"math inline\">\\((A \\bigotimes B)(C \\bigotimes D) = AC \\bigotimes BD\\)</span></li>\r\n<li><span class=\"math inline\">\\(K_{pm}(A \\bigotimes B)K_{nq} = B \\bigotimes A\\)</span></li>\r\n</ul>\r\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"标量对标量求导\">标量对标量求导</h1>\r\n<p><span class=\"math inline\">\\(f(x)：R \\rightarrow R, x \\in R, df = f&#39;(x)dx, f&#39;(x) = \\frac{\\partial f}{\\partial x} \\in R\\)</span></p>\r\n<h1 id=\"标量对向量求导\">标量对向量求导</h1>\r\n<p><span class=\"math inline\">\\(f(x)：R^n \\rightarrow R, x \\in R^n, df = (\\frac{\\partial f}{\\partial x}) ^ T dx, \\frac{\\partial f}{\\partial x} \\in R^n\\)</span></p>\r\n<h1 id=\"向量对向量求导\">向量对向量求导</h1>\r\n<p><span class=\"math inline\">\\(f(x)：R^m \\rightarrow R^n, x \\in R^m, df = (\\frac{\\partial f}{\\partial x}) ^ T dx, \\frac{\\partial f}{\\partial x} \\in R^{m \\times n}\\)</span>，这样得到的结果是个矩阵，<span class=\"math inline\">\\(f(x)\\)</span>的每个元素对应一列，<span class=\"math inline\">\\(x\\)</span>的每个元素对应一行，即： <span class=\"math display\">\\[\r\n\\frac{\\partial f}{\\partial x} = \r\n\\begin{bmatrix}\r\n    \\frac{\\partial f(x)_1}{\\partial x_1}&amp;\\cdots&amp;\\frac{\\partial f(x)_n}{\\partial x_1}\\\\\r\n    \\vdots&amp;\\ddots&amp;\\vdots\\\\\r\n    \\frac{\\partial f(x)_1}{\\partial x_m}&amp;\\cdots&amp;\\frac{\\partial f(x)_n}{\\partial x_m}\\\\\r\n\\end{bmatrix}\r\n\\]</span></p>\r\n<h1 id=\"标量对矩阵求导\">标量对矩阵求导</h1>\r\n<p><span class=\"math inline\">\\(f(X)：R^{m \\times n} \\rightarrow R, X \\in R^{m \\times n}, d f = tr((\\frac{\\partial f}{\\partial X}) ^ T dX)\\)</span></p>\r\n<h2 id=\"矩阵微分操作\">矩阵微分操作：</h2>\r\n<ul>\r\n<li><span class=\"math inline\">\\(d(X \\pm Y) = dX \\pm dY\\)</span></li>\r\n<li><span class=\"math inline\">\\(d(XY) = XdY + YdX\\)</span></li>\r\n<li><span class=\"math inline\">\\(d(X^T) = (dX)^T\\)</span></li>\r\n<li><span class=\"math inline\">\\(dtr(X)=tr(dX)\\)</span></li>\r\n<li><span class=\"math inline\">\\(tr(AdX) = A^TdX\\)</span></li>\r\n<li><span class=\"math inline\">\\(d(X^{-1})=-X^{-1}dXX^{-1}\\)</span>，由<span class=\"math inline\">\\(XX^{-1}=I\\)</span>两侧微分可得。</li>\r\n<li><span class=\"math inline\">\\(d|X| = tr(X ^ \\# d X)\\)</span>, <span class=\"math inline\">\\(X^\\#\\)</span>为<span class=\"math inline\">\\(X\\)</span>的伴随矩阵，当<span class=\"math inline\">\\(X\\)</span>可逆时，<span class=\"math inline\">\\(d|X| = tr(|X|X ^ {-1} d X)\\)</span></li>\r\n<li><span class=\"math inline\">\\(d(X \\odot Y) = X \\odot dY + dX \\odot Y\\)</span>，其中<span class=\"math inline\">\\(\\odot\\)</span>为逐元素乘法</li>\r\n<li><span class=\"math inline\">\\(d\\sigma(x)=\\sigma&#39;(X) \\odot dX\\)</span>，其中<span class=\"math inline\">\\(\\sigma\\)</span>为逐元素函数</li>\r\n</ul>\r\n<h2 id=\"迹技巧\">迹技巧</h2>\r\n<ul>\r\n<li><span class=\"math inline\">\\(a = tr(a)\\)</span>，<span class=\"math inline\">\\(a\\)</span>为标量</li>\r\n<li><span class=\"math inline\">\\(tr(A^T) = tr(A)\\)</span></li>\r\n<li><span class=\"math inline\">\\(tr(A \\pm B) = tr(A) \\pm tr(B)\\)</span></li>\r\n<li><span class=\"math inline\">\\(tr(A B) = tr(B A)\\)</span>，其中<span class=\"math inline\">\\(A\\)</span>和<span class=\"math inline\">\\(B^T\\)</span>大小相同</li>\r\n<li><span class=\"math inline\">\\(tr(A^TB \\odot C) = tr((A \\odot B)^T C)\\)</span>，其中<span class=\"math inline\">\\(A\\)</span>、<span class=\"math inline\">\\(B\\)</span>、<span class=\"math inline\">\\(C\\)</span>尺寸相同</li>\r\n</ul>\r\n<h2 id=\"逐元素乘法的转换技巧\">逐元素乘法的转换技巧</h2>\r\n<ul>\r\n<li><span class=\"math inline\">\\(X \\odot Y = Y \\odot X\\)</span></li>\r\n<li><span class=\"math inline\">\\(X \\odot Y = diag(X)Y, X,Y \\in R^n\\)</span></li>\r\n<li><span class=\"math inline\">\\(X \\odot Y = vec^{-1}(diag(vec(X))vec(Y)), X,Y \\in R^{m \\times n}\\)</span></li>\r\n</ul>\r\n<h1 id=\"矩阵对矩阵求导\">矩阵对矩阵求导</h1>\r\n<p>矩阵函数<span class=\"math inline\">\\(F(X): R^{m \\times n} \\rightarrow R^{p \\times q}\\)</span></p>\r\n<p>定义矩阵的向量化：<span class=\"math inline\">\\(vec(X) = \\begin{bmatrix}X_{1,1}&amp;X_{2,1}&amp;X_{3,1}&amp; \\dots &amp;X_{m,n}\\end{bmatrix}\\)</span>，长度为<span class=\"math inline\">\\(m \\times n\\)</span>。</p>\r\n<p>矩阵对矩阵求导可以作为向量对向量的求导来处理：</p>\r\n<p><span class=\"math inline\">\\(vec(F)：R^{mn} \\rightarrow R^{pq}, x \\in R^{m \\times n}, dvec(F) = (\\frac{\\partial vec(F)}{\\partial vec(x)}) ^ T dvec(x), \\frac{\\partial vec(F)}{\\partial vec(x)} \\in R^{mn \\times pq}\\)</span></p>\r\n<p>其实还有其他矩阵对矩阵的导数定义方式，这里选择这种定义，可以兼容微分运算，在求导时比较方便。</p>\r\n<h2 id=\"向量化的一些性质\">向量化的一些性质：</h2>\r\n<ul>\r\n<li><span class=\"math inline\">\\(vec(A \\pm B) = vec(A) \\pm vec(B)\\)</span></li>\r\n<li><span class=\"math inline\">\\(vec(AXB) = (B^T \\bigotimes A)vec(X)\\)</span>，其中<span class=\"math inline\">\\(\\bigotimes\\)</span>表示kronecker积</li>\r\n<li><span class=\"math inline\">\\(vec(A^T) = K_{mn} vec(A)\\)</span>，其中<span class=\"math inline\">\\(K_{mn} \\in R^{mn \\times mn}\\)</span>是交换矩阵，作用是将按列优先的向量化变为按行优先的向量化，满足性质<span class=\"math inline\">\\(K_{mn} = K_{mn}^T, K_{mn}K_{mn} = I\\)</span></li>\r\n<li><span class=\"math inline\">\\(vec(A \\odot X) = diag(vec(A))vec(X)\\)</span>，其中<span class=\"math inline\">\\(diag(vec(A)) \\in R^{mn \\times mn}\\)</span></li>\r\n<li><span class=\"math inline\">\\(vec(ab^T) = b \\bigotimes a\\)</span></li>\r\n</ul>\r\n<h2 id=\"kronecker积的性质\">kronecker积的性质</h2>\r\n<ul>\r\n<li><span class=\"math inline\">\\((A \\bigotimes B)^T = A^T \\bigotimes B^T\\)</span></li>\r\n<li><span class=\"math inline\">\\((A \\bigotimes B)(C \\bigotimes D) = AC \\bigotimes BD\\)</span></li>\r\n<li><span class=\"math inline\">\\(K_{pm}(A \\bigotimes B)K_{nq} = B \\bigotimes A\\)</span></li>\r\n</ul>\r\n"},{"title":"hexo安装记录","date":"2020-05-19T07:21:29.000Z","_content":"\n# hexo安装过程\n参考资料 https://zhuanlan.zhihu.com/p/187435941\n## nodejs\n版本：12.19.0，安装过程略\n\n## 安装hexo命令\n运行命令:\n\n\t npm install -g hexo-cli\n\n## 查看hexo版本\n运行命令:\n\n\t hexo --version\n\n得到以下输出\n\n\t hexo-cli: 4.2.0\n\t os: Windows_NT 10.0.19041 win32 x64\n\t node: 12.19.0\n\t v8: 7.8.279.23-node.44\n\t uv: 1.39.0\n\t zlib: 1.2.11\n\t brotli: 1.0.9\n\t ares: 1.16.0\n\t modules: 72\n\t nghttp2: 1.41.0\n\t napi: 7\n\t llhttp: 2.1.2\n\t http_parser: 2.9.3\n\t openssl: 1.1.1g\n\t cldr: 37.0\n\t icu: 67.1\n\t tz: 2019c\n\t unicode: 13.0\n\n## 初始化博客文件夹\n运行命令:\n\n\t hexo init blogDev\n\n## 安装package.json中的依赖\n\n运行命令：\n\n\t cd blogDev\n\t npm install\n\n## 修改_config.yml文件\n\n使用hexo-autonofollow插件来给外链添加nofollow属性\n\n在站点根目录下执行下列命令：\n\n\t npm install hexo-autonofollow --save\n\n## 修改git init文件\n去掉以下三项：\n\n\t db.json,\n\t Thumbs.db\n\t node-modules/\n\n## 支持latex\n卸载hexo自带的makrdown渲染工具：\n\n\t npm uninstall hexo-renderer-markded --save\n\n安装hexo-renderer-pandoc：\n\n\t npm install hexo-renderer-pandoc --save\n\n下载next主题：\n\n\t git clone https://github.com/iissnan/hexo-theme-next themes/next\n\n在_config.yml中将主题修改为next\n\n修改next主题下的_config.yml文件中mathjax部分。\n\n\t mathjax:\n\t\tenable: true\n\t\tper_page: false\n\t\tcdn: //cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML\n\n在执行hexo s时遇到报错：\n\n\t pandoc exited with code null\n\n因此在 https://github.com/jgm/pandoc/releases 重新下载pandoc安装，这个问题解决后，访问网页时又遇到输出代码的问题，于是又安装swig：\n\n\t npm i hexo-renderer-swig\n\n网页可以正常访问，但是不能加载图片，又安装asset-image:\n\n\t npm install hexo-asset-image --save\n\n还需要在package.json中手动修改asset-image版本如下，并重新执行npm install：\n\n\t \"hexo-asset-image\": \"^0.0.1\"\n\n\n## 侧边栏中文乱码\n是hexo-asset-image导致的，删除掉就好\n\n## 设置首页不显示全文\n在next的配置文件中，将下面的enable改成true\n\n\t auto_excerpt:\n\t\tenable: false\n\t\tlength: 150\n\n## 设置导航栏\n除了修改next主题下的config文件menu字段之外，还需要执行以下命令：\n\n\t hexo new page \"about\"\n\t hexo new page \"tags\"\n\t hexo new page \"categories\"\n\n## 翻页按钮显示不正常\n修改代码的位置: themes\\next\\layout\\_partials\\pagination.swig\n\n\n## Hexo next 主题配置右侧栏的分类和标签打开的是空白\ncategories 文件夹里面的 index.md 文件打开，修改（即添加一行）为：\n\n\t ---\n\t title: categories\n\t date: 2018-01-23 17:14:51\n\t type: \"categories\"   #新添加的\n\t ---\n同理，tags\n\n\t ---\n\t title: tags\n\t date: 2018-01-23 17:14:51\n\t type: \"tags\"     #新添加的\n\t ---\n","source":"_posts/环境搭建/hexo安装.md","raw":"---\ntitle: hexo安装记录\ndate: 2020-05-19 15:21:29\ntags: [杂项]\n---\n\n# hexo安装过程\n参考资料 https://zhuanlan.zhihu.com/p/187435941\n## nodejs\n版本：12.19.0，安装过程略\n\n## 安装hexo命令\n运行命令:\n\n\t npm install -g hexo-cli\n\n## 查看hexo版本\n运行命令:\n\n\t hexo --version\n\n得到以下输出\n\n\t hexo-cli: 4.2.0\n\t os: Windows_NT 10.0.19041 win32 x64\n\t node: 12.19.0\n\t v8: 7.8.279.23-node.44\n\t uv: 1.39.0\n\t zlib: 1.2.11\n\t brotli: 1.0.9\n\t ares: 1.16.0\n\t modules: 72\n\t nghttp2: 1.41.0\n\t napi: 7\n\t llhttp: 2.1.2\n\t http_parser: 2.9.3\n\t openssl: 1.1.1g\n\t cldr: 37.0\n\t icu: 67.1\n\t tz: 2019c\n\t unicode: 13.0\n\n## 初始化博客文件夹\n运行命令:\n\n\t hexo init blogDev\n\n## 安装package.json中的依赖\n\n运行命令：\n\n\t cd blogDev\n\t npm install\n\n## 修改_config.yml文件\n\n使用hexo-autonofollow插件来给外链添加nofollow属性\n\n在站点根目录下执行下列命令：\n\n\t npm install hexo-autonofollow --save\n\n## 修改git init文件\n去掉以下三项：\n\n\t db.json,\n\t Thumbs.db\n\t node-modules/\n\n## 支持latex\n卸载hexo自带的makrdown渲染工具：\n\n\t npm uninstall hexo-renderer-markded --save\n\n安装hexo-renderer-pandoc：\n\n\t npm install hexo-renderer-pandoc --save\n\n下载next主题：\n\n\t git clone https://github.com/iissnan/hexo-theme-next themes/next\n\n在_config.yml中将主题修改为next\n\n修改next主题下的_config.yml文件中mathjax部分。\n\n\t mathjax:\n\t\tenable: true\n\t\tper_page: false\n\t\tcdn: //cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML\n\n在执行hexo s时遇到报错：\n\n\t pandoc exited with code null\n\n因此在 https://github.com/jgm/pandoc/releases 重新下载pandoc安装，这个问题解决后，访问网页时又遇到输出代码的问题，于是又安装swig：\n\n\t npm i hexo-renderer-swig\n\n网页可以正常访问，但是不能加载图片，又安装asset-image:\n\n\t npm install hexo-asset-image --save\n\n还需要在package.json中手动修改asset-image版本如下，并重新执行npm install：\n\n\t \"hexo-asset-image\": \"^0.0.1\"\n\n\n## 侧边栏中文乱码\n是hexo-asset-image导致的，删除掉就好\n\n## 设置首页不显示全文\n在next的配置文件中，将下面的enable改成true\n\n\t auto_excerpt:\n\t\tenable: false\n\t\tlength: 150\n\n## 设置导航栏\n除了修改next主题下的config文件menu字段之外，还需要执行以下命令：\n\n\t hexo new page \"about\"\n\t hexo new page \"tags\"\n\t hexo new page \"categories\"\n\n## 翻页按钮显示不正常\n修改代码的位置: themes\\next\\layout\\_partials\\pagination.swig\n\n\n## Hexo next 主题配置右侧栏的分类和标签打开的是空白\ncategories 文件夹里面的 index.md 文件打开，修改（即添加一行）为：\n\n\t ---\n\t title: categories\n\t date: 2018-01-23 17:14:51\n\t type: \"categories\"   #新添加的\n\t ---\n同理，tags\n\n\t ---\n\t title: tags\n\t date: 2018-01-23 17:14:51\n\t type: \"tags\"     #新添加的\n\t ---\n","slug":"环境搭建/hexo安装","published":1,"updated":"2021-01-15T03:26:09.775Z","_id":"ckjxpc3s5001p44mq3smgc2y6","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"hexo安装过程\">hexo安装过程</h1>\r\n<p>参考资料 https://zhuanlan.zhihu.com/p/187435941 ## nodejs 版本：12.19.0，安装过程略</p>\r\n<h2 id=\"安装hexo命令\">安装hexo命令</h2>\r\n<p>运行命令:</p>\r\n<pre><code> npm install -g hexo-cli</code></pre>\r\n<h2 id=\"查看hexo版本\">查看hexo版本</h2>\r\n<p>运行命令:</p>\r\n<pre><code> hexo --version</code></pre>\r\n<p>得到以下输出</p>\r\n<pre><code> hexo-cli: 4.2.0\r\n os: Windows_NT 10.0.19041 win32 x64\r\n node: 12.19.0\r\n v8: 7.8.279.23-node.44\r\n uv: 1.39.0\r\n zlib: 1.2.11\r\n brotli: 1.0.9\r\n ares: 1.16.0\r\n modules: 72\r\n nghttp2: 1.41.0\r\n napi: 7\r\n llhttp: 2.1.2\r\n http_parser: 2.9.3\r\n openssl: 1.1.1g\r\n cldr: 37.0\r\n icu: 67.1\r\n tz: 2019c\r\n unicode: 13.0</code></pre>\r\n<h2 id=\"初始化博客文件夹\">初始化博客文件夹</h2>\r\n<p>运行命令:</p>\r\n<pre><code> hexo init blogDev</code></pre>\r\n<h2 id=\"安装package.json中的依赖\">安装package.json中的依赖</h2>\r\n<p>运行命令：</p>\r\n<pre><code> cd blogDev\r\n npm install</code></pre>\r\n<h2 id=\"修改_config.yml文件\">修改_config.yml文件</h2>\r\n<p>使用hexo-autonofollow插件来给外链添加nofollow属性</p>\r\n<p>在站点根目录下执行下列命令：</p>\r\n<pre><code> npm install hexo-autonofollow --save</code></pre>\r\n<h2 id=\"修改git-init文件\">修改git init文件</h2>\r\n<p>去掉以下三项：</p>\r\n<pre><code> db.json,\r\n Thumbs.db\r\n node-modules/</code></pre>\r\n<h2 id=\"支持latex\">支持latex</h2>\r\n<p>卸载hexo自带的makrdown渲染工具：</p>\r\n<pre><code> npm uninstall hexo-renderer-markded --save</code></pre>\r\n<p>安装hexo-renderer-pandoc：</p>\r\n<pre><code> npm install hexo-renderer-pandoc --save</code></pre>\r\n<p>下载next主题：</p>\r\n<pre><code> git clone https://github.com/iissnan/hexo-theme-next themes/next</code></pre>\r\n<p>在_config.yml中将主题修改为next</p>\r\n<p>修改next主题下的_config.yml文件中mathjax部分。</p>\r\n<pre><code> mathjax:\r\n    enable: true\r\n    per_page: false\r\n    cdn: //cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML</code></pre>\r\n<p>在执行hexo s时遇到报错：</p>\r\n<pre><code> pandoc exited with code null</code></pre>\r\n<p>因此在 https://github.com/jgm/pandoc/releases 重新下载pandoc安装，这个问题解决后，访问网页时又遇到输出代码的问题，于是又安装swig：</p>\r\n<pre><code> npm i hexo-renderer-swig</code></pre>\r\n<p>网页可以正常访问，但是不能加载图片，又安装asset-image:</p>\r\n<pre><code> npm install hexo-asset-image --save</code></pre>\r\n<p>还需要在package.json中手动修改asset-image版本如下，并重新执行npm install：</p>\r\n<pre><code> &quot;hexo-asset-image&quot;: &quot;^0.0.1&quot;</code></pre>\r\n<h2 id=\"侧边栏中文乱码\">侧边栏中文乱码</h2>\r\n<p>是hexo-asset-image导致的，删除掉就好</p>\r\n<h2 id=\"设置首页不显示全文\">设置首页不显示全文</h2>\r\n<p>在next的配置文件中，将下面的enable改成true</p>\r\n<pre><code> auto_excerpt:\r\n    enable: false\r\n    length: 150</code></pre>\r\n<h2 id=\"设置导航栏\">设置导航栏</h2>\r\n<p>除了修改next主题下的config文件menu字段之外，还需要执行以下命令：</p>\r\n<pre><code> hexo new page &quot;about&quot;\r\n hexo new page &quot;tags&quot;\r\n hexo new page &quot;categories&quot;</code></pre>\r\n<h2 id=\"翻页按钮显示不正常\">翻页按钮显示不正常</h2>\r\n<p>修改代码的位置: themes_partials.swig</p>\r\n<h2 id=\"hexo-next-主题配置右侧栏的分类和标签打开的是空白\">Hexo next 主题配置右侧栏的分类和标签打开的是空白</h2>\r\n<p>categories 文件夹里面的 index.md 文件打开，修改（即添加一行）为：</p>\r\n<pre><code> ---\r\n title: categories\r\n date: 2018-01-23 17:14:51\r\n type: &quot;categories&quot;   #新添加的\r\n ---</code></pre>\r\n<p>同理，tags</p>\r\n<pre><code> ---\r\n title: tags\r\n date: 2018-01-23 17:14:51\r\n type: &quot;tags&quot;     #新添加的\r\n ---</code></pre>\r\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"hexo安装过程\">hexo安装过程</h1>\r\n<p>参考资料 https://zhuanlan.zhihu.com/p/187435941 ## nodejs 版本：12.19.0，安装过程略</p>\r\n<h2 id=\"安装hexo命令\">安装hexo命令</h2>\r\n<p>运行命令:</p>\r\n<pre><code> npm install -g hexo-cli</code></pre>\r\n<h2 id=\"查看hexo版本\">查看hexo版本</h2>\r\n<p>运行命令:</p>\r\n<pre><code> hexo --version</code></pre>\r\n<p>得到以下输出</p>\r\n<pre><code> hexo-cli: 4.2.0\r\n os: Windows_NT 10.0.19041 win32 x64\r\n node: 12.19.0\r\n v8: 7.8.279.23-node.44\r\n uv: 1.39.0\r\n zlib: 1.2.11\r\n brotli: 1.0.9\r\n ares: 1.16.0\r\n modules: 72\r\n nghttp2: 1.41.0\r\n napi: 7\r\n llhttp: 2.1.2\r\n http_parser: 2.9.3\r\n openssl: 1.1.1g\r\n cldr: 37.0\r\n icu: 67.1\r\n tz: 2019c\r\n unicode: 13.0</code></pre>\r\n<h2 id=\"初始化博客文件夹\">初始化博客文件夹</h2>\r\n<p>运行命令:</p>\r\n<pre><code> hexo init blogDev</code></pre>\r\n<h2 id=\"安装package.json中的依赖\">安装package.json中的依赖</h2>\r\n<p>运行命令：</p>\r\n<pre><code> cd blogDev\r\n npm install</code></pre>\r\n<h2 id=\"修改_config.yml文件\">修改_config.yml文件</h2>\r\n<p>使用hexo-autonofollow插件来给外链添加nofollow属性</p>\r\n<p>在站点根目录下执行下列命令：</p>\r\n<pre><code> npm install hexo-autonofollow --save</code></pre>\r\n<h2 id=\"修改git-init文件\">修改git init文件</h2>\r\n<p>去掉以下三项：</p>\r\n<pre><code> db.json,\r\n Thumbs.db\r\n node-modules/</code></pre>\r\n<h2 id=\"支持latex\">支持latex</h2>\r\n<p>卸载hexo自带的makrdown渲染工具：</p>\r\n<pre><code> npm uninstall hexo-renderer-markded --save</code></pre>\r\n<p>安装hexo-renderer-pandoc：</p>\r\n<pre><code> npm install hexo-renderer-pandoc --save</code></pre>\r\n<p>下载next主题：</p>\r\n<pre><code> git clone https://github.com/iissnan/hexo-theme-next themes/next</code></pre>\r\n<p>在_config.yml中将主题修改为next</p>\r\n<p>修改next主题下的_config.yml文件中mathjax部分。</p>\r\n<pre><code> mathjax:\r\n    enable: true\r\n    per_page: false\r\n    cdn: //cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML</code></pre>\r\n<p>在执行hexo s时遇到报错：</p>\r\n<pre><code> pandoc exited with code null</code></pre>\r\n<p>因此在 https://github.com/jgm/pandoc/releases 重新下载pandoc安装，这个问题解决后，访问网页时又遇到输出代码的问题，于是又安装swig：</p>\r\n<pre><code> npm i hexo-renderer-swig</code></pre>\r\n<p>网页可以正常访问，但是不能加载图片，又安装asset-image:</p>\r\n<pre><code> npm install hexo-asset-image --save</code></pre>\r\n<p>还需要在package.json中手动修改asset-image版本如下，并重新执行npm install：</p>\r\n<pre><code> &quot;hexo-asset-image&quot;: &quot;^0.0.1&quot;</code></pre>\r\n<h2 id=\"侧边栏中文乱码\">侧边栏中文乱码</h2>\r\n<p>是hexo-asset-image导致的，删除掉就好</p>\r\n<h2 id=\"设置首页不显示全文\">设置首页不显示全文</h2>\r\n<p>在next的配置文件中，将下面的enable改成true</p>\r\n<pre><code> auto_excerpt:\r\n    enable: false\r\n    length: 150</code></pre>\r\n<h2 id=\"设置导航栏\">设置导航栏</h2>\r\n<p>除了修改next主题下的config文件menu字段之外，还需要执行以下命令：</p>\r\n<pre><code> hexo new page &quot;about&quot;\r\n hexo new page &quot;tags&quot;\r\n hexo new page &quot;categories&quot;</code></pre>\r\n<h2 id=\"翻页按钮显示不正常\">翻页按钮显示不正常</h2>\r\n<p>修改代码的位置: themes_partials.swig</p>\r\n<h2 id=\"hexo-next-主题配置右侧栏的分类和标签打开的是空白\">Hexo next 主题配置右侧栏的分类和标签打开的是空白</h2>\r\n<p>categories 文件夹里面的 index.md 文件打开，修改（即添加一行）为：</p>\r\n<pre><code> ---\r\n title: categories\r\n date: 2018-01-23 17:14:51\r\n type: &quot;categories&quot;   #新添加的\r\n ---</code></pre>\r\n<p>同理，tags</p>\r\n<pre><code> ---\r\n title: tags\r\n date: 2018-01-23 17:14:51\r\n type: &quot;tags&quot;     #新添加的\r\n ---</code></pre>\r\n"},{"title":"在win10上搭建python3和tensorflow-gpu环境","date":"2018-03-10T09:15:35.000Z","_content":"## 安装Anaconda2和Anaconda3\n因为需要用到一些python2的代码，因此决定同时安装Anaconda2和3，搭建python2和3共存的环境。\n首先安装Anaconda3作为主要环境，安装文件可以在Anaconda官网上下载，我把它安装在C盘下的Anaconda目录下，一路上没有什么特别需要注意的，两个选项都选中就好了，安装完后在Anaconda3安装目录下的envs文件夹下新建py2目录，如下图：\n{% asset_img Anaconda_install.png Anaconda2安装目录 %}\n安装好后就可以正常使用python2和Python3的环境了，并且可以非常简单的切换。\n只需在命令行输入如下命令即可切换到python2环境,这个py2就是上面在Anaconda3安装目录下的envs文件夹下的py2目录名。\n\n     activate py2\n\n然后用\n\n     deactivate py2\n\n就可以切换回python3环境\n## 安装cuda\n在cuda官网选择下载版本，我选择cuda9.0版本，我的电脑适合版本如下：\n{% asset_img cuda_download.png cuda版本 %}\n然后开始下载，1.4G，需要一定时间。\n下载完成后直接安装，我直接使用的精简安装，更改目录选在D盘下，然而并没有什么效果，还是装到了C盘，但是C盘下面还是会有一个文件夹，如图：\n{% asset_img cuda_dir.png cuda在C盘下的安装目录 %}\n安装完成后还需要下载cudnn，这个也在官网可以找到，需要注册并填写问卷才能下载。\n下载相应版本，之后解压出来，发现有几个文件夹和Cuda安装目录下的文件夹同名，那么直接把文件夹下的内容拷贝到Cuda安装目录中的对应文件夹下就好了。\n还要注意的是把下面的路径加入到path环境变量中：\n\n     C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\lib\\x64\n\ncuda的安装基本上就结束了。\n命令行使用\n\n     nvcc --version\n\n即可查看cuda版本如下：\n{% asset_img cuda_version.png cuda版本查看 %}\n\n## 安装tensorflow-gpu\n这个是最简单的一步，只需要在命令行下使用如下命令即可完成:\n\n     pip install tensorflow-gpu\n\n{% asset_img install_tensorflow-gpu.png 安装tensorflow-gpu %}\n等待即可\n\n这个时候遇到了一个bug，说是pip安装一些包的时候有编码问题，google了一下，按照[这里](https://segmentfault.com/q/1010000008071661/a-1020000008073594)的办法，修改了pip的__init__.py文件，完美解决。\n\n## 测试环境\n在交互模式下，引入tensorflow然后创建一个session，可以发现找到了一颗GPU（我电脑上就一个），说明环境搭建成功，接下来就可以开始愉快的玩耍了。\n{% asset_img test.png 环境测试 %}\n\n原创文章，引用请注明出处。","source":"_posts/环境搭建/在win10上搭建python3和tensorflow-gpu环境.md","raw":"---\ntitle: 在win10上搭建python3和tensorflow-gpu环境\ndate: 2018-03-10 17:15:35\ntags: [tensorflow-gpu, python3, win10]\ncategories: 环境搭建\n---\n## 安装Anaconda2和Anaconda3\n因为需要用到一些python2的代码，因此决定同时安装Anaconda2和3，搭建python2和3共存的环境。\n首先安装Anaconda3作为主要环境，安装文件可以在Anaconda官网上下载，我把它安装在C盘下的Anaconda目录下，一路上没有什么特别需要注意的，两个选项都选中就好了，安装完后在Anaconda3安装目录下的envs文件夹下新建py2目录，如下图：\n{% asset_img Anaconda_install.png Anaconda2安装目录 %}\n安装好后就可以正常使用python2和Python3的环境了，并且可以非常简单的切换。\n只需在命令行输入如下命令即可切换到python2环境,这个py2就是上面在Anaconda3安装目录下的envs文件夹下的py2目录名。\n\n     activate py2\n\n然后用\n\n     deactivate py2\n\n就可以切换回python3环境\n## 安装cuda\n在cuda官网选择下载版本，我选择cuda9.0版本，我的电脑适合版本如下：\n{% asset_img cuda_download.png cuda版本 %}\n然后开始下载，1.4G，需要一定时间。\n下载完成后直接安装，我直接使用的精简安装，更改目录选在D盘下，然而并没有什么效果，还是装到了C盘，但是C盘下面还是会有一个文件夹，如图：\n{% asset_img cuda_dir.png cuda在C盘下的安装目录 %}\n安装完成后还需要下载cudnn，这个也在官网可以找到，需要注册并填写问卷才能下载。\n下载相应版本，之后解压出来，发现有几个文件夹和Cuda安装目录下的文件夹同名，那么直接把文件夹下的内容拷贝到Cuda安装目录中的对应文件夹下就好了。\n还要注意的是把下面的路径加入到path环境变量中：\n\n     C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\lib\\x64\n\ncuda的安装基本上就结束了。\n命令行使用\n\n     nvcc --version\n\n即可查看cuda版本如下：\n{% asset_img cuda_version.png cuda版本查看 %}\n\n## 安装tensorflow-gpu\n这个是最简单的一步，只需要在命令行下使用如下命令即可完成:\n\n     pip install tensorflow-gpu\n\n{% asset_img install_tensorflow-gpu.png 安装tensorflow-gpu %}\n等待即可\n\n这个时候遇到了一个bug，说是pip安装一些包的时候有编码问题，google了一下，按照[这里](https://segmentfault.com/q/1010000008071661/a-1020000008073594)的办法，修改了pip的__init__.py文件，完美解决。\n\n## 测试环境\n在交互模式下，引入tensorflow然后创建一个session，可以发现找到了一颗GPU（我电脑上就一个），说明环境搭建成功，接下来就可以开始愉快的玩耍了。\n{% asset_img test.png 环境测试 %}\n\n原创文章，引用请注明出处。","slug":"环境搭建/在win10上搭建python3和tensorflow-gpu环境","published":1,"updated":"2018-08-17T02:57:17.781Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckjxpc3s6001r44mq05cy8v51","content":"<h2 id=\"安装anaconda2和anaconda3\">安装Anaconda2和Anaconda3</h2>\r\n<p>因为需要用到一些python2的代码，因此决定同时安装Anaconda2和3，搭建python2和3共存的环境。 首先安装Anaconda3作为主要环境，安装文件可以在Anaconda官网上下载，我把它安装在C盘下的Anaconda目录下，一路上没有什么特别需要注意的，两个选项都选中就好了，安装完后在Anaconda3安装目录下的envs文件夹下新建py2目录，如下图： <img src=\"/2018/03/10/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/%E5%9C%A8win10%E4%B8%8A%E6%90%AD%E5%BB%BApython3%E5%92%8Ctensorflow-gpu%E7%8E%AF%E5%A2%83/Anaconda_install.png\" class=\"\" title=\"Anaconda2安装目录\"> 安装好后就可以正常使用python2和Python3的环境了，并且可以非常简单的切换。 只需在命令行输入如下命令即可切换到python2环境,这个py2就是上面在Anaconda3安装目录下的envs文件夹下的py2目录名。</p>\r\n<pre><code> activate py2</code></pre>\r\n<p>然后用</p>\r\n<pre><code> deactivate py2</code></pre>\r\n<p>就可以切换回python3环境 ## 安装cuda 在cuda官网选择下载版本，我选择cuda9.0版本，我的电脑适合版本如下： <img src=\"/2018/03/10/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/%E5%9C%A8win10%E4%B8%8A%E6%90%AD%E5%BB%BApython3%E5%92%8Ctensorflow-gpu%E7%8E%AF%E5%A2%83/cuda_download.png\" class=\"\" title=\"cuda版本\"> 然后开始下载，1.4G，需要一定时间。 下载完成后直接安装，我直接使用的精简安装，更改目录选在D盘下，然而并没有什么效果，还是装到了C盘，但是C盘下面还是会有一个文件夹，如图： <img src=\"/2018/03/10/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/%E5%9C%A8win10%E4%B8%8A%E6%90%AD%E5%BB%BApython3%E5%92%8Ctensorflow-gpu%E7%8E%AF%E5%A2%83/cuda_dir.png\" class=\"\" title=\"cuda在C盘下的安装目录\"> 安装完成后还需要下载cudnn，这个也在官网可以找到，需要注册并填写问卷才能下载。 下载相应版本，之后解压出来，发现有几个文件夹和Cuda安装目录下的文件夹同名，那么直接把文件夹下的内容拷贝到Cuda安装目录中的对应文件夹下就好了。 还要注意的是把下面的路径加入到path环境变量中：</p>\r\n<pre><code> C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\lib\\x64</code></pre>\r\n<p>cuda的安装基本上就结束了。 命令行使用</p>\r\n<pre><code> nvcc --version</code></pre>\r\n<p>即可查看cuda版本如下： <img src=\"/2018/03/10/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/%E5%9C%A8win10%E4%B8%8A%E6%90%AD%E5%BB%BApython3%E5%92%8Ctensorflow-gpu%E7%8E%AF%E5%A2%83/cuda_version.png\" class=\"\" title=\"cuda版本查看\"></p>\r\n<h2 id=\"安装tensorflow-gpu\">安装tensorflow-gpu</h2>\r\n<p>这个是最简单的一步，只需要在命令行下使用如下命令即可完成:</p>\r\n<pre><code> pip install tensorflow-gpu</code></pre>\r\n<img src=\"/2018/03/10/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/%E5%9C%A8win10%E4%B8%8A%E6%90%AD%E5%BB%BApython3%E5%92%8Ctensorflow-gpu%E7%8E%AF%E5%A2%83/install_tensorflow-gpu.png\" class=\"\" title=\"安装tensorflow-gpu\">\r\n<p>等待即可</p>\r\n<p>这个时候遇到了一个bug，说是pip安装一些包的时候有编码问题，google了一下，按照<a href=\"https://segmentfault.com/q/1010000008071661/a-1020000008073594\">这里</a>的办法，修改了pip的__init__.py文件，完美解决。</p>\r\n<h2 id=\"测试环境\">测试环境</h2>\r\n<p>在交互模式下，引入tensorflow然后创建一个session，可以发现找到了一颗GPU（我电脑上就一个），说明环境搭建成功，接下来就可以开始愉快的玩耍了。 <img src=\"/2018/03/10/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/%E5%9C%A8win10%E4%B8%8A%E6%90%AD%E5%BB%BApython3%E5%92%8Ctensorflow-gpu%E7%8E%AF%E5%A2%83/test.png\" class=\"\" title=\"环境测试\"></p>\r\n<p>原创文章，引用请注明出处。</p>\r\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"安装anaconda2和anaconda3\">安装Anaconda2和Anaconda3</h2>\r\n<p>因为需要用到一些python2的代码，因此决定同时安装Anaconda2和3，搭建python2和3共存的环境。 首先安装Anaconda3作为主要环境，安装文件可以在Anaconda官网上下载，我把它安装在C盘下的Anaconda目录下，一路上没有什么特别需要注意的，两个选项都选中就好了，安装完后在Anaconda3安装目录下的envs文件夹下新建py2目录，如下图： <img src=\"/2018/03/10/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/%E5%9C%A8win10%E4%B8%8A%E6%90%AD%E5%BB%BApython3%E5%92%8Ctensorflow-gpu%E7%8E%AF%E5%A2%83/Anaconda_install.png\" class=\"\" title=\"Anaconda2安装目录\"> 安装好后就可以正常使用python2和Python3的环境了，并且可以非常简单的切换。 只需在命令行输入如下命令即可切换到python2环境,这个py2就是上面在Anaconda3安装目录下的envs文件夹下的py2目录名。</p>\r\n<pre><code> activate py2</code></pre>\r\n<p>然后用</p>\r\n<pre><code> deactivate py2</code></pre>\r\n<p>就可以切换回python3环境 ## 安装cuda 在cuda官网选择下载版本，我选择cuda9.0版本，我的电脑适合版本如下： <img src=\"/2018/03/10/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/%E5%9C%A8win10%E4%B8%8A%E6%90%AD%E5%BB%BApython3%E5%92%8Ctensorflow-gpu%E7%8E%AF%E5%A2%83/cuda_download.png\" class=\"\" title=\"cuda版本\"> 然后开始下载，1.4G，需要一定时间。 下载完成后直接安装，我直接使用的精简安装，更改目录选在D盘下，然而并没有什么效果，还是装到了C盘，但是C盘下面还是会有一个文件夹，如图： <img src=\"/2018/03/10/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/%E5%9C%A8win10%E4%B8%8A%E6%90%AD%E5%BB%BApython3%E5%92%8Ctensorflow-gpu%E7%8E%AF%E5%A2%83/cuda_dir.png\" class=\"\" title=\"cuda在C盘下的安装目录\"> 安装完成后还需要下载cudnn，这个也在官网可以找到，需要注册并填写问卷才能下载。 下载相应版本，之后解压出来，发现有几个文件夹和Cuda安装目录下的文件夹同名，那么直接把文件夹下的内容拷贝到Cuda安装目录中的对应文件夹下就好了。 还要注意的是把下面的路径加入到path环境变量中：</p>\r\n<pre><code> C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\lib\\x64</code></pre>\r\n<p>cuda的安装基本上就结束了。 命令行使用</p>\r\n<pre><code> nvcc --version</code></pre>\r\n<p>即可查看cuda版本如下： <img src=\"/2018/03/10/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/%E5%9C%A8win10%E4%B8%8A%E6%90%AD%E5%BB%BApython3%E5%92%8Ctensorflow-gpu%E7%8E%AF%E5%A2%83/cuda_version.png\" class=\"\" title=\"cuda版本查看\"></p>\r\n<h2 id=\"安装tensorflow-gpu\">安装tensorflow-gpu</h2>\r\n<p>这个是最简单的一步，只需要在命令行下使用如下命令即可完成:</p>\r\n<pre><code> pip install tensorflow-gpu</code></pre>\r\n<img src=\"/2018/03/10/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/%E5%9C%A8win10%E4%B8%8A%E6%90%AD%E5%BB%BApython3%E5%92%8Ctensorflow-gpu%E7%8E%AF%E5%A2%83/install_tensorflow-gpu.png\" class=\"\" title=\"安装tensorflow-gpu\">\r\n<p>等待即可</p>\r\n<p>这个时候遇到了一个bug，说是pip安装一些包的时候有编码问题，google了一下，按照<a href=\"https://segmentfault.com/q/1010000008071661/a-1020000008073594\">这里</a>的办法，修改了pip的__init__.py文件，完美解决。</p>\r\n<h2 id=\"测试环境\">测试环境</h2>\r\n<p>在交互模式下，引入tensorflow然后创建一个session，可以发现找到了一颗GPU（我电脑上就一个），说明环境搭建成功，接下来就可以开始愉快的玩耍了。 <img src=\"/2018/03/10/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/%E5%9C%A8win10%E4%B8%8A%E6%90%AD%E5%BB%BApython3%E5%92%8Ctensorflow-gpu%E7%8E%AF%E5%A2%83/test.png\" class=\"\" title=\"环境测试\"></p>\r\n<p>原创文章，引用请注明出处。</p>\r\n"},{"title":"论文翻译之《Perceptual Losses for Real-Time Style Transferand Super-Resolution》","date":"2018-05-08T12:24:09.000Z","_content":"# 在实时风格转换和超分辨率上使用感知损失函数\n\n### 作者：Justin Johnson, Alexandre Alahi, and Li Fei-Fei\n\n### 摘要\n\n本文研究图像转换问题，即将一个输入图像转化到一个输出图像，最近在这个问题上使用的典型方法是训练一个使用在输出和目标图像上逐像素计算损失函数（per-pixel loss）的卷积神经网络。有研究表明高质量的图像可以通过基于预训练网络提取的高级特征定义和优化感知损失函数（perceptual loss）来生成。我们结合这些方法的有点，并且提出使用干事损失函数来训练前向网络来做图像风格转换任务，我们展示一个由Gatys等人提出的训练于解决优化问题的前向网络来做图像风格转换器的结果，相比于这个方法，我们的方法可以得到质量大致相同的图像，但是我们的速度比这个方法大约快三个数量级。我们还在单图像超分辨率问题上进行的实验，将逐像素计算的损失函数替换为感知损失函数，我们得到了视觉上令人满意的效果。\n\n### 关键词：\n\n风格转换器，超分辨率，深度学习\n\n## 简介\n\n许多经典问题可以被构筑为图像风格转换任务，即一个系统接收一些输入图像，并且将其转换到输出图像，图像处理的例子包括去躁、超分辨率、着色，这些问题中，输入图像是一个低质量图像（噪声多、分辨率低或者灰度图），而输出是一个高质量彩色图像。机器视觉问题中的例子包括语义分隔和深度估计（depth estimation）,这里的输入时彩色图像，输出图像编码了场景中的语义或者集合信息。\n一个解决图像转换任务的方法是用有监督方法训练一个前向卷积神经网络，使用逐像素计算的损失函数来测量输出和目标图像之间的距离，例如被Dong等人用于超分辨率问题[1]，被Cheng等人用于着色问题[2,3]，被Long等人用于分割问题[4]，被Eigen等人用于深度和表面正则预测（depth and surface normal prediction）[5,6]，这些方法在测试阶段非常高效，仅仅需要一个前向的通过一个训练好的网络。\n\n然而，使用这些方法逐像素计算的损失函数不能捕获输出图像和目标图像感知上的差异。例如，考虑两个在对方基础上平移一个像素的独立图像，虽然他们在感知上相似，但是在他们之间按照逐像素计算的损失函数来度量差距的话，会显示两个图像非常不同。\n\n{% asset_img fig1.png fig.1 风格转换（上）和4倍超分辨率（下）结果示例，在风格转换上，我们实现了和Gatys等人相似的结果[11],但是比他们的速度快三个数量级。在超分辨率问题上，我们的方法使用感知损失函数可以比其他使用逐像素损失函数的方法更好的重构细节%}\n\n最近的研究显示使用感知损失函数生成的高质量图像不是基于图像像素之间的差异，而是基于预训练的卷积神经网络提取的图像的高级特征之间的差异。图像通过最小化一个损失函数来生成。这种策略已经被Mahendran等人用于特征反演[7]，被Simonyan和Yosinski等人用于特征可视化[8][9],被Gatys等人用于上下文分析和风格转换[10][11][12]，这些方法生成了高质量的图像，但是在用于解决一个优化问题的时候很慢。\n\n在本文中我们结合了两种方法的优点，我们训练一个用于图像转换任务的前向转换网络，但是不仅只是在低级的像素信息上使用逐像素的损失函数，我们在预训练的损失网络提取出的高级特征上使用感知损失函数。在训练过程中，感知损失函数在测试实时转换网络运行时在度量图像相似度的效果上比逐像素损失函数有更好的鲁棒性。\n\n我们在两个任务上进行了实验：风格转换和单图像超分辨率，两者本质上是病态的。在风格转换任务上，这里没有唯一正确的输出，而在超分辨率图像上，一个低分辨率输入可以生成很多高分辨率输出图像。l两个任务都需要在输入图像上进行语意推理。在风格转换任务中，虽然在纹理和颜色上有很大变化，但输出必须在语义上和输入相似。在超分辨率任务上，需要从视觉模糊的低分辨率图片上推断出细节。原则上，训练好的用于这两个任务的一个大容量的神经网络可以隐式的学习到推断相关的语义；然而实际上我们不需要从头学习，使用感知损失函数允许语义转换的知识从损失网络传递到转换网络。\n\n在图像风格转换优化问题[11]上；我们的结果在使用目标函数值衡量的质量上和[11]相似，但是快了三个数量级。对于超分辨率问题，我们发现使用感知损失函数替代逐像素计算的损失给出了在放大4倍和8倍的超分辨率问题上的令人满意的结果。\n\n## 相关工作\n\n     前馈的图像转换\n\n在最近几年中，很多不同的图像转换任务使用逐像素计算的损失函数来训练。\n\n语义分隔方法[4,6,14-17]使用一个逐像素的分类损失，通过全卷积的方式在输入图像上运行网络来产生密集的场景标签，最近的深度和表层正态估计（normal estimation）方法[6,19]都是相似的，依靠使用一个前向的用逐像素的回归[5,6]或者分类损失函数[19]训练的前向卷积网络。一些方法在使用逐像素损失函数之外还使用图像梯度的惩罚[6]，将复发层和网络的其余部分一起训练来构建CRF推断[17]。或者使用一个CRF损失层[8]来加强输出中的局部一致性。\n\n我们的风格转换网络的架构来源于[4]和[16]，使用网络内下采样来减少\n特征映射的空间范围，然后进行网络内上采样来产生最终的输出图像。\n\n     感知优化\n\n最近大量的论文已经着手使用感知目标来优化图像的生成，依赖于从一个卷积网络中提取出的高级特征。图像可以根据最大化分类预测值[8,9]或者为了理解函数编码而是用个别特征[9]的方向来生成。相似的优化技术可以被用于生成高度混淆的图片[20,21]。\n\nMahendran和Vedaldi[7]使用最小化一个特征重构损失反转卷积网络的特征来理解不同网络层保留的图像信息;相似的方法更早被用于反转局部二值描述[22.23]以及HOG特征[24]\n\nDosovitskiy和Brox的研究[25]和我们非常相关，他们训练了一个前向神经网络来反转卷积特征，迅速的估计一个由[7]提出的优化问题的解。然而，他们的前向网络使用的是逐像素计算的重构损失，而我们直接优化了[7]中的特征重构损失。\n\n     风格转换\n\nGatys等人[11]实现了艺术风格的转换，将一个图片的内容通过最小化[7]中的特征重构损失函数以及一个同样基于预训练的卷积网络提取的特征的风格重构损失来实现和另一个图片的风格结合；在这之前，有人用类似的方法来做纹理合成[10]。他们在这个问题上得到了非常好的结果，但是由于优化问题的每一步都需要在预训练的网络上前向和反向传播共两次，他们的方法需要非常大的计算量。为了减少这个计算负担，我们训练了一个前向网络来迅速估计他们的优化问题的解。论文[26,27]和我们同时提出了前向方法来快速转换图像风格。\n\n     图像超分辨率\n\n图像差分辨率问题是产生了许多技术的一个经典问题。Yang等人[28]在广泛采用卷积神经网络之前，对之前的技术做了广泛的评估。他们将超分辨率技术分组为基于预测的技术（biliner，bicubic，Lanczos，[29]）,基于边界的方法[30,31]，统计方法[32-34]，贴片型方法[30,35-41]，以及稀疏字典方法[42,43]。最近论文[1]在但图像超分辨率问题上使用了一个使用逐像素欧几里得损失的三层卷积神经网络实现了非常好的效果。其他近期的方法包括[44-46]。\n\n## 方法\n\n如fig.2所示，我们的系统包括两个部分：一个图像转换网络以及一个损失网络\n\n{% asset_img fig2.png fig.2 系统概况，我们训练了一个图像转换网络来把输入图像转换到输出图像。我们使用一个预训练的图像分类网络来定义测量图像之间内容和风格上感知差异的感知损失函数。这个感知网络在训练过程中是固定不变的%}\n\n三个月过去了，发现翻译这篇论文没有啥意义，因此终止于此。\n\n引用请注明出处。\n","source":"_posts/论文阅读/论文翻译之《Perceptual-Losses-for-Real-Time-Style-Transferand-Super-Resolution》.md","raw":"---\ntitle: 论文翻译之《Perceptual Losses for Real-Time Style Transferand Super-Resolution》\ndate: 2018-05-08 20:24:09\ntags: 论文翻译\n---\n# 在实时风格转换和超分辨率上使用感知损失函数\n\n### 作者：Justin Johnson, Alexandre Alahi, and Li Fei-Fei\n\n### 摘要\n\n本文研究图像转换问题，即将一个输入图像转化到一个输出图像，最近在这个问题上使用的典型方法是训练一个使用在输出和目标图像上逐像素计算损失函数（per-pixel loss）的卷积神经网络。有研究表明高质量的图像可以通过基于预训练网络提取的高级特征定义和优化感知损失函数（perceptual loss）来生成。我们结合这些方法的有点，并且提出使用干事损失函数来训练前向网络来做图像风格转换任务，我们展示一个由Gatys等人提出的训练于解决优化问题的前向网络来做图像风格转换器的结果，相比于这个方法，我们的方法可以得到质量大致相同的图像，但是我们的速度比这个方法大约快三个数量级。我们还在单图像超分辨率问题上进行的实验，将逐像素计算的损失函数替换为感知损失函数，我们得到了视觉上令人满意的效果。\n\n### 关键词：\n\n风格转换器，超分辨率，深度学习\n\n## 简介\n\n许多经典问题可以被构筑为图像风格转换任务，即一个系统接收一些输入图像，并且将其转换到输出图像，图像处理的例子包括去躁、超分辨率、着色，这些问题中，输入图像是一个低质量图像（噪声多、分辨率低或者灰度图），而输出是一个高质量彩色图像。机器视觉问题中的例子包括语义分隔和深度估计（depth estimation）,这里的输入时彩色图像，输出图像编码了场景中的语义或者集合信息。\n一个解决图像转换任务的方法是用有监督方法训练一个前向卷积神经网络，使用逐像素计算的损失函数来测量输出和目标图像之间的距离，例如被Dong等人用于超分辨率问题[1]，被Cheng等人用于着色问题[2,3]，被Long等人用于分割问题[4]，被Eigen等人用于深度和表面正则预测（depth and surface normal prediction）[5,6]，这些方法在测试阶段非常高效，仅仅需要一个前向的通过一个训练好的网络。\n\n然而，使用这些方法逐像素计算的损失函数不能捕获输出图像和目标图像感知上的差异。例如，考虑两个在对方基础上平移一个像素的独立图像，虽然他们在感知上相似，但是在他们之间按照逐像素计算的损失函数来度量差距的话，会显示两个图像非常不同。\n\n{% asset_img fig1.png fig.1 风格转换（上）和4倍超分辨率（下）结果示例，在风格转换上，我们实现了和Gatys等人相似的结果[11],但是比他们的速度快三个数量级。在超分辨率问题上，我们的方法使用感知损失函数可以比其他使用逐像素损失函数的方法更好的重构细节%}\n\n最近的研究显示使用感知损失函数生成的高质量图像不是基于图像像素之间的差异，而是基于预训练的卷积神经网络提取的图像的高级特征之间的差异。图像通过最小化一个损失函数来生成。这种策略已经被Mahendran等人用于特征反演[7]，被Simonyan和Yosinski等人用于特征可视化[8][9],被Gatys等人用于上下文分析和风格转换[10][11][12]，这些方法生成了高质量的图像，但是在用于解决一个优化问题的时候很慢。\n\n在本文中我们结合了两种方法的优点，我们训练一个用于图像转换任务的前向转换网络，但是不仅只是在低级的像素信息上使用逐像素的损失函数，我们在预训练的损失网络提取出的高级特征上使用感知损失函数。在训练过程中，感知损失函数在测试实时转换网络运行时在度量图像相似度的效果上比逐像素损失函数有更好的鲁棒性。\n\n我们在两个任务上进行了实验：风格转换和单图像超分辨率，两者本质上是病态的。在风格转换任务上，这里没有唯一正确的输出，而在超分辨率图像上，一个低分辨率输入可以生成很多高分辨率输出图像。l两个任务都需要在输入图像上进行语意推理。在风格转换任务中，虽然在纹理和颜色上有很大变化，但输出必须在语义上和输入相似。在超分辨率任务上，需要从视觉模糊的低分辨率图片上推断出细节。原则上，训练好的用于这两个任务的一个大容量的神经网络可以隐式的学习到推断相关的语义；然而实际上我们不需要从头学习，使用感知损失函数允许语义转换的知识从损失网络传递到转换网络。\n\n在图像风格转换优化问题[11]上；我们的结果在使用目标函数值衡量的质量上和[11]相似，但是快了三个数量级。对于超分辨率问题，我们发现使用感知损失函数替代逐像素计算的损失给出了在放大4倍和8倍的超分辨率问题上的令人满意的结果。\n\n## 相关工作\n\n     前馈的图像转换\n\n在最近几年中，很多不同的图像转换任务使用逐像素计算的损失函数来训练。\n\n语义分隔方法[4,6,14-17]使用一个逐像素的分类损失，通过全卷积的方式在输入图像上运行网络来产生密集的场景标签，最近的深度和表层正态估计（normal estimation）方法[6,19]都是相似的，依靠使用一个前向的用逐像素的回归[5,6]或者分类损失函数[19]训练的前向卷积网络。一些方法在使用逐像素损失函数之外还使用图像梯度的惩罚[6]，将复发层和网络的其余部分一起训练来构建CRF推断[17]。或者使用一个CRF损失层[8]来加强输出中的局部一致性。\n\n我们的风格转换网络的架构来源于[4]和[16]，使用网络内下采样来减少\n特征映射的空间范围，然后进行网络内上采样来产生最终的输出图像。\n\n     感知优化\n\n最近大量的论文已经着手使用感知目标来优化图像的生成，依赖于从一个卷积网络中提取出的高级特征。图像可以根据最大化分类预测值[8,9]或者为了理解函数编码而是用个别特征[9]的方向来生成。相似的优化技术可以被用于生成高度混淆的图片[20,21]。\n\nMahendran和Vedaldi[7]使用最小化一个特征重构损失反转卷积网络的特征来理解不同网络层保留的图像信息;相似的方法更早被用于反转局部二值描述[22.23]以及HOG特征[24]\n\nDosovitskiy和Brox的研究[25]和我们非常相关，他们训练了一个前向神经网络来反转卷积特征，迅速的估计一个由[7]提出的优化问题的解。然而，他们的前向网络使用的是逐像素计算的重构损失，而我们直接优化了[7]中的特征重构损失。\n\n     风格转换\n\nGatys等人[11]实现了艺术风格的转换，将一个图片的内容通过最小化[7]中的特征重构损失函数以及一个同样基于预训练的卷积网络提取的特征的风格重构损失来实现和另一个图片的风格结合；在这之前，有人用类似的方法来做纹理合成[10]。他们在这个问题上得到了非常好的结果，但是由于优化问题的每一步都需要在预训练的网络上前向和反向传播共两次，他们的方法需要非常大的计算量。为了减少这个计算负担，我们训练了一个前向网络来迅速估计他们的优化问题的解。论文[26,27]和我们同时提出了前向方法来快速转换图像风格。\n\n     图像超分辨率\n\n图像差分辨率问题是产生了许多技术的一个经典问题。Yang等人[28]在广泛采用卷积神经网络之前，对之前的技术做了广泛的评估。他们将超分辨率技术分组为基于预测的技术（biliner，bicubic，Lanczos，[29]）,基于边界的方法[30,31]，统计方法[32-34]，贴片型方法[30,35-41]，以及稀疏字典方法[42,43]。最近论文[1]在但图像超分辨率问题上使用了一个使用逐像素欧几里得损失的三层卷积神经网络实现了非常好的效果。其他近期的方法包括[44-46]。\n\n## 方法\n\n如fig.2所示，我们的系统包括两个部分：一个图像转换网络以及一个损失网络\n\n{% asset_img fig2.png fig.2 系统概况，我们训练了一个图像转换网络来把输入图像转换到输出图像。我们使用一个预训练的图像分类网络来定义测量图像之间内容和风格上感知差异的感知损失函数。这个感知网络在训练过程中是固定不变的%}\n\n三个月过去了，发现翻译这篇论文没有啥意义，因此终止于此。\n\n引用请注明出处。\n","slug":"论文阅读/论文翻译之《Perceptual-Losses-for-Real-Time-Style-Transferand-Super-Resolution》","published":1,"updated":"2019-01-14T12:38:54.736Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckjxpc3s7001u44mq3qtn5s11","content":"<h1 id=\"在实时风格转换和超分辨率上使用感知损失函数\">在实时风格转换和超分辨率上使用感知损失函数</h1>\r\n<h3 id=\"作者justin-johnson-alexandre-alahi-and-li-fei-fei\">作者：Justin Johnson, Alexandre Alahi, and Li Fei-Fei</h3>\r\n<h3 id=\"摘要\">摘要</h3>\r\n<p>本文研究图像转换问题，即将一个输入图像转化到一个输出图像，最近在这个问题上使用的典型方法是训练一个使用在输出和目标图像上逐像素计算损失函数（per-pixel loss）的卷积神经网络。有研究表明高质量的图像可以通过基于预训练网络提取的高级特征定义和优化感知损失函数（perceptual loss）来生成。我们结合这些方法的有点，并且提出使用干事损失函数来训练前向网络来做图像风格转换任务，我们展示一个由Gatys等人提出的训练于解决优化问题的前向网络来做图像风格转换器的结果，相比于这个方法，我们的方法可以得到质量大致相同的图像，但是我们的速度比这个方法大约快三个数量级。我们还在单图像超分辨率问题上进行的实验，将逐像素计算的损失函数替换为感知损失函数，我们得到了视觉上令人满意的效果。</p>\r\n<h3 id=\"关键词\">关键词：</h3>\r\n<p>风格转换器，超分辨率，深度学习</p>\r\n<h2 id=\"简介\">简介</h2>\r\n<p>许多经典问题可以被构筑为图像风格转换任务，即一个系统接收一些输入图像，并且将其转换到输出图像，图像处理的例子包括去躁、超分辨率、着色，这些问题中，输入图像是一个低质量图像（噪声多、分辨率低或者灰度图），而输出是一个高质量彩色图像。机器视觉问题中的例子包括语义分隔和深度估计（depth estimation）,这里的输入时彩色图像，输出图像编码了场景中的语义或者集合信息。 一个解决图像转换任务的方法是用有监督方法训练一个前向卷积神经网络，使用逐像素计算的损失函数来测量输出和目标图像之间的距离，例如被Dong等人用于超分辨率问题[1]，被Cheng等人用于着色问题[2,3]，被Long等人用于分割问题[4]，被Eigen等人用于深度和表面正则预测（depth and surface normal prediction）[5,6]，这些方法在测试阶段非常高效，仅仅需要一个前向的通过一个训练好的网络。</p>\r\n<p>然而，使用这些方法逐像素计算的损失函数不能捕获输出图像和目标图像感知上的差异。例如，考虑两个在对方基础上平移一个像素的独立图像，虽然他们在感知上相似，但是在他们之间按照逐像素计算的损失函数来度量差距的话，会显示两个图像非常不同。</p>\r\n<img src=\"/2018/05/08/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E4%B9%8B%E3%80%8APerceptual-Losses-for-Real-Time-Style-Transferand-Super-Resolution%E3%80%8B/fig1.png\" class=\"\" title=\"fig.1 风格转换（上）和4倍超分辨率（下）结果示例，在风格转换上，我们实现了和Gatys等人相似的结果[11],但是比他们的速度快三个数量级。在超分辨率问题上，我们的方法使用感知损失函数可以比其他使用逐像素损失函数的方法更好的重构细节\">\r\n<p>最近的研究显示使用感知损失函数生成的高质量图像不是基于图像像素之间的差异，而是基于预训练的卷积神经网络提取的图像的高级特征之间的差异。图像通过最小化一个损失函数来生成。这种策略已经被Mahendran等人用于特征反演[7]，被Simonyan和Yosinski等人用于特征可视化[8][9],被Gatys等人用于上下文分析和风格转换[10][11][12]，这些方法生成了高质量的图像，但是在用于解决一个优化问题的时候很慢。</p>\r\n<p>在本文中我们结合了两种方法的优点，我们训练一个用于图像转换任务的前向转换网络，但是不仅只是在低级的像素信息上使用逐像素的损失函数，我们在预训练的损失网络提取出的高级特征上使用感知损失函数。在训练过程中，感知损失函数在测试实时转换网络运行时在度量图像相似度的效果上比逐像素损失函数有更好的鲁棒性。</p>\r\n<p>我们在两个任务上进行了实验：风格转换和单图像超分辨率，两者本质上是病态的。在风格转换任务上，这里没有唯一正确的输出，而在超分辨率图像上，一个低分辨率输入可以生成很多高分辨率输出图像。l两个任务都需要在输入图像上进行语意推理。在风格转换任务中，虽然在纹理和颜色上有很大变化，但输出必须在语义上和输入相似。在超分辨率任务上，需要从视觉模糊的低分辨率图片上推断出细节。原则上，训练好的用于这两个任务的一个大容量的神经网络可以隐式的学习到推断相关的语义；然而实际上我们不需要从头学习，使用感知损失函数允许语义转换的知识从损失网络传递到转换网络。</p>\r\n<p>在图像风格转换优化问题[11]上；我们的结果在使用目标函数值衡量的质量上和[11]相似，但是快了三个数量级。对于超分辨率问题，我们发现使用感知损失函数替代逐像素计算的损失给出了在放大4倍和8倍的超分辨率问题上的令人满意的结果。</p>\r\n<h2 id=\"相关工作\">相关工作</h2>\r\n<pre><code> 前馈的图像转换</code></pre>\r\n<p>在最近几年中，很多不同的图像转换任务使用逐像素计算的损失函数来训练。</p>\r\n<p>语义分隔方法[4,6,14-17]使用一个逐像素的分类损失，通过全卷积的方式在输入图像上运行网络来产生密集的场景标签，最近的深度和表层正态估计（normal estimation）方法[6,19]都是相似的，依靠使用一个前向的用逐像素的回归[5,6]或者分类损失函数[19]训练的前向卷积网络。一些方法在使用逐像素损失函数之外还使用图像梯度的惩罚[6]，将复发层和网络的其余部分一起训练来构建CRF推断[17]。或者使用一个CRF损失层[8]来加强输出中的局部一致性。</p>\r\n<p>我们的风格转换网络的架构来源于[4]和[16]，使用网络内下采样来减少 特征映射的空间范围，然后进行网络内上采样来产生最终的输出图像。</p>\r\n<pre><code> 感知优化</code></pre>\r\n<p>最近大量的论文已经着手使用感知目标来优化图像的生成，依赖于从一个卷积网络中提取出的高级特征。图像可以根据最大化分类预测值[8,9]或者为了理解函数编码而是用个别特征[9]的方向来生成。相似的优化技术可以被用于生成高度混淆的图片[20,21]。</p>\r\n<p>Mahendran和Vedaldi[7]使用最小化一个特征重构损失反转卷积网络的特征来理解不同网络层保留的图像信息;相似的方法更早被用于反转局部二值描述[22.23]以及HOG特征[24]</p>\r\n<p>Dosovitskiy和Brox的研究[25]和我们非常相关，他们训练了一个前向神经网络来反转卷积特征，迅速的估计一个由[7]提出的优化问题的解。然而，他们的前向网络使用的是逐像素计算的重构损失，而我们直接优化了[7]中的特征重构损失。</p>\r\n<pre><code> 风格转换</code></pre>\r\n<p>Gatys等人[11]实现了艺术风格的转换，将一个图片的内容通过最小化[7]中的特征重构损失函数以及一个同样基于预训练的卷积网络提取的特征的风格重构损失来实现和另一个图片的风格结合；在这之前，有人用类似的方法来做纹理合成[10]。他们在这个问题上得到了非常好的结果，但是由于优化问题的每一步都需要在预训练的网络上前向和反向传播共两次，他们的方法需要非常大的计算量。为了减少这个计算负担，我们训练了一个前向网络来迅速估计他们的优化问题的解。论文[26,27]和我们同时提出了前向方法来快速转换图像风格。</p>\r\n<pre><code> 图像超分辨率</code></pre>\r\n<p>图像差分辨率问题是产生了许多技术的一个经典问题。Yang等人[28]在广泛采用卷积神经网络之前，对之前的技术做了广泛的评估。他们将超分辨率技术分组为基于预测的技术（biliner，bicubic，Lanczos，[29]）,基于边界的方法[30,31]，统计方法[32-34]，贴片型方法[30,35-41]，以及稀疏字典方法[42,43]。最近论文[1]在但图像超分辨率问题上使用了一个使用逐像素欧几里得损失的三层卷积神经网络实现了非常好的效果。其他近期的方法包括[44-46]。</p>\r\n<h2 id=\"方法\">方法</h2>\r\n<p>如fig.2所示，我们的系统包括两个部分：一个图像转换网络以及一个损失网络</p>\r\n<img src=\"/2018/05/08/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E4%B9%8B%E3%80%8APerceptual-Losses-for-Real-Time-Style-Transferand-Super-Resolution%E3%80%8B/fig2.png\" class=\"\" title=\"fig.2 系统概况，我们训练了一个图像转换网络来把输入图像转换到输出图像。我们使用一个预训练的图像分类网络来定义测量图像之间内容和风格上感知差异的感知损失函数。这个感知网络在训练过程中是固定不变的\">\r\n<p>三个月过去了，发现翻译这篇论文没有啥意义，因此终止于此。</p>\r\n<p>引用请注明出处。</p>\r\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"在实时风格转换和超分辨率上使用感知损失函数\">在实时风格转换和超分辨率上使用感知损失函数</h1>\r\n<h3 id=\"作者justin-johnson-alexandre-alahi-and-li-fei-fei\">作者：Justin Johnson, Alexandre Alahi, and Li Fei-Fei</h3>\r\n<h3 id=\"摘要\">摘要</h3>\r\n<p>本文研究图像转换问题，即将一个输入图像转化到一个输出图像，最近在这个问题上使用的典型方法是训练一个使用在输出和目标图像上逐像素计算损失函数（per-pixel loss）的卷积神经网络。有研究表明高质量的图像可以通过基于预训练网络提取的高级特征定义和优化感知损失函数（perceptual loss）来生成。我们结合这些方法的有点，并且提出使用干事损失函数来训练前向网络来做图像风格转换任务，我们展示一个由Gatys等人提出的训练于解决优化问题的前向网络来做图像风格转换器的结果，相比于这个方法，我们的方法可以得到质量大致相同的图像，但是我们的速度比这个方法大约快三个数量级。我们还在单图像超分辨率问题上进行的实验，将逐像素计算的损失函数替换为感知损失函数，我们得到了视觉上令人满意的效果。</p>\r\n<h3 id=\"关键词\">关键词：</h3>\r\n<p>风格转换器，超分辨率，深度学习</p>\r\n<h2 id=\"简介\">简介</h2>\r\n<p>许多经典问题可以被构筑为图像风格转换任务，即一个系统接收一些输入图像，并且将其转换到输出图像，图像处理的例子包括去躁、超分辨率、着色，这些问题中，输入图像是一个低质量图像（噪声多、分辨率低或者灰度图），而输出是一个高质量彩色图像。机器视觉问题中的例子包括语义分隔和深度估计（depth estimation）,这里的输入时彩色图像，输出图像编码了场景中的语义或者集合信息。 一个解决图像转换任务的方法是用有监督方法训练一个前向卷积神经网络，使用逐像素计算的损失函数来测量输出和目标图像之间的距离，例如被Dong等人用于超分辨率问题[1]，被Cheng等人用于着色问题[2,3]，被Long等人用于分割问题[4]，被Eigen等人用于深度和表面正则预测（depth and surface normal prediction）[5,6]，这些方法在测试阶段非常高效，仅仅需要一个前向的通过一个训练好的网络。</p>\r\n<p>然而，使用这些方法逐像素计算的损失函数不能捕获输出图像和目标图像感知上的差异。例如，考虑两个在对方基础上平移一个像素的独立图像，虽然他们在感知上相似，但是在他们之间按照逐像素计算的损失函数来度量差距的话，会显示两个图像非常不同。</p>\r\n<img src=\"/2018/05/08/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E4%B9%8B%E3%80%8APerceptual-Losses-for-Real-Time-Style-Transferand-Super-Resolution%E3%80%8B/fig1.png\" class=\"\" title=\"fig.1 风格转换（上）和4倍超分辨率（下）结果示例，在风格转换上，我们实现了和Gatys等人相似的结果[11],但是比他们的速度快三个数量级。在超分辨率问题上，我们的方法使用感知损失函数可以比其他使用逐像素损失函数的方法更好的重构细节\">\r\n<p>最近的研究显示使用感知损失函数生成的高质量图像不是基于图像像素之间的差异，而是基于预训练的卷积神经网络提取的图像的高级特征之间的差异。图像通过最小化一个损失函数来生成。这种策略已经被Mahendran等人用于特征反演[7]，被Simonyan和Yosinski等人用于特征可视化[8][9],被Gatys等人用于上下文分析和风格转换[10][11][12]，这些方法生成了高质量的图像，但是在用于解决一个优化问题的时候很慢。</p>\r\n<p>在本文中我们结合了两种方法的优点，我们训练一个用于图像转换任务的前向转换网络，但是不仅只是在低级的像素信息上使用逐像素的损失函数，我们在预训练的损失网络提取出的高级特征上使用感知损失函数。在训练过程中，感知损失函数在测试实时转换网络运行时在度量图像相似度的效果上比逐像素损失函数有更好的鲁棒性。</p>\r\n<p>我们在两个任务上进行了实验：风格转换和单图像超分辨率，两者本质上是病态的。在风格转换任务上，这里没有唯一正确的输出，而在超分辨率图像上，一个低分辨率输入可以生成很多高分辨率输出图像。l两个任务都需要在输入图像上进行语意推理。在风格转换任务中，虽然在纹理和颜色上有很大变化，但输出必须在语义上和输入相似。在超分辨率任务上，需要从视觉模糊的低分辨率图片上推断出细节。原则上，训练好的用于这两个任务的一个大容量的神经网络可以隐式的学习到推断相关的语义；然而实际上我们不需要从头学习，使用感知损失函数允许语义转换的知识从损失网络传递到转换网络。</p>\r\n<p>在图像风格转换优化问题[11]上；我们的结果在使用目标函数值衡量的质量上和[11]相似，但是快了三个数量级。对于超分辨率问题，我们发现使用感知损失函数替代逐像素计算的损失给出了在放大4倍和8倍的超分辨率问题上的令人满意的结果。</p>\r\n<h2 id=\"相关工作\">相关工作</h2>\r\n<pre><code> 前馈的图像转换</code></pre>\r\n<p>在最近几年中，很多不同的图像转换任务使用逐像素计算的损失函数来训练。</p>\r\n<p>语义分隔方法[4,6,14-17]使用一个逐像素的分类损失，通过全卷积的方式在输入图像上运行网络来产生密集的场景标签，最近的深度和表层正态估计（normal estimation）方法[6,19]都是相似的，依靠使用一个前向的用逐像素的回归[5,6]或者分类损失函数[19]训练的前向卷积网络。一些方法在使用逐像素损失函数之外还使用图像梯度的惩罚[6]，将复发层和网络的其余部分一起训练来构建CRF推断[17]。或者使用一个CRF损失层[8]来加强输出中的局部一致性。</p>\r\n<p>我们的风格转换网络的架构来源于[4]和[16]，使用网络内下采样来减少 特征映射的空间范围，然后进行网络内上采样来产生最终的输出图像。</p>\r\n<pre><code> 感知优化</code></pre>\r\n<p>最近大量的论文已经着手使用感知目标来优化图像的生成，依赖于从一个卷积网络中提取出的高级特征。图像可以根据最大化分类预测值[8,9]或者为了理解函数编码而是用个别特征[9]的方向来生成。相似的优化技术可以被用于生成高度混淆的图片[20,21]。</p>\r\n<p>Mahendran和Vedaldi[7]使用最小化一个特征重构损失反转卷积网络的特征来理解不同网络层保留的图像信息;相似的方法更早被用于反转局部二值描述[22.23]以及HOG特征[24]</p>\r\n<p>Dosovitskiy和Brox的研究[25]和我们非常相关，他们训练了一个前向神经网络来反转卷积特征，迅速的估计一个由[7]提出的优化问题的解。然而，他们的前向网络使用的是逐像素计算的重构损失，而我们直接优化了[7]中的特征重构损失。</p>\r\n<pre><code> 风格转换</code></pre>\r\n<p>Gatys等人[11]实现了艺术风格的转换，将一个图片的内容通过最小化[7]中的特征重构损失函数以及一个同样基于预训练的卷积网络提取的特征的风格重构损失来实现和另一个图片的风格结合；在这之前，有人用类似的方法来做纹理合成[10]。他们在这个问题上得到了非常好的结果，但是由于优化问题的每一步都需要在预训练的网络上前向和反向传播共两次，他们的方法需要非常大的计算量。为了减少这个计算负担，我们训练了一个前向网络来迅速估计他们的优化问题的解。论文[26,27]和我们同时提出了前向方法来快速转换图像风格。</p>\r\n<pre><code> 图像超分辨率</code></pre>\r\n<p>图像差分辨率问题是产生了许多技术的一个经典问题。Yang等人[28]在广泛采用卷积神经网络之前，对之前的技术做了广泛的评估。他们将超分辨率技术分组为基于预测的技术（biliner，bicubic，Lanczos，[29]）,基于边界的方法[30,31]，统计方法[32-34]，贴片型方法[30,35-41]，以及稀疏字典方法[42,43]。最近论文[1]在但图像超分辨率问题上使用了一个使用逐像素欧几里得损失的三层卷积神经网络实现了非常好的效果。其他近期的方法包括[44-46]。</p>\r\n<h2 id=\"方法\">方法</h2>\r\n<p>如fig.2所示，我们的系统包括两个部分：一个图像转换网络以及一个损失网络</p>\r\n<img src=\"/2018/05/08/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E4%B9%8B%E3%80%8APerceptual-Losses-for-Real-Time-Style-Transferand-Super-Resolution%E3%80%8B/fig2.png\" class=\"\" title=\"fig.2 系统概况，我们训练了一个图像转换网络来把输入图像转换到输出图像。我们使用一个预训练的图像分类网络来定义测量图像之间内容和风格上感知差异的感知损失函数。这个感知网络在训练过程中是固定不变的\">\r\n<p>三个月过去了，发现翻译这篇论文没有啥意义，因此终止于此。</p>\r\n<p>引用请注明出处。</p>\r\n"},{"title":"论文阅读《ACNet——Strengthening the Kernel Skeletons for Powerful CNN via AsymmetricConvolution Blocks》","date":"2019-12-31T07:02:12.000Z","mathjax":true,"_content":"\n## 主要工作\n提出了一种非对称卷积块（Asymmetric Convolution Block），使用一维的非对称卷积来加强方形卷积。\n\n## 具体实现\n在训练过程中，将原始的$3 \\times 3$卷积替换为一种非对称卷积块的结构，如下图所示。\n\n![训练时的ACB模块](ACB_training.png)\n\n在预测过程中，将ACB模块中的三个卷积核加起来，重新变成一个$3 \\times 3$卷积，如下图所示。\n\n![预测时的ACB模块](ACB_evaluating.png)\n\n对于预测过程，这样做并不会引起任何其他开销。但是实验结果表明，这样做可以提升模型的效果。\n\n看起来这个ACB模块和直接训练一个$3 \\times 3$卷积没有什么区别，但是一个细节的地方是，在实现ACB时，三个卷积分支在卷积层之后都会有一个BN层，在预测过程，混合卷积层权重时，BN层权重也需要进行混合，如下图所示。\n![混合操作](BN_fusion.png)\n\n那么这样分析之后，结果就很明显了，这就相当于为卷积核不同位置分配了可训练的权重。\n\n## 我的实验结果\n我自己在最近做的乳腺肿块分割任务上，对我的baseline模型使用了ACB，发现dice score确实有1个百分点的提升。\n\n我还尝试直接为卷积核构造一个空间权重参数，但是发现效果反而不如直接使用ACB，可能是因为ACB只为十字上的卷积核参数分配权重，算是一种约束条件，更加容易优化吧。","source":"_posts/论文阅读/论文阅读《ACNet——Strengthening-the-Kernel-Skeletons-for-Powerful-CNN-via-AsymmetricConvolution-Blocks》.md","raw":"---\ntitle: >-\n  论文阅读《ACNet——Strengthening the Kernel Skeletons for Powerful CNN via\n  AsymmetricConvolution Blocks》\ndate: 2019-12-31 15:02:12\ntags: 论文阅读\nmathjax: true\n---\n\n## 主要工作\n提出了一种非对称卷积块（Asymmetric Convolution Block），使用一维的非对称卷积来加强方形卷积。\n\n## 具体实现\n在训练过程中，将原始的$3 \\times 3$卷积替换为一种非对称卷积块的结构，如下图所示。\n\n![训练时的ACB模块](ACB_training.png)\n\n在预测过程中，将ACB模块中的三个卷积核加起来，重新变成一个$3 \\times 3$卷积，如下图所示。\n\n![预测时的ACB模块](ACB_evaluating.png)\n\n对于预测过程，这样做并不会引起任何其他开销。但是实验结果表明，这样做可以提升模型的效果。\n\n看起来这个ACB模块和直接训练一个$3 \\times 3$卷积没有什么区别，但是一个细节的地方是，在实现ACB时，三个卷积分支在卷积层之后都会有一个BN层，在预测过程，混合卷积层权重时，BN层权重也需要进行混合，如下图所示。\n![混合操作](BN_fusion.png)\n\n那么这样分析之后，结果就很明显了，这就相当于为卷积核不同位置分配了可训练的权重。\n\n## 我的实验结果\n我自己在最近做的乳腺肿块分割任务上，对我的baseline模型使用了ACB，发现dice score确实有1个百分点的提升。\n\n我还尝试直接为卷积核构造一个空间权重参数，但是发现效果反而不如直接使用ACB，可能是因为ACB只为十字上的卷积核参数分配权重，算是一种约束条件，更加容易优化吧。","slug":"论文阅读/论文阅读《ACNet——Strengthening-the-Kernel-Skeletons-for-Powerful-CNN-via-AsymmetricConvolution-Blocks》","published":1,"updated":"2020-01-02T07:25:44.739Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckjxpc3s8001x44mq8zfy5bd8","content":"<h2 id=\"主要工作\">主要工作</h2>\r\n<p>提出了一种非对称卷积块（Asymmetric Convolution Block），使用一维的非对称卷积来加强方形卷积。</p>\r\n<h2 id=\"具体实现\">具体实现</h2>\r\n<p>在训练过程中，将原始的<span class=\"math inline\">\\(3 \\times 3\\)</span>卷积替换为一种非对称卷积块的结构，如下图所示。</p>\r\n<figure>\r\n<img src=\"ACB_training.png\" alt=\"训练时的ACB模块\" /><figcaption aria-hidden=\"true\">训练时的ACB模块</figcaption>\r\n</figure>\r\n<p>在预测过程中，将ACB模块中的三个卷积核加起来，重新变成一个<span class=\"math inline\">\\(3 \\times 3\\)</span>卷积，如下图所示。</p>\r\n<figure>\r\n<img src=\"ACB_evaluating.png\" alt=\"预测时的ACB模块\" /><figcaption aria-hidden=\"true\">预测时的ACB模块</figcaption>\r\n</figure>\r\n<p>对于预测过程，这样做并不会引起任何其他开销。但是实验结果表明，这样做可以提升模型的效果。</p>\r\n<p>看起来这个ACB模块和直接训练一个<span class=\"math inline\">\\(3 \\times 3\\)</span>卷积没有什么区别，但是一个细节的地方是，在实现ACB时，三个卷积分支在卷积层之后都会有一个BN层，在预测过程，混合卷积层权重时，BN层权重也需要进行混合，如下图所示。 <img src=\"BN_fusion.png\" alt=\"混合操作\" /></p>\r\n<p>那么这样分析之后，结果就很明显了，这就相当于为卷积核不同位置分配了可训练的权重。</p>\r\n<h2 id=\"我的实验结果\">我的实验结果</h2>\r\n<p>我自己在最近做的乳腺肿块分割任务上，对我的baseline模型使用了ACB，发现dice score确实有1个百分点的提升。</p>\r\n<p>我还尝试直接为卷积核构造一个空间权重参数，但是发现效果反而不如直接使用ACB，可能是因为ACB只为十字上的卷积核参数分配权重，算是一种约束条件，更加容易优化吧。</p>\r\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"主要工作\">主要工作</h2>\r\n<p>提出了一种非对称卷积块（Asymmetric Convolution Block），使用一维的非对称卷积来加强方形卷积。</p>\r\n<h2 id=\"具体实现\">具体实现</h2>\r\n<p>在训练过程中，将原始的<span class=\"math inline\">\\(3 \\times 3\\)</span>卷积替换为一种非对称卷积块的结构，如下图所示。</p>\r\n<figure>\r\n<img src=\"ACB_training.png\" alt=\"训练时的ACB模块\" /><figcaption aria-hidden=\"true\">训练时的ACB模块</figcaption>\r\n</figure>\r\n<p>在预测过程中，将ACB模块中的三个卷积核加起来，重新变成一个<span class=\"math inline\">\\(3 \\times 3\\)</span>卷积，如下图所示。</p>\r\n<figure>\r\n<img src=\"ACB_evaluating.png\" alt=\"预测时的ACB模块\" /><figcaption aria-hidden=\"true\">预测时的ACB模块</figcaption>\r\n</figure>\r\n<p>对于预测过程，这样做并不会引起任何其他开销。但是实验结果表明，这样做可以提升模型的效果。</p>\r\n<p>看起来这个ACB模块和直接训练一个<span class=\"math inline\">\\(3 \\times 3\\)</span>卷积没有什么区别，但是一个细节的地方是，在实现ACB时，三个卷积分支在卷积层之后都会有一个BN层，在预测过程，混合卷积层权重时，BN层权重也需要进行混合，如下图所示。 <img src=\"BN_fusion.png\" alt=\"混合操作\" /></p>\r\n<p>那么这样分析之后，结果就很明显了，这就相当于为卷积核不同位置分配了可训练的权重。</p>\r\n<h2 id=\"我的实验结果\">我的实验结果</h2>\r\n<p>我自己在最近做的乳腺肿块分割任务上，对我的baseline模型使用了ACB，发现dice score确实有1个百分点的提升。</p>\r\n<p>我还尝试直接为卷积核构造一个空间权重参数，但是发现效果反而不如直接使用ACB，可能是因为ACB只为十字上的卷积核参数分配权重，算是一种约束条件，更加容易优化吧。</p>\r\n"},{"title":"论文阅读《CondenseNet: An Efficient DenseNet using Learned Group Convolutions》","date":"2019-06-04T06:27:49.000Z","mathjax":true,"_content":"\n## 主要工作\n本文主要贡献是提出了一种比ShuffleNets还要高效很多的CondenseNets。\n\n## 主要思想\n该论文认为普通的卷积模型:layer-by-layer的连接模式，模型需要在每一层都去重复最前面层的特征，因此DenseNet的短连接所引导的特征重用对这个问题有一定的缓解，但论文中猜想对于DenseNet这样的密集短连接，会引入一些冗余的浅层特征，因此论文提出了一种方法来减少这种冗余，并形成了一种高效的模型结构。\n\n## 主要方法\n将一层中的输出特征分成多组（分组也是学习得到），并在训练过程中按照分组逐渐移除组中与一些不重要的输入特征之间的连接。\n\n## 效率对比\n在ImageNet数据上进行图像分类实验， CondenseNets相比于同样正确率的DenseNets模型，计算量减少到了DenseNets的$\\frac{1}{10}$，相比于相近top-1错误率的MobileNet，CondenseNets计算量为MobileNet的$\\frac{1}{2}$。\n\n## 具体实现\n![Learn Group Convolution的学习和预测示意](GroupConv_Learn_Test.png)\n如图，在Condensing阶段训练过程中，使用sparsity inducing regularization重复训练模型， 并丢弃权重较低的特征（在丢弃时，保证同一个Group的卷积核有相同的稀疏连接，方便在测试阶段使用一个标准的group convolution实现计算），在optimization阶段，固定分组，并训练卷积核。**图中解释可能有误，图上说是的每个Condensing stage 移除$(C-1)/C$比例的连接，但后面又说每个Condensing stage之后移除$1/C$比例的连接，我觉得应该是后面一种说法正确**\n\n### 特征凝聚原则(Condensation Criterion)\n普通的卷积核是四维的$O \\times R \\times W \\times H$，其中$O$代表输出的特征个数，$R$代表输入的特征个数，$W$代表卷积核宽度，$H$代表卷积核高度，如果简化为$1 \\times 1$卷积，则卷积核可以看成一个$O \\times R$的矩阵$F$, 对于某个分组$G$, 其卷积权重表示为$F^G$，$F^g_{ij}$则表示分组$g$中第$j$个输入到第$i$个输出的权重。\n\n在筛选的过程中，第$j$输入特征的重要程度由$\\sum_{i=1}^{O/G}|F^g_{ij}|$来衡量，丢弃的过程可以简单描述为：对于矩阵$F^g$，如果其某一列的$L_1$正则项较小，则把这一列的全都置0。\n\n为了激励同一组的卷积倾向于同样的一组输入特征，作者没有使用逐元素的$L_1$正则化，而是使用其提出的一种group-lasso正则化，其正则化项计算如下：\n$$\n\\sum_{g=1}^G \\sum_{j=1}^R \\sqrt{\\sum_{i=1}^{O/G}{F^g_{ij}}^2}\n$$\n这样的的正则化项由于其值由每个$F^g$中的每一列的最大值来主导，因此会倾向于把$F^g$中某一列整体变小，满足要求。\n\n### 特征凝聚因子(Condensation Factor)\n一组输出所连接到的输入个数比例不一定是$\\frac{1}{G}$，这里定义特征凝聚因子$C$，允许每一组输出连接到$\\lfloor\\frac{R}{C}\\rfloor$个输入特征。\n\n### 特征凝聚过程(Condensation Procedure)\n在上面的Learn Group Convolution的学习和预测示意图中，在经过$C - 1$个Condensing stage之后，只有$1 - (C - 1) \\times \\frac{1}{C}=\\frac{1}{C}$比例的输入特征保留下来。\n\nepoch设置：Condensing stage $C-1$的训练epoch设置为$\\frac{M}{2(C-1)}$,其中M表示总共的训练epoch数。\n\n学习速率：使用余弦状学习速率，初始值为0.1,300个epoch下降到0，如下图所示。\n\n![训练loss和学习速率](训练loss和学习速率.png)\n\n上图在150代的时候，训练loss跳跃上升，论文解释说是因为在最后一个Condensing stage，去掉了当前一半的连接导致。\n\n### 特征选择和重排(Index Layer)\n在经过训练丢弃一些卷积核权重之后，层间的连接非常杂乱，为了高效的在硬件上实现组卷积操作，增加了Index Layer用于特征选择和重排。(暂时没有看到有关Index Layer的具体实现方法的描述)\n\n### 模型结构设计(Architecture Design)\n1. 普通的DenseNet结构在每个dense block之后特征个数增加$k$(常数)个，作者在一篇论文中看到在DenseNet中更深的层更多的依赖深层特征而非浅层特征，因此作者决定将k改为指数增长，加大深层特征所占的比例，所以设置$k=2^{m-1}k_0$，其中$m$表示层的编号，$k_0$是一个常数。\n2. 为了提高特征重用率，作者将原始输入（或者原始输入对应大小的平均池化下采样）连接到了所有的层。\n\n## 在CIFAR-10数据上进行的对比实验\n\n### 特征凝聚因子对比实验\n\n![特征凝聚因子实验](特征凝聚因子实验.png)\n\n在DenseNet50的基础上，对比了不同模型权重丢弃程度(特征凝聚因子)下的测试集错误率，如下图，蓝色表示原始模型，黄色表示其他的模型剪枝方法，红色表示作者提出的凝聚模型，三个凝聚模型的分组个数$G$都设置为4，这里将特征凝聚因子$C$作为横坐标，表示使用不同的特征凝聚因子的效果，**其它的模型剪枝方法最终也是保留$\\frac{1}{C}$比例的连接作为对比**\n\n### 分组个数实验\n\n![分组个数实验](分组个数实验.png)\n\n在四个对比实验中，特征凝聚因子$C$使用固定值：8。\n\n### 特征凝聚因子效率对比\n\n![特征凝聚因子效率对比](特征凝聚因子效率对比.png)\n\n这里作者说所有实验固定分组个数$G=4$，然后观察不同的凝聚因子$C$下的测试集错误率和FLOPs的关系。**这里没搞清楚，按理说$G$和$C$一旦确定，那么FLOPs就是固定的，为什么这里对于每个$C$可以变化FLOPs？模型不同？暂时没有看到相应的描述**\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","source":"_posts/论文阅读/论文阅读《CondenseNet-An-Efficient-DenseNet-using-Learned-Group-Convolutions》.md","raw":"---\ntitle: '论文阅读《CondenseNet: An Efficient DenseNet using Learned Group Convolutions》'\ndate: 2019-06-04 14:27:49\ntags: 论文阅读\nmathjax: true\n---\n\n## 主要工作\n本文主要贡献是提出了一种比ShuffleNets还要高效很多的CondenseNets。\n\n## 主要思想\n该论文认为普通的卷积模型:layer-by-layer的连接模式，模型需要在每一层都去重复最前面层的特征，因此DenseNet的短连接所引导的特征重用对这个问题有一定的缓解，但论文中猜想对于DenseNet这样的密集短连接，会引入一些冗余的浅层特征，因此论文提出了一种方法来减少这种冗余，并形成了一种高效的模型结构。\n\n## 主要方法\n将一层中的输出特征分成多组（分组也是学习得到），并在训练过程中按照分组逐渐移除组中与一些不重要的输入特征之间的连接。\n\n## 效率对比\n在ImageNet数据上进行图像分类实验， CondenseNets相比于同样正确率的DenseNets模型，计算量减少到了DenseNets的$\\frac{1}{10}$，相比于相近top-1错误率的MobileNet，CondenseNets计算量为MobileNet的$\\frac{1}{2}$。\n\n## 具体实现\n![Learn Group Convolution的学习和预测示意](GroupConv_Learn_Test.png)\n如图，在Condensing阶段训练过程中，使用sparsity inducing regularization重复训练模型， 并丢弃权重较低的特征（在丢弃时，保证同一个Group的卷积核有相同的稀疏连接，方便在测试阶段使用一个标准的group convolution实现计算），在optimization阶段，固定分组，并训练卷积核。**图中解释可能有误，图上说是的每个Condensing stage 移除$(C-1)/C$比例的连接，但后面又说每个Condensing stage之后移除$1/C$比例的连接，我觉得应该是后面一种说法正确**\n\n### 特征凝聚原则(Condensation Criterion)\n普通的卷积核是四维的$O \\times R \\times W \\times H$，其中$O$代表输出的特征个数，$R$代表输入的特征个数，$W$代表卷积核宽度，$H$代表卷积核高度，如果简化为$1 \\times 1$卷积，则卷积核可以看成一个$O \\times R$的矩阵$F$, 对于某个分组$G$, 其卷积权重表示为$F^G$，$F^g_{ij}$则表示分组$g$中第$j$个输入到第$i$个输出的权重。\n\n在筛选的过程中，第$j$输入特征的重要程度由$\\sum_{i=1}^{O/G}|F^g_{ij}|$来衡量，丢弃的过程可以简单描述为：对于矩阵$F^g$，如果其某一列的$L_1$正则项较小，则把这一列的全都置0。\n\n为了激励同一组的卷积倾向于同样的一组输入特征，作者没有使用逐元素的$L_1$正则化，而是使用其提出的一种group-lasso正则化，其正则化项计算如下：\n$$\n\\sum_{g=1}^G \\sum_{j=1}^R \\sqrt{\\sum_{i=1}^{O/G}{F^g_{ij}}^2}\n$$\n这样的的正则化项由于其值由每个$F^g$中的每一列的最大值来主导，因此会倾向于把$F^g$中某一列整体变小，满足要求。\n\n### 特征凝聚因子(Condensation Factor)\n一组输出所连接到的输入个数比例不一定是$\\frac{1}{G}$，这里定义特征凝聚因子$C$，允许每一组输出连接到$\\lfloor\\frac{R}{C}\\rfloor$个输入特征。\n\n### 特征凝聚过程(Condensation Procedure)\n在上面的Learn Group Convolution的学习和预测示意图中，在经过$C - 1$个Condensing stage之后，只有$1 - (C - 1) \\times \\frac{1}{C}=\\frac{1}{C}$比例的输入特征保留下来。\n\nepoch设置：Condensing stage $C-1$的训练epoch设置为$\\frac{M}{2(C-1)}$,其中M表示总共的训练epoch数。\n\n学习速率：使用余弦状学习速率，初始值为0.1,300个epoch下降到0，如下图所示。\n\n![训练loss和学习速率](训练loss和学习速率.png)\n\n上图在150代的时候，训练loss跳跃上升，论文解释说是因为在最后一个Condensing stage，去掉了当前一半的连接导致。\n\n### 特征选择和重排(Index Layer)\n在经过训练丢弃一些卷积核权重之后，层间的连接非常杂乱，为了高效的在硬件上实现组卷积操作，增加了Index Layer用于特征选择和重排。(暂时没有看到有关Index Layer的具体实现方法的描述)\n\n### 模型结构设计(Architecture Design)\n1. 普通的DenseNet结构在每个dense block之后特征个数增加$k$(常数)个，作者在一篇论文中看到在DenseNet中更深的层更多的依赖深层特征而非浅层特征，因此作者决定将k改为指数增长，加大深层特征所占的比例，所以设置$k=2^{m-1}k_0$，其中$m$表示层的编号，$k_0$是一个常数。\n2. 为了提高特征重用率，作者将原始输入（或者原始输入对应大小的平均池化下采样）连接到了所有的层。\n\n## 在CIFAR-10数据上进行的对比实验\n\n### 特征凝聚因子对比实验\n\n![特征凝聚因子实验](特征凝聚因子实验.png)\n\n在DenseNet50的基础上，对比了不同模型权重丢弃程度(特征凝聚因子)下的测试集错误率，如下图，蓝色表示原始模型，黄色表示其他的模型剪枝方法，红色表示作者提出的凝聚模型，三个凝聚模型的分组个数$G$都设置为4，这里将特征凝聚因子$C$作为横坐标，表示使用不同的特征凝聚因子的效果，**其它的模型剪枝方法最终也是保留$\\frac{1}{C}$比例的连接作为对比**\n\n### 分组个数实验\n\n![分组个数实验](分组个数实验.png)\n\n在四个对比实验中，特征凝聚因子$C$使用固定值：8。\n\n### 特征凝聚因子效率对比\n\n![特征凝聚因子效率对比](特征凝聚因子效率对比.png)\n\n这里作者说所有实验固定分组个数$G=4$，然后观察不同的凝聚因子$C$下的测试集错误率和FLOPs的关系。**这里没搞清楚，按理说$G$和$C$一旦确定，那么FLOPs就是固定的，为什么这里对于每个$C$可以变化FLOPs？模型不同？暂时没有看到相应的描述**\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","slug":"论文阅读/论文阅读《CondenseNet-An-Efficient-DenseNet-using-Learned-Group-Convolutions》","published":1,"updated":"2020-08-31T06:39:20.785Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckjxpc3s9002044mq8crq844j","content":"<h2 id=\"主要工作\">主要工作</h2>\r\n<p>本文主要贡献是提出了一种比ShuffleNets还要高效很多的CondenseNets。</p>\r\n<h2 id=\"主要思想\">主要思想</h2>\r\n<p>该论文认为普通的卷积模型:layer-by-layer的连接模式，模型需要在每一层都去重复最前面层的特征，因此DenseNet的短连接所引导的特征重用对这个问题有一定的缓解，但论文中猜想对于DenseNet这样的密集短连接，会引入一些冗余的浅层特征，因此论文提出了一种方法来减少这种冗余，并形成了一种高效的模型结构。</p>\r\n<h2 id=\"主要方法\">主要方法</h2>\r\n<p>将一层中的输出特征分成多组（分组也是学习得到），并在训练过程中按照分组逐渐移除组中与一些不重要的输入特征之间的连接。</p>\r\n<h2 id=\"效率对比\">效率对比</h2>\r\n<p>在ImageNet数据上进行图像分类实验， CondenseNets相比于同样正确率的DenseNets模型，计算量减少到了DenseNets的<span class=\"math inline\">\\(\\frac{1}{10}\\)</span>，相比于相近top-1错误率的MobileNet，CondenseNets计算量为MobileNet的<span class=\"math inline\">\\(\\frac{1}{2}\\)</span>。</p>\r\n<h2 id=\"具体实现\">具体实现</h2>\r\n<p><img src=\"GroupConv_Learn_Test.png\" alt=\"Learn Group Convolution的学习和预测示意\" /> 如图，在Condensing阶段训练过程中，使用sparsity inducing regularization重复训练模型， 并丢弃权重较低的特征（在丢弃时，保证同一个Group的卷积核有相同的稀疏连接，方便在测试阶段使用一个标准的group convolution实现计算），在optimization阶段，固定分组，并训练卷积核。<strong>图中解释可能有误，图上说是的每个Condensing stage 移除<span class=\"math inline\">\\((C-1)/C\\)</span>比例的连接，但后面又说每个Condensing stage之后移除<span class=\"math inline\">\\(1/C\\)</span>比例的连接，我觉得应该是后面一种说法正确</strong></p>\r\n<h3 id=\"特征凝聚原则condensation-criterion\">特征凝聚原则(Condensation Criterion)</h3>\r\n<p>普通的卷积核是四维的<span class=\"math inline\">\\(O \\times R \\times W \\times H\\)</span>，其中<span class=\"math inline\">\\(O\\)</span>代表输出的特征个数，<span class=\"math inline\">\\(R\\)</span>代表输入的特征个数，<span class=\"math inline\">\\(W\\)</span>代表卷积核宽度，<span class=\"math inline\">\\(H\\)</span>代表卷积核高度，如果简化为<span class=\"math inline\">\\(1 \\times 1\\)</span>卷积，则卷积核可以看成一个<span class=\"math inline\">\\(O \\times R\\)</span>的矩阵<span class=\"math inline\">\\(F\\)</span>, 对于某个分组<span class=\"math inline\">\\(G\\)</span>, 其卷积权重表示为<span class=\"math inline\">\\(F^G\\)</span>，<span class=\"math inline\">\\(F^g_{ij}\\)</span>则表示分组<span class=\"math inline\">\\(g\\)</span>中第<span class=\"math inline\">\\(j\\)</span>个输入到第<span class=\"math inline\">\\(i\\)</span>个输出的权重。</p>\r\n<p>在筛选的过程中，第<span class=\"math inline\">\\(j\\)</span>输入特征的重要程度由<span class=\"math inline\">\\(\\sum_{i=1}^{O/G}|F^g_{ij}|\\)</span>来衡量，丢弃的过程可以简单描述为：对于矩阵<span class=\"math inline\">\\(F^g\\)</span>，如果其某一列的<span class=\"math inline\">\\(L_1\\)</span>正则项较小，则把这一列的全都置0。</p>\r\n<p>为了激励同一组的卷积倾向于同样的一组输入特征，作者没有使用逐元素的<span class=\"math inline\">\\(L_1\\)</span>正则化，而是使用其提出的一种group-lasso正则化，其正则化项计算如下： <span class=\"math display\">\\[\r\n\\sum_{g=1}^G \\sum_{j=1}^R \\sqrt{\\sum_{i=1}^{O/G}{F^g_{ij}}^2}\r\n\\]</span> 这样的的正则化项由于其值由每个<span class=\"math inline\">\\(F^g\\)</span>中的每一列的最大值来主导，因此会倾向于把<span class=\"math inline\">\\(F^g\\)</span>中某一列整体变小，满足要求。</p>\r\n<h3 id=\"特征凝聚因子condensation-factor\">特征凝聚因子(Condensation Factor)</h3>\r\n<p>一组输出所连接到的输入个数比例不一定是<span class=\"math inline\">\\(\\frac{1}{G}\\)</span>，这里定义特征凝聚因子<span class=\"math inline\">\\(C\\)</span>，允许每一组输出连接到<span class=\"math inline\">\\(\\lfloor\\frac{R}{C}\\rfloor\\)</span>个输入特征。</p>\r\n<h3 id=\"特征凝聚过程condensation-procedure\">特征凝聚过程(Condensation Procedure)</h3>\r\n<p>在上面的Learn Group Convolution的学习和预测示意图中，在经过<span class=\"math inline\">\\(C - 1\\)</span>个Condensing stage之后，只有<span class=\"math inline\">\\(1 - (C - 1) \\times \\frac{1}{C}=\\frac{1}{C}\\)</span>比例的输入特征保留下来。</p>\r\n<p>epoch设置：Condensing stage <span class=\"math inline\">\\(C-1\\)</span>的训练epoch设置为<span class=\"math inline\">\\(\\frac{M}{2(C-1)}\\)</span>,其中M表示总共的训练epoch数。</p>\r\n<p>学习速率：使用余弦状学习速率，初始值为0.1,300个epoch下降到0，如下图所示。</p>\r\n<figure>\r\n<img src=\"训练loss和学习速率.png\" alt=\"训练loss和学习速率\" /><figcaption aria-hidden=\"true\">训练loss和学习速率</figcaption>\r\n</figure>\r\n<p>上图在150代的时候，训练loss跳跃上升，论文解释说是因为在最后一个Condensing stage，去掉了当前一半的连接导致。</p>\r\n<h3 id=\"特征选择和重排index-layer\">特征选择和重排(Index Layer)</h3>\r\n<p>在经过训练丢弃一些卷积核权重之后，层间的连接非常杂乱，为了高效的在硬件上实现组卷积操作，增加了Index Layer用于特征选择和重排。(暂时没有看到有关Index Layer的具体实现方法的描述)</p>\r\n<h3 id=\"模型结构设计architecture-design\">模型结构设计(Architecture Design)</h3>\r\n<ol type=\"1\">\r\n<li>普通的DenseNet结构在每个dense block之后特征个数增加<span class=\"math inline\">\\(k\\)</span>(常数)个，作者在一篇论文中看到在DenseNet中更深的层更多的依赖深层特征而非浅层特征，因此作者决定将k改为指数增长，加大深层特征所占的比例，所以设置<span class=\"math inline\">\\(k=2^{m-1}k_0\\)</span>，其中<span class=\"math inline\">\\(m\\)</span>表示层的编号，<span class=\"math inline\">\\(k_0\\)</span>是一个常数。</li>\r\n<li>为了提高特征重用率，作者将原始输入（或者原始输入对应大小的平均池化下采样）连接到了所有的层。</li>\r\n</ol>\r\n<h2 id=\"在cifar-10数据上进行的对比实验\">在CIFAR-10数据上进行的对比实验</h2>\r\n<h3 id=\"特征凝聚因子对比实验\">特征凝聚因子对比实验</h3>\r\n<figure>\r\n<img src=\"特征凝聚因子实验.png\" alt=\"特征凝聚因子实验\" /><figcaption aria-hidden=\"true\">特征凝聚因子实验</figcaption>\r\n</figure>\r\n<p>在DenseNet50的基础上，对比了不同模型权重丢弃程度(特征凝聚因子)下的测试集错误率，如下图，蓝色表示原始模型，黄色表示其他的模型剪枝方法，红色表示作者提出的凝聚模型，三个凝聚模型的分组个数<span class=\"math inline\">\\(G\\)</span>都设置为4，这里将特征凝聚因子<span class=\"math inline\">\\(C\\)</span>作为横坐标，表示使用不同的特征凝聚因子的效果，<strong>其它的模型剪枝方法最终也是保留<span class=\"math inline\">\\(\\frac{1}{C}\\)</span>比例的连接作为对比</strong></p>\r\n<h3 id=\"分组个数实验\">分组个数实验</h3>\r\n<figure>\r\n<img src=\"分组个数实验.png\" alt=\"分组个数实验\" /><figcaption aria-hidden=\"true\">分组个数实验</figcaption>\r\n</figure>\r\n<p>在四个对比实验中，特征凝聚因子<span class=\"math inline\">\\(C\\)</span>使用固定值：8。</p>\r\n<h3 id=\"特征凝聚因子效率对比\">特征凝聚因子效率对比</h3>\r\n<figure>\r\n<img src=\"特征凝聚因子效率对比.png\" alt=\"特征凝聚因子效率对比\" /><figcaption aria-hidden=\"true\">特征凝聚因子效率对比</figcaption>\r\n</figure>\r\n<p>这里作者说所有实验固定分组个数<span class=\"math inline\">\\(G=4\\)</span>，然后观察不同的凝聚因子<span class=\"math inline\">\\(C\\)</span>下的测试集错误率和FLOPs的关系。<strong>这里没搞清楚，按理说<span class=\"math inline\">\\(G\\)</span>和<span class=\"math inline\">\\(C\\)</span>一旦确定，那么FLOPs就是固定的，为什么这里对于每个<span class=\"math inline\">\\(C\\)</span>可以变化FLOPs？模型不同？暂时没有看到相应的描述</strong></p>\r\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"主要工作\">主要工作</h2>\r\n<p>本文主要贡献是提出了一种比ShuffleNets还要高效很多的CondenseNets。</p>\r\n<h2 id=\"主要思想\">主要思想</h2>\r\n<p>该论文认为普通的卷积模型:layer-by-layer的连接模式，模型需要在每一层都去重复最前面层的特征，因此DenseNet的短连接所引导的特征重用对这个问题有一定的缓解，但论文中猜想对于DenseNet这样的密集短连接，会引入一些冗余的浅层特征，因此论文提出了一种方法来减少这种冗余，并形成了一种高效的模型结构。</p>\r\n<h2 id=\"主要方法\">主要方法</h2>\r\n<p>将一层中的输出特征分成多组（分组也是学习得到），并在训练过程中按照分组逐渐移除组中与一些不重要的输入特征之间的连接。</p>\r\n<h2 id=\"效率对比\">效率对比</h2>\r\n<p>在ImageNet数据上进行图像分类实验， CondenseNets相比于同样正确率的DenseNets模型，计算量减少到了DenseNets的<span class=\"math inline\">\\(\\frac{1}{10}\\)</span>，相比于相近top-1错误率的MobileNet，CondenseNets计算量为MobileNet的<span class=\"math inline\">\\(\\frac{1}{2}\\)</span>。</p>\r\n<h2 id=\"具体实现\">具体实现</h2>\r\n<p><img src=\"GroupConv_Learn_Test.png\" alt=\"Learn Group Convolution的学习和预测示意\" /> 如图，在Condensing阶段训练过程中，使用sparsity inducing regularization重复训练模型， 并丢弃权重较低的特征（在丢弃时，保证同一个Group的卷积核有相同的稀疏连接，方便在测试阶段使用一个标准的group convolution实现计算），在optimization阶段，固定分组，并训练卷积核。<strong>图中解释可能有误，图上说是的每个Condensing stage 移除<span class=\"math inline\">\\((C-1)/C\\)</span>比例的连接，但后面又说每个Condensing stage之后移除<span class=\"math inline\">\\(1/C\\)</span>比例的连接，我觉得应该是后面一种说法正确</strong></p>\r\n<h3 id=\"特征凝聚原则condensation-criterion\">特征凝聚原则(Condensation Criterion)</h3>\r\n<p>普通的卷积核是四维的<span class=\"math inline\">\\(O \\times R \\times W \\times H\\)</span>，其中<span class=\"math inline\">\\(O\\)</span>代表输出的特征个数，<span class=\"math inline\">\\(R\\)</span>代表输入的特征个数，<span class=\"math inline\">\\(W\\)</span>代表卷积核宽度，<span class=\"math inline\">\\(H\\)</span>代表卷积核高度，如果简化为<span class=\"math inline\">\\(1 \\times 1\\)</span>卷积，则卷积核可以看成一个<span class=\"math inline\">\\(O \\times R\\)</span>的矩阵<span class=\"math inline\">\\(F\\)</span>, 对于某个分组<span class=\"math inline\">\\(G\\)</span>, 其卷积权重表示为<span class=\"math inline\">\\(F^G\\)</span>，<span class=\"math inline\">\\(F^g_{ij}\\)</span>则表示分组<span class=\"math inline\">\\(g\\)</span>中第<span class=\"math inline\">\\(j\\)</span>个输入到第<span class=\"math inline\">\\(i\\)</span>个输出的权重。</p>\r\n<p>在筛选的过程中，第<span class=\"math inline\">\\(j\\)</span>输入特征的重要程度由<span class=\"math inline\">\\(\\sum_{i=1}^{O/G}|F^g_{ij}|\\)</span>来衡量，丢弃的过程可以简单描述为：对于矩阵<span class=\"math inline\">\\(F^g\\)</span>，如果其某一列的<span class=\"math inline\">\\(L_1\\)</span>正则项较小，则把这一列的全都置0。</p>\r\n<p>为了激励同一组的卷积倾向于同样的一组输入特征，作者没有使用逐元素的<span class=\"math inline\">\\(L_1\\)</span>正则化，而是使用其提出的一种group-lasso正则化，其正则化项计算如下： <span class=\"math display\">\\[\r\n\\sum_{g=1}^G \\sum_{j=1}^R \\sqrt{\\sum_{i=1}^{O/G}{F^g_{ij}}^2}\r\n\\]</span> 这样的的正则化项由于其值由每个<span class=\"math inline\">\\(F^g\\)</span>中的每一列的最大值来主导，因此会倾向于把<span class=\"math inline\">\\(F^g\\)</span>中某一列整体变小，满足要求。</p>\r\n<h3 id=\"特征凝聚因子condensation-factor\">特征凝聚因子(Condensation Factor)</h3>\r\n<p>一组输出所连接到的输入个数比例不一定是<span class=\"math inline\">\\(\\frac{1}{G}\\)</span>，这里定义特征凝聚因子<span class=\"math inline\">\\(C\\)</span>，允许每一组输出连接到<span class=\"math inline\">\\(\\lfloor\\frac{R}{C}\\rfloor\\)</span>个输入特征。</p>\r\n<h3 id=\"特征凝聚过程condensation-procedure\">特征凝聚过程(Condensation Procedure)</h3>\r\n<p>在上面的Learn Group Convolution的学习和预测示意图中，在经过<span class=\"math inline\">\\(C - 1\\)</span>个Condensing stage之后，只有<span class=\"math inline\">\\(1 - (C - 1) \\times \\frac{1}{C}=\\frac{1}{C}\\)</span>比例的输入特征保留下来。</p>\r\n<p>epoch设置：Condensing stage <span class=\"math inline\">\\(C-1\\)</span>的训练epoch设置为<span class=\"math inline\">\\(\\frac{M}{2(C-1)}\\)</span>,其中M表示总共的训练epoch数。</p>\r\n<p>学习速率：使用余弦状学习速率，初始值为0.1,300个epoch下降到0，如下图所示。</p>\r\n<figure>\r\n<img src=\"训练loss和学习速率.png\" alt=\"训练loss和学习速率\" /><figcaption aria-hidden=\"true\">训练loss和学习速率</figcaption>\r\n</figure>\r\n<p>上图在150代的时候，训练loss跳跃上升，论文解释说是因为在最后一个Condensing stage，去掉了当前一半的连接导致。</p>\r\n<h3 id=\"特征选择和重排index-layer\">特征选择和重排(Index Layer)</h3>\r\n<p>在经过训练丢弃一些卷积核权重之后，层间的连接非常杂乱，为了高效的在硬件上实现组卷积操作，增加了Index Layer用于特征选择和重排。(暂时没有看到有关Index Layer的具体实现方法的描述)</p>\r\n<h3 id=\"模型结构设计architecture-design\">模型结构设计(Architecture Design)</h3>\r\n<ol type=\"1\">\r\n<li>普通的DenseNet结构在每个dense block之后特征个数增加<span class=\"math inline\">\\(k\\)</span>(常数)个，作者在一篇论文中看到在DenseNet中更深的层更多的依赖深层特征而非浅层特征，因此作者决定将k改为指数增长，加大深层特征所占的比例，所以设置<span class=\"math inline\">\\(k=2^{m-1}k_0\\)</span>，其中<span class=\"math inline\">\\(m\\)</span>表示层的编号，<span class=\"math inline\">\\(k_0\\)</span>是一个常数。</li>\r\n<li>为了提高特征重用率，作者将原始输入（或者原始输入对应大小的平均池化下采样）连接到了所有的层。</li>\r\n</ol>\r\n<h2 id=\"在cifar-10数据上进行的对比实验\">在CIFAR-10数据上进行的对比实验</h2>\r\n<h3 id=\"特征凝聚因子对比实验\">特征凝聚因子对比实验</h3>\r\n<figure>\r\n<img src=\"特征凝聚因子实验.png\" alt=\"特征凝聚因子实验\" /><figcaption aria-hidden=\"true\">特征凝聚因子实验</figcaption>\r\n</figure>\r\n<p>在DenseNet50的基础上，对比了不同模型权重丢弃程度(特征凝聚因子)下的测试集错误率，如下图，蓝色表示原始模型，黄色表示其他的模型剪枝方法，红色表示作者提出的凝聚模型，三个凝聚模型的分组个数<span class=\"math inline\">\\(G\\)</span>都设置为4，这里将特征凝聚因子<span class=\"math inline\">\\(C\\)</span>作为横坐标，表示使用不同的特征凝聚因子的效果，<strong>其它的模型剪枝方法最终也是保留<span class=\"math inline\">\\(\\frac{1}{C}\\)</span>比例的连接作为对比</strong></p>\r\n<h3 id=\"分组个数实验\">分组个数实验</h3>\r\n<figure>\r\n<img src=\"分组个数实验.png\" alt=\"分组个数实验\" /><figcaption aria-hidden=\"true\">分组个数实验</figcaption>\r\n</figure>\r\n<p>在四个对比实验中，特征凝聚因子<span class=\"math inline\">\\(C\\)</span>使用固定值：8。</p>\r\n<h3 id=\"特征凝聚因子效率对比\">特征凝聚因子效率对比</h3>\r\n<figure>\r\n<img src=\"特征凝聚因子效率对比.png\" alt=\"特征凝聚因子效率对比\" /><figcaption aria-hidden=\"true\">特征凝聚因子效率对比</figcaption>\r\n</figure>\r\n<p>这里作者说所有实验固定分组个数<span class=\"math inline\">\\(G=4\\)</span>，然后观察不同的凝聚因子<span class=\"math inline\">\\(C\\)</span>下的测试集错误率和FLOPs的关系。<strong>这里没搞清楚，按理说<span class=\"math inline\">\\(G\\)</span>和<span class=\"math inline\">\\(C\\)</span>一旦确定，那么FLOPs就是固定的，为什么这里对于每个<span class=\"math inline\">\\(C\\)</span>可以变化FLOPs？模型不同？暂时没有看到相应的描述</strong></p>\r\n"},{"title":"“论文阅读《CornerNet-Detecting-Objects-as-Paired-Keypoints》”","date":"2019-08-07T12:17:13.000Z","mathjax":true,"_content":"\n## 提出的问题\n\n单阶段目标检测大多基于anchor实现，但是用anchor存在两个问题：\n\n- 为了保证所有ground truth都有匹配的anchor，不得不使用非常多的anchor，例如retinanet anchor个数100K, DSSD anchor个数40K，但是真正与ground truth iou较大的anchor非常少，导致了很严重的正负样本不平衡问题。\n- 设置anchor时需要非常多的超参数，anchor的大小、长宽比、个数，使得找到合适的anchor设置非常困难\n\n## 解决思想\n丢弃anchor，模型在回归目标位置时，不是去预测基于anchor的偏移量，而是使用两个feature map同时预测两个角点（左上、右下）即可，在feature map上的点，只有为目标角点的时候才进行激活预测其目标类别，只是这里又有三个问题：\n\n- 两个feature map预测出来的角点，如何进行匹配，从而合并成一个目标？\n- 目标在两个角点之间，因此feature map上的角点位置缺少关于目标的信息，如何准确的进行分类？\n- feature map在经过下采样之后，feature map上的点的位置要映射回原图，这个时候存在位置精度的损失，可能造成预测位置的偏差。\n\n## 具体做法\n针对第一个角点匹配的问题，这里引用了另外一篇论文的方法（《ssociative embedding: End-to-end learning forjoint detection and grouping》），在模型中，两个角点坐标的feature map都会额外预测一个一维的嵌入向量（embedding vector），希望当两个角点匹配时，两个角点位置预测的嵌入向量的距离尽可能接近，否则尽可能远离（**这里嵌入向量的想法没看得太懂，可能还要看看原论文才能理解，除此之外计算量貌似有点大**），因此定义了两个loss如下所示，其中$e_{t_k}$代表第k个目标左上角点的嵌入向量，$e_{b_k}$代表第k个目标右下角点的嵌入向量，$e_k = \\frac{(e_{t_k} + e_{b_k})}{2}$，N代表目标的个数，$\\Delta$在论文的所有实验中均为1。\n$$\n\\begin{aligned}\n    L_{pull} &= \\frac{1}{N}\\sum_{k=1}^N[(e_{t_k} - e_k) ^ 2 + (e_{b_k} - e_k) ^ 2]\\\\\n    L_{push} &= \\frac{1}{N(N - 1)}\\sum_{k=1}^N\\sum_{\\begin{aligned}\n        j = 1 \\\\\n        j \\neq k\n    \\end{aligned}}max(0, \\Delta - |e_k - e_j|)\n\\end{aligned}\n$$\n\n针对第二个角点处没有目标信息的问题，论文中提出了Corner Pooling，如下图所示，在左上角角点预测过程中，进行pooling时，每个点水平向右和垂直向下搜索，找到最大的值，之后加起来作为这个点pooling之后的值，这样给出了当前点是否是角点的判断信息，并且引入了一个非常强的先验：一个点是否是左上角点，需要看其右边的信息和下面的信息来判断。而如果是在预测右下角点，那么Corner Pooling变为每个点水平向左和垂直向上搜索。\n\n![CornerPooling示意图](CornerPooling.png)\n\n这里为了加快Corner Pooling的计算，如果是左上角点的Corner Pooling，可以从右下点开始计算，计算方式如下，$t_{ij}$和$l_{ij}$分别代表水平向右搜索和垂直向下搜索的结果，最终pooling结果$p_{ij} = t_{ij} + l_{ij}$。\n$$\n\\begin{aligned}\n    t_{ij} &= \\begin{cases}\n        max(f_{t_{ij}}, t_{(i+1)j}) & if \\quad i < H\\\\\n        f_{t_{Hj}} & otherwise\n    \\end{cases}\\\\\n    l_{ij} &= \\begin{cases}\n        max(f_{t_{ij}}, t_{i(j+1)}) & if \\quad j < W\\\\\n        f_{t_{iW}} & otherwise\n    \\end{cases}\n\\end{aligned}\n$$\n\n针对第三个问题，在模型中还预测两组偏移量，分别对左上角点和右下角点进行偏移，对于一组偏移量，其目标$o_k$计算如下，并使用SmoothL1Loss进行训练。\n$$\no_k = (\\frac{x}{n} - \\lfloor\\frac{x}{n}\\rfloor, \\frac{y}{n} - \\lfloor\\frac{y}{n}\\rfloor)\n$$\n\n最后要说的就是ground truth匹配问题，一个geound truth只匹配一个左上角点和一个右下角点，其余点都为负样本，但在训练时，在正样本周围画一个半径为$r$的圈，这里有个约束：半径$r$范围内的点，要保证可以和ground truth生成iou大于0.7的检测框，从而决定$r$的大小。\n\n这个圈中除中心点之外，其余点在计算loss时，会降低其loss权重，作者定义了一个focal loss的变种，作为每个点的分类损失函数，如下所示，其中$C、H、W$分别表示类别数、图片高度、图片宽度，N表示图片上目标个数，$p_{cij}$表示位置$ij$处的$c$类预测概率，在论文所有实验中，$\\alpha = 2$，$\\beta = 4$，$y_{cij}$不是纯粹的标签，还编码了刚才说的loss函数权重信息，对于正样本点，$y_{cij} = 1$，对于正样本周围半径为$r$的圈内的点，$y_{cij} = 1 - e^{-\\frac{x^2+y^2}{\\sigma}}$(这里的$xy$是相对于中心点的坐标)，其中$\\sigma = \\frac{r}{3}$，而对于其余负样本点，$y_{cij} = 0$。\n$$\n\\begin{aligned}\n    L_{det} = \\frac{-1}{N}\\sum_{c=1}^C\\sum_{i=1}^H\\sum_{j=1}^W\\begin{cases}\n        (1 - p_{cij})^\\alpha log(p_{cij}) & if \\quad y = 1\\\\\n        (1 - y_{cij})^\\beta p_{cij}^\\alpha log(1 - p_{cij}) & otherwise\n    \\end{cases}\n\\end{aligned}\n$$\n\n## 存在的问题\n个人觉得这个模型的计算可能非常慢，首先是嵌入向量的匹配过程，我没有想到比较好的实现方式，暴力匹配的话应该非常慢，还有一个问题是Corner Pooling的计算，这里如果需要按顺序计算，那么GPU并行加速提升不大。\n\n","source":"_posts/论文阅读/论文阅读《CornerNet-Detecting-Objects-as-Paired-Keypoints》.md","raw":"---\ntitle: “论文阅读《CornerNet-Detecting-Objects-as-Paired-Keypoints》”\ndate: 2019-08-07 20:17:13\ntags: 论文阅读\nmathjax: true\n---\n\n## 提出的问题\n\n单阶段目标检测大多基于anchor实现，但是用anchor存在两个问题：\n\n- 为了保证所有ground truth都有匹配的anchor，不得不使用非常多的anchor，例如retinanet anchor个数100K, DSSD anchor个数40K，但是真正与ground truth iou较大的anchor非常少，导致了很严重的正负样本不平衡问题。\n- 设置anchor时需要非常多的超参数，anchor的大小、长宽比、个数，使得找到合适的anchor设置非常困难\n\n## 解决思想\n丢弃anchor，模型在回归目标位置时，不是去预测基于anchor的偏移量，而是使用两个feature map同时预测两个角点（左上、右下）即可，在feature map上的点，只有为目标角点的时候才进行激活预测其目标类别，只是这里又有三个问题：\n\n- 两个feature map预测出来的角点，如何进行匹配，从而合并成一个目标？\n- 目标在两个角点之间，因此feature map上的角点位置缺少关于目标的信息，如何准确的进行分类？\n- feature map在经过下采样之后，feature map上的点的位置要映射回原图，这个时候存在位置精度的损失，可能造成预测位置的偏差。\n\n## 具体做法\n针对第一个角点匹配的问题，这里引用了另外一篇论文的方法（《ssociative embedding: End-to-end learning forjoint detection and grouping》），在模型中，两个角点坐标的feature map都会额外预测一个一维的嵌入向量（embedding vector），希望当两个角点匹配时，两个角点位置预测的嵌入向量的距离尽可能接近，否则尽可能远离（**这里嵌入向量的想法没看得太懂，可能还要看看原论文才能理解，除此之外计算量貌似有点大**），因此定义了两个loss如下所示，其中$e_{t_k}$代表第k个目标左上角点的嵌入向量，$e_{b_k}$代表第k个目标右下角点的嵌入向量，$e_k = \\frac{(e_{t_k} + e_{b_k})}{2}$，N代表目标的个数，$\\Delta$在论文的所有实验中均为1。\n$$\n\\begin{aligned}\n    L_{pull} &= \\frac{1}{N}\\sum_{k=1}^N[(e_{t_k} - e_k) ^ 2 + (e_{b_k} - e_k) ^ 2]\\\\\n    L_{push} &= \\frac{1}{N(N - 1)}\\sum_{k=1}^N\\sum_{\\begin{aligned}\n        j = 1 \\\\\n        j \\neq k\n    \\end{aligned}}max(0, \\Delta - |e_k - e_j|)\n\\end{aligned}\n$$\n\n针对第二个角点处没有目标信息的问题，论文中提出了Corner Pooling，如下图所示，在左上角角点预测过程中，进行pooling时，每个点水平向右和垂直向下搜索，找到最大的值，之后加起来作为这个点pooling之后的值，这样给出了当前点是否是角点的判断信息，并且引入了一个非常强的先验：一个点是否是左上角点，需要看其右边的信息和下面的信息来判断。而如果是在预测右下角点，那么Corner Pooling变为每个点水平向左和垂直向上搜索。\n\n![CornerPooling示意图](CornerPooling.png)\n\n这里为了加快Corner Pooling的计算，如果是左上角点的Corner Pooling，可以从右下点开始计算，计算方式如下，$t_{ij}$和$l_{ij}$分别代表水平向右搜索和垂直向下搜索的结果，最终pooling结果$p_{ij} = t_{ij} + l_{ij}$。\n$$\n\\begin{aligned}\n    t_{ij} &= \\begin{cases}\n        max(f_{t_{ij}}, t_{(i+1)j}) & if \\quad i < H\\\\\n        f_{t_{Hj}} & otherwise\n    \\end{cases}\\\\\n    l_{ij} &= \\begin{cases}\n        max(f_{t_{ij}}, t_{i(j+1)}) & if \\quad j < W\\\\\n        f_{t_{iW}} & otherwise\n    \\end{cases}\n\\end{aligned}\n$$\n\n针对第三个问题，在模型中还预测两组偏移量，分别对左上角点和右下角点进行偏移，对于一组偏移量，其目标$o_k$计算如下，并使用SmoothL1Loss进行训练。\n$$\no_k = (\\frac{x}{n} - \\lfloor\\frac{x}{n}\\rfloor, \\frac{y}{n} - \\lfloor\\frac{y}{n}\\rfloor)\n$$\n\n最后要说的就是ground truth匹配问题，一个geound truth只匹配一个左上角点和一个右下角点，其余点都为负样本，但在训练时，在正样本周围画一个半径为$r$的圈，这里有个约束：半径$r$范围内的点，要保证可以和ground truth生成iou大于0.7的检测框，从而决定$r$的大小。\n\n这个圈中除中心点之外，其余点在计算loss时，会降低其loss权重，作者定义了一个focal loss的变种，作为每个点的分类损失函数，如下所示，其中$C、H、W$分别表示类别数、图片高度、图片宽度，N表示图片上目标个数，$p_{cij}$表示位置$ij$处的$c$类预测概率，在论文所有实验中，$\\alpha = 2$，$\\beta = 4$，$y_{cij}$不是纯粹的标签，还编码了刚才说的loss函数权重信息，对于正样本点，$y_{cij} = 1$，对于正样本周围半径为$r$的圈内的点，$y_{cij} = 1 - e^{-\\frac{x^2+y^2}{\\sigma}}$(这里的$xy$是相对于中心点的坐标)，其中$\\sigma = \\frac{r}{3}$，而对于其余负样本点，$y_{cij} = 0$。\n$$\n\\begin{aligned}\n    L_{det} = \\frac{-1}{N}\\sum_{c=1}^C\\sum_{i=1}^H\\sum_{j=1}^W\\begin{cases}\n        (1 - p_{cij})^\\alpha log(p_{cij}) & if \\quad y = 1\\\\\n        (1 - y_{cij})^\\beta p_{cij}^\\alpha log(1 - p_{cij}) & otherwise\n    \\end{cases}\n\\end{aligned}\n$$\n\n## 存在的问题\n个人觉得这个模型的计算可能非常慢，首先是嵌入向量的匹配过程，我没有想到比较好的实现方式，暴力匹配的话应该非常慢，还有一个问题是Corner Pooling的计算，这里如果需要按顺序计算，那么GPU并行加速提升不大。\n\n","slug":"论文阅读/论文阅读《CornerNet-Detecting-Objects-as-Paired-Keypoints》","published":1,"updated":"2020-08-31T06:39:20.786Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckjxpc3sa002244mqbfhh1fdm","content":"<h2 id=\"提出的问题\">提出的问题</h2>\r\n<p>单阶段目标检测大多基于anchor实现，但是用anchor存在两个问题：</p>\r\n<ul>\r\n<li>为了保证所有ground truth都有匹配的anchor，不得不使用非常多的anchor，例如retinanet anchor个数100K, DSSD anchor个数40K，但是真正与ground truth iou较大的anchor非常少，导致了很严重的正负样本不平衡问题。</li>\r\n<li>设置anchor时需要非常多的超参数，anchor的大小、长宽比、个数，使得找到合适的anchor设置非常困难</li>\r\n</ul>\r\n<h2 id=\"解决思想\">解决思想</h2>\r\n<p>丢弃anchor，模型在回归目标位置时，不是去预测基于anchor的偏移量，而是使用两个feature map同时预测两个角点（左上、右下）即可，在feature map上的点，只有为目标角点的时候才进行激活预测其目标类别，只是这里又有三个问题：</p>\r\n<ul>\r\n<li>两个feature map预测出来的角点，如何进行匹配，从而合并成一个目标？</li>\r\n<li>目标在两个角点之间，因此feature map上的角点位置缺少关于目标的信息，如何准确的进行分类？</li>\r\n<li>feature map在经过下采样之后，feature map上的点的位置要映射回原图，这个时候存在位置精度的损失，可能造成预测位置的偏差。</li>\r\n</ul>\r\n<h2 id=\"具体做法\">具体做法</h2>\r\n<p>针对第一个角点匹配的问题，这里引用了另外一篇论文的方法（《ssociative embedding: End-to-end learning forjoint detection and grouping》），在模型中，两个角点坐标的feature map都会额外预测一个一维的嵌入向量（embedding vector），希望当两个角点匹配时，两个角点位置预测的嵌入向量的距离尽可能接近，否则尽可能远离（<strong>这里嵌入向量的想法没看得太懂，可能还要看看原论文才能理解，除此之外计算量貌似有点大</strong>），因此定义了两个loss如下所示，其中<span class=\"math inline\">\\(e_{t_k}\\)</span>代表第k个目标左上角点的嵌入向量，<span class=\"math inline\">\\(e_{b_k}\\)</span>代表第k个目标右下角点的嵌入向量，<span class=\"math inline\">\\(e_k = \\frac{(e_{t_k} + e_{b_k})}{2}\\)</span>，N代表目标的个数，<span class=\"math inline\">\\(\\Delta\\)</span>在论文的所有实验中均为1。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    L_{pull} &amp;= \\frac{1}{N}\\sum_{k=1}^N[(e_{t_k} - e_k) ^ 2 + (e_{b_k} - e_k) ^ 2]\\\\\r\n    L_{push} &amp;= \\frac{1}{N(N - 1)}\\sum_{k=1}^N\\sum_{\\begin{aligned}\r\n        j = 1 \\\\\r\n        j \\neq k\r\n    \\end{aligned}}max(0, \\Delta - |e_k - e_j|)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>针对第二个角点处没有目标信息的问题，论文中提出了Corner Pooling，如下图所示，在左上角角点预测过程中，进行pooling时，每个点水平向右和垂直向下搜索，找到最大的值，之后加起来作为这个点pooling之后的值，这样给出了当前点是否是角点的判断信息，并且引入了一个非常强的先验：一个点是否是左上角点，需要看其右边的信息和下面的信息来判断。而如果是在预测右下角点，那么Corner Pooling变为每个点水平向左和垂直向上搜索。</p>\r\n<figure>\r\n<img src=\"CornerPooling.png\" alt=\"CornerPooling示意图\" /><figcaption aria-hidden=\"true\">CornerPooling示意图</figcaption>\r\n</figure>\r\n<p>这里为了加快Corner Pooling的计算，如果是左上角点的Corner Pooling，可以从右下点开始计算，计算方式如下，<span class=\"math inline\">\\(t_{ij}\\)</span>和<span class=\"math inline\">\\(l_{ij}\\)</span>分别代表水平向右搜索和垂直向下搜索的结果，最终pooling结果<span class=\"math inline\">\\(p_{ij} = t_{ij} + l_{ij}\\)</span>。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    t_{ij} &amp;= \\begin{cases}\r\n        max(f_{t_{ij}}, t_{(i+1)j}) &amp; if \\quad i &lt; H\\\\\r\n        f_{t_{Hj}} &amp; otherwise\r\n    \\end{cases}\\\\\r\n    l_{ij} &amp;= \\begin{cases}\r\n        max(f_{t_{ij}}, t_{i(j+1)}) &amp; if \\quad j &lt; W\\\\\r\n        f_{t_{iW}} &amp; otherwise\r\n    \\end{cases}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>针对第三个问题，在模型中还预测两组偏移量，分别对左上角点和右下角点进行偏移，对于一组偏移量，其目标<span class=\"math inline\">\\(o_k\\)</span>计算如下，并使用SmoothL1Loss进行训练。 <span class=\"math display\">\\[\r\no_k = (\\frac{x}{n} - \\lfloor\\frac{x}{n}\\rfloor, \\frac{y}{n} - \\lfloor\\frac{y}{n}\\rfloor)\r\n\\]</span></p>\r\n<p>最后要说的就是ground truth匹配问题，一个geound truth只匹配一个左上角点和一个右下角点，其余点都为负样本，但在训练时，在正样本周围画一个半径为<span class=\"math inline\">\\(r\\)</span>的圈，这里有个约束：半径<span class=\"math inline\">\\(r\\)</span>范围内的点，要保证可以和ground truth生成iou大于0.7的检测框，从而决定<span class=\"math inline\">\\(r\\)</span>的大小。</p>\r\n<p>这个圈中除中心点之外，其余点在计算loss时，会降低其loss权重，作者定义了一个focal loss的变种，作为每个点的分类损失函数，如下所示，其中<span class=\"math inline\">\\(C、H、W\\)</span>分别表示类别数、图片高度、图片宽度，N表示图片上目标个数，<span class=\"math inline\">\\(p_{cij}\\)</span>表示位置<span class=\"math inline\">\\(ij\\)</span>处的<span class=\"math inline\">\\(c\\)</span>类预测概率，在论文所有实验中，<span class=\"math inline\">\\(\\alpha = 2\\)</span>，<span class=\"math inline\">\\(\\beta = 4\\)</span>，<span class=\"math inline\">\\(y_{cij}\\)</span>不是纯粹的标签，还编码了刚才说的loss函数权重信息，对于正样本点，<span class=\"math inline\">\\(y_{cij} = 1\\)</span>，对于正样本周围半径为<span class=\"math inline\">\\(r\\)</span>的圈内的点，<span class=\"math inline\">\\(y_{cij} = 1 - e^{-\\frac{x^2+y^2}{\\sigma}}\\)</span>(这里的<span class=\"math inline\">\\(xy\\)</span>是相对于中心点的坐标)，其中<span class=\"math inline\">\\(\\sigma = \\frac{r}{3}\\)</span>，而对于其余负样本点，<span class=\"math inline\">\\(y_{cij} = 0\\)</span>。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    L_{det} = \\frac{-1}{N}\\sum_{c=1}^C\\sum_{i=1}^H\\sum_{j=1}^W\\begin{cases}\r\n        (1 - p_{cij})^\\alpha log(p_{cij}) &amp; if \\quad y = 1\\\\\r\n        (1 - y_{cij})^\\beta p_{cij}^\\alpha log(1 - p_{cij}) &amp; otherwise\r\n    \\end{cases}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<h2 id=\"存在的问题\">存在的问题</h2>\r\n<p>个人觉得这个模型的计算可能非常慢，首先是嵌入向量的匹配过程，我没有想到比较好的实现方式，暴力匹配的话应该非常慢，还有一个问题是Corner Pooling的计算，这里如果需要按顺序计算，那么GPU并行加速提升不大。</p>\r\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"提出的问题\">提出的问题</h2>\r\n<p>单阶段目标检测大多基于anchor实现，但是用anchor存在两个问题：</p>\r\n<ul>\r\n<li>为了保证所有ground truth都有匹配的anchor，不得不使用非常多的anchor，例如retinanet anchor个数100K, DSSD anchor个数40K，但是真正与ground truth iou较大的anchor非常少，导致了很严重的正负样本不平衡问题。</li>\r\n<li>设置anchor时需要非常多的超参数，anchor的大小、长宽比、个数，使得找到合适的anchor设置非常困难</li>\r\n</ul>\r\n<h2 id=\"解决思想\">解决思想</h2>\r\n<p>丢弃anchor，模型在回归目标位置时，不是去预测基于anchor的偏移量，而是使用两个feature map同时预测两个角点（左上、右下）即可，在feature map上的点，只有为目标角点的时候才进行激活预测其目标类别，只是这里又有三个问题：</p>\r\n<ul>\r\n<li>两个feature map预测出来的角点，如何进行匹配，从而合并成一个目标？</li>\r\n<li>目标在两个角点之间，因此feature map上的角点位置缺少关于目标的信息，如何准确的进行分类？</li>\r\n<li>feature map在经过下采样之后，feature map上的点的位置要映射回原图，这个时候存在位置精度的损失，可能造成预测位置的偏差。</li>\r\n</ul>\r\n<h2 id=\"具体做法\">具体做法</h2>\r\n<p>针对第一个角点匹配的问题，这里引用了另外一篇论文的方法（《ssociative embedding: End-to-end learning forjoint detection and grouping》），在模型中，两个角点坐标的feature map都会额外预测一个一维的嵌入向量（embedding vector），希望当两个角点匹配时，两个角点位置预测的嵌入向量的距离尽可能接近，否则尽可能远离（<strong>这里嵌入向量的想法没看得太懂，可能还要看看原论文才能理解，除此之外计算量貌似有点大</strong>），因此定义了两个loss如下所示，其中<span class=\"math inline\">\\(e_{t_k}\\)</span>代表第k个目标左上角点的嵌入向量，<span class=\"math inline\">\\(e_{b_k}\\)</span>代表第k个目标右下角点的嵌入向量，<span class=\"math inline\">\\(e_k = \\frac{(e_{t_k} + e_{b_k})}{2}\\)</span>，N代表目标的个数，<span class=\"math inline\">\\(\\Delta\\)</span>在论文的所有实验中均为1。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    L_{pull} &amp;= \\frac{1}{N}\\sum_{k=1}^N[(e_{t_k} - e_k) ^ 2 + (e_{b_k} - e_k) ^ 2]\\\\\r\n    L_{push} &amp;= \\frac{1}{N(N - 1)}\\sum_{k=1}^N\\sum_{\\begin{aligned}\r\n        j = 1 \\\\\r\n        j \\neq k\r\n    \\end{aligned}}max(0, \\Delta - |e_k - e_j|)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>针对第二个角点处没有目标信息的问题，论文中提出了Corner Pooling，如下图所示，在左上角角点预测过程中，进行pooling时，每个点水平向右和垂直向下搜索，找到最大的值，之后加起来作为这个点pooling之后的值，这样给出了当前点是否是角点的判断信息，并且引入了一个非常强的先验：一个点是否是左上角点，需要看其右边的信息和下面的信息来判断。而如果是在预测右下角点，那么Corner Pooling变为每个点水平向左和垂直向上搜索。</p>\r\n<figure>\r\n<img src=\"CornerPooling.png\" alt=\"CornerPooling示意图\" /><figcaption aria-hidden=\"true\">CornerPooling示意图</figcaption>\r\n</figure>\r\n<p>这里为了加快Corner Pooling的计算，如果是左上角点的Corner Pooling，可以从右下点开始计算，计算方式如下，<span class=\"math inline\">\\(t_{ij}\\)</span>和<span class=\"math inline\">\\(l_{ij}\\)</span>分别代表水平向右搜索和垂直向下搜索的结果，最终pooling结果<span class=\"math inline\">\\(p_{ij} = t_{ij} + l_{ij}\\)</span>。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    t_{ij} &amp;= \\begin{cases}\r\n        max(f_{t_{ij}}, t_{(i+1)j}) &amp; if \\quad i &lt; H\\\\\r\n        f_{t_{Hj}} &amp; otherwise\r\n    \\end{cases}\\\\\r\n    l_{ij} &amp;= \\begin{cases}\r\n        max(f_{t_{ij}}, t_{i(j+1)}) &amp; if \\quad j &lt; W\\\\\r\n        f_{t_{iW}} &amp; otherwise\r\n    \\end{cases}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>针对第三个问题，在模型中还预测两组偏移量，分别对左上角点和右下角点进行偏移，对于一组偏移量，其目标<span class=\"math inline\">\\(o_k\\)</span>计算如下，并使用SmoothL1Loss进行训练。 <span class=\"math display\">\\[\r\no_k = (\\frac{x}{n} - \\lfloor\\frac{x}{n}\\rfloor, \\frac{y}{n} - \\lfloor\\frac{y}{n}\\rfloor)\r\n\\]</span></p>\r\n<p>最后要说的就是ground truth匹配问题，一个geound truth只匹配一个左上角点和一个右下角点，其余点都为负样本，但在训练时，在正样本周围画一个半径为<span class=\"math inline\">\\(r\\)</span>的圈，这里有个约束：半径<span class=\"math inline\">\\(r\\)</span>范围内的点，要保证可以和ground truth生成iou大于0.7的检测框，从而决定<span class=\"math inline\">\\(r\\)</span>的大小。</p>\r\n<p>这个圈中除中心点之外，其余点在计算loss时，会降低其loss权重，作者定义了一个focal loss的变种，作为每个点的分类损失函数，如下所示，其中<span class=\"math inline\">\\(C、H、W\\)</span>分别表示类别数、图片高度、图片宽度，N表示图片上目标个数，<span class=\"math inline\">\\(p_{cij}\\)</span>表示位置<span class=\"math inline\">\\(ij\\)</span>处的<span class=\"math inline\">\\(c\\)</span>类预测概率，在论文所有实验中，<span class=\"math inline\">\\(\\alpha = 2\\)</span>，<span class=\"math inline\">\\(\\beta = 4\\)</span>，<span class=\"math inline\">\\(y_{cij}\\)</span>不是纯粹的标签，还编码了刚才说的loss函数权重信息，对于正样本点，<span class=\"math inline\">\\(y_{cij} = 1\\)</span>，对于正样本周围半径为<span class=\"math inline\">\\(r\\)</span>的圈内的点，<span class=\"math inline\">\\(y_{cij} = 1 - e^{-\\frac{x^2+y^2}{\\sigma}}\\)</span>(这里的<span class=\"math inline\">\\(xy\\)</span>是相对于中心点的坐标)，其中<span class=\"math inline\">\\(\\sigma = \\frac{r}{3}\\)</span>，而对于其余负样本点，<span class=\"math inline\">\\(y_{cij} = 0\\)</span>。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    L_{det} = \\frac{-1}{N}\\sum_{c=1}^C\\sum_{i=1}^H\\sum_{j=1}^W\\begin{cases}\r\n        (1 - p_{cij})^\\alpha log(p_{cij}) &amp; if \\quad y = 1\\\\\r\n        (1 - y_{cij})^\\beta p_{cij}^\\alpha log(1 - p_{cij}) &amp; otherwise\r\n    \\end{cases}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<h2 id=\"存在的问题\">存在的问题</h2>\r\n<p>个人觉得这个模型的计算可能非常慢，首先是嵌入向量的匹配过程，我没有想到比较好的实现方式，暴力匹配的话应该非常慢，还有一个问题是Corner Pooling的计算，这里如果需要按顺序计算，那么GPU并行加速提升不大。</p>\r\n"},{"title":"论文阅读《Deep-Bilateral-Learning-for-Real-Time-Image-Enhancement》","date":"2020-08-27T12:57:39.000Z","mathjax":true,"_content":"\n最近在看风格转换的时候看了一篇文章叫《Joint Bilateral Learning for Real-time UniversalPhotorealistic Style Transfer》，然后顺着这方面的研究往上捋，发现了这篇文章，这里进行一些记录。\n\n## 提出的问题\n在移动端的图像处理任务上，对于高分辨率图像，一些操作可能耗时很久。\n\n## 解决思路\n对于一个高分辨率图像，首先下采样得到低分辨率图像，在低分辨率图像上训练一个CNN，让其预测双边空间（bilateral space）下的局部仿射变换的系数，然后通过slice操作将这个预测出的局部仿射变换系数插值到高分辨率下，最后通过这个仿射变换对高分辨率图像进行处理，得到处理之后的高分辨率图像。\n\n## 具体实现\n模型的整体结构如下图所示，其中对于一个高分辨率输入图$I$，首先下采样得到低分辨率图像$\\tilde{I}$，用一个CNN模型（VGG-19）在这个低分辨率图像上提取特征（在共享的vgg结构结束之后，有两个分支，分别提取局部特征$L^i$和全局特征$G^i$），之后将局部特征和全局特征混合，输出双边网格方式的图像变换系数矩阵$A$，另一边，高分辨率图像也要做一个简单的pointwise卷积和激活层，转换成高分辨率的引导图像$g$，然后根据$g$对$A$进行插值，得到高分辨率的变换系数矩阵$\\bar{A}$，将这个变换系数矩阵应用在原图上，即可完成对原图的处理操作，得到输出图像$O$。\n\n![模型整体结构](模型整体结构.png)\n\n## 实验结果\n这个模型可以专门训练用于模拟复杂的一些图像操作，例如HDR+、Local Laplacian filter、Face brightening等，在移动设备上，可以节约这些操作的时间，作者的实验中显示，在同样的设备上，传统算法对一张$1920\\times 1080$的图像进行Local Laplacian filter，大概需要200ms的时间，而这个模型只需要20ms左右。\n\n## 存在的问题\n由于这个模型预测的变换系数是针对每个像素的，例如输入图片是三通道，那么每个像素的变换系数是一个$3\\times 4$的矩阵（包含一个bias），这样也就限制这个模型只能输出单像素输出只依赖于单像素输入的变换系数。另外对于去雾等操作，这个模型的效果也不是很好，因为pointwise卷积得到的guide图像在去雾方面不能做一个很好的上采样引导。","source":"_posts/论文阅读/论文阅读《Deep-Bilateral-Learning-for-Real-Time-Image-Enhancement》.md","raw":"---\ntitle: 论文阅读《Deep-Bilateral-Learning-for-Real-Time-Image-Enhancement》\ndate: 2020-08-27 20:57:39\ntags: [论文阅读]\nmathjax: true\n---\n\n最近在看风格转换的时候看了一篇文章叫《Joint Bilateral Learning for Real-time UniversalPhotorealistic Style Transfer》，然后顺着这方面的研究往上捋，发现了这篇文章，这里进行一些记录。\n\n## 提出的问题\n在移动端的图像处理任务上，对于高分辨率图像，一些操作可能耗时很久。\n\n## 解决思路\n对于一个高分辨率图像，首先下采样得到低分辨率图像，在低分辨率图像上训练一个CNN，让其预测双边空间（bilateral space）下的局部仿射变换的系数，然后通过slice操作将这个预测出的局部仿射变换系数插值到高分辨率下，最后通过这个仿射变换对高分辨率图像进行处理，得到处理之后的高分辨率图像。\n\n## 具体实现\n模型的整体结构如下图所示，其中对于一个高分辨率输入图$I$，首先下采样得到低分辨率图像$\\tilde{I}$，用一个CNN模型（VGG-19）在这个低分辨率图像上提取特征（在共享的vgg结构结束之后，有两个分支，分别提取局部特征$L^i$和全局特征$G^i$），之后将局部特征和全局特征混合，输出双边网格方式的图像变换系数矩阵$A$，另一边，高分辨率图像也要做一个简单的pointwise卷积和激活层，转换成高分辨率的引导图像$g$，然后根据$g$对$A$进行插值，得到高分辨率的变换系数矩阵$\\bar{A}$，将这个变换系数矩阵应用在原图上，即可完成对原图的处理操作，得到输出图像$O$。\n\n![模型整体结构](模型整体结构.png)\n\n## 实验结果\n这个模型可以专门训练用于模拟复杂的一些图像操作，例如HDR+、Local Laplacian filter、Face brightening等，在移动设备上，可以节约这些操作的时间，作者的实验中显示，在同样的设备上，传统算法对一张$1920\\times 1080$的图像进行Local Laplacian filter，大概需要200ms的时间，而这个模型只需要20ms左右。\n\n## 存在的问题\n由于这个模型预测的变换系数是针对每个像素的，例如输入图片是三通道，那么每个像素的变换系数是一个$3\\times 4$的矩阵（包含一个bias），这样也就限制这个模型只能输出单像素输出只依赖于单像素输入的变换系数。另外对于去雾等操作，这个模型的效果也不是很好，因为pointwise卷积得到的guide图像在去雾方面不能做一个很好的上采样引导。","slug":"论文阅读/论文阅读《Deep-Bilateral-Learning-for-Real-Time-Image-Enhancement》","published":1,"updated":"2020-08-31T06:39:20.787Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckjxpc3sb002544mq12tv2ev0","content":"<p>最近在看风格转换的时候看了一篇文章叫《Joint Bilateral Learning for Real-time UniversalPhotorealistic Style Transfer》，然后顺着这方面的研究往上捋，发现了这篇文章，这里进行一些记录。</p>\r\n<h2 id=\"提出的问题\">提出的问题</h2>\r\n<p>在移动端的图像处理任务上，对于高分辨率图像，一些操作可能耗时很久。</p>\r\n<h2 id=\"解决思路\">解决思路</h2>\r\n<p>对于一个高分辨率图像，首先下采样得到低分辨率图像，在低分辨率图像上训练一个CNN，让其预测双边空间（bilateral space）下的局部仿射变换的系数，然后通过slice操作将这个预测出的局部仿射变换系数插值到高分辨率下，最后通过这个仿射变换对高分辨率图像进行处理，得到处理之后的高分辨率图像。</p>\r\n<h2 id=\"具体实现\">具体实现</h2>\r\n<p>模型的整体结构如下图所示，其中对于一个高分辨率输入图<span class=\"math inline\">\\(I\\)</span>，首先下采样得到低分辨率图像<span class=\"math inline\">\\(\\tilde{I}\\)</span>，用一个CNN模型（VGG-19）在这个低分辨率图像上提取特征（在共享的vgg结构结束之后，有两个分支，分别提取局部特征<span class=\"math inline\">\\(L^i\\)</span>和全局特征<span class=\"math inline\">\\(G^i\\)</span>），之后将局部特征和全局特征混合，输出双边网格方式的图像变换系数矩阵<span class=\"math inline\">\\(A\\)</span>，另一边，高分辨率图像也要做一个简单的pointwise卷积和激活层，转换成高分辨率的引导图像<span class=\"math inline\">\\(g\\)</span>，然后根据<span class=\"math inline\">\\(g\\)</span>对<span class=\"math inline\">\\(A\\)</span>进行插值，得到高分辨率的变换系数矩阵<span class=\"math inline\">\\(\\bar{A}\\)</span>，将这个变换系数矩阵应用在原图上，即可完成对原图的处理操作，得到输出图像<span class=\"math inline\">\\(O\\)</span>。</p>\r\n<figure>\r\n<img src=\"模型整体结构.png\" alt=\"模型整体结构\" /><figcaption aria-hidden=\"true\">模型整体结构</figcaption>\r\n</figure>\r\n<h2 id=\"实验结果\">实验结果</h2>\r\n<p>这个模型可以专门训练用于模拟复杂的一些图像操作，例如HDR+、Local Laplacian filter、Face brightening等，在移动设备上，可以节约这些操作的时间，作者的实验中显示，在同样的设备上，传统算法对一张<span class=\"math inline\">\\(1920\\times 1080\\)</span>的图像进行Local Laplacian filter，大概需要200ms的时间，而这个模型只需要20ms左右。</p>\r\n<h2 id=\"存在的问题\">存在的问题</h2>\r\n<p>由于这个模型预测的变换系数是针对每个像素的，例如输入图片是三通道，那么每个像素的变换系数是一个<span class=\"math inline\">\\(3\\times 4\\)</span>的矩阵（包含一个bias），这样也就限制这个模型只能输出单像素输出只依赖于单像素输入的变换系数。另外对于去雾等操作，这个模型的效果也不是很好，因为pointwise卷积得到的guide图像在去雾方面不能做一个很好的上采样引导。</p>\r\n","site":{"data":{}},"excerpt":"","more":"<p>最近在看风格转换的时候看了一篇文章叫《Joint Bilateral Learning for Real-time UniversalPhotorealistic Style Transfer》，然后顺着这方面的研究往上捋，发现了这篇文章，这里进行一些记录。</p>\r\n<h2 id=\"提出的问题\">提出的问题</h2>\r\n<p>在移动端的图像处理任务上，对于高分辨率图像，一些操作可能耗时很久。</p>\r\n<h2 id=\"解决思路\">解决思路</h2>\r\n<p>对于一个高分辨率图像，首先下采样得到低分辨率图像，在低分辨率图像上训练一个CNN，让其预测双边空间（bilateral space）下的局部仿射变换的系数，然后通过slice操作将这个预测出的局部仿射变换系数插值到高分辨率下，最后通过这个仿射变换对高分辨率图像进行处理，得到处理之后的高分辨率图像。</p>\r\n<h2 id=\"具体实现\">具体实现</h2>\r\n<p>模型的整体结构如下图所示，其中对于一个高分辨率输入图<span class=\"math inline\">\\(I\\)</span>，首先下采样得到低分辨率图像<span class=\"math inline\">\\(\\tilde{I}\\)</span>，用一个CNN模型（VGG-19）在这个低分辨率图像上提取特征（在共享的vgg结构结束之后，有两个分支，分别提取局部特征<span class=\"math inline\">\\(L^i\\)</span>和全局特征<span class=\"math inline\">\\(G^i\\)</span>），之后将局部特征和全局特征混合，输出双边网格方式的图像变换系数矩阵<span class=\"math inline\">\\(A\\)</span>，另一边，高分辨率图像也要做一个简单的pointwise卷积和激活层，转换成高分辨率的引导图像<span class=\"math inline\">\\(g\\)</span>，然后根据<span class=\"math inline\">\\(g\\)</span>对<span class=\"math inline\">\\(A\\)</span>进行插值，得到高分辨率的变换系数矩阵<span class=\"math inline\">\\(\\bar{A}\\)</span>，将这个变换系数矩阵应用在原图上，即可完成对原图的处理操作，得到输出图像<span class=\"math inline\">\\(O\\)</span>。</p>\r\n<figure>\r\n<img src=\"模型整体结构.png\" alt=\"模型整体结构\" /><figcaption aria-hidden=\"true\">模型整体结构</figcaption>\r\n</figure>\r\n<h2 id=\"实验结果\">实验结果</h2>\r\n<p>这个模型可以专门训练用于模拟复杂的一些图像操作，例如HDR+、Local Laplacian filter、Face brightening等，在移动设备上，可以节约这些操作的时间，作者的实验中显示，在同样的设备上，传统算法对一张<span class=\"math inline\">\\(1920\\times 1080\\)</span>的图像进行Local Laplacian filter，大概需要200ms的时间，而这个模型只需要20ms左右。</p>\r\n<h2 id=\"存在的问题\">存在的问题</h2>\r\n<p>由于这个模型预测的变换系数是针对每个像素的，例如输入图片是三通道，那么每个像素的变换系数是一个<span class=\"math inline\">\\(3\\times 4\\)</span>的矩阵（包含一个bias），这样也就限制这个模型只能输出单像素输出只依赖于单像素输入的变换系数。另外对于去雾等操作，这个模型的效果也不是很好，因为pointwise卷积得到的guide图像在去雾方面不能做一个很好的上采样引导。</p>\r\n"},{"title":"论文阅读《Deformable-Convolutional-Networks》","date":"2019-09-03T01:59:56.000Z","mathjax":true,"_content":"\n## 提出的问题\n常规卷积层因为卷积核的固定几何结构，限制了模型的几何变换能力，而视觉任务中的一大难点就是如何适应目标的尺度缩放、变形等干扰，常规的卷积层在面对这类干扰较大的任务时，效果不好。\n\n## 解决思想\n常规卷积可以分为两个步骤：采样和矩阵乘法，在采样阶段，可以加入一个自主学习的偏移，使得采样点组成的不再是固定的方形区域，从而达到可变形卷积的效果。\n\n## 具体做法\n论文中提出了两个可变形模块：可变形卷积和可变形roi pooling。\n\n### 可变形卷积\n使用$x$表示feature map，$w$表示卷积权重, $p_0$表示中心点位置坐标，假设卷积核大小为$3 \\times 3$，那么采样区域相对于中心点的偏移区域$\\mathcal{R} = \\{(-1, -1), (-1, 0), ...,(0, 1), (1, 1)\\}$，因此传统的卷积可以写成如下：\n$$\n\\begin{aligned}\n    y(p_0) = \\sum_{p_n \\in \\mathcal{R}}w(p_n)\\cdot x(p_0 + p_n)\n\\end{aligned}\n$$\n可变形卷积的操作可以写成如下，其中$\\Delta p_n$是偏移量：\n$$\n\\begin{aligned}\n    y(p_0) = \\sum_{p_n \\in \\mathcal{R}}w(p_n)\\cdot x(p_0 + p_n + \\Delta p_n)\n\\end{aligned}\n$$\n\n令$p = p_0 + p_n + \\Delta p_n$，$q$表示为图像上的所有位置，那么可以通过线性插值来计算$x(p)$：$x(p)=\\sum_q G(q,p)\\cdot x(q)$，其中$G(q,p)$表示线性插值核，$G(q,p)=g(q_x, p_x) \\cdot g(q_y, p_y)$，$g(a,b) = max(0, 1-|a-b|)$\n\n**在我的实验中，我将retinanet改为了一个带fpn的centernet（没有anchor，只用P2进行回归和分类输出），我在fpn的P2之后先叠加了三个可变形卷积层，然后再来分类和回归，发现训练根本不收敛，可能对于检测任务，可变形卷积层不是那么好用**\n\n由于我这边实验效果比较差，因此这篇文章后面的可变形ROI池化之类的内容就不往下读了。\n","source":"_posts/论文阅读/论文阅读《Deformable-Convolutional-Networks》.md","raw":"---\ntitle: 论文阅读《Deformable-Convolutional-Networks》\ndate: 2019-09-03 09:59:56\ntags: 论文阅读\nmathjax: true\n---\n\n## 提出的问题\n常规卷积层因为卷积核的固定几何结构，限制了模型的几何变换能力，而视觉任务中的一大难点就是如何适应目标的尺度缩放、变形等干扰，常规的卷积层在面对这类干扰较大的任务时，效果不好。\n\n## 解决思想\n常规卷积可以分为两个步骤：采样和矩阵乘法，在采样阶段，可以加入一个自主学习的偏移，使得采样点组成的不再是固定的方形区域，从而达到可变形卷积的效果。\n\n## 具体做法\n论文中提出了两个可变形模块：可变形卷积和可变形roi pooling。\n\n### 可变形卷积\n使用$x$表示feature map，$w$表示卷积权重, $p_0$表示中心点位置坐标，假设卷积核大小为$3 \\times 3$，那么采样区域相对于中心点的偏移区域$\\mathcal{R} = \\{(-1, -1), (-1, 0), ...,(0, 1), (1, 1)\\}$，因此传统的卷积可以写成如下：\n$$\n\\begin{aligned}\n    y(p_0) = \\sum_{p_n \\in \\mathcal{R}}w(p_n)\\cdot x(p_0 + p_n)\n\\end{aligned}\n$$\n可变形卷积的操作可以写成如下，其中$\\Delta p_n$是偏移量：\n$$\n\\begin{aligned}\n    y(p_0) = \\sum_{p_n \\in \\mathcal{R}}w(p_n)\\cdot x(p_0 + p_n + \\Delta p_n)\n\\end{aligned}\n$$\n\n令$p = p_0 + p_n + \\Delta p_n$，$q$表示为图像上的所有位置，那么可以通过线性插值来计算$x(p)$：$x(p)=\\sum_q G(q,p)\\cdot x(q)$，其中$G(q,p)$表示线性插值核，$G(q,p)=g(q_x, p_x) \\cdot g(q_y, p_y)$，$g(a,b) = max(0, 1-|a-b|)$\n\n**在我的实验中，我将retinanet改为了一个带fpn的centernet（没有anchor，只用P2进行回归和分类输出），我在fpn的P2之后先叠加了三个可变形卷积层，然后再来分类和回归，发现训练根本不收敛，可能对于检测任务，可变形卷积层不是那么好用**\n\n由于我这边实验效果比较差，因此这篇文章后面的可变形ROI池化之类的内容就不往下读了。\n","slug":"论文阅读/论文阅读《Deformable-Convolutional-Networks》","published":1,"updated":"2019-09-07T01:57:05.426Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckjxpc3sc002744mq0ssb1g1g","content":"<h2 id=\"提出的问题\">提出的问题</h2>\r\n<p>常规卷积层因为卷积核的固定几何结构，限制了模型的几何变换能力，而视觉任务中的一大难点就是如何适应目标的尺度缩放、变形等干扰，常规的卷积层在面对这类干扰较大的任务时，效果不好。</p>\r\n<h2 id=\"解决思想\">解决思想</h2>\r\n<p>常规卷积可以分为两个步骤：采样和矩阵乘法，在采样阶段，可以加入一个自主学习的偏移，使得采样点组成的不再是固定的方形区域，从而达到可变形卷积的效果。</p>\r\n<h2 id=\"具体做法\">具体做法</h2>\r\n<p>论文中提出了两个可变形模块：可变形卷积和可变形roi pooling。</p>\r\n<h3 id=\"可变形卷积\">可变形卷积</h3>\r\n<p>使用<span class=\"math inline\">\\(x\\)</span>表示feature map，<span class=\"math inline\">\\(w\\)</span>表示卷积权重, <span class=\"math inline\">\\(p_0\\)</span>表示中心点位置坐标，假设卷积核大小为<span class=\"math inline\">\\(3 \\times 3\\)</span>，那么采样区域相对于中心点的偏移区域<span class=\"math inline\">\\(\\mathcal{R} = \\{(-1, -1), (-1, 0), ...,(0, 1), (1, 1)\\}\\)</span>，因此传统的卷积可以写成如下： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    y(p_0) = \\sum_{p_n \\in \\mathcal{R}}w(p_n)\\cdot x(p_0 + p_n)\r\n\\end{aligned}\r\n\\]</span> 可变形卷积的操作可以写成如下，其中<span class=\"math inline\">\\(\\Delta p_n\\)</span>是偏移量： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    y(p_0) = \\sum_{p_n \\in \\mathcal{R}}w(p_n)\\cdot x(p_0 + p_n + \\Delta p_n)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>令<span class=\"math inline\">\\(p = p_0 + p_n + \\Delta p_n\\)</span>，<span class=\"math inline\">\\(q\\)</span>表示为图像上的所有位置，那么可以通过线性插值来计算<span class=\"math inline\">\\(x(p)\\)</span>：<span class=\"math inline\">\\(x(p)=\\sum_q G(q,p)\\cdot x(q)\\)</span>，其中<span class=\"math inline\">\\(G(q,p)\\)</span>表示线性插值核，<span class=\"math inline\">\\(G(q,p)=g(q_x, p_x) \\cdot g(q_y, p_y)\\)</span>，<span class=\"math inline\">\\(g(a,b) = max(0, 1-|a-b|)\\)</span></p>\r\n<p><strong>在我的实验中，我将retinanet改为了一个带fpn的centernet（没有anchor，只用P2进行回归和分类输出），我在fpn的P2之后先叠加了三个可变形卷积层，然后再来分类和回归，发现训练根本不收敛，可能对于检测任务，可变形卷积层不是那么好用</strong></p>\r\n<p>由于我这边实验效果比较差，因此这篇文章后面的可变形ROI池化之类的内容就不往下读了。</p>\r\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"提出的问题\">提出的问题</h2>\r\n<p>常规卷积层因为卷积核的固定几何结构，限制了模型的几何变换能力，而视觉任务中的一大难点就是如何适应目标的尺度缩放、变形等干扰，常规的卷积层在面对这类干扰较大的任务时，效果不好。</p>\r\n<h2 id=\"解决思想\">解决思想</h2>\r\n<p>常规卷积可以分为两个步骤：采样和矩阵乘法，在采样阶段，可以加入一个自主学习的偏移，使得采样点组成的不再是固定的方形区域，从而达到可变形卷积的效果。</p>\r\n<h2 id=\"具体做法\">具体做法</h2>\r\n<p>论文中提出了两个可变形模块：可变形卷积和可变形roi pooling。</p>\r\n<h3 id=\"可变形卷积\">可变形卷积</h3>\r\n<p>使用<span class=\"math inline\">\\(x\\)</span>表示feature map，<span class=\"math inline\">\\(w\\)</span>表示卷积权重, <span class=\"math inline\">\\(p_0\\)</span>表示中心点位置坐标，假设卷积核大小为<span class=\"math inline\">\\(3 \\times 3\\)</span>，那么采样区域相对于中心点的偏移区域<span class=\"math inline\">\\(\\mathcal{R} = \\{(-1, -1), (-1, 0), ...,(0, 1), (1, 1)\\}\\)</span>，因此传统的卷积可以写成如下： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    y(p_0) = \\sum_{p_n \\in \\mathcal{R}}w(p_n)\\cdot x(p_0 + p_n)\r\n\\end{aligned}\r\n\\]</span> 可变形卷积的操作可以写成如下，其中<span class=\"math inline\">\\(\\Delta p_n\\)</span>是偏移量： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    y(p_0) = \\sum_{p_n \\in \\mathcal{R}}w(p_n)\\cdot x(p_0 + p_n + \\Delta p_n)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>令<span class=\"math inline\">\\(p = p_0 + p_n + \\Delta p_n\\)</span>，<span class=\"math inline\">\\(q\\)</span>表示为图像上的所有位置，那么可以通过线性插值来计算<span class=\"math inline\">\\(x(p)\\)</span>：<span class=\"math inline\">\\(x(p)=\\sum_q G(q,p)\\cdot x(q)\\)</span>，其中<span class=\"math inline\">\\(G(q,p)\\)</span>表示线性插值核，<span class=\"math inline\">\\(G(q,p)=g(q_x, p_x) \\cdot g(q_y, p_y)\\)</span>，<span class=\"math inline\">\\(g(a,b) = max(0, 1-|a-b|)\\)</span></p>\r\n<p><strong>在我的实验中，我将retinanet改为了一个带fpn的centernet（没有anchor，只用P2进行回归和分类输出），我在fpn的P2之后先叠加了三个可变形卷积层，然后再来分类和回归，发现训练根本不收敛，可能对于检测任务，可变形卷积层不是那么好用</strong></p>\r\n<p>由于我这边实验效果比较差，因此这篇文章后面的可变形ROI池化之类的内容就不往下读了。</p>\r\n"},{"title":"论文阅读《Expectation-Maximization Attention Networks for Semantic Segmentation》","date":"2020-01-03T07:31:52.000Z","mathjax":true,"_content":"## 主要工作\n基于高斯混合模型和EM优化算法，提出了一种自注意力机制，用一组基向量来对特征图进行重构，达到简化特征图以及去除噪声的目的。\n\n## 主要思想\n将$C \\times H \\times W$大小的特征图看做$N \\times C$的$N$个$C$维随机变量$X$，其中$x_i \\in \\mathbb{R}^{C}$表示像素$i$对应的特征向量。同构构造一组基向量$\\mu \\in \\mathbb{R}^{K \\times C}$以及一个隐变量$Z \\in \\mathbb{R}^{N \\times K}$，其中$\\mu_i \\in \\mathbb{R}^C$表示一个其中一个基向量，$z_i \\in \\mathbb{R}^K$表示第$i$个像素点在这组基向量下的坐标，文中最终把输入的特征图$X$进行了重构，得到$\\hat{X} = Z \\times \\mu$。\n\n## 具体实现\n通过高斯混合模型，对$x_n$建模得到$p(x_n) = \\sum_{k=1}^K z_{nk}\\mathcal{N}(x_n|\\mu_k,\\Sigma_k)$，再通过EM算法（可见我之前的文章{% post_link EM算法学习笔记 %}），求出$\\mu$和$Z$，这里将EM算法，分解为$A_E$和$A_M$，对应EM算法的E步骤和M步骤，再加上重构步骤$A_R$，构成一种self-attention操作，称为Expectation-Maximization Attention（EMA）。\n\n在实现过程中，EMA Unit的结构类似于带有bottleneck的Residual结构，只是将$3 \\times 3$卷积换成了循环的$A_E$和$A_M$操作，整体结构如下。\n\n![EMA Unit示意图](EMA_Unit.png)\n\n在这个结构中，第一个$1 \\times 1$卷积之后，不能有ReLU激活函数，作者解释说这是因为一旦使用ReLU激活，输入的范围将从$(-\\infty, \\infty)$变为$[0, \\infty)$，导致EM算法得出的结果中，$\\mu$将是一个半正定矩阵，其表达能力只相当于普通卷积操作的一半。\n\n在$A_E$阶段，计算$Z^{(t)}=softmax(\\lambda X(\\mu^{(t-1)})^T)$，这里$\\lambda$是控制$Z$分布的一个超参数，作者将$\\lambda$设置为1。\n\n在$A_M$阶段，计算$\\mu^{(t)}_k=\\frac{z^{(t)}_{nk}x_n}{\\sum_{m=1}^Nz^{(t)}_{mk}}$\n\n在训练过程中，对于batch数据，$\\mu$的值不是每个数据都去计算一次，在第一个mini-batch中，使用Kaiming初始化方法对$\\mu^{(0)}$进行初始化（这里将矩阵乘法看做1*1的卷积），之后的mini-batch中，$\\mu^{(0)}$不能简单的使用反向传播来更新，作者解释说是因为在不断的迭代过程中，容易引起梯度消失或者梯度爆炸等问题，这样更新会导致$\\mu^{(0)}$不稳定，因此论文中提出，使用滑动平均对$\\mu^{(0)}$进行学习，每个图像在EM迭代之后所得到的$\\mu^{(T)}$将用于更新$\\mu^{(0)}$：$\\mu^{(0)} \\leftarrow  \\alpha\\mu^{(0)} + (1-\\alpha)\\mu^{(T)}$，而在推断过程中，$\\mu^{(0)}$的值是固定的。\n\n最后，由于$\\mu^{(T)}$和$\\mu^{(0)}$偏差不能太大，因此对于$\\mu$还有一个L2Norm处理，将$\\mu_k$的长度归一化。","source":"_posts/论文阅读/论文阅读《Expectation-Maximization-Attention-Networks-for-Semantic-Segmentation》.md","raw":"---\ntitle: 论文阅读《Expectation-Maximization Attention Networks for Semantic Segmentation》\ndate: 2020-01-03 15:31:52\ntags: 论文阅读\nmathjax: true\n---\n## 主要工作\n基于高斯混合模型和EM优化算法，提出了一种自注意力机制，用一组基向量来对特征图进行重构，达到简化特征图以及去除噪声的目的。\n\n## 主要思想\n将$C \\times H \\times W$大小的特征图看做$N \\times C$的$N$个$C$维随机变量$X$，其中$x_i \\in \\mathbb{R}^{C}$表示像素$i$对应的特征向量。同构构造一组基向量$\\mu \\in \\mathbb{R}^{K \\times C}$以及一个隐变量$Z \\in \\mathbb{R}^{N \\times K}$，其中$\\mu_i \\in \\mathbb{R}^C$表示一个其中一个基向量，$z_i \\in \\mathbb{R}^K$表示第$i$个像素点在这组基向量下的坐标，文中最终把输入的特征图$X$进行了重构，得到$\\hat{X} = Z \\times \\mu$。\n\n## 具体实现\n通过高斯混合模型，对$x_n$建模得到$p(x_n) = \\sum_{k=1}^K z_{nk}\\mathcal{N}(x_n|\\mu_k,\\Sigma_k)$，再通过EM算法（可见我之前的文章{% post_link EM算法学习笔记 %}），求出$\\mu$和$Z$，这里将EM算法，分解为$A_E$和$A_M$，对应EM算法的E步骤和M步骤，再加上重构步骤$A_R$，构成一种self-attention操作，称为Expectation-Maximization Attention（EMA）。\n\n在实现过程中，EMA Unit的结构类似于带有bottleneck的Residual结构，只是将$3 \\times 3$卷积换成了循环的$A_E$和$A_M$操作，整体结构如下。\n\n![EMA Unit示意图](EMA_Unit.png)\n\n在这个结构中，第一个$1 \\times 1$卷积之后，不能有ReLU激活函数，作者解释说这是因为一旦使用ReLU激活，输入的范围将从$(-\\infty, \\infty)$变为$[0, \\infty)$，导致EM算法得出的结果中，$\\mu$将是一个半正定矩阵，其表达能力只相当于普通卷积操作的一半。\n\n在$A_E$阶段，计算$Z^{(t)}=softmax(\\lambda X(\\mu^{(t-1)})^T)$，这里$\\lambda$是控制$Z$分布的一个超参数，作者将$\\lambda$设置为1。\n\n在$A_M$阶段，计算$\\mu^{(t)}_k=\\frac{z^{(t)}_{nk}x_n}{\\sum_{m=1}^Nz^{(t)}_{mk}}$\n\n在训练过程中，对于batch数据，$\\mu$的值不是每个数据都去计算一次，在第一个mini-batch中，使用Kaiming初始化方法对$\\mu^{(0)}$进行初始化（这里将矩阵乘法看做1*1的卷积），之后的mini-batch中，$\\mu^{(0)}$不能简单的使用反向传播来更新，作者解释说是因为在不断的迭代过程中，容易引起梯度消失或者梯度爆炸等问题，这样更新会导致$\\mu^{(0)}$不稳定，因此论文中提出，使用滑动平均对$\\mu^{(0)}$进行学习，每个图像在EM迭代之后所得到的$\\mu^{(T)}$将用于更新$\\mu^{(0)}$：$\\mu^{(0)} \\leftarrow  \\alpha\\mu^{(0)} + (1-\\alpha)\\mu^{(T)}$，而在推断过程中，$\\mu^{(0)}$的值是固定的。\n\n最后，由于$\\mu^{(T)}$和$\\mu^{(0)}$偏差不能太大，因此对于$\\mu$还有一个L2Norm处理，将$\\mu_k$的长度归一化。","slug":"论文阅读/论文阅读《Expectation-Maximization-Attention-Networks-for-Semantic-Segmentation》","published":1,"updated":"2020-01-11T01:53:34.088Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckjxpc3sd002a44mq16bgcb9i","content":"<h2 id=\"主要工作\">主要工作</h2>\r\n<p>基于高斯混合模型和EM优化算法，提出了一种自注意力机制，用一组基向量来对特征图进行重构，达到简化特征图以及去除噪声的目的。</p>\r\n<h2 id=\"主要思想\">主要思想</h2>\r\n<p>将<span class=\"math inline\">\\(C \\times H \\times W\\)</span>大小的特征图看做<span class=\"math inline\">\\(N \\times C\\)</span>的<span class=\"math inline\">\\(N\\)</span>个<span class=\"math inline\">\\(C\\)</span>维随机变量<span class=\"math inline\">\\(X\\)</span>，其中<span class=\"math inline\">\\(x_i \\in \\mathbb{R}^{C}\\)</span>表示像素<span class=\"math inline\">\\(i\\)</span>对应的特征向量。同构构造一组基向量<span class=\"math inline\">\\(\\mu \\in \\mathbb{R}^{K \\times C}\\)</span>以及一个隐变量<span class=\"math inline\">\\(Z \\in \\mathbb{R}^{N \\times K}\\)</span>，其中<span class=\"math inline\">\\(\\mu_i \\in \\mathbb{R}^C\\)</span>表示一个其中一个基向量，<span class=\"math inline\">\\(z_i \\in \\mathbb{R}^K\\)</span>表示第<span class=\"math inline\">\\(i\\)</span>个像素点在这组基向量下的坐标，文中最终把输入的特征图<span class=\"math inline\">\\(X\\)</span>进行了重构，得到<span class=\"math inline\">\\(\\hat{X} = Z \\times \\mu\\)</span>。</p>\r\n<h2 id=\"具体实现\">具体实现</h2>\r\n<p>通过高斯混合模型，对<span class=\"math inline\">\\(x_n\\)</span>建模得到<span class=\"math inline\">\\(p(x_n) = \\sum_{k=1}^K z_{nk}\\mathcal{N}(x_n|\\mu_k,\\Sigma_k)\\)</span>，再通过EM算法（可见我之前的文章<a href=\"#\">Post not found: EM算法学习笔记</a>），求出<span class=\"math inline\">\\(\\mu\\)</span>和<span class=\"math inline\">\\(Z\\)</span>，这里将EM算法，分解为<span class=\"math inline\">\\(A_E\\)</span>和<span class=\"math inline\">\\(A_M\\)</span>，对应EM算法的E步骤和M步骤，再加上重构步骤<span class=\"math inline\">\\(A_R\\)</span>，构成一种self-attention操作，称为Expectation-Maximization Attention（EMA）。</p>\r\n<p>在实现过程中，EMA Unit的结构类似于带有bottleneck的Residual结构，只是将<span class=\"math inline\">\\(3 \\times 3\\)</span>卷积换成了循环的<span class=\"math inline\">\\(A_E\\)</span>和<span class=\"math inline\">\\(A_M\\)</span>操作，整体结构如下。</p>\r\n<figure>\r\n<img src=\"EMA_Unit.png\" alt=\"EMA Unit示意图\" /><figcaption aria-hidden=\"true\">EMA Unit示意图</figcaption>\r\n</figure>\r\n<p>在这个结构中，第一个<span class=\"math inline\">\\(1 \\times 1\\)</span>卷积之后，不能有ReLU激活函数，作者解释说这是因为一旦使用ReLU激活，输入的范围将从<span class=\"math inline\">\\((-\\infty, \\infty)\\)</span>变为<span class=\"math inline\">\\([0, \\infty)\\)</span>，导致EM算法得出的结果中，<span class=\"math inline\">\\(\\mu\\)</span>将是一个半正定矩阵，其表达能力只相当于普通卷积操作的一半。</p>\r\n<p>在<span class=\"math inline\">\\(A_E\\)</span>阶段，计算<span class=\"math inline\">\\(Z^{(t)}=softmax(\\lambda X(\\mu^{(t-1)})^T)\\)</span>，这里<span class=\"math inline\">\\(\\lambda\\)</span>是控制<span class=\"math inline\">\\(Z\\)</span>分布的一个超参数，作者将<span class=\"math inline\">\\(\\lambda\\)</span>设置为1。</p>\r\n<p>在<span class=\"math inline\">\\(A_M\\)</span>阶段，计算<span class=\"math inline\">\\(\\mu^{(t)}_k=\\frac{z^{(t)}_{nk}x_n}{\\sum_{m=1}^Nz^{(t)}_{mk}}\\)</span></p>\r\n<p>在训练过程中，对于batch数据，<span class=\"math inline\">\\(\\mu\\)</span>的值不是每个数据都去计算一次，在第一个mini-batch中，使用Kaiming初始化方法对<span class=\"math inline\">\\(\\mu^{(0)}\\)</span>进行初始化（这里将矩阵乘法看做1*1的卷积），之后的mini-batch中，<span class=\"math inline\">\\(\\mu^{(0)}\\)</span>不能简单的使用反向传播来更新，作者解释说是因为在不断的迭代过程中，容易引起梯度消失或者梯度爆炸等问题，这样更新会导致<span class=\"math inline\">\\(\\mu^{(0)}\\)</span>不稳定，因此论文中提出，使用滑动平均对<span class=\"math inline\">\\(\\mu^{(0)}\\)</span>进行学习，每个图像在EM迭代之后所得到的<span class=\"math inline\">\\(\\mu^{(T)}\\)</span>将用于更新<span class=\"math inline\">\\(\\mu^{(0)}\\)</span>：<span class=\"math inline\">\\(\\mu^{(0)} \\leftarrow \\alpha\\mu^{(0)} + (1-\\alpha)\\mu^{(T)}\\)</span>，而在推断过程中，<span class=\"math inline\">\\(\\mu^{(0)}\\)</span>的值是固定的。</p>\r\n<p>最后，由于<span class=\"math inline\">\\(\\mu^{(T)}\\)</span>和<span class=\"math inline\">\\(\\mu^{(0)}\\)</span>偏差不能太大，因此对于<span class=\"math inline\">\\(\\mu\\)</span>还有一个L2Norm处理，将<span class=\"math inline\">\\(\\mu_k\\)</span>的长度归一化。</p>\r\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"主要工作\">主要工作</h2>\r\n<p>基于高斯混合模型和EM优化算法，提出了一种自注意力机制，用一组基向量来对特征图进行重构，达到简化特征图以及去除噪声的目的。</p>\r\n<h2 id=\"主要思想\">主要思想</h2>\r\n<p>将<span class=\"math inline\">\\(C \\times H \\times W\\)</span>大小的特征图看做<span class=\"math inline\">\\(N \\times C\\)</span>的<span class=\"math inline\">\\(N\\)</span>个<span class=\"math inline\">\\(C\\)</span>维随机变量<span class=\"math inline\">\\(X\\)</span>，其中<span class=\"math inline\">\\(x_i \\in \\mathbb{R}^{C}\\)</span>表示像素<span class=\"math inline\">\\(i\\)</span>对应的特征向量。同构构造一组基向量<span class=\"math inline\">\\(\\mu \\in \\mathbb{R}^{K \\times C}\\)</span>以及一个隐变量<span class=\"math inline\">\\(Z \\in \\mathbb{R}^{N \\times K}\\)</span>，其中<span class=\"math inline\">\\(\\mu_i \\in \\mathbb{R}^C\\)</span>表示一个其中一个基向量，<span class=\"math inline\">\\(z_i \\in \\mathbb{R}^K\\)</span>表示第<span class=\"math inline\">\\(i\\)</span>个像素点在这组基向量下的坐标，文中最终把输入的特征图<span class=\"math inline\">\\(X\\)</span>进行了重构，得到<span class=\"math inline\">\\(\\hat{X} = Z \\times \\mu\\)</span>。</p>\r\n<h2 id=\"具体实现\">具体实现</h2>\r\n<p>通过高斯混合模型，对<span class=\"math inline\">\\(x_n\\)</span>建模得到<span class=\"math inline\">\\(p(x_n) = \\sum_{k=1}^K z_{nk}\\mathcal{N}(x_n|\\mu_k,\\Sigma_k)\\)</span>，再通过EM算法（可见我之前的文章<a href=\"#\">Post not found: EM算法学习笔记</a>），求出<span class=\"math inline\">\\(\\mu\\)</span>和<span class=\"math inline\">\\(Z\\)</span>，这里将EM算法，分解为<span class=\"math inline\">\\(A_E\\)</span>和<span class=\"math inline\">\\(A_M\\)</span>，对应EM算法的E步骤和M步骤，再加上重构步骤<span class=\"math inline\">\\(A_R\\)</span>，构成一种self-attention操作，称为Expectation-Maximization Attention（EMA）。</p>\r\n<p>在实现过程中，EMA Unit的结构类似于带有bottleneck的Residual结构，只是将<span class=\"math inline\">\\(3 \\times 3\\)</span>卷积换成了循环的<span class=\"math inline\">\\(A_E\\)</span>和<span class=\"math inline\">\\(A_M\\)</span>操作，整体结构如下。</p>\r\n<figure>\r\n<img src=\"EMA_Unit.png\" alt=\"EMA Unit示意图\" /><figcaption aria-hidden=\"true\">EMA Unit示意图</figcaption>\r\n</figure>\r\n<p>在这个结构中，第一个<span class=\"math inline\">\\(1 \\times 1\\)</span>卷积之后，不能有ReLU激活函数，作者解释说这是因为一旦使用ReLU激活，输入的范围将从<span class=\"math inline\">\\((-\\infty, \\infty)\\)</span>变为<span class=\"math inline\">\\([0, \\infty)\\)</span>，导致EM算法得出的结果中，<span class=\"math inline\">\\(\\mu\\)</span>将是一个半正定矩阵，其表达能力只相当于普通卷积操作的一半。</p>\r\n<p>在<span class=\"math inline\">\\(A_E\\)</span>阶段，计算<span class=\"math inline\">\\(Z^{(t)}=softmax(\\lambda X(\\mu^{(t-1)})^T)\\)</span>，这里<span class=\"math inline\">\\(\\lambda\\)</span>是控制<span class=\"math inline\">\\(Z\\)</span>分布的一个超参数，作者将<span class=\"math inline\">\\(\\lambda\\)</span>设置为1。</p>\r\n<p>在<span class=\"math inline\">\\(A_M\\)</span>阶段，计算<span class=\"math inline\">\\(\\mu^{(t)}_k=\\frac{z^{(t)}_{nk}x_n}{\\sum_{m=1}^Nz^{(t)}_{mk}}\\)</span></p>\r\n<p>在训练过程中，对于batch数据，<span class=\"math inline\">\\(\\mu\\)</span>的值不是每个数据都去计算一次，在第一个mini-batch中，使用Kaiming初始化方法对<span class=\"math inline\">\\(\\mu^{(0)}\\)</span>进行初始化（这里将矩阵乘法看做1*1的卷积），之后的mini-batch中，<span class=\"math inline\">\\(\\mu^{(0)}\\)</span>不能简单的使用反向传播来更新，作者解释说是因为在不断的迭代过程中，容易引起梯度消失或者梯度爆炸等问题，这样更新会导致<span class=\"math inline\">\\(\\mu^{(0)}\\)</span>不稳定，因此论文中提出，使用滑动平均对<span class=\"math inline\">\\(\\mu^{(0)}\\)</span>进行学习，每个图像在EM迭代之后所得到的<span class=\"math inline\">\\(\\mu^{(T)}\\)</span>将用于更新<span class=\"math inline\">\\(\\mu^{(0)}\\)</span>：<span class=\"math inline\">\\(\\mu^{(0)} \\leftarrow \\alpha\\mu^{(0)} + (1-\\alpha)\\mu^{(T)}\\)</span>，而在推断过程中，<span class=\"math inline\">\\(\\mu^{(0)}\\)</span>的值是固定的。</p>\r\n<p>最后，由于<span class=\"math inline\">\\(\\mu^{(T)}\\)</span>和<span class=\"math inline\">\\(\\mu^{(0)}\\)</span>偏差不能太大，因此对于<span class=\"math inline\">\\(\\mu\\)</span>还有一个L2Norm处理，将<span class=\"math inline\">\\(\\mu_k\\)</span>的长度归一化。</p>\r\n"},{"title":"论文阅读《Improving-neural-networks-by-preventingco-adaptation-of-feature-detectors》","date":"2019-07-31T02:08:12.000Z","mathjax":true,"_content":"\n## 提出的问题\n在小数据集上使用非常大的神经网络模型训练，往往会出现过拟合的情况。\n\n## 解决思想\n随机忽视掉一半的特征检测器，即drop rate为0.5的Dropout，作者解释这样做的原因是减少特征之间的相互依赖性，并展示了在CNN中使用和不使用Dropout两种情况所提取到的特征图，如下所示，作者解释说:“ The features that dropout learns are simpler and looklike strokes, whereas the ones learned by standard backpropagation are difficult to interpret”。\n![学习到的特征对比](feature_map_show.png)\n\n**我觉得这个解释很勉强，关于Dropout的有效性，另外一篇论文《Dropout: a simple way to prevent neural networks from overfitting》有更多的描述，其中提到了Dropout可以让feature激活值变得更加稀疏,如下所示,我认为这是Dropout抑制过拟合的一个原因**\n![Dropout对激活值的影响](feature_map_activate_show.png)\n\n## 实现细节\n- MNIST:使用全连接神经网络,在输入层使用drop rate为0.2的Dropout，在隐藏层使用drop rate为0.5的Dropout,学习速率指数衰减，初始学习率为10，不使用权重衰减，但是使用权重裁剪策略，将权重的“squared length”限制在15以内。\n- CIFAR-10:使用CNN模型，在最后的卷积层之后加入一个局部全连接层，作为一个不共享权重的卷积层，并在这个层上使用Dropout。\n- CIFAR-10:使用CNN模型，在最后的两个全连接层上进行Dropout。","source":"_posts/论文阅读/论文阅读《Improving-neural-networks-by-preventingco-adaptation-of-feature-detectors》.md","raw":"---\ntitle: >-\n  论文阅读《Improving-neural-networks-by-preventingco-adaptation-of-feature-detectors》\ndate: 2019-07-31 10:08:12\ntags: 论文阅读\nmathjax: true\n---\n\n## 提出的问题\n在小数据集上使用非常大的神经网络模型训练，往往会出现过拟合的情况。\n\n## 解决思想\n随机忽视掉一半的特征检测器，即drop rate为0.5的Dropout，作者解释这样做的原因是减少特征之间的相互依赖性，并展示了在CNN中使用和不使用Dropout两种情况所提取到的特征图，如下所示，作者解释说:“ The features that dropout learns are simpler and looklike strokes, whereas the ones learned by standard backpropagation are difficult to interpret”。\n![学习到的特征对比](feature_map_show.png)\n\n**我觉得这个解释很勉强，关于Dropout的有效性，另外一篇论文《Dropout: a simple way to prevent neural networks from overfitting》有更多的描述，其中提到了Dropout可以让feature激活值变得更加稀疏,如下所示,我认为这是Dropout抑制过拟合的一个原因**\n![Dropout对激活值的影响](feature_map_activate_show.png)\n\n## 实现细节\n- MNIST:使用全连接神经网络,在输入层使用drop rate为0.2的Dropout，在隐藏层使用drop rate为0.5的Dropout,学习速率指数衰减，初始学习率为10，不使用权重衰减，但是使用权重裁剪策略，将权重的“squared length”限制在15以内。\n- CIFAR-10:使用CNN模型，在最后的卷积层之后加入一个局部全连接层，作为一个不共享权重的卷积层，并在这个层上使用Dropout。\n- CIFAR-10:使用CNN模型，在最后的两个全连接层上进行Dropout。","slug":"论文阅读/论文阅读《Improving-neural-networks-by-preventingco-adaptation-of-feature-detectors》","published":1,"updated":"2019-07-31T03:27:49.209Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckjxpc3sn002t44mq3rp61ggi","content":"<h2 id=\"提出的问题\">提出的问题</h2>\r\n<p>在小数据集上使用非常大的神经网络模型训练，往往会出现过拟合的情况。</p>\r\n<h2 id=\"解决思想\">解决思想</h2>\r\n<p>随机忽视掉一半的特征检测器，即drop rate为0.5的Dropout，作者解释这样做的原因是减少特征之间的相互依赖性，并展示了在CNN中使用和不使用Dropout两种情况所提取到的特征图，如下所示，作者解释说:“ The features that dropout learns are simpler and looklike strokes, whereas the ones learned by standard backpropagation are difficult to interpret”。 <img src=\"feature_map_show.png\" alt=\"学习到的特征对比\" /></p>\r\n<p><strong>我觉得这个解释很勉强，关于Dropout的有效性，另外一篇论文《Dropout: a simple way to prevent neural networks from overfitting》有更多的描述，其中提到了Dropout可以让feature激活值变得更加稀疏,如下所示,我认为这是Dropout抑制过拟合的一个原因</strong> <img src=\"feature_map_activate_show.png\" alt=\"Dropout对激活值的影响\" /></p>\r\n<h2 id=\"实现细节\">实现细节</h2>\r\n<ul>\r\n<li>MNIST:使用全连接神经网络,在输入层使用drop rate为0.2的Dropout，在隐藏层使用drop rate为0.5的Dropout,学习速率指数衰减，初始学习率为10，不使用权重衰减，但是使用权重裁剪策略，将权重的“squared length”限制在15以内。</li>\r\n<li>CIFAR-10:使用CNN模型，在最后的卷积层之后加入一个局部全连接层，作为一个不共享权重的卷积层，并在这个层上使用Dropout。</li>\r\n<li>CIFAR-10:使用CNN模型，在最后的两个全连接层上进行Dropout。</li>\r\n</ul>\r\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"提出的问题\">提出的问题</h2>\r\n<p>在小数据集上使用非常大的神经网络模型训练，往往会出现过拟合的情况。</p>\r\n<h2 id=\"解决思想\">解决思想</h2>\r\n<p>随机忽视掉一半的特征检测器，即drop rate为0.5的Dropout，作者解释这样做的原因是减少特征之间的相互依赖性，并展示了在CNN中使用和不使用Dropout两种情况所提取到的特征图，如下所示，作者解释说:“ The features that dropout learns are simpler and looklike strokes, whereas the ones learned by standard backpropagation are difficult to interpret”。 <img src=\"feature_map_show.png\" alt=\"学习到的特征对比\" /></p>\r\n<p><strong>我觉得这个解释很勉强，关于Dropout的有效性，另外一篇论文《Dropout: a simple way to prevent neural networks from overfitting》有更多的描述，其中提到了Dropout可以让feature激活值变得更加稀疏,如下所示,我认为这是Dropout抑制过拟合的一个原因</strong> <img src=\"feature_map_activate_show.png\" alt=\"Dropout对激活值的影响\" /></p>\r\n<h2 id=\"实现细节\">实现细节</h2>\r\n<ul>\r\n<li>MNIST:使用全连接神经网络,在输入层使用drop rate为0.2的Dropout，在隐藏层使用drop rate为0.5的Dropout,学习速率指数衰减，初始学习率为10，不使用权重衰减，但是使用权重裁剪策略，将权重的“squared length”限制在15以内。</li>\r\n<li>CIFAR-10:使用CNN模型，在最后的卷积层之后加入一个局部全连接层，作为一个不共享权重的卷积层，并在这个层上使用Dropout。</li>\r\n<li>CIFAR-10:使用CNN模型，在最后的两个全连接层上进行Dropout。</li>\r\n</ul>\r\n"},{"title":"论文阅读《Learning-a-Discriminative-Feature-Network-for-Semantic-Segmentation》","date":"2019-07-26T07:59:06.000Z","_content":"\n## 提出的问题\n- 类内不一致问题：被分配到同一个标签的一块区域，像素之间表现不同\n- 类间模糊问题：不同标签的邻近区域，像素之间表现相似\n\n## 解决思想\n从宏观角度考虑语义分割问题，将语义分割问题当做将一致的语义标签分配给事物类别的任务，将一个类别的像素当成一个整体去考虑，使用一种Discriminative Feature Network(DFN)，由Smooth Network和Border Network组成\n\nSmooth Network用于解决类内不一致问题，目的是学习针对类内不一致性的鲁棒特征，主要考虑全局上下文信息和多尺度的特征，因此Smooth Network基于U型结构来获取多尺度上下文信息，并使用Channel Attention Block(CAB)逐步利用高级特征来选择低级特征。\n\nBoder Network用于解决类间模糊问题，通过整合语义边界损失来使模型发现能增大类间距离的更具描述性的特征。\n\n整体结构如下图所示：\n\n![结构示意图](total_model.png)\n\n## 一些问题\n原文：\"Accordingto our observation, the different stages have different recognition abilities resulting in diverse consistency manifestation.\"，这里作者是如何观察到的，还有这里的\"consistency\"是指什么？","source":"_posts/论文阅读/论文阅读《Learning-a-Discriminative-Feature-Network-for-Semantic-Segmentation》.md","raw":"---\ntitle: 论文阅读《Learning-a-Discriminative-Feature-Network-for-Semantic-Segmentation》\ndate: 2019-07-26 15:59:06\ntags: 论文阅读\n---\n\n## 提出的问题\n- 类内不一致问题：被分配到同一个标签的一块区域，像素之间表现不同\n- 类间模糊问题：不同标签的邻近区域，像素之间表现相似\n\n## 解决思想\n从宏观角度考虑语义分割问题，将语义分割问题当做将一致的语义标签分配给事物类别的任务，将一个类别的像素当成一个整体去考虑，使用一种Discriminative Feature Network(DFN)，由Smooth Network和Border Network组成\n\nSmooth Network用于解决类内不一致问题，目的是学习针对类内不一致性的鲁棒特征，主要考虑全局上下文信息和多尺度的特征，因此Smooth Network基于U型结构来获取多尺度上下文信息，并使用Channel Attention Block(CAB)逐步利用高级特征来选择低级特征。\n\nBoder Network用于解决类间模糊问题，通过整合语义边界损失来使模型发现能增大类间距离的更具描述性的特征。\n\n整体结构如下图所示：\n\n![结构示意图](total_model.png)\n\n## 一些问题\n原文：\"Accordingto our observation, the different stages have different recognition abilities resulting in diverse consistency manifestation.\"，这里作者是如何观察到的，还有这里的\"consistency\"是指什么？","slug":"论文阅读/论文阅读《Learning-a-Discriminative-Feature-Network-for-Semantic-Segmentation》","published":1,"updated":"2019-07-30T11:39:11.271Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckjxpc3sn002u44mq07sm90dv","content":"<h2 id=\"提出的问题\">提出的问题</h2>\r\n<ul>\r\n<li>类内不一致问题：被分配到同一个标签的一块区域，像素之间表现不同</li>\r\n<li>类间模糊问题：不同标签的邻近区域，像素之间表现相似</li>\r\n</ul>\r\n<h2 id=\"解决思想\">解决思想</h2>\r\n<p>从宏观角度考虑语义分割问题，将语义分割问题当做将一致的语义标签分配给事物类别的任务，将一个类别的像素当成一个整体去考虑，使用一种Discriminative Feature Network(DFN)，由Smooth Network和Border Network组成</p>\r\n<p>Smooth Network用于解决类内不一致问题，目的是学习针对类内不一致性的鲁棒特征，主要考虑全局上下文信息和多尺度的特征，因此Smooth Network基于U型结构来获取多尺度上下文信息，并使用Channel Attention Block(CAB)逐步利用高级特征来选择低级特征。</p>\r\n<p>Boder Network用于解决类间模糊问题，通过整合语义边界损失来使模型发现能增大类间距离的更具描述性的特征。</p>\r\n<p>整体结构如下图所示：</p>\r\n<figure>\r\n<img src=\"total_model.png\" alt=\"结构示意图\" /><figcaption aria-hidden=\"true\">结构示意图</figcaption>\r\n</figure>\r\n<h2 id=\"一些问题\">一些问题</h2>\r\n<p>原文：\"Accordingto our observation, the different stages have different recognition abilities resulting in diverse consistency manifestation.\"，这里作者是如何观察到的，还有这里的\"consistency\"是指什么？</p>\r\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"提出的问题\">提出的问题</h2>\r\n<ul>\r\n<li>类内不一致问题：被分配到同一个标签的一块区域，像素之间表现不同</li>\r\n<li>类间模糊问题：不同标签的邻近区域，像素之间表现相似</li>\r\n</ul>\r\n<h2 id=\"解决思想\">解决思想</h2>\r\n<p>从宏观角度考虑语义分割问题，将语义分割问题当做将一致的语义标签分配给事物类别的任务，将一个类别的像素当成一个整体去考虑，使用一种Discriminative Feature Network(DFN)，由Smooth Network和Border Network组成</p>\r\n<p>Smooth Network用于解决类内不一致问题，目的是学习针对类内不一致性的鲁棒特征，主要考虑全局上下文信息和多尺度的特征，因此Smooth Network基于U型结构来获取多尺度上下文信息，并使用Channel Attention Block(CAB)逐步利用高级特征来选择低级特征。</p>\r\n<p>Boder Network用于解决类间模糊问题，通过整合语义边界损失来使模型发现能增大类间距离的更具描述性的特征。</p>\r\n<p>整体结构如下图所示：</p>\r\n<figure>\r\n<img src=\"total_model.png\" alt=\"结构示意图\" /><figcaption aria-hidden=\"true\">结构示意图</figcaption>\r\n</figure>\r\n<h2 id=\"一些问题\">一些问题</h2>\r\n<p>原文：\"Accordingto our observation, the different stages have different recognition abilities resulting in diverse consistency manifestation.\"，这里作者是如何观察到的，还有这里的\"consistency\"是指什么？</p>\r\n"},{"title":"论文阅读《Multi-Task-Learning-Using-Uncertainty-to-Weigh-Lossesfor-Scene-Geometry-and-Semantics》","date":"2019-07-29T07:39:55.000Z","mathjax":true,"_content":"\n## 提出的问题\n在多任务学习的过程中，不同的损失函数权重设置非常困难。\n\n## 解决思想\n使用同方差不确定性来对多任务损失函数进行组合。\n\n## 具体方法\n$f^{W}(x)$代表输入x经过参数为$W$的神经网络之后得到的输出，那么对于回归任务，可以定义其高斯似然概率：$p(y|f^{W}(x)) = \\mathcal{N}(f^{W}(x), \\sigma^2)$,对于分类任务，因为常用Softmax来进行输出的处理，所以定义其似然概率为$p(y|f^{W}(x)) = Softmax(\\frac{1}{\\sigma^2}f^{W}(x))$，这里的$\\frac{1}{\\sigma^2}$不会影响Softmax的结果。\n\n如果一个模型有多个任务，可以用最大似然法来定义其损失函数，假设模型存在一个回归任务和一个分类任务，其对数似然定义如下：\n$$\n\\begin{aligned}\n\tlog(p(y_1,y_2=c|f^{W}(x))) &= log(p(y_1|f^{W}(x))) + log(p(y_2=c|f^{W}(x), \\sigma_2^2))\\\\\n\t&= log(\\mathcal{N}(f^{W}(x), \\sigma_1^2)) + log(Softmax(y_2=c; f^{W}(x)))\\\\\n\t&= log(\\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{(x - f^{W}(x))^2}{2\\sigma_1^2}}) + log(\\frac{e^{\\frac{1}{\\sigma ^ 2}f_c^W(x)}}{\\sum_{\\hat{c}}e^{\\frac{1}{\\sigma_2 ^ 2}f_{\\hat{c}}^W(x)}})\\\\\n\t&= -\\frac{(x - f^{W}(x))^2}{2\\sigma_1^2} - log\\sigma_1 - log\\sqrt{2\\pi} + log(\\frac{e^{\\frac{1}{\\sigma ^ 2}f_c^W(x)}}{\\sum_{\\hat{c}}e^{\\frac{1}{\\sigma_2 ^ 2}f_{\\hat{c}}^W(x)}})\\\\\n\\end{aligned}\n$$\n当$\\sigma_2$接近1时，$\\frac{1}{\\sigma_2}\\sum_{\\hat{c}}e^{\\frac{1}{\\sigma_2 ^ 2}f_{\\hat{c}}^W(x)} \\approx (\\sum_{\\hat{c}}e^{f_{\\hat{c}}^W(x)})^\\frac{1}{\\sigma_2 ^ 2}$，上式可以写为：\n$$\n\\begin{aligned}\n\t&\\approx -\\frac{(x - f^{W}(x))^2}{2\\sigma_1^2} - log\\sigma_1 - log\\sqrt{2\\pi} + log(\\frac{e^{\\frac{1}{\\sigma ^ 2}f_c^W(x)}}{\\sigma_2(\\sum_{\\hat{c}}e^{f_{\\hat{c}}^W(x)})^\\frac{1}{\\sigma_2 ^ 2}})\\\\\n\t&= -\\frac{(x - f^{W}(x))^2}{2\\sigma_1^2} - log\\sigma_1 - log\\sqrt{2\\pi} + \\frac{1}{\\sigma_2 ^ 2}log(\\frac{e^{f_c^W(x)}}{\\sum_{\\hat{c}}e^{f_{\\hat{c}}^W(x)}}) - log\\sigma_2\\\\\n\t&= \\frac{1}{\\sigma_1 ^ 2}L_1(W) + \\frac{1}{\\sigma_2 ^ 2}L_2(W) - log\\sigma_1 - log\\sigma_2 - log\\sqrt{2\\pi}\n\\end{aligned}\n$$\n\n因此，在模型中设置两个可训练权重$\\sigma_1$和$\\sigma_2$，使用$-\\frac{1}{\\sigma_1 ^ 2}L_1(W) - \\frac{1}{\\sigma_2 ^ 2}L_2(W) + log\\sigma_1 + log\\sigma_2$作为多任务损失函数，即可自动学习模型的损失的权重。\n\n## 一些问题\n- **上式的简化是说在$\\sigma_2$接近1的时候有用，但是模型学出来的权重很可能不在1附近，或者可以将$\\sigma_2$固定为1？**\n- **从论文贴出的结果看，组合多任务学习的效果比单任务的效果要差？结果图如下**\n\n![论文中的实验结果对比](paper_result.png)\n","source":"_posts/论文阅读/论文阅读《Multi-Task-Learning-Using-Uncertainty-to-Weigh-Lossesfor-Scene-Geometry-and-Semantics》.md","raw":"---\ntitle: 论文阅读《Multi-Task-Learning-Using-Uncertainty-to-Weigh-Lossesfor-Scene-Geometry-and-Semantics》\ndate: 2019-07-29 15:39:55\ntags: 论文阅读\nmathjax: true\n---\n\n## 提出的问题\n在多任务学习的过程中，不同的损失函数权重设置非常困难。\n\n## 解决思想\n使用同方差不确定性来对多任务损失函数进行组合。\n\n## 具体方法\n$f^{W}(x)$代表输入x经过参数为$W$的神经网络之后得到的输出，那么对于回归任务，可以定义其高斯似然概率：$p(y|f^{W}(x)) = \\mathcal{N}(f^{W}(x), \\sigma^2)$,对于分类任务，因为常用Softmax来进行输出的处理，所以定义其似然概率为$p(y|f^{W}(x)) = Softmax(\\frac{1}{\\sigma^2}f^{W}(x))$，这里的$\\frac{1}{\\sigma^2}$不会影响Softmax的结果。\n\n如果一个模型有多个任务，可以用最大似然法来定义其损失函数，假设模型存在一个回归任务和一个分类任务，其对数似然定义如下：\n$$\n\\begin{aligned}\n\tlog(p(y_1,y_2=c|f^{W}(x))) &= log(p(y_1|f^{W}(x))) + log(p(y_2=c|f^{W}(x), \\sigma_2^2))\\\\\n\t&= log(\\mathcal{N}(f^{W}(x), \\sigma_1^2)) + log(Softmax(y_2=c; f^{W}(x)))\\\\\n\t&= log(\\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{(x - f^{W}(x))^2}{2\\sigma_1^2}}) + log(\\frac{e^{\\frac{1}{\\sigma ^ 2}f_c^W(x)}}{\\sum_{\\hat{c}}e^{\\frac{1}{\\sigma_2 ^ 2}f_{\\hat{c}}^W(x)}})\\\\\n\t&= -\\frac{(x - f^{W}(x))^2}{2\\sigma_1^2} - log\\sigma_1 - log\\sqrt{2\\pi} + log(\\frac{e^{\\frac{1}{\\sigma ^ 2}f_c^W(x)}}{\\sum_{\\hat{c}}e^{\\frac{1}{\\sigma_2 ^ 2}f_{\\hat{c}}^W(x)}})\\\\\n\\end{aligned}\n$$\n当$\\sigma_2$接近1时，$\\frac{1}{\\sigma_2}\\sum_{\\hat{c}}e^{\\frac{1}{\\sigma_2 ^ 2}f_{\\hat{c}}^W(x)} \\approx (\\sum_{\\hat{c}}e^{f_{\\hat{c}}^W(x)})^\\frac{1}{\\sigma_2 ^ 2}$，上式可以写为：\n$$\n\\begin{aligned}\n\t&\\approx -\\frac{(x - f^{W}(x))^2}{2\\sigma_1^2} - log\\sigma_1 - log\\sqrt{2\\pi} + log(\\frac{e^{\\frac{1}{\\sigma ^ 2}f_c^W(x)}}{\\sigma_2(\\sum_{\\hat{c}}e^{f_{\\hat{c}}^W(x)})^\\frac{1}{\\sigma_2 ^ 2}})\\\\\n\t&= -\\frac{(x - f^{W}(x))^2}{2\\sigma_1^2} - log\\sigma_1 - log\\sqrt{2\\pi} + \\frac{1}{\\sigma_2 ^ 2}log(\\frac{e^{f_c^W(x)}}{\\sum_{\\hat{c}}e^{f_{\\hat{c}}^W(x)}}) - log\\sigma_2\\\\\n\t&= \\frac{1}{\\sigma_1 ^ 2}L_1(W) + \\frac{1}{\\sigma_2 ^ 2}L_2(W) - log\\sigma_1 - log\\sigma_2 - log\\sqrt{2\\pi}\n\\end{aligned}\n$$\n\n因此，在模型中设置两个可训练权重$\\sigma_1$和$\\sigma_2$，使用$-\\frac{1}{\\sigma_1 ^ 2}L_1(W) - \\frac{1}{\\sigma_2 ^ 2}L_2(W) + log\\sigma_1 + log\\sigma_2$作为多任务损失函数，即可自动学习模型的损失的权重。\n\n## 一些问题\n- **上式的简化是说在$\\sigma_2$接近1的时候有用，但是模型学出来的权重很可能不在1附近，或者可以将$\\sigma_2$固定为1？**\n- **从论文贴出的结果看，组合多任务学习的效果比单任务的效果要差？结果图如下**\n\n![论文中的实验结果对比](paper_result.png)\n","slug":"论文阅读/论文阅读《Multi-Task-Learning-Using-Uncertainty-to-Weigh-Lossesfor-Scene-Geometry-and-Semantics》","published":1,"updated":"2019-07-30T11:46:08.376Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckjxpc3sq002w44mq79uf4tc2","content":"<h2 id=\"提出的问题\">提出的问题</h2>\r\n<p>在多任务学习的过程中，不同的损失函数权重设置非常困难。</p>\r\n<h2 id=\"解决思想\">解决思想</h2>\r\n<p>使用同方差不确定性来对多任务损失函数进行组合。</p>\r\n<h2 id=\"具体方法\">具体方法</h2>\r\n<p><span class=\"math inline\">\\(f^{W}(x)\\)</span>代表输入x经过参数为<span class=\"math inline\">\\(W\\)</span>的神经网络之后得到的输出，那么对于回归任务，可以定义其高斯似然概率：<span class=\"math inline\">\\(p(y|f^{W}(x)) = \\mathcal{N}(f^{W}(x), \\sigma^2)\\)</span>,对于分类任务，因为常用Softmax来进行输出的处理，所以定义其似然概率为<span class=\"math inline\">\\(p(y|f^{W}(x)) = Softmax(\\frac{1}{\\sigma^2}f^{W}(x))\\)</span>，这里的<span class=\"math inline\">\\(\\frac{1}{\\sigma^2}\\)</span>不会影响Softmax的结果。</p>\r\n<p>如果一个模型有多个任务，可以用最大似然法来定义其损失函数，假设模型存在一个回归任务和一个分类任务，其对数似然定义如下： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    log(p(y_1,y_2=c|f^{W}(x))) &amp;= log(p(y_1|f^{W}(x))) + log(p(y_2=c|f^{W}(x), \\sigma_2^2))\\\\\r\n    &amp;= log(\\mathcal{N}(f^{W}(x), \\sigma_1^2)) + log(Softmax(y_2=c; f^{W}(x)))\\\\\r\n    &amp;= log(\\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{(x - f^{W}(x))^2}{2\\sigma_1^2}}) + log(\\frac{e^{\\frac{1}{\\sigma ^ 2}f_c^W(x)}}{\\sum_{\\hat{c}}e^{\\frac{1}{\\sigma_2 ^ 2}f_{\\hat{c}}^W(x)}})\\\\\r\n    &amp;= -\\frac{(x - f^{W}(x))^2}{2\\sigma_1^2} - log\\sigma_1 - log\\sqrt{2\\pi} + log(\\frac{e^{\\frac{1}{\\sigma ^ 2}f_c^W(x)}}{\\sum_{\\hat{c}}e^{\\frac{1}{\\sigma_2 ^ 2}f_{\\hat{c}}^W(x)}})\\\\\r\n\\end{aligned}\r\n\\]</span> 当<span class=\"math inline\">\\(\\sigma_2\\)</span>接近1时，<span class=\"math inline\">\\(\\frac{1}{\\sigma_2}\\sum_{\\hat{c}}e^{\\frac{1}{\\sigma_2 ^ 2}f_{\\hat{c}}^W(x)} \\approx (\\sum_{\\hat{c}}e^{f_{\\hat{c}}^W(x)})^\\frac{1}{\\sigma_2 ^ 2}\\)</span>，上式可以写为： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    &amp;\\approx -\\frac{(x - f^{W}(x))^2}{2\\sigma_1^2} - log\\sigma_1 - log\\sqrt{2\\pi} + log(\\frac{e^{\\frac{1}{\\sigma ^ 2}f_c^W(x)}}{\\sigma_2(\\sum_{\\hat{c}}e^{f_{\\hat{c}}^W(x)})^\\frac{1}{\\sigma_2 ^ 2}})\\\\\r\n    &amp;= -\\frac{(x - f^{W}(x))^2}{2\\sigma_1^2} - log\\sigma_1 - log\\sqrt{2\\pi} + \\frac{1}{\\sigma_2 ^ 2}log(\\frac{e^{f_c^W(x)}}{\\sum_{\\hat{c}}e^{f_{\\hat{c}}^W(x)}}) - log\\sigma_2\\\\\r\n    &amp;= \\frac{1}{\\sigma_1 ^ 2}L_1(W) + \\frac{1}{\\sigma_2 ^ 2}L_2(W) - log\\sigma_1 - log\\sigma_2 - log\\sqrt{2\\pi}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>因此，在模型中设置两个可训练权重<span class=\"math inline\">\\(\\sigma_1\\)</span>和<span class=\"math inline\">\\(\\sigma_2\\)</span>，使用<span class=\"math inline\">\\(-\\frac{1}{\\sigma_1 ^ 2}L_1(W) - \\frac{1}{\\sigma_2 ^ 2}L_2(W) + log\\sigma_1 + log\\sigma_2\\)</span>作为多任务损失函数，即可自动学习模型的损失的权重。</p>\r\n<h2 id=\"一些问题\">一些问题</h2>\r\n<ul>\r\n<li><strong>上式的简化是说在<span class=\"math inline\">\\(\\sigma_2\\)</span>接近1的时候有用，但是模型学出来的权重很可能不在1附近，或者可以将<span class=\"math inline\">\\(\\sigma_2\\)</span>固定为1？</strong></li>\r\n<li><strong>从论文贴出的结果看，组合多任务学习的效果比单任务的效果要差？结果图如下</strong></li>\r\n</ul>\r\n<figure>\r\n<img src=\"paper_result.png\" alt=\"论文中的实验结果对比\" /><figcaption aria-hidden=\"true\">论文中的实验结果对比</figcaption>\r\n</figure>\r\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"提出的问题\">提出的问题</h2>\r\n<p>在多任务学习的过程中，不同的损失函数权重设置非常困难。</p>\r\n<h2 id=\"解决思想\">解决思想</h2>\r\n<p>使用同方差不确定性来对多任务损失函数进行组合。</p>\r\n<h2 id=\"具体方法\">具体方法</h2>\r\n<p><span class=\"math inline\">\\(f^{W}(x)\\)</span>代表输入x经过参数为<span class=\"math inline\">\\(W\\)</span>的神经网络之后得到的输出，那么对于回归任务，可以定义其高斯似然概率：<span class=\"math inline\">\\(p(y|f^{W}(x)) = \\mathcal{N}(f^{W}(x), \\sigma^2)\\)</span>,对于分类任务，因为常用Softmax来进行输出的处理，所以定义其似然概率为<span class=\"math inline\">\\(p(y|f^{W}(x)) = Softmax(\\frac{1}{\\sigma^2}f^{W}(x))\\)</span>，这里的<span class=\"math inline\">\\(\\frac{1}{\\sigma^2}\\)</span>不会影响Softmax的结果。</p>\r\n<p>如果一个模型有多个任务，可以用最大似然法来定义其损失函数，假设模型存在一个回归任务和一个分类任务，其对数似然定义如下： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    log(p(y_1,y_2=c|f^{W}(x))) &amp;= log(p(y_1|f^{W}(x))) + log(p(y_2=c|f^{W}(x), \\sigma_2^2))\\\\\r\n    &amp;= log(\\mathcal{N}(f^{W}(x), \\sigma_1^2)) + log(Softmax(y_2=c; f^{W}(x)))\\\\\r\n    &amp;= log(\\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{(x - f^{W}(x))^2}{2\\sigma_1^2}}) + log(\\frac{e^{\\frac{1}{\\sigma ^ 2}f_c^W(x)}}{\\sum_{\\hat{c}}e^{\\frac{1}{\\sigma_2 ^ 2}f_{\\hat{c}}^W(x)}})\\\\\r\n    &amp;= -\\frac{(x - f^{W}(x))^2}{2\\sigma_1^2} - log\\sigma_1 - log\\sqrt{2\\pi} + log(\\frac{e^{\\frac{1}{\\sigma ^ 2}f_c^W(x)}}{\\sum_{\\hat{c}}e^{\\frac{1}{\\sigma_2 ^ 2}f_{\\hat{c}}^W(x)}})\\\\\r\n\\end{aligned}\r\n\\]</span> 当<span class=\"math inline\">\\(\\sigma_2\\)</span>接近1时，<span class=\"math inline\">\\(\\frac{1}{\\sigma_2}\\sum_{\\hat{c}}e^{\\frac{1}{\\sigma_2 ^ 2}f_{\\hat{c}}^W(x)} \\approx (\\sum_{\\hat{c}}e^{f_{\\hat{c}}^W(x)})^\\frac{1}{\\sigma_2 ^ 2}\\)</span>，上式可以写为： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    &amp;\\approx -\\frac{(x - f^{W}(x))^2}{2\\sigma_1^2} - log\\sigma_1 - log\\sqrt{2\\pi} + log(\\frac{e^{\\frac{1}{\\sigma ^ 2}f_c^W(x)}}{\\sigma_2(\\sum_{\\hat{c}}e^{f_{\\hat{c}}^W(x)})^\\frac{1}{\\sigma_2 ^ 2}})\\\\\r\n    &amp;= -\\frac{(x - f^{W}(x))^2}{2\\sigma_1^2} - log\\sigma_1 - log\\sqrt{2\\pi} + \\frac{1}{\\sigma_2 ^ 2}log(\\frac{e^{f_c^W(x)}}{\\sum_{\\hat{c}}e^{f_{\\hat{c}}^W(x)}}) - log\\sigma_2\\\\\r\n    &amp;= \\frac{1}{\\sigma_1 ^ 2}L_1(W) + \\frac{1}{\\sigma_2 ^ 2}L_2(W) - log\\sigma_1 - log\\sigma_2 - log\\sqrt{2\\pi}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>因此，在模型中设置两个可训练权重<span class=\"math inline\">\\(\\sigma_1\\)</span>和<span class=\"math inline\">\\(\\sigma_2\\)</span>，使用<span class=\"math inline\">\\(-\\frac{1}{\\sigma_1 ^ 2}L_1(W) - \\frac{1}{\\sigma_2 ^ 2}L_2(W) + log\\sigma_1 + log\\sigma_2\\)</span>作为多任务损失函数，即可自动学习模型的损失的权重。</p>\r\n<h2 id=\"一些问题\">一些问题</h2>\r\n<ul>\r\n<li><strong>上式的简化是说在<span class=\"math inline\">\\(\\sigma_2\\)</span>接近1的时候有用，但是模型学出来的权重很可能不在1附近，或者可以将<span class=\"math inline\">\\(\\sigma_2\\)</span>固定为1？</strong></li>\r\n<li><strong>从论文贴出的结果看，组合多任务学习的效果比单任务的效果要差？结果图如下</strong></li>\r\n</ul>\r\n<figure>\r\n<img src=\"paper_result.png\" alt=\"论文中的实验结果对比\" /><figcaption aria-hidden=\"true\">论文中的实验结果对比</figcaption>\r\n</figure>\r\n"},{"title":"论文阅读《Reducing-the-Dimensionality-of-Data-with-Neural-Networks》","date":"2019-08-01T09:09:54.000Z","mathjax":true,"_content":"\n## 提出的问题\n使用多层神经网络进行数据降维编码和解码的时候，使用梯度下降法的效果非常依赖权重的初始化，难以训练。\n\n## 解决思想\n定义一种两层的神经网络模型（**注意，只有一层权重，但是有两个偏置变量**），称为限制玻尔兹曼机（RBM）,多个RBM串联组成一个自动编/解码器，训练时，每个RBM单独训练（每个玻尔兹曼机的目标都是使编码解码之后的输出和原始输入尽可能相同，具体模型结构图如下所示。\n![RBM与编码/解码模型](RBM_and_multilayer_model.png)\n\n## 实现细节\n定义RBM的能量函数$E(v,h) = - \\sum_{i \\in pixels}b_i v_i - \\sum_{j \\in features}b_j h_j - \\sum_{i,j}v_iw_{ij}h_j$，其中$v$表示可见单元(visible unit)，$h$表示隐藏单元(hidden unit)，$i$表示可见单元编号，$j$表示隐藏单元编号，$w_{ij}$表示权重矩阵中的相应值，$b_i$表示可见层的偏置，$b_j$表示隐藏层的偏置，训练每个RBM就是最小化其能量函数。","source":"_posts/论文阅读/论文阅读《Reducing-the-Dimensionality-of-Data-with-Neural-Networks》.md","raw":"---\ntitle: 论文阅读《Reducing-the-Dimensionality-of-Data-with-Neural-Networks》\ndate: 2019-08-01 17:09:54\ntags: 论文阅读\nmathjax: true\n---\n\n## 提出的问题\n使用多层神经网络进行数据降维编码和解码的时候，使用梯度下降法的效果非常依赖权重的初始化，难以训练。\n\n## 解决思想\n定义一种两层的神经网络模型（**注意，只有一层权重，但是有两个偏置变量**），称为限制玻尔兹曼机（RBM）,多个RBM串联组成一个自动编/解码器，训练时，每个RBM单独训练（每个玻尔兹曼机的目标都是使编码解码之后的输出和原始输入尽可能相同，具体模型结构图如下所示。\n![RBM与编码/解码模型](RBM_and_multilayer_model.png)\n\n## 实现细节\n定义RBM的能量函数$E(v,h) = - \\sum_{i \\in pixels}b_i v_i - \\sum_{j \\in features}b_j h_j - \\sum_{i,j}v_iw_{ij}h_j$，其中$v$表示可见单元(visible unit)，$h$表示隐藏单元(hidden unit)，$i$表示可见单元编号，$j$表示隐藏单元编号，$w_{ij}$表示权重矩阵中的相应值，$b_i$表示可见层的偏置，$b_j$表示隐藏层的偏置，训练每个RBM就是最小化其能量函数。","slug":"论文阅读/论文阅读《Reducing-the-Dimensionality-of-Data-with-Neural-Networks》","published":1,"updated":"2019-08-08T00:42:53.467Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckjxpc3sr002y44mq3ml0h7ip","content":"<h2 id=\"提出的问题\">提出的问题</h2>\r\n<p>使用多层神经网络进行数据降维编码和解码的时候，使用梯度下降法的效果非常依赖权重的初始化，难以训练。</p>\r\n<h2 id=\"解决思想\">解决思想</h2>\r\n<p>定义一种两层的神经网络模型（<strong>注意，只有一层权重，但是有两个偏置变量</strong>），称为限制玻尔兹曼机（RBM）,多个RBM串联组成一个自动编/解码器，训练时，每个RBM单独训练（每个玻尔兹曼机的目标都是使编码解码之后的输出和原始输入尽可能相同，具体模型结构图如下所示。 <img src=\"RBM_and_multilayer_model.png\" alt=\"RBM与编码/解码模型\" /></p>\r\n<h2 id=\"实现细节\">实现细节</h2>\r\n<p>定义RBM的能量函数<span class=\"math inline\">\\(E(v,h) = - \\sum_{i \\in pixels}b_i v_i - \\sum_{j \\in features}b_j h_j - \\sum_{i,j}v_iw_{ij}h_j\\)</span>，其中<span class=\"math inline\">\\(v\\)</span>表示可见单元(visible unit)，<span class=\"math inline\">\\(h\\)</span>表示隐藏单元(hidden unit)，<span class=\"math inline\">\\(i\\)</span>表示可见单元编号，<span class=\"math inline\">\\(j\\)</span>表示隐藏单元编号，<span class=\"math inline\">\\(w_{ij}\\)</span>表示权重矩阵中的相应值，<span class=\"math inline\">\\(b_i\\)</span>表示可见层的偏置，<span class=\"math inline\">\\(b_j\\)</span>表示隐藏层的偏置，训练每个RBM就是最小化其能量函数。</p>\r\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"提出的问题\">提出的问题</h2>\r\n<p>使用多层神经网络进行数据降维编码和解码的时候，使用梯度下降法的效果非常依赖权重的初始化，难以训练。</p>\r\n<h2 id=\"解决思想\">解决思想</h2>\r\n<p>定义一种两层的神经网络模型（<strong>注意，只有一层权重，但是有两个偏置变量</strong>），称为限制玻尔兹曼机（RBM）,多个RBM串联组成一个自动编/解码器，训练时，每个RBM单独训练（每个玻尔兹曼机的目标都是使编码解码之后的输出和原始输入尽可能相同，具体模型结构图如下所示。 <img src=\"RBM_and_multilayer_model.png\" alt=\"RBM与编码/解码模型\" /></p>\r\n<h2 id=\"实现细节\">实现细节</h2>\r\n<p>定义RBM的能量函数<span class=\"math inline\">\\(E(v,h) = - \\sum_{i \\in pixels}b_i v_i - \\sum_{j \\in features}b_j h_j - \\sum_{i,j}v_iw_{ij}h_j\\)</span>，其中<span class=\"math inline\">\\(v\\)</span>表示可见单元(visible unit)，<span class=\"math inline\">\\(h\\)</span>表示隐藏单元(hidden unit)，<span class=\"math inline\">\\(i\\)</span>表示可见单元编号，<span class=\"math inline\">\\(j\\)</span>表示隐藏单元编号，<span class=\"math inline\">\\(w_{ij}\\)</span>表示权重矩阵中的相应值，<span class=\"math inline\">\\(b_i\\)</span>表示可见层的偏置，<span class=\"math inline\">\\(b_j\\)</span>表示隐藏层的偏置，训练每个RBM就是最小化其能量函数。</p>\r\n"},{"title":"GAN概述","date":"2020-08-22T08:08:21.000Z","mathjax":true,"_content":"GAN在深度学习领域是生成对抗网络（Generative Adversarial Network）的简称，第一次是在论文《Generative Adversarial Nets》中提出，论文Abstract中第一句话就点出了GAN的本质“a new framework for estimating grnerative models via an adversarial process”，即GAN是通过一种对抗过程来训练（估计）生成模型的一种框架。\n\n# GAN的基础结构\nGAN的简陋示意图如下所示。\n\n![GAN结构示意](GAN模型结构.png)\n\n其中$D$表示判别器模型，$G$表示生成器模型，$G$从一个隐空间中随机采样得到隐变量$z$，将其生成为$G(z)$，用于模拟真实数据$x$，判别器$D$则接收一个输入，这里不是同时接收$x$和$G(z)$，判别器本质上是个分类模型，他不知道自己接收到的数据是生成器生成的$G(z)$还是真实数据$x$，判别器的工作就是将$G(z)$和$x$分成两类，即判别数据的真假。在GAN的架构中，生成器$G$努力构造虚假数据，其最终目的是让判别器无法判别$G(z)$和$x$的差别，而判别器$D$的目的则努力将$G(z)$找出来，判别器和生成器构成了一种对抗的关系，在训练过程中，不断提高两种模型的效果，最终就可以得到效果以假乱真的生成器$G$，这样的生成器就可以用于数据（图像、音频等）的生成。\n\n# GAN的损失函数\n在原始论文《Generative Adversarial Nets》中，GAN中的判别器和生成器的损失函数分别定义如下，其中$P_R$代表数据的真实分布，$P_G$代表生成器所生成的数据分布，从损失函数中就可以看出两个模型的对抗关系。\n\n$$\n\\begin{aligned}\n    L_D &= \\frac{1}{m} \\sum\\limits_{i=1}^m[log(D(x^{(i)})) + log(1 - D(G(z^{(i)})))]\\\\\n    &= E_{x \\sim P_R}log(D(x)) + E_{x \\sim P_G}log(1 - D(x))\\\\\n    L_G &= \\frac{1}{m} \\sum\\limits_{i=1}^m log(D(G(z^{(i)})))\\\\\n    &= E_{x \\sim P_G} log(D(x))\n\\end{aligned}\n$$\n\n在生成器$G$固定的时候，最优的判别器可以写为$D^\\star(x) = \\frac{P_R(x)}{P_R(x) + P_G(x)}$，这里$P_R(x)，P_G(x)$分别表示数据来源于真实数据和生成数据的概率，当分布$P_R$和$P_G$很近或者直接重叠的时候，就代表生成器达到最优，即生成数据的分布和真实数据分布相同了。\n\n# 条件生成对抗网络（CGAN， Conditional Generative Adversarial Network）\nGAN可以生成样本，那么是否可以指定生成的样本是什么，例如能够指定生成一个包含猫的图片？\n\n在论文《Conditional Generative Adversarial Nets》中，条件GAN提供了一种思路。\n\n原始的GAN可以表达为$\\min\\limits_G\\max\\limits_D V(D, G) = E_{x \\sim P_R}log(D(x)) + E_{z \\sim P_Z}log(1 - D(G(z)))$\n\n而在条件GAN中，变成了$\\min\\limits_G\\max\\limits_D V(D, G) = E_{x \\sim P_R}log(D(x|y)) + E_{z \\sim P_Z}log(1 - D(G(z, y) | y))$\n\n在CGAN论文中，使用CGAN来生成手写数字，其生成器输入包括100维的随机隐变量以及10维的one-hot编码类别向量，判别器输入包括784维的图像以及10维的one-hot类别向量。\n\n# 深度卷积生成对抗网络（DCGAN，Deep Convolutional Generative Adversarial Networks）\n在论文《Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks》中第一次采用深度卷积神经网络来代替GAN中的生成器和判别器（以前都是使用MLP），对于生成器，可以使用上采样加卷积的方式来生成最终的图像，而对于判别器，类似于普通的分类模型，只不过在下采样的时候不适用池化操作，而是使用步长大于1的卷积操作，另外不论是判别器还是生成器，除了输入输出层，其他层都使用Batch Normalization层来稳定梯度，加速模型的收敛。\n\n# WGAN\nGAN的思路看起来很完美，但是原始的GAN loss存在一些问题，这些问题在后面的论文《Wassertein GAN》中提到并解决，这一部分内容可以参考知乎上的一篇文章分析：[令人拍案叫绝的Wasserstein GAN](https://zhuanlan.zhihu.com/p/25071913)，或者直接看原论文。\n\n总的来说，分析可以发现原GAN的论文中生成器模型的损失函数其实就是在最小化分布$P_G$和$P_R$之间的KL散度，而KL散度存在的一个问题是：当分布$P_G$和分布$P_R$没有交集时，KL散度始终为$log2$，不能提供有效的梯度，从而导致GAN训练困难，而WGAN利用Wasserstein距离来代替KL散度以度量两个分布$P_G$、$P_R$之间的距离。\n\nWasserstein距离也称作Earch-Mover(EM)距离，可以写为\n$$\nW(P_R, P_G) = \\inf\\limits_{\\gamma \\sim \\prod(P_R, P_G)} E_{\\{x,y\\}\\sim \\gamma}||x - y||\n$$\n这里$\\prod(P_R, P_G)$表示$P_G$、$P_R$的联合分布，对于所有的联合分布，求$\\{x,y\\}$的距离期望的最小值，就是Wasserstein距离，这里可以仔细理解下为什么Wasserstein距离不存在KL散度的那种无法适用于无交集的两个分布的问题。\n\n这里有个问题是Wassertein距离在模型中是无法计算的，因此WGAN论文中用一些近似手段，将Wassertein距离近似表示为如下计算方式。\n$$\nK \\times W(P_R, P_G) \\approx \\max\\limits_{|f_w|_L \\le K} E_{x \\sim P_R}f_w(x) - E_{x \\sim P_G}f_w(x)\n$$\n这里$f_w$表示一个神经网络或者CNN所构成的判别器所表示的函数，$|f_w|_L$表示这个函数的Lipschitz常数（如果$\\exists K, \\forall x_1, x_2 \\ |f(x_1) - f(x_2)| \\le K|x_1 - x_2|$那么$K$就是函数$f$的Lipschitz常数）。\n\n因此我们将GAN的判别器损失改为$-E_{x \\sim P_R}f_w(x) + E_{x \\sim P_G}f_w(x)$，这样一来，判别器的目标不再是一个分类问题，而是求$\\max\\limits_{|f_w|_L \\le K} E_{x \\sim P_R}f_w(x) - E_{x \\sim P_G}f_w(x)$以近似Wasserstein距离，因此需要去掉判别器中的sigmoid层，同时为了保证$|f_w|_L \\le K$，这里需要将判别器模型中的参数值限定在一定范围内，这可以通过参数的clip操作来实现。\n\n判别器训练好之后，训练生成器时，损失函数使用$-E_{x \\sim P_G}f_w(x)$（这个意思是将近似的Wasserstein距离作为损失函数来优化，缩小Wasserstein距离）即可。\n\n和之前的GAN loss相比的话，WGAN相当于仅仅做了三个修改：\n- 去掉判别器损失和生成器损失中的log\n- 去掉判别器的sigmoid\n- 判别器的参数每次更新之后需要进行clip，以保证参数在一定范围内。\n\n另外一个trick是：GAN的训练不适合使用Adam这类基于动量的算法，因为每次生成器的更新后，判别器的loss梯度非常不稳定，甚至和之前的方向完全相反，基于动量优化容易导致收敛缓慢。\n\n\n# GAN的应用方向以及发展\n上面介绍了GAN的几个主要的基础发展方向，接下来看看GAN应用领域方面的具体问题。\n\n## 图像生成\nGAN的老本行就是生成数据，图像数据自然包括在内，但是早期的GAN生成的图像清晰度低且内容混乱，近年来在高清图像生成方面有了一些进展。\n\n未完待续...\n\n## 图像风格迁移\n下面是在论文阅读过程中遇到的一些基础概念。\n### Gram矩阵\nGram矩阵可以看做是feature之间的偏心协方差矩阵，例如对于一个$C \\times H \\times W$的特征图，首先将特征图进行resize得到特征矩阵$M \\in R^{C \\times HW}$，然后计算Gram矩阵为$M \\times M^T$，Gram矩阵对角线上的元素表示不同特征在图像上出现的强度，非对角线上的元素则表示不同特征之间的相关性，因此Gram矩阵可用于表示图像的整体风格。\n### 双边滤波（BF，Bilateral Filter）\n首先，一般的高斯滤波器可以表示如下：\n$$\n\\begin{aligned}\n    J_p = \\frac{1}{K_p} \\sum\\limits_{q \\in \\Omega_p} I_q \\times f(||p-q||)\n\\end{aligned}\n$$\n其中$J$表示输出图像，$I$表示输入图像，$p,q$表示位置，$\\Omega_p$表示$p$的一个邻域，$f$表示高斯核函数，$K_p$表示该位置的归一化因子，这样的滤波考虑到了像素之间的相对位置关系，虽然可以有效的去躁，但是对于一些图像边缘非常不友好，容易将边缘模糊化。\n\n基于上面的问题，双边滤波考虑再引入像素之间的像素值关系，将滤波过程表达为如下：\n$$\n\\begin{aligned}\n    J_p = \\frac{1}{K_p} \\sum\\limits_{q \\in \\Omega_p} I_q \\times f(||p-q||) \\times g(|I_p-I_q|)\n\\end{aligned}\n$$\n\n这里的$g$同样为一个高斯核函数，$||I_p - I_q||$表达的是两个位置之间的像素值差异，双边的意思即同时考虑相对位置信息和像素值信息，用这种方式进行滤波，对于边缘比较友好。\n\n### 双边网格（Bilateral Grid）\n双边滤波其实运行起来很慢，需要对其进行加速，因此就有了双边网格的概念，一个灰度图像$I$大小为$H \\times W$，其实可以表示为$H \\times W \\times B$大小的一个三维格式，这个三维格式就是双边网格，其中最后一维表示灰度值，图像$I$上的一个位置$(x, y)$的点，其像素值为$I_{xy}$，那么在双边网格中，这个点的位置就变成了$(y, x, I_{xy})$，另外如果是一个uint8类型的灰度图，那么似乎$B$必须为256，但是这里其实可以做一定的区间划分以压缩$B$的大小，例如划分为10个区间，那么$B$的大小就只需要10了。同理，对$H$和$W$两个维度也可以进行压缩，从而将一个二维图像表示为双边空间（可以理解为双边网格对应的大小为$H \\times W \\times B$的空间）中的点的集合，这个过程叫做splat，在双边空间中做完双边滤波之后，再通过插值（一般使用三线性插值）的方式，从双边空间中恢复原始的大图像，这个过程称为slice。\n\nslice的具体操作我没有找到说明，按照我的理解，大概是对于原始图像$I$上的一个点$(x,y)$，那么可以通过对双边空间中$(x, y, I_{xy})$这个坐标进行三线性插值，得到输出图像上$(x,y)$位置的值。\n\n\n### 联合双边滤波（JBF，Joint Bilateral Filter）以及联合双边上采样（JBU，Joint Bilateral Upsampling）\n在双边滤波的基础上，引入一张其他图像来引导滤波过程，可以写作下式：\n\n$$\n\\begin{aligned}\n    J_p = \\frac{1}{K_p} \\sum\\limits_{q \\in \\Omega_p} I_q \\times f(||p-q||) \\times g(|\\hat{I}_p-\\hat{I}_q|)\n\\end{aligned}\n$$\n\n其中$\\hat{I}$表示一张其他图像。\n\n联合双边上采样使用的就是联合双边滤波的思路，假设现在有一张很大的图像$I$，对其进行某种处理非常耗时间，如果想要加速这个处理过程，那么可以考虑先将图像缩小得到图像$S$，然后在缩小的图像上进行处理,得到图像$\\hat{S}$，最后将缩小的图像resize回原大小得到图像$\\hat{I}$。\n\n上面的思路很简单，但是这里有个主要的问题是如果使用传统的插值方式（例如双线性插值、最近邻插值等），$\\hat{S}$直接上采样得到的$\\hat{I}$往往非常不清晰，但这里不是一个传统的resize问题，因为这里还有图像$I$可以用于参考，因此就可以考虑在插值之后使用联合双边滤波对插值之后的图像进行进一步处理，提高清晰度，首先$\\hat{S}$直接上采样(一般使用最近邻插值就可以)得到$U(\\hat{S})$，然后用原始图像$I$引导进行联合双边滤波，得到最终的$\\hat{I}$，如下所示：\n\n$$\n\\begin{aligned}\n    \\hat{I}_p = \\frac{1}{K_p} \\sum\\limits_{q \\in \\Omega_p} U(\\hat{S})_q \\times f(||p-q||) \\times g(|I_p-I_q|)\n\\end{aligned}\n$$\n\n下面记录一下看过的一些风格转换相关的论文\n\n### 论文《Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization》\n这篇论文中构建了一个端到端的进行风格转换的模型，模型在预测时可以选择任意的style图片，而不是固定为训练过程中使用的style图片。其主要思想是认为style在CNN特征图中表达为特征的方差和均值，因此这个论文中使用一个Adaptive Instance Normalization（AdaIN）模块，通过计算style图片的特征图的Instance Normalization均值和方差，然后将content图片的特征图均值和方差按照style图片的均值和方差进行缩放，然后将该特征图进行解码，得到风格转换之后的图片。\n\n该方案的整体结构图如下所示，content图片和style图片都首先经过一个固定的预训练VGG-19模型编码，得到两个特征图，两个特征图输入到AdaIN模块中，用于根据style图片的特征图调整content图片的特征图的特征均值和方差，调整后的content特征图再通过一个解码器得到风格转换之后的输出，最后将输出图片再通过同样的VGG-19编码器进行编码，使用编码之后的输出计算content损失和style损失。\n\n\n![AdaIN模型结构示意图](AdaIN模型结构示意图.png)\n\n\n其损失函数设计如下，其中$L$是总的损失，由两部分组成，一个是content损失$L_c$，一个是style损失$L_s$，$L_c$是由模型输出$f(g(t))$和经过了AdaIN的特征图$t$计算，$f$表示编码器，$g$表示解码器。$L_s$是由编码器VGG的不同层特征$\\phi_i$计算，这里只监督不同层特征的统计量，例如$\\mu$表示特征的均值，$\\sigma$表示特征的标准差。\n\n$$\n\\begin{aligned}\n    L &= L_c + \\lambda L_s\\\\\n    L_c &= ||f(g(t)) - t||_2\\\\\n    L_s &= \\sum\\limits_{i=1}^L ||\\mu(\\phi_i(g(t))) - \\mu(\\phi_i(s))||_2 + \\sum\\limits_{i=1}^L ||\\sigma(\\phi_i(g(t))) - \\sigma(\\phi_i(s))||_2\n\\end{aligned}\n$$\n\n该文章实现了一个端到端的，可以使用任意style图片的风格转换任务，但是其缺点在于容易造成content的改变，我认为是因为这里没有一个独立的content编码分支造成的，或许可以从这里入手做一些改变，但是另一篇论文从双边联合滤波的角度给出了一种不同的方案，如下。\n\n### 论文《Joint Bilateral Learning for Real-time Universal Photorealistic Style Transfer》\n该论文尝试将风格转换问题设计为一个图像局部transform的问题，让模型去在低分辨率图像上去学习出一个transform系数，然后将transform系数应用于高分辨率图像以完成快速的高分辨率图像的风格转换处理工作，很大程度上借鉴了HDRnet，关于HDRnet的简单介绍可以参考我写的另一篇论文阅读记录{% post_link 论文阅读《Deep-Bilateral-Learning-for-Real-Time-Image-Enhancement》 论文阅读《Deep-Bilateral-Learning-for-Real-Time-Image-Enhancement》%}。\n\n论文中整体的模型结构如下图所示。\n\n![基于联合双边学习的图像风格转换模型结构示意图](基于联合双边学习的图像风格转换模型结构示意图.png)\n\n首先将低分辨率的content图片和低分辨率的style图片用VGG-19进行encoding，上面的top path部分主要是将VGG-19中conv2_1、conv3_1、conv4_1对应的特征图拿出来分别经过AdaIN层，以根据style图特征调整content图的特征，最终得到调整style后的不同分辨率的特征，下面的一个分支主要有三个splatting block组成，其结构可以参考图片右下角，主要是在学习Bilateral grid的splat操作，同时考虑到了style信息，因此使用了AdaIN层来对不同阶段的特征进行调整，因此这个叫做Style-base splatting，最终的特征还是和HDRnet一样，分成局部特征的学习和全局特征的学习，最后将局部特征和全局特征混合为双边网格$\\Gamma$，作为即将对原图进行局部变换的系数，最后将双边网格以原分辨率图为guide图像进行slice操作插值到原分辨率大小的变换系数图，然后apply到原分辨图上，得到风格转换之后的输出图。\n\n该论文的损失函数设计如下，这里的$L_c$和$L_sa$其实和论文《Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization》中的$L_c$和$L_s$类似，这里的$F_i$表示VGG的中间层特征输出，$N_c$表示取的中间层的个数，$I_c$表示低分辨率的content图片，$I_s$表示低分辨率的style图片，$\\mu$和$\\sigma$分别表示统计均值和标准差。这里新增了一个$L_r$损失，其中$s$表示双边网格上的一个位置，$N(s)$表示双边网格的$s$位置的邻域（论文里用的是6邻域），这一项损失主要是希望相邻的位置的仿射变换差别不大，使得其变换更加平滑。另外对于三个损失的系数，论文中使用的是$\\lambda_c= 0.5，\\lambda_sa =1，\\lambda_r = 0.15$\n\n$$\n\\begin{aligned}\n    L &= \\lambda_c L_c + \\lambda_sa L_{sa} + \\lambda_r L_r\\\\\n    L_c &= \\sum\\limits_{i=1}^{N_c}||F_i[O] - F_i[I_c]||^2_2\\\\\n    L_{sa} &= \\sum\\limits_{i=1}^{N_s} ||\\mu(F_i[O]) - \\mu(F_i[I_s])||^2_2 + \\sum\\limits_{i=1}^{N_s} ||\\sigma(F_i[O]) - \\sigma(F_i[I_s])||^2_2\\\\\n    L_r &= \\sum\\limits_s \\sum\\limits_{t \\in N(s)} ||\\Gamma[s] - \\Gamma[t]||^2_2\n\\end{aligned}\n$$\n\n这个方法在style转换任务中主要的好处是速度快，而且很大程度上保留图片的内容信息，因为其transform过程仅仅针对原始图片上单像素的颜色变换，但是也正是这个原因，这个transform也存在和HDRnet一样的限制，例如在艺术风格的转换任务上（这个不是简单的颜色变换可以完成的）上效果就不是那么明显了，不过对于一般的颜色风格转换任务，这个方法还算是有了较好的效果和速度。\n\n## 超分辨率\n\n未完待续...\n\n\n\n","source":"_posts/学习笔记/GAN概述.md","raw":"---\ntitle: GAN概述\ndate: 2020-08-22 16:08:21\ntags: [深度学习]\nmathjax: true\n---\nGAN在深度学习领域是生成对抗网络（Generative Adversarial Network）的简称，第一次是在论文《Generative Adversarial Nets》中提出，论文Abstract中第一句话就点出了GAN的本质“a new framework for estimating grnerative models via an adversarial process”，即GAN是通过一种对抗过程来训练（估计）生成模型的一种框架。\n\n# GAN的基础结构\nGAN的简陋示意图如下所示。\n\n![GAN结构示意](GAN模型结构.png)\n\n其中$D$表示判别器模型，$G$表示生成器模型，$G$从一个隐空间中随机采样得到隐变量$z$，将其生成为$G(z)$，用于模拟真实数据$x$，判别器$D$则接收一个输入，这里不是同时接收$x$和$G(z)$，判别器本质上是个分类模型，他不知道自己接收到的数据是生成器生成的$G(z)$还是真实数据$x$，判别器的工作就是将$G(z)$和$x$分成两类，即判别数据的真假。在GAN的架构中，生成器$G$努力构造虚假数据，其最终目的是让判别器无法判别$G(z)$和$x$的差别，而判别器$D$的目的则努力将$G(z)$找出来，判别器和生成器构成了一种对抗的关系，在训练过程中，不断提高两种模型的效果，最终就可以得到效果以假乱真的生成器$G$，这样的生成器就可以用于数据（图像、音频等）的生成。\n\n# GAN的损失函数\n在原始论文《Generative Adversarial Nets》中，GAN中的判别器和生成器的损失函数分别定义如下，其中$P_R$代表数据的真实分布，$P_G$代表生成器所生成的数据分布，从损失函数中就可以看出两个模型的对抗关系。\n\n$$\n\\begin{aligned}\n    L_D &= \\frac{1}{m} \\sum\\limits_{i=1}^m[log(D(x^{(i)})) + log(1 - D(G(z^{(i)})))]\\\\\n    &= E_{x \\sim P_R}log(D(x)) + E_{x \\sim P_G}log(1 - D(x))\\\\\n    L_G &= \\frac{1}{m} \\sum\\limits_{i=1}^m log(D(G(z^{(i)})))\\\\\n    &= E_{x \\sim P_G} log(D(x))\n\\end{aligned}\n$$\n\n在生成器$G$固定的时候，最优的判别器可以写为$D^\\star(x) = \\frac{P_R(x)}{P_R(x) + P_G(x)}$，这里$P_R(x)，P_G(x)$分别表示数据来源于真实数据和生成数据的概率，当分布$P_R$和$P_G$很近或者直接重叠的时候，就代表生成器达到最优，即生成数据的分布和真实数据分布相同了。\n\n# 条件生成对抗网络（CGAN， Conditional Generative Adversarial Network）\nGAN可以生成样本，那么是否可以指定生成的样本是什么，例如能够指定生成一个包含猫的图片？\n\n在论文《Conditional Generative Adversarial Nets》中，条件GAN提供了一种思路。\n\n原始的GAN可以表达为$\\min\\limits_G\\max\\limits_D V(D, G) = E_{x \\sim P_R}log(D(x)) + E_{z \\sim P_Z}log(1 - D(G(z)))$\n\n而在条件GAN中，变成了$\\min\\limits_G\\max\\limits_D V(D, G) = E_{x \\sim P_R}log(D(x|y)) + E_{z \\sim P_Z}log(1 - D(G(z, y) | y))$\n\n在CGAN论文中，使用CGAN来生成手写数字，其生成器输入包括100维的随机隐变量以及10维的one-hot编码类别向量，判别器输入包括784维的图像以及10维的one-hot类别向量。\n\n# 深度卷积生成对抗网络（DCGAN，Deep Convolutional Generative Adversarial Networks）\n在论文《Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks》中第一次采用深度卷积神经网络来代替GAN中的生成器和判别器（以前都是使用MLP），对于生成器，可以使用上采样加卷积的方式来生成最终的图像，而对于判别器，类似于普通的分类模型，只不过在下采样的时候不适用池化操作，而是使用步长大于1的卷积操作，另外不论是判别器还是生成器，除了输入输出层，其他层都使用Batch Normalization层来稳定梯度，加速模型的收敛。\n\n# WGAN\nGAN的思路看起来很完美，但是原始的GAN loss存在一些问题，这些问题在后面的论文《Wassertein GAN》中提到并解决，这一部分内容可以参考知乎上的一篇文章分析：[令人拍案叫绝的Wasserstein GAN](https://zhuanlan.zhihu.com/p/25071913)，或者直接看原论文。\n\n总的来说，分析可以发现原GAN的论文中生成器模型的损失函数其实就是在最小化分布$P_G$和$P_R$之间的KL散度，而KL散度存在的一个问题是：当分布$P_G$和分布$P_R$没有交集时，KL散度始终为$log2$，不能提供有效的梯度，从而导致GAN训练困难，而WGAN利用Wasserstein距离来代替KL散度以度量两个分布$P_G$、$P_R$之间的距离。\n\nWasserstein距离也称作Earch-Mover(EM)距离，可以写为\n$$\nW(P_R, P_G) = \\inf\\limits_{\\gamma \\sim \\prod(P_R, P_G)} E_{\\{x,y\\}\\sim \\gamma}||x - y||\n$$\n这里$\\prod(P_R, P_G)$表示$P_G$、$P_R$的联合分布，对于所有的联合分布，求$\\{x,y\\}$的距离期望的最小值，就是Wasserstein距离，这里可以仔细理解下为什么Wasserstein距离不存在KL散度的那种无法适用于无交集的两个分布的问题。\n\n这里有个问题是Wassertein距离在模型中是无法计算的，因此WGAN论文中用一些近似手段，将Wassertein距离近似表示为如下计算方式。\n$$\nK \\times W(P_R, P_G) \\approx \\max\\limits_{|f_w|_L \\le K} E_{x \\sim P_R}f_w(x) - E_{x \\sim P_G}f_w(x)\n$$\n这里$f_w$表示一个神经网络或者CNN所构成的判别器所表示的函数，$|f_w|_L$表示这个函数的Lipschitz常数（如果$\\exists K, \\forall x_1, x_2 \\ |f(x_1) - f(x_2)| \\le K|x_1 - x_2|$那么$K$就是函数$f$的Lipschitz常数）。\n\n因此我们将GAN的判别器损失改为$-E_{x \\sim P_R}f_w(x) + E_{x \\sim P_G}f_w(x)$，这样一来，判别器的目标不再是一个分类问题，而是求$\\max\\limits_{|f_w|_L \\le K} E_{x \\sim P_R}f_w(x) - E_{x \\sim P_G}f_w(x)$以近似Wasserstein距离，因此需要去掉判别器中的sigmoid层，同时为了保证$|f_w|_L \\le K$，这里需要将判别器模型中的参数值限定在一定范围内，这可以通过参数的clip操作来实现。\n\n判别器训练好之后，训练生成器时，损失函数使用$-E_{x \\sim P_G}f_w(x)$（这个意思是将近似的Wasserstein距离作为损失函数来优化，缩小Wasserstein距离）即可。\n\n和之前的GAN loss相比的话，WGAN相当于仅仅做了三个修改：\n- 去掉判别器损失和生成器损失中的log\n- 去掉判别器的sigmoid\n- 判别器的参数每次更新之后需要进行clip，以保证参数在一定范围内。\n\n另外一个trick是：GAN的训练不适合使用Adam这类基于动量的算法，因为每次生成器的更新后，判别器的loss梯度非常不稳定，甚至和之前的方向完全相反，基于动量优化容易导致收敛缓慢。\n\n\n# GAN的应用方向以及发展\n上面介绍了GAN的几个主要的基础发展方向，接下来看看GAN应用领域方面的具体问题。\n\n## 图像生成\nGAN的老本行就是生成数据，图像数据自然包括在内，但是早期的GAN生成的图像清晰度低且内容混乱，近年来在高清图像生成方面有了一些进展。\n\n未完待续...\n\n## 图像风格迁移\n下面是在论文阅读过程中遇到的一些基础概念。\n### Gram矩阵\nGram矩阵可以看做是feature之间的偏心协方差矩阵，例如对于一个$C \\times H \\times W$的特征图，首先将特征图进行resize得到特征矩阵$M \\in R^{C \\times HW}$，然后计算Gram矩阵为$M \\times M^T$，Gram矩阵对角线上的元素表示不同特征在图像上出现的强度，非对角线上的元素则表示不同特征之间的相关性，因此Gram矩阵可用于表示图像的整体风格。\n### 双边滤波（BF，Bilateral Filter）\n首先，一般的高斯滤波器可以表示如下：\n$$\n\\begin{aligned}\n    J_p = \\frac{1}{K_p} \\sum\\limits_{q \\in \\Omega_p} I_q \\times f(||p-q||)\n\\end{aligned}\n$$\n其中$J$表示输出图像，$I$表示输入图像，$p,q$表示位置，$\\Omega_p$表示$p$的一个邻域，$f$表示高斯核函数，$K_p$表示该位置的归一化因子，这样的滤波考虑到了像素之间的相对位置关系，虽然可以有效的去躁，但是对于一些图像边缘非常不友好，容易将边缘模糊化。\n\n基于上面的问题，双边滤波考虑再引入像素之间的像素值关系，将滤波过程表达为如下：\n$$\n\\begin{aligned}\n    J_p = \\frac{1}{K_p} \\sum\\limits_{q \\in \\Omega_p} I_q \\times f(||p-q||) \\times g(|I_p-I_q|)\n\\end{aligned}\n$$\n\n这里的$g$同样为一个高斯核函数，$||I_p - I_q||$表达的是两个位置之间的像素值差异，双边的意思即同时考虑相对位置信息和像素值信息，用这种方式进行滤波，对于边缘比较友好。\n\n### 双边网格（Bilateral Grid）\n双边滤波其实运行起来很慢，需要对其进行加速，因此就有了双边网格的概念，一个灰度图像$I$大小为$H \\times W$，其实可以表示为$H \\times W \\times B$大小的一个三维格式，这个三维格式就是双边网格，其中最后一维表示灰度值，图像$I$上的一个位置$(x, y)$的点，其像素值为$I_{xy}$，那么在双边网格中，这个点的位置就变成了$(y, x, I_{xy})$，另外如果是一个uint8类型的灰度图，那么似乎$B$必须为256，但是这里其实可以做一定的区间划分以压缩$B$的大小，例如划分为10个区间，那么$B$的大小就只需要10了。同理，对$H$和$W$两个维度也可以进行压缩，从而将一个二维图像表示为双边空间（可以理解为双边网格对应的大小为$H \\times W \\times B$的空间）中的点的集合，这个过程叫做splat，在双边空间中做完双边滤波之后，再通过插值（一般使用三线性插值）的方式，从双边空间中恢复原始的大图像，这个过程称为slice。\n\nslice的具体操作我没有找到说明，按照我的理解，大概是对于原始图像$I$上的一个点$(x,y)$，那么可以通过对双边空间中$(x, y, I_{xy})$这个坐标进行三线性插值，得到输出图像上$(x,y)$位置的值。\n\n\n### 联合双边滤波（JBF，Joint Bilateral Filter）以及联合双边上采样（JBU，Joint Bilateral Upsampling）\n在双边滤波的基础上，引入一张其他图像来引导滤波过程，可以写作下式：\n\n$$\n\\begin{aligned}\n    J_p = \\frac{1}{K_p} \\sum\\limits_{q \\in \\Omega_p} I_q \\times f(||p-q||) \\times g(|\\hat{I}_p-\\hat{I}_q|)\n\\end{aligned}\n$$\n\n其中$\\hat{I}$表示一张其他图像。\n\n联合双边上采样使用的就是联合双边滤波的思路，假设现在有一张很大的图像$I$，对其进行某种处理非常耗时间，如果想要加速这个处理过程，那么可以考虑先将图像缩小得到图像$S$，然后在缩小的图像上进行处理,得到图像$\\hat{S}$，最后将缩小的图像resize回原大小得到图像$\\hat{I}$。\n\n上面的思路很简单，但是这里有个主要的问题是如果使用传统的插值方式（例如双线性插值、最近邻插值等），$\\hat{S}$直接上采样得到的$\\hat{I}$往往非常不清晰，但这里不是一个传统的resize问题，因为这里还有图像$I$可以用于参考，因此就可以考虑在插值之后使用联合双边滤波对插值之后的图像进行进一步处理，提高清晰度，首先$\\hat{S}$直接上采样(一般使用最近邻插值就可以)得到$U(\\hat{S})$，然后用原始图像$I$引导进行联合双边滤波，得到最终的$\\hat{I}$，如下所示：\n\n$$\n\\begin{aligned}\n    \\hat{I}_p = \\frac{1}{K_p} \\sum\\limits_{q \\in \\Omega_p} U(\\hat{S})_q \\times f(||p-q||) \\times g(|I_p-I_q|)\n\\end{aligned}\n$$\n\n下面记录一下看过的一些风格转换相关的论文\n\n### 论文《Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization》\n这篇论文中构建了一个端到端的进行风格转换的模型，模型在预测时可以选择任意的style图片，而不是固定为训练过程中使用的style图片。其主要思想是认为style在CNN特征图中表达为特征的方差和均值，因此这个论文中使用一个Adaptive Instance Normalization（AdaIN）模块，通过计算style图片的特征图的Instance Normalization均值和方差，然后将content图片的特征图均值和方差按照style图片的均值和方差进行缩放，然后将该特征图进行解码，得到风格转换之后的图片。\n\n该方案的整体结构图如下所示，content图片和style图片都首先经过一个固定的预训练VGG-19模型编码，得到两个特征图，两个特征图输入到AdaIN模块中，用于根据style图片的特征图调整content图片的特征图的特征均值和方差，调整后的content特征图再通过一个解码器得到风格转换之后的输出，最后将输出图片再通过同样的VGG-19编码器进行编码，使用编码之后的输出计算content损失和style损失。\n\n\n![AdaIN模型结构示意图](AdaIN模型结构示意图.png)\n\n\n其损失函数设计如下，其中$L$是总的损失，由两部分组成，一个是content损失$L_c$，一个是style损失$L_s$，$L_c$是由模型输出$f(g(t))$和经过了AdaIN的特征图$t$计算，$f$表示编码器，$g$表示解码器。$L_s$是由编码器VGG的不同层特征$\\phi_i$计算，这里只监督不同层特征的统计量，例如$\\mu$表示特征的均值，$\\sigma$表示特征的标准差。\n\n$$\n\\begin{aligned}\n    L &= L_c + \\lambda L_s\\\\\n    L_c &= ||f(g(t)) - t||_2\\\\\n    L_s &= \\sum\\limits_{i=1}^L ||\\mu(\\phi_i(g(t))) - \\mu(\\phi_i(s))||_2 + \\sum\\limits_{i=1}^L ||\\sigma(\\phi_i(g(t))) - \\sigma(\\phi_i(s))||_2\n\\end{aligned}\n$$\n\n该文章实现了一个端到端的，可以使用任意style图片的风格转换任务，但是其缺点在于容易造成content的改变，我认为是因为这里没有一个独立的content编码分支造成的，或许可以从这里入手做一些改变，但是另一篇论文从双边联合滤波的角度给出了一种不同的方案，如下。\n\n### 论文《Joint Bilateral Learning for Real-time Universal Photorealistic Style Transfer》\n该论文尝试将风格转换问题设计为一个图像局部transform的问题，让模型去在低分辨率图像上去学习出一个transform系数，然后将transform系数应用于高分辨率图像以完成快速的高分辨率图像的风格转换处理工作，很大程度上借鉴了HDRnet，关于HDRnet的简单介绍可以参考我写的另一篇论文阅读记录{% post_link 论文阅读《Deep-Bilateral-Learning-for-Real-Time-Image-Enhancement》 论文阅读《Deep-Bilateral-Learning-for-Real-Time-Image-Enhancement》%}。\n\n论文中整体的模型结构如下图所示。\n\n![基于联合双边学习的图像风格转换模型结构示意图](基于联合双边学习的图像风格转换模型结构示意图.png)\n\n首先将低分辨率的content图片和低分辨率的style图片用VGG-19进行encoding，上面的top path部分主要是将VGG-19中conv2_1、conv3_1、conv4_1对应的特征图拿出来分别经过AdaIN层，以根据style图特征调整content图的特征，最终得到调整style后的不同分辨率的特征，下面的一个分支主要有三个splatting block组成，其结构可以参考图片右下角，主要是在学习Bilateral grid的splat操作，同时考虑到了style信息，因此使用了AdaIN层来对不同阶段的特征进行调整，因此这个叫做Style-base splatting，最终的特征还是和HDRnet一样，分成局部特征的学习和全局特征的学习，最后将局部特征和全局特征混合为双边网格$\\Gamma$，作为即将对原图进行局部变换的系数，最后将双边网格以原分辨率图为guide图像进行slice操作插值到原分辨率大小的变换系数图，然后apply到原分辨图上，得到风格转换之后的输出图。\n\n该论文的损失函数设计如下，这里的$L_c$和$L_sa$其实和论文《Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization》中的$L_c$和$L_s$类似，这里的$F_i$表示VGG的中间层特征输出，$N_c$表示取的中间层的个数，$I_c$表示低分辨率的content图片，$I_s$表示低分辨率的style图片，$\\mu$和$\\sigma$分别表示统计均值和标准差。这里新增了一个$L_r$损失，其中$s$表示双边网格上的一个位置，$N(s)$表示双边网格的$s$位置的邻域（论文里用的是6邻域），这一项损失主要是希望相邻的位置的仿射变换差别不大，使得其变换更加平滑。另外对于三个损失的系数，论文中使用的是$\\lambda_c= 0.5，\\lambda_sa =1，\\lambda_r = 0.15$\n\n$$\n\\begin{aligned}\n    L &= \\lambda_c L_c + \\lambda_sa L_{sa} + \\lambda_r L_r\\\\\n    L_c &= \\sum\\limits_{i=1}^{N_c}||F_i[O] - F_i[I_c]||^2_2\\\\\n    L_{sa} &= \\sum\\limits_{i=1}^{N_s} ||\\mu(F_i[O]) - \\mu(F_i[I_s])||^2_2 + \\sum\\limits_{i=1}^{N_s} ||\\sigma(F_i[O]) - \\sigma(F_i[I_s])||^2_2\\\\\n    L_r &= \\sum\\limits_s \\sum\\limits_{t \\in N(s)} ||\\Gamma[s] - \\Gamma[t]||^2_2\n\\end{aligned}\n$$\n\n这个方法在style转换任务中主要的好处是速度快，而且很大程度上保留图片的内容信息，因为其transform过程仅仅针对原始图片上单像素的颜色变换，但是也正是这个原因，这个transform也存在和HDRnet一样的限制，例如在艺术风格的转换任务上（这个不是简单的颜色变换可以完成的）上效果就不是那么明显了，不过对于一般的颜色风格转换任务，这个方法还算是有了较好的效果和速度。\n\n## 超分辨率\n\n未完待续...\n\n\n\n","slug":"学习笔记/GAN概述","published":1,"updated":"2020-11-30T09:23:09.581Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckjxpc3sr003044mq2qk9babf","content":"<p>GAN在深度学习领域是生成对抗网络（Generative Adversarial Network）的简称，第一次是在论文《Generative Adversarial Nets》中提出，论文Abstract中第一句话就点出了GAN的本质“a new framework for estimating grnerative models via an adversarial process”，即GAN是通过一种对抗过程来训练（估计）生成模型的一种框架。</p>\r\n<h1 id=\"gan的基础结构\">GAN的基础结构</h1>\r\n<p>GAN的简陋示意图如下所示。</p>\r\n<figure>\r\n<img src=\"GAN模型结构.png\" alt=\"GAN结构示意\" /><figcaption aria-hidden=\"true\">GAN结构示意</figcaption>\r\n</figure>\r\n<p>其中<span class=\"math inline\">\\(D\\)</span>表示判别器模型，<span class=\"math inline\">\\(G\\)</span>表示生成器模型，<span class=\"math inline\">\\(G\\)</span>从一个隐空间中随机采样得到隐变量<span class=\"math inline\">\\(z\\)</span>，将其生成为<span class=\"math inline\">\\(G(z)\\)</span>，用于模拟真实数据<span class=\"math inline\">\\(x\\)</span>，判别器<span class=\"math inline\">\\(D\\)</span>则接收一个输入，这里不是同时接收<span class=\"math inline\">\\(x\\)</span>和<span class=\"math inline\">\\(G(z)\\)</span>，判别器本质上是个分类模型，他不知道自己接收到的数据是生成器生成的<span class=\"math inline\">\\(G(z)\\)</span>还是真实数据<span class=\"math inline\">\\(x\\)</span>，判别器的工作就是将<span class=\"math inline\">\\(G(z)\\)</span>和<span class=\"math inline\">\\(x\\)</span>分成两类，即判别数据的真假。在GAN的架构中，生成器<span class=\"math inline\">\\(G\\)</span>努力构造虚假数据，其最终目的是让判别器无法判别<span class=\"math inline\">\\(G(z)\\)</span>和<span class=\"math inline\">\\(x\\)</span>的差别，而判别器<span class=\"math inline\">\\(D\\)</span>的目的则努力将<span class=\"math inline\">\\(G(z)\\)</span>找出来，判别器和生成器构成了一种对抗的关系，在训练过程中，不断提高两种模型的效果，最终就可以得到效果以假乱真的生成器<span class=\"math inline\">\\(G\\)</span>，这样的生成器就可以用于数据（图像、音频等）的生成。</p>\r\n<h1 id=\"gan的损失函数\">GAN的损失函数</h1>\r\n<p>在原始论文《Generative Adversarial Nets》中，GAN中的判别器和生成器的损失函数分别定义如下，其中<span class=\"math inline\">\\(P_R\\)</span>代表数据的真实分布，<span class=\"math inline\">\\(P_G\\)</span>代表生成器所生成的数据分布，从损失函数中就可以看出两个模型的对抗关系。</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    L_D &amp;= \\frac{1}{m} \\sum\\limits_{i=1}^m[log(D(x^{(i)})) + log(1 - D(G(z^{(i)})))]\\\\\r\n    &amp;= E_{x \\sim P_R}log(D(x)) + E_{x \\sim P_G}log(1 - D(x))\\\\\r\n    L_G &amp;= \\frac{1}{m} \\sum\\limits_{i=1}^m log(D(G(z^{(i)})))\\\\\r\n    &amp;= E_{x \\sim P_G} log(D(x))\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>在生成器<span class=\"math inline\">\\(G\\)</span>固定的时候，最优的判别器可以写为<span class=\"math inline\">\\(D^\\star(x) = \\frac{P_R(x)}{P_R(x) + P_G(x)}\\)</span>，这里<span class=\"math inline\">\\(P_R(x)，P_G(x)\\)</span>分别表示数据来源于真实数据和生成数据的概率，当分布<span class=\"math inline\">\\(P_R\\)</span>和<span class=\"math inline\">\\(P_G\\)</span>很近或者直接重叠的时候，就代表生成器达到最优，即生成数据的分布和真实数据分布相同了。</p>\r\n<h1 id=\"条件生成对抗网络cgan-conditional-generative-adversarial-network\">条件生成对抗网络（CGAN， Conditional Generative Adversarial Network）</h1>\r\n<p>GAN可以生成样本，那么是否可以指定生成的样本是什么，例如能够指定生成一个包含猫的图片？</p>\r\n<p>在论文《Conditional Generative Adversarial Nets》中，条件GAN提供了一种思路。</p>\r\n<p>原始的GAN可以表达为<span class=\"math inline\">\\(\\min\\limits_G\\max\\limits_D V(D, G) = E_{x \\sim P_R}log(D(x)) + E_{z \\sim P_Z}log(1 - D(G(z)))\\)</span></p>\r\n<p>而在条件GAN中，变成了<span class=\"math inline\">\\(\\min\\limits_G\\max\\limits_D V(D, G) = E_{x \\sim P_R}log(D(x|y)) + E_{z \\sim P_Z}log(1 - D(G(z, y) | y))\\)</span></p>\r\n<p>在CGAN论文中，使用CGAN来生成手写数字，其生成器输入包括100维的随机隐变量以及10维的one-hot编码类别向量，判别器输入包括784维的图像以及10维的one-hot类别向量。</p>\r\n<h1 id=\"深度卷积生成对抗网络dcgandeep-convolutional-generative-adversarial-networks\">深度卷积生成对抗网络（DCGAN，Deep Convolutional Generative Adversarial Networks）</h1>\r\n<p>在论文《Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks》中第一次采用深度卷积神经网络来代替GAN中的生成器和判别器（以前都是使用MLP），对于生成器，可以使用上采样加卷积的方式来生成最终的图像，而对于判别器，类似于普通的分类模型，只不过在下采样的时候不适用池化操作，而是使用步长大于1的卷积操作，另外不论是判别器还是生成器，除了输入输出层，其他层都使用Batch Normalization层来稳定梯度，加速模型的收敛。</p>\r\n<h1 id=\"wgan\">WGAN</h1>\r\n<p>GAN的思路看起来很完美，但是原始的GAN loss存在一些问题，这些问题在后面的论文《Wassertein GAN》中提到并解决，这一部分内容可以参考知乎上的一篇文章分析：<a href=\"https://zhuanlan.zhihu.com/p/25071913\">令人拍案叫绝的Wasserstein GAN</a>，或者直接看原论文。</p>\r\n<p>总的来说，分析可以发现原GAN的论文中生成器模型的损失函数其实就是在最小化分布<span class=\"math inline\">\\(P_G\\)</span>和<span class=\"math inline\">\\(P_R\\)</span>之间的KL散度，而KL散度存在的一个问题是：当分布<span class=\"math inline\">\\(P_G\\)</span>和分布<span class=\"math inline\">\\(P_R\\)</span>没有交集时，KL散度始终为<span class=\"math inline\">\\(log2\\)</span>，不能提供有效的梯度，从而导致GAN训练困难，而WGAN利用Wasserstein距离来代替KL散度以度量两个分布<span class=\"math inline\">\\(P_G\\)</span>、<span class=\"math inline\">\\(P_R\\)</span>之间的距离。</p>\r\n<p>Wasserstein距离也称作Earch-Mover(EM)距离，可以写为 <span class=\"math display\">\\[\r\nW(P_R, P_G) = \\inf\\limits_{\\gamma \\sim \\prod(P_R, P_G)} E_{\\{x,y\\}\\sim \\gamma}||x - y||\r\n\\]</span> 这里<span class=\"math inline\">\\(\\prod(P_R, P_G)\\)</span>表示<span class=\"math inline\">\\(P_G\\)</span>、<span class=\"math inline\">\\(P_R\\)</span>的联合分布，对于所有的联合分布，求<span class=\"math inline\">\\(\\{x,y\\}\\)</span>的距离期望的最小值，就是Wasserstein距离，这里可以仔细理解下为什么Wasserstein距离不存在KL散度的那种无法适用于无交集的两个分布的问题。</p>\r\n<p>这里有个问题是Wassertein距离在模型中是无法计算的，因此WGAN论文中用一些近似手段，将Wassertein距离近似表示为如下计算方式。 <span class=\"math display\">\\[\r\nK \\times W(P_R, P_G) \\approx \\max\\limits_{|f_w|_L \\le K} E_{x \\sim P_R}f_w(x) - E_{x \\sim P_G}f_w(x)\r\n\\]</span> 这里<span class=\"math inline\">\\(f_w\\)</span>表示一个神经网络或者CNN所构成的判别器所表示的函数，<span class=\"math inline\">\\(|f_w|_L\\)</span>表示这个函数的Lipschitz常数（如果<span class=\"math inline\">\\(\\exists K, \\forall x_1, x_2 \\ |f(x_1) - f(x_2)| \\le K|x_1 - x_2|\\)</span>那么<span class=\"math inline\">\\(K\\)</span>就是函数<span class=\"math inline\">\\(f\\)</span>的Lipschitz常数）。</p>\r\n<p>因此我们将GAN的判别器损失改为<span class=\"math inline\">\\(-E_{x \\sim P_R}f_w(x) + E_{x \\sim P_G}f_w(x)\\)</span>，这样一来，判别器的目标不再是一个分类问题，而是求<span class=\"math inline\">\\(\\max\\limits_{|f_w|_L \\le K} E_{x \\sim P_R}f_w(x) - E_{x \\sim P_G}f_w(x)\\)</span>以近似Wasserstein距离，因此需要去掉判别器中的sigmoid层，同时为了保证<span class=\"math inline\">\\(|f_w|_L \\le K\\)</span>，这里需要将判别器模型中的参数值限定在一定范围内，这可以通过参数的clip操作来实现。</p>\r\n<p>判别器训练好之后，训练生成器时，损失函数使用<span class=\"math inline\">\\(-E_{x \\sim P_G}f_w(x)\\)</span>（这个意思是将近似的Wasserstein距离作为损失函数来优化，缩小Wasserstein距离）即可。</p>\r\n<p>和之前的GAN loss相比的话，WGAN相当于仅仅做了三个修改： - 去掉判别器损失和生成器损失中的log - 去掉判别器的sigmoid - 判别器的参数每次更新之后需要进行clip，以保证参数在一定范围内。</p>\r\n<p>另外一个trick是：GAN的训练不适合使用Adam这类基于动量的算法，因为每次生成器的更新后，判别器的loss梯度非常不稳定，甚至和之前的方向完全相反，基于动量优化容易导致收敛缓慢。</p>\r\n<h1 id=\"gan的应用方向以及发展\">GAN的应用方向以及发展</h1>\r\n<p>上面介绍了GAN的几个主要的基础发展方向，接下来看看GAN应用领域方面的具体问题。</p>\r\n<h2 id=\"图像生成\">图像生成</h2>\r\n<p>GAN的老本行就是生成数据，图像数据自然包括在内，但是早期的GAN生成的图像清晰度低且内容混乱，近年来在高清图像生成方面有了一些进展。</p>\r\n<p>未完待续...</p>\r\n<h2 id=\"图像风格迁移\">图像风格迁移</h2>\r\n<p>下面是在论文阅读过程中遇到的一些基础概念。 ### Gram矩阵 Gram矩阵可以看做是feature之间的偏心协方差矩阵，例如对于一个<span class=\"math inline\">\\(C \\times H \\times W\\)</span>的特征图，首先将特征图进行resize得到特征矩阵<span class=\"math inline\">\\(M \\in R^{C \\times HW}\\)</span>，然后计算Gram矩阵为<span class=\"math inline\">\\(M \\times M^T\\)</span>，Gram矩阵对角线上的元素表示不同特征在图像上出现的强度，非对角线上的元素则表示不同特征之间的相关性，因此Gram矩阵可用于表示图像的整体风格。 ### 双边滤波（BF，Bilateral Filter） 首先，一般的高斯滤波器可以表示如下： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    J_p = \\frac{1}{K_p} \\sum\\limits_{q \\in \\Omega_p} I_q \\times f(||p-q||)\r\n\\end{aligned}\r\n\\]</span> 其中<span class=\"math inline\">\\(J\\)</span>表示输出图像，<span class=\"math inline\">\\(I\\)</span>表示输入图像，<span class=\"math inline\">\\(p,q\\)</span>表示位置，<span class=\"math inline\">\\(\\Omega_p\\)</span>表示<span class=\"math inline\">\\(p\\)</span>的一个邻域，<span class=\"math inline\">\\(f\\)</span>表示高斯核函数，<span class=\"math inline\">\\(K_p\\)</span>表示该位置的归一化因子，这样的滤波考虑到了像素之间的相对位置关系，虽然可以有效的去躁，但是对于一些图像边缘非常不友好，容易将边缘模糊化。</p>\r\n<p>基于上面的问题，双边滤波考虑再引入像素之间的像素值关系，将滤波过程表达为如下： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    J_p = \\frac{1}{K_p} \\sum\\limits_{q \\in \\Omega_p} I_q \\times f(||p-q||) \\times g(|I_p-I_q|)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>这里的<span class=\"math inline\">\\(g\\)</span>同样为一个高斯核函数，<span class=\"math inline\">\\(||I_p - I_q||\\)</span>表达的是两个位置之间的像素值差异，双边的意思即同时考虑相对位置信息和像素值信息，用这种方式进行滤波，对于边缘比较友好。</p>\r\n<h3 id=\"双边网格bilateral-grid\">双边网格（Bilateral Grid）</h3>\r\n<p>双边滤波其实运行起来很慢，需要对其进行加速，因此就有了双边网格的概念，一个灰度图像<span class=\"math inline\">\\(I\\)</span>大小为<span class=\"math inline\">\\(H \\times W\\)</span>，其实可以表示为<span class=\"math inline\">\\(H \\times W \\times B\\)</span>大小的一个三维格式，这个三维格式就是双边网格，其中最后一维表示灰度值，图像<span class=\"math inline\">\\(I\\)</span>上的一个位置<span class=\"math inline\">\\((x, y)\\)</span>的点，其像素值为<span class=\"math inline\">\\(I_{xy}\\)</span>，那么在双边网格中，这个点的位置就变成了<span class=\"math inline\">\\((y, x, I_{xy})\\)</span>，另外如果是一个uint8类型的灰度图，那么似乎<span class=\"math inline\">\\(B\\)</span>必须为256，但是这里其实可以做一定的区间划分以压缩<span class=\"math inline\">\\(B\\)</span>的大小，例如划分为10个区间，那么<span class=\"math inline\">\\(B\\)</span>的大小就只需要10了。同理，对<span class=\"math inline\">\\(H\\)</span>和<span class=\"math inline\">\\(W\\)</span>两个维度也可以进行压缩，从而将一个二维图像表示为双边空间（可以理解为双边网格对应的大小为<span class=\"math inline\">\\(H \\times W \\times B\\)</span>的空间）中的点的集合，这个过程叫做splat，在双边空间中做完双边滤波之后，再通过插值（一般使用三线性插值）的方式，从双边空间中恢复原始的大图像，这个过程称为slice。</p>\r\n<p>slice的具体操作我没有找到说明，按照我的理解，大概是对于原始图像<span class=\"math inline\">\\(I\\)</span>上的一个点<span class=\"math inline\">\\((x,y)\\)</span>，那么可以通过对双边空间中<span class=\"math inline\">\\((x, y, I_{xy})\\)</span>这个坐标进行三线性插值，得到输出图像上<span class=\"math inline\">\\((x,y)\\)</span>位置的值。</p>\r\n<h3 id=\"联合双边滤波jbfjoint-bilateral-filter以及联合双边上采样jbujoint-bilateral-upsampling\">联合双边滤波（JBF，Joint Bilateral Filter）以及联合双边上采样（JBU，Joint Bilateral Upsampling）</h3>\r\n<p>在双边滤波的基础上，引入一张其他图像来引导滤波过程，可以写作下式：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    J_p = \\frac{1}{K_p} \\sum\\limits_{q \\in \\Omega_p} I_q \\times f(||p-q||) \\times g(|\\hat{I}_p-\\hat{I}_q|)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>其中<span class=\"math inline\">\\(\\hat{I}\\)</span>表示一张其他图像。</p>\r\n<p>联合双边上采样使用的就是联合双边滤波的思路，假设现在有一张很大的图像<span class=\"math inline\">\\(I\\)</span>，对其进行某种处理非常耗时间，如果想要加速这个处理过程，那么可以考虑先将图像缩小得到图像<span class=\"math inline\">\\(S\\)</span>，然后在缩小的图像上进行处理,得到图像<span class=\"math inline\">\\(\\hat{S}\\)</span>，最后将缩小的图像resize回原大小得到图像<span class=\"math inline\">\\(\\hat{I}\\)</span>。</p>\r\n<p>上面的思路很简单，但是这里有个主要的问题是如果使用传统的插值方式（例如双线性插值、最近邻插值等），<span class=\"math inline\">\\(\\hat{S}\\)</span>直接上采样得到的<span class=\"math inline\">\\(\\hat{I}\\)</span>往往非常不清晰，但这里不是一个传统的resize问题，因为这里还有图像<span class=\"math inline\">\\(I\\)</span>可以用于参考，因此就可以考虑在插值之后使用联合双边滤波对插值之后的图像进行进一步处理，提高清晰度，首先<span class=\"math inline\">\\(\\hat{S}\\)</span>直接上采样(一般使用最近邻插值就可以)得到<span class=\"math inline\">\\(U(\\hat{S})\\)</span>，然后用原始图像<span class=\"math inline\">\\(I\\)</span>引导进行联合双边滤波，得到最终的<span class=\"math inline\">\\(\\hat{I}\\)</span>，如下所示：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\hat{I}_p = \\frac{1}{K_p} \\sum\\limits_{q \\in \\Omega_p} U(\\hat{S})_q \\times f(||p-q||) \\times g(|I_p-I_q|)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>下面记录一下看过的一些风格转换相关的论文</p>\r\n<h3 id=\"论文arbitrary-style-transfer-in-real-time-with-adaptive-instance-normalization\">论文《Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization》</h3>\r\n<p>这篇论文中构建了一个端到端的进行风格转换的模型，模型在预测时可以选择任意的style图片，而不是固定为训练过程中使用的style图片。其主要思想是认为style在CNN特征图中表达为特征的方差和均值，因此这个论文中使用一个Adaptive Instance Normalization（AdaIN）模块，通过计算style图片的特征图的Instance Normalization均值和方差，然后将content图片的特征图均值和方差按照style图片的均值和方差进行缩放，然后将该特征图进行解码，得到风格转换之后的图片。</p>\r\n<p>该方案的整体结构图如下所示，content图片和style图片都首先经过一个固定的预训练VGG-19模型编码，得到两个特征图，两个特征图输入到AdaIN模块中，用于根据style图片的特征图调整content图片的特征图的特征均值和方差，调整后的content特征图再通过一个解码器得到风格转换之后的输出，最后将输出图片再通过同样的VGG-19编码器进行编码，使用编码之后的输出计算content损失和style损失。</p>\r\n<figure>\r\n<img src=\"AdaIN模型结构示意图.png\" alt=\"AdaIN模型结构示意图\" /><figcaption aria-hidden=\"true\">AdaIN模型结构示意图</figcaption>\r\n</figure>\r\n<p>其损失函数设计如下，其中<span class=\"math inline\">\\(L\\)</span>是总的损失，由两部分组成，一个是content损失<span class=\"math inline\">\\(L_c\\)</span>，一个是style损失<span class=\"math inline\">\\(L_s\\)</span>，<span class=\"math inline\">\\(L_c\\)</span>是由模型输出<span class=\"math inline\">\\(f(g(t))\\)</span>和经过了AdaIN的特征图<span class=\"math inline\">\\(t\\)</span>计算，<span class=\"math inline\">\\(f\\)</span>表示编码器，<span class=\"math inline\">\\(g\\)</span>表示解码器。<span class=\"math inline\">\\(L_s\\)</span>是由编码器VGG的不同层特征<span class=\"math inline\">\\(\\phi_i\\)</span>计算，这里只监督不同层特征的统计量，例如<span class=\"math inline\">\\(\\mu\\)</span>表示特征的均值，<span class=\"math inline\">\\(\\sigma\\)</span>表示特征的标准差。</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    L &amp;= L_c + \\lambda L_s\\\\\r\n    L_c &amp;= ||f(g(t)) - t||_2\\\\\r\n    L_s &amp;= \\sum\\limits_{i=1}^L ||\\mu(\\phi_i(g(t))) - \\mu(\\phi_i(s))||_2 + \\sum\\limits_{i=1}^L ||\\sigma(\\phi_i(g(t))) - \\sigma(\\phi_i(s))||_2\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>该文章实现了一个端到端的，可以使用任意style图片的风格转换任务，但是其缺点在于容易造成content的改变，我认为是因为这里没有一个独立的content编码分支造成的，或许可以从这里入手做一些改变，但是另一篇论文从双边联合滤波的角度给出了一种不同的方案，如下。</p>\r\n<h3 id=\"论文joint-bilateral-learning-for-real-time-universal-photorealistic-style-transfer\">论文《Joint Bilateral Learning for Real-time Universal Photorealistic Style Transfer》</h3>\r\n<p>该论文尝试将风格转换问题设计为一个图像局部transform的问题，让模型去在低分辨率图像上去学习出一个transform系数，然后将transform系数应用于高分辨率图像以完成快速的高分辨率图像的风格转换处理工作，很大程度上借鉴了HDRnet，关于HDRnet的简单介绍可以参考我写的另一篇论文阅读记录<a href=\"#\">Post not found: 论文阅读《Deep-Bilateral-Learning-for-Real-Time-Image-Enhancement》 论文阅读《Deep-Bilateral-Learning-for-Real-Time-Image-Enhancement》</a>。</p>\r\n<p>论文中整体的模型结构如下图所示。</p>\r\n<figure>\r\n<img src=\"基于联合双边学习的图像风格转换模型结构示意图.png\" alt=\"基于联合双边学习的图像风格转换模型结构示意图\" /><figcaption aria-hidden=\"true\">基于联合双边学习的图像风格转换模型结构示意图</figcaption>\r\n</figure>\r\n<p>首先将低分辨率的content图片和低分辨率的style图片用VGG-19进行encoding，上面的top path部分主要是将VGG-19中conv2_1、conv3_1、conv4_1对应的特征图拿出来分别经过AdaIN层，以根据style图特征调整content图的特征，最终得到调整style后的不同分辨率的特征，下面的一个分支主要有三个splatting block组成，其结构可以参考图片右下角，主要是在学习Bilateral grid的splat操作，同时考虑到了style信息，因此使用了AdaIN层来对不同阶段的特征进行调整，因此这个叫做Style-base splatting，最终的特征还是和HDRnet一样，分成局部特征的学习和全局特征的学习，最后将局部特征和全局特征混合为双边网格<span class=\"math inline\">\\(\\Gamma\\)</span>，作为即将对原图进行局部变换的系数，最后将双边网格以原分辨率图为guide图像进行slice操作插值到原分辨率大小的变换系数图，然后apply到原分辨图上，得到风格转换之后的输出图。</p>\r\n<p>该论文的损失函数设计如下，这里的<span class=\"math inline\">\\(L_c\\)</span>和<span class=\"math inline\">\\(L_sa\\)</span>其实和论文《Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization》中的<span class=\"math inline\">\\(L_c\\)</span>和<span class=\"math inline\">\\(L_s\\)</span>类似，这里的<span class=\"math inline\">\\(F_i\\)</span>表示VGG的中间层特征输出，<span class=\"math inline\">\\(N_c\\)</span>表示取的中间层的个数，<span class=\"math inline\">\\(I_c\\)</span>表示低分辨率的content图片，<span class=\"math inline\">\\(I_s\\)</span>表示低分辨率的style图片，<span class=\"math inline\">\\(\\mu\\)</span>和<span class=\"math inline\">\\(\\sigma\\)</span>分别表示统计均值和标准差。这里新增了一个<span class=\"math inline\">\\(L_r\\)</span>损失，其中<span class=\"math inline\">\\(s\\)</span>表示双边网格上的一个位置，<span class=\"math inline\">\\(N(s)\\)</span>表示双边网格的<span class=\"math inline\">\\(s\\)</span>位置的邻域（论文里用的是6邻域），这一项损失主要是希望相邻的位置的仿射变换差别不大，使得其变换更加平滑。另外对于三个损失的系数，论文中使用的是<span class=\"math inline\">\\(\\lambda_c= 0.5，\\lambda_sa =1，\\lambda_r = 0.15\\)</span></p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    L &amp;= \\lambda_c L_c + \\lambda_sa L_{sa} + \\lambda_r L_r\\\\\r\n    L_c &amp;= \\sum\\limits_{i=1}^{N_c}||F_i[O] - F_i[I_c]||^2_2\\\\\r\n    L_{sa} &amp;= \\sum\\limits_{i=1}^{N_s} ||\\mu(F_i[O]) - \\mu(F_i[I_s])||^2_2 + \\sum\\limits_{i=1}^{N_s} ||\\sigma(F_i[O]) - \\sigma(F_i[I_s])||^2_2\\\\\r\n    L_r &amp;= \\sum\\limits_s \\sum\\limits_{t \\in N(s)} ||\\Gamma[s] - \\Gamma[t]||^2_2\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>这个方法在style转换任务中主要的好处是速度快，而且很大程度上保留图片的内容信息，因为其transform过程仅仅针对原始图片上单像素的颜色变换，但是也正是这个原因，这个transform也存在和HDRnet一样的限制，例如在艺术风格的转换任务上（这个不是简单的颜色变换可以完成的）上效果就不是那么明显了，不过对于一般的颜色风格转换任务，这个方法还算是有了较好的效果和速度。</p>\r\n<h2 id=\"超分辨率\">超分辨率</h2>\r\n<p>未完待续...</p>\r\n","site":{"data":{}},"excerpt":"","more":"<p>GAN在深度学习领域是生成对抗网络（Generative Adversarial Network）的简称，第一次是在论文《Generative Adversarial Nets》中提出，论文Abstract中第一句话就点出了GAN的本质“a new framework for estimating grnerative models via an adversarial process”，即GAN是通过一种对抗过程来训练（估计）生成模型的一种框架。</p>\r\n<h1 id=\"gan的基础结构\">GAN的基础结构</h1>\r\n<p>GAN的简陋示意图如下所示。</p>\r\n<figure>\r\n<img src=\"GAN模型结构.png\" alt=\"GAN结构示意\" /><figcaption aria-hidden=\"true\">GAN结构示意</figcaption>\r\n</figure>\r\n<p>其中<span class=\"math inline\">\\(D\\)</span>表示判别器模型，<span class=\"math inline\">\\(G\\)</span>表示生成器模型，<span class=\"math inline\">\\(G\\)</span>从一个隐空间中随机采样得到隐变量<span class=\"math inline\">\\(z\\)</span>，将其生成为<span class=\"math inline\">\\(G(z)\\)</span>，用于模拟真实数据<span class=\"math inline\">\\(x\\)</span>，判别器<span class=\"math inline\">\\(D\\)</span>则接收一个输入，这里不是同时接收<span class=\"math inline\">\\(x\\)</span>和<span class=\"math inline\">\\(G(z)\\)</span>，判别器本质上是个分类模型，他不知道自己接收到的数据是生成器生成的<span class=\"math inline\">\\(G(z)\\)</span>还是真实数据<span class=\"math inline\">\\(x\\)</span>，判别器的工作就是将<span class=\"math inline\">\\(G(z)\\)</span>和<span class=\"math inline\">\\(x\\)</span>分成两类，即判别数据的真假。在GAN的架构中，生成器<span class=\"math inline\">\\(G\\)</span>努力构造虚假数据，其最终目的是让判别器无法判别<span class=\"math inline\">\\(G(z)\\)</span>和<span class=\"math inline\">\\(x\\)</span>的差别，而判别器<span class=\"math inline\">\\(D\\)</span>的目的则努力将<span class=\"math inline\">\\(G(z)\\)</span>找出来，判别器和生成器构成了一种对抗的关系，在训练过程中，不断提高两种模型的效果，最终就可以得到效果以假乱真的生成器<span class=\"math inline\">\\(G\\)</span>，这样的生成器就可以用于数据（图像、音频等）的生成。</p>\r\n<h1 id=\"gan的损失函数\">GAN的损失函数</h1>\r\n<p>在原始论文《Generative Adversarial Nets》中，GAN中的判别器和生成器的损失函数分别定义如下，其中<span class=\"math inline\">\\(P_R\\)</span>代表数据的真实分布，<span class=\"math inline\">\\(P_G\\)</span>代表生成器所生成的数据分布，从损失函数中就可以看出两个模型的对抗关系。</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    L_D &amp;= \\frac{1}{m} \\sum\\limits_{i=1}^m[log(D(x^{(i)})) + log(1 - D(G(z^{(i)})))]\\\\\r\n    &amp;= E_{x \\sim P_R}log(D(x)) + E_{x \\sim P_G}log(1 - D(x))\\\\\r\n    L_G &amp;= \\frac{1}{m} \\sum\\limits_{i=1}^m log(D(G(z^{(i)})))\\\\\r\n    &amp;= E_{x \\sim P_G} log(D(x))\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>在生成器<span class=\"math inline\">\\(G\\)</span>固定的时候，最优的判别器可以写为<span class=\"math inline\">\\(D^\\star(x) = \\frac{P_R(x)}{P_R(x) + P_G(x)}\\)</span>，这里<span class=\"math inline\">\\(P_R(x)，P_G(x)\\)</span>分别表示数据来源于真实数据和生成数据的概率，当分布<span class=\"math inline\">\\(P_R\\)</span>和<span class=\"math inline\">\\(P_G\\)</span>很近或者直接重叠的时候，就代表生成器达到最优，即生成数据的分布和真实数据分布相同了。</p>\r\n<h1 id=\"条件生成对抗网络cgan-conditional-generative-adversarial-network\">条件生成对抗网络（CGAN， Conditional Generative Adversarial Network）</h1>\r\n<p>GAN可以生成样本，那么是否可以指定生成的样本是什么，例如能够指定生成一个包含猫的图片？</p>\r\n<p>在论文《Conditional Generative Adversarial Nets》中，条件GAN提供了一种思路。</p>\r\n<p>原始的GAN可以表达为<span class=\"math inline\">\\(\\min\\limits_G\\max\\limits_D V(D, G) = E_{x \\sim P_R}log(D(x)) + E_{z \\sim P_Z}log(1 - D(G(z)))\\)</span></p>\r\n<p>而在条件GAN中，变成了<span class=\"math inline\">\\(\\min\\limits_G\\max\\limits_D V(D, G) = E_{x \\sim P_R}log(D(x|y)) + E_{z \\sim P_Z}log(1 - D(G(z, y) | y))\\)</span></p>\r\n<p>在CGAN论文中，使用CGAN来生成手写数字，其生成器输入包括100维的随机隐变量以及10维的one-hot编码类别向量，判别器输入包括784维的图像以及10维的one-hot类别向量。</p>\r\n<h1 id=\"深度卷积生成对抗网络dcgandeep-convolutional-generative-adversarial-networks\">深度卷积生成对抗网络（DCGAN，Deep Convolutional Generative Adversarial Networks）</h1>\r\n<p>在论文《Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks》中第一次采用深度卷积神经网络来代替GAN中的生成器和判别器（以前都是使用MLP），对于生成器，可以使用上采样加卷积的方式来生成最终的图像，而对于判别器，类似于普通的分类模型，只不过在下采样的时候不适用池化操作，而是使用步长大于1的卷积操作，另外不论是判别器还是生成器，除了输入输出层，其他层都使用Batch Normalization层来稳定梯度，加速模型的收敛。</p>\r\n<h1 id=\"wgan\">WGAN</h1>\r\n<p>GAN的思路看起来很完美，但是原始的GAN loss存在一些问题，这些问题在后面的论文《Wassertein GAN》中提到并解决，这一部分内容可以参考知乎上的一篇文章分析：<a href=\"https://zhuanlan.zhihu.com/p/25071913\">令人拍案叫绝的Wasserstein GAN</a>，或者直接看原论文。</p>\r\n<p>总的来说，分析可以发现原GAN的论文中生成器模型的损失函数其实就是在最小化分布<span class=\"math inline\">\\(P_G\\)</span>和<span class=\"math inline\">\\(P_R\\)</span>之间的KL散度，而KL散度存在的一个问题是：当分布<span class=\"math inline\">\\(P_G\\)</span>和分布<span class=\"math inline\">\\(P_R\\)</span>没有交集时，KL散度始终为<span class=\"math inline\">\\(log2\\)</span>，不能提供有效的梯度，从而导致GAN训练困难，而WGAN利用Wasserstein距离来代替KL散度以度量两个分布<span class=\"math inline\">\\(P_G\\)</span>、<span class=\"math inline\">\\(P_R\\)</span>之间的距离。</p>\r\n<p>Wasserstein距离也称作Earch-Mover(EM)距离，可以写为 <span class=\"math display\">\\[\r\nW(P_R, P_G) = \\inf\\limits_{\\gamma \\sim \\prod(P_R, P_G)} E_{\\{x,y\\}\\sim \\gamma}||x - y||\r\n\\]</span> 这里<span class=\"math inline\">\\(\\prod(P_R, P_G)\\)</span>表示<span class=\"math inline\">\\(P_G\\)</span>、<span class=\"math inline\">\\(P_R\\)</span>的联合分布，对于所有的联合分布，求<span class=\"math inline\">\\(\\{x,y\\}\\)</span>的距离期望的最小值，就是Wasserstein距离，这里可以仔细理解下为什么Wasserstein距离不存在KL散度的那种无法适用于无交集的两个分布的问题。</p>\r\n<p>这里有个问题是Wassertein距离在模型中是无法计算的，因此WGAN论文中用一些近似手段，将Wassertein距离近似表示为如下计算方式。 <span class=\"math display\">\\[\r\nK \\times W(P_R, P_G) \\approx \\max\\limits_{|f_w|_L \\le K} E_{x \\sim P_R}f_w(x) - E_{x \\sim P_G}f_w(x)\r\n\\]</span> 这里<span class=\"math inline\">\\(f_w\\)</span>表示一个神经网络或者CNN所构成的判别器所表示的函数，<span class=\"math inline\">\\(|f_w|_L\\)</span>表示这个函数的Lipschitz常数（如果<span class=\"math inline\">\\(\\exists K, \\forall x_1, x_2 \\ |f(x_1) - f(x_2)| \\le K|x_1 - x_2|\\)</span>那么<span class=\"math inline\">\\(K\\)</span>就是函数<span class=\"math inline\">\\(f\\)</span>的Lipschitz常数）。</p>\r\n<p>因此我们将GAN的判别器损失改为<span class=\"math inline\">\\(-E_{x \\sim P_R}f_w(x) + E_{x \\sim P_G}f_w(x)\\)</span>，这样一来，判别器的目标不再是一个分类问题，而是求<span class=\"math inline\">\\(\\max\\limits_{|f_w|_L \\le K} E_{x \\sim P_R}f_w(x) - E_{x \\sim P_G}f_w(x)\\)</span>以近似Wasserstein距离，因此需要去掉判别器中的sigmoid层，同时为了保证<span class=\"math inline\">\\(|f_w|_L \\le K\\)</span>，这里需要将判别器模型中的参数值限定在一定范围内，这可以通过参数的clip操作来实现。</p>\r\n<p>判别器训练好之后，训练生成器时，损失函数使用<span class=\"math inline\">\\(-E_{x \\sim P_G}f_w(x)\\)</span>（这个意思是将近似的Wasserstein距离作为损失函数来优化，缩小Wasserstein距离）即可。</p>\r\n<p>和之前的GAN loss相比的话，WGAN相当于仅仅做了三个修改： - 去掉判别器损失和生成器损失中的log - 去掉判别器的sigmoid - 判别器的参数每次更新之后需要进行clip，以保证参数在一定范围内。</p>\r\n<p>另外一个trick是：GAN的训练不适合使用Adam这类基于动量的算法，因为每次生成器的更新后，判别器的loss梯度非常不稳定，甚至和之前的方向完全相反，基于动量优化容易导致收敛缓慢。</p>\r\n<h1 id=\"gan的应用方向以及发展\">GAN的应用方向以及发展</h1>\r\n<p>上面介绍了GAN的几个主要的基础发展方向，接下来看看GAN应用领域方面的具体问题。</p>\r\n<h2 id=\"图像生成\">图像生成</h2>\r\n<p>GAN的老本行就是生成数据，图像数据自然包括在内，但是早期的GAN生成的图像清晰度低且内容混乱，近年来在高清图像生成方面有了一些进展。</p>\r\n<p>未完待续...</p>\r\n<h2 id=\"图像风格迁移\">图像风格迁移</h2>\r\n<p>下面是在论文阅读过程中遇到的一些基础概念。 ### Gram矩阵 Gram矩阵可以看做是feature之间的偏心协方差矩阵，例如对于一个<span class=\"math inline\">\\(C \\times H \\times W\\)</span>的特征图，首先将特征图进行resize得到特征矩阵<span class=\"math inline\">\\(M \\in R^{C \\times HW}\\)</span>，然后计算Gram矩阵为<span class=\"math inline\">\\(M \\times M^T\\)</span>，Gram矩阵对角线上的元素表示不同特征在图像上出现的强度，非对角线上的元素则表示不同特征之间的相关性，因此Gram矩阵可用于表示图像的整体风格。 ### 双边滤波（BF，Bilateral Filter） 首先，一般的高斯滤波器可以表示如下： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    J_p = \\frac{1}{K_p} \\sum\\limits_{q \\in \\Omega_p} I_q \\times f(||p-q||)\r\n\\end{aligned}\r\n\\]</span> 其中<span class=\"math inline\">\\(J\\)</span>表示输出图像，<span class=\"math inline\">\\(I\\)</span>表示输入图像，<span class=\"math inline\">\\(p,q\\)</span>表示位置，<span class=\"math inline\">\\(\\Omega_p\\)</span>表示<span class=\"math inline\">\\(p\\)</span>的一个邻域，<span class=\"math inline\">\\(f\\)</span>表示高斯核函数，<span class=\"math inline\">\\(K_p\\)</span>表示该位置的归一化因子，这样的滤波考虑到了像素之间的相对位置关系，虽然可以有效的去躁，但是对于一些图像边缘非常不友好，容易将边缘模糊化。</p>\r\n<p>基于上面的问题，双边滤波考虑再引入像素之间的像素值关系，将滤波过程表达为如下： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    J_p = \\frac{1}{K_p} \\sum\\limits_{q \\in \\Omega_p} I_q \\times f(||p-q||) \\times g(|I_p-I_q|)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>这里的<span class=\"math inline\">\\(g\\)</span>同样为一个高斯核函数，<span class=\"math inline\">\\(||I_p - I_q||\\)</span>表达的是两个位置之间的像素值差异，双边的意思即同时考虑相对位置信息和像素值信息，用这种方式进行滤波，对于边缘比较友好。</p>\r\n<h3 id=\"双边网格bilateral-grid\">双边网格（Bilateral Grid）</h3>\r\n<p>双边滤波其实运行起来很慢，需要对其进行加速，因此就有了双边网格的概念，一个灰度图像<span class=\"math inline\">\\(I\\)</span>大小为<span class=\"math inline\">\\(H \\times W\\)</span>，其实可以表示为<span class=\"math inline\">\\(H \\times W \\times B\\)</span>大小的一个三维格式，这个三维格式就是双边网格，其中最后一维表示灰度值，图像<span class=\"math inline\">\\(I\\)</span>上的一个位置<span class=\"math inline\">\\((x, y)\\)</span>的点，其像素值为<span class=\"math inline\">\\(I_{xy}\\)</span>，那么在双边网格中，这个点的位置就变成了<span class=\"math inline\">\\((y, x, I_{xy})\\)</span>，另外如果是一个uint8类型的灰度图，那么似乎<span class=\"math inline\">\\(B\\)</span>必须为256，但是这里其实可以做一定的区间划分以压缩<span class=\"math inline\">\\(B\\)</span>的大小，例如划分为10个区间，那么<span class=\"math inline\">\\(B\\)</span>的大小就只需要10了。同理，对<span class=\"math inline\">\\(H\\)</span>和<span class=\"math inline\">\\(W\\)</span>两个维度也可以进行压缩，从而将一个二维图像表示为双边空间（可以理解为双边网格对应的大小为<span class=\"math inline\">\\(H \\times W \\times B\\)</span>的空间）中的点的集合，这个过程叫做splat，在双边空间中做完双边滤波之后，再通过插值（一般使用三线性插值）的方式，从双边空间中恢复原始的大图像，这个过程称为slice。</p>\r\n<p>slice的具体操作我没有找到说明，按照我的理解，大概是对于原始图像<span class=\"math inline\">\\(I\\)</span>上的一个点<span class=\"math inline\">\\((x,y)\\)</span>，那么可以通过对双边空间中<span class=\"math inline\">\\((x, y, I_{xy})\\)</span>这个坐标进行三线性插值，得到输出图像上<span class=\"math inline\">\\((x,y)\\)</span>位置的值。</p>\r\n<h3 id=\"联合双边滤波jbfjoint-bilateral-filter以及联合双边上采样jbujoint-bilateral-upsampling\">联合双边滤波（JBF，Joint Bilateral Filter）以及联合双边上采样（JBU，Joint Bilateral Upsampling）</h3>\r\n<p>在双边滤波的基础上，引入一张其他图像来引导滤波过程，可以写作下式：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    J_p = \\frac{1}{K_p} \\sum\\limits_{q \\in \\Omega_p} I_q \\times f(||p-q||) \\times g(|\\hat{I}_p-\\hat{I}_q|)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>其中<span class=\"math inline\">\\(\\hat{I}\\)</span>表示一张其他图像。</p>\r\n<p>联合双边上采样使用的就是联合双边滤波的思路，假设现在有一张很大的图像<span class=\"math inline\">\\(I\\)</span>，对其进行某种处理非常耗时间，如果想要加速这个处理过程，那么可以考虑先将图像缩小得到图像<span class=\"math inline\">\\(S\\)</span>，然后在缩小的图像上进行处理,得到图像<span class=\"math inline\">\\(\\hat{S}\\)</span>，最后将缩小的图像resize回原大小得到图像<span class=\"math inline\">\\(\\hat{I}\\)</span>。</p>\r\n<p>上面的思路很简单，但是这里有个主要的问题是如果使用传统的插值方式（例如双线性插值、最近邻插值等），<span class=\"math inline\">\\(\\hat{S}\\)</span>直接上采样得到的<span class=\"math inline\">\\(\\hat{I}\\)</span>往往非常不清晰，但这里不是一个传统的resize问题，因为这里还有图像<span class=\"math inline\">\\(I\\)</span>可以用于参考，因此就可以考虑在插值之后使用联合双边滤波对插值之后的图像进行进一步处理，提高清晰度，首先<span class=\"math inline\">\\(\\hat{S}\\)</span>直接上采样(一般使用最近邻插值就可以)得到<span class=\"math inline\">\\(U(\\hat{S})\\)</span>，然后用原始图像<span class=\"math inline\">\\(I\\)</span>引导进行联合双边滤波，得到最终的<span class=\"math inline\">\\(\\hat{I}\\)</span>，如下所示：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\hat{I}_p = \\frac{1}{K_p} \\sum\\limits_{q \\in \\Omega_p} U(\\hat{S})_q \\times f(||p-q||) \\times g(|I_p-I_q|)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>下面记录一下看过的一些风格转换相关的论文</p>\r\n<h3 id=\"论文arbitrary-style-transfer-in-real-time-with-adaptive-instance-normalization\">论文《Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization》</h3>\r\n<p>这篇论文中构建了一个端到端的进行风格转换的模型，模型在预测时可以选择任意的style图片，而不是固定为训练过程中使用的style图片。其主要思想是认为style在CNN特征图中表达为特征的方差和均值，因此这个论文中使用一个Adaptive Instance Normalization（AdaIN）模块，通过计算style图片的特征图的Instance Normalization均值和方差，然后将content图片的特征图均值和方差按照style图片的均值和方差进行缩放，然后将该特征图进行解码，得到风格转换之后的图片。</p>\r\n<p>该方案的整体结构图如下所示，content图片和style图片都首先经过一个固定的预训练VGG-19模型编码，得到两个特征图，两个特征图输入到AdaIN模块中，用于根据style图片的特征图调整content图片的特征图的特征均值和方差，调整后的content特征图再通过一个解码器得到风格转换之后的输出，最后将输出图片再通过同样的VGG-19编码器进行编码，使用编码之后的输出计算content损失和style损失。</p>\r\n<figure>\r\n<img src=\"AdaIN模型结构示意图.png\" alt=\"AdaIN模型结构示意图\" /><figcaption aria-hidden=\"true\">AdaIN模型结构示意图</figcaption>\r\n</figure>\r\n<p>其损失函数设计如下，其中<span class=\"math inline\">\\(L\\)</span>是总的损失，由两部分组成，一个是content损失<span class=\"math inline\">\\(L_c\\)</span>，一个是style损失<span class=\"math inline\">\\(L_s\\)</span>，<span class=\"math inline\">\\(L_c\\)</span>是由模型输出<span class=\"math inline\">\\(f(g(t))\\)</span>和经过了AdaIN的特征图<span class=\"math inline\">\\(t\\)</span>计算，<span class=\"math inline\">\\(f\\)</span>表示编码器，<span class=\"math inline\">\\(g\\)</span>表示解码器。<span class=\"math inline\">\\(L_s\\)</span>是由编码器VGG的不同层特征<span class=\"math inline\">\\(\\phi_i\\)</span>计算，这里只监督不同层特征的统计量，例如<span class=\"math inline\">\\(\\mu\\)</span>表示特征的均值，<span class=\"math inline\">\\(\\sigma\\)</span>表示特征的标准差。</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    L &amp;= L_c + \\lambda L_s\\\\\r\n    L_c &amp;= ||f(g(t)) - t||_2\\\\\r\n    L_s &amp;= \\sum\\limits_{i=1}^L ||\\mu(\\phi_i(g(t))) - \\mu(\\phi_i(s))||_2 + \\sum\\limits_{i=1}^L ||\\sigma(\\phi_i(g(t))) - \\sigma(\\phi_i(s))||_2\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>该文章实现了一个端到端的，可以使用任意style图片的风格转换任务，但是其缺点在于容易造成content的改变，我认为是因为这里没有一个独立的content编码分支造成的，或许可以从这里入手做一些改变，但是另一篇论文从双边联合滤波的角度给出了一种不同的方案，如下。</p>\r\n<h3 id=\"论文joint-bilateral-learning-for-real-time-universal-photorealistic-style-transfer\">论文《Joint Bilateral Learning for Real-time Universal Photorealistic Style Transfer》</h3>\r\n<p>该论文尝试将风格转换问题设计为一个图像局部transform的问题，让模型去在低分辨率图像上去学习出一个transform系数，然后将transform系数应用于高分辨率图像以完成快速的高分辨率图像的风格转换处理工作，很大程度上借鉴了HDRnet，关于HDRnet的简单介绍可以参考我写的另一篇论文阅读记录<a href=\"#\">Post not found: 论文阅读《Deep-Bilateral-Learning-for-Real-Time-Image-Enhancement》 论文阅读《Deep-Bilateral-Learning-for-Real-Time-Image-Enhancement》</a>。</p>\r\n<p>论文中整体的模型结构如下图所示。</p>\r\n<figure>\r\n<img src=\"基于联合双边学习的图像风格转换模型结构示意图.png\" alt=\"基于联合双边学习的图像风格转换模型结构示意图\" /><figcaption aria-hidden=\"true\">基于联合双边学习的图像风格转换模型结构示意图</figcaption>\r\n</figure>\r\n<p>首先将低分辨率的content图片和低分辨率的style图片用VGG-19进行encoding，上面的top path部分主要是将VGG-19中conv2_1、conv3_1、conv4_1对应的特征图拿出来分别经过AdaIN层，以根据style图特征调整content图的特征，最终得到调整style后的不同分辨率的特征，下面的一个分支主要有三个splatting block组成，其结构可以参考图片右下角，主要是在学习Bilateral grid的splat操作，同时考虑到了style信息，因此使用了AdaIN层来对不同阶段的特征进行调整，因此这个叫做Style-base splatting，最终的特征还是和HDRnet一样，分成局部特征的学习和全局特征的学习，最后将局部特征和全局特征混合为双边网格<span class=\"math inline\">\\(\\Gamma\\)</span>，作为即将对原图进行局部变换的系数，最后将双边网格以原分辨率图为guide图像进行slice操作插值到原分辨率大小的变换系数图，然后apply到原分辨图上，得到风格转换之后的输出图。</p>\r\n<p>该论文的损失函数设计如下，这里的<span class=\"math inline\">\\(L_c\\)</span>和<span class=\"math inline\">\\(L_sa\\)</span>其实和论文《Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization》中的<span class=\"math inline\">\\(L_c\\)</span>和<span class=\"math inline\">\\(L_s\\)</span>类似，这里的<span class=\"math inline\">\\(F_i\\)</span>表示VGG的中间层特征输出，<span class=\"math inline\">\\(N_c\\)</span>表示取的中间层的个数，<span class=\"math inline\">\\(I_c\\)</span>表示低分辨率的content图片，<span class=\"math inline\">\\(I_s\\)</span>表示低分辨率的style图片，<span class=\"math inline\">\\(\\mu\\)</span>和<span class=\"math inline\">\\(\\sigma\\)</span>分别表示统计均值和标准差。这里新增了一个<span class=\"math inline\">\\(L_r\\)</span>损失，其中<span class=\"math inline\">\\(s\\)</span>表示双边网格上的一个位置，<span class=\"math inline\">\\(N(s)\\)</span>表示双边网格的<span class=\"math inline\">\\(s\\)</span>位置的邻域（论文里用的是6邻域），这一项损失主要是希望相邻的位置的仿射变换差别不大，使得其变换更加平滑。另外对于三个损失的系数，论文中使用的是<span class=\"math inline\">\\(\\lambda_c= 0.5，\\lambda_sa =1，\\lambda_r = 0.15\\)</span></p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    L &amp;= \\lambda_c L_c + \\lambda_sa L_{sa} + \\lambda_r L_r\\\\\r\n    L_c &amp;= \\sum\\limits_{i=1}^{N_c}||F_i[O] - F_i[I_c]||^2_2\\\\\r\n    L_{sa} &amp;= \\sum\\limits_{i=1}^{N_s} ||\\mu(F_i[O]) - \\mu(F_i[I_s])||^2_2 + \\sum\\limits_{i=1}^{N_s} ||\\sigma(F_i[O]) - \\sigma(F_i[I_s])||^2_2\\\\\r\n    L_r &amp;= \\sum\\limits_s \\sum\\limits_{t \\in N(s)} ||\\Gamma[s] - \\Gamma[t]||^2_2\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>这个方法在style转换任务中主要的好处是速度快，而且很大程度上保留图片的内容信息，因为其transform过程仅仅针对原始图片上单像素的颜色变换，但是也正是这个原因，这个transform也存在和HDRnet一样的限制，例如在艺术风格的转换任务上（这个不是简单的颜色变换可以完成的）上效果就不是那么明显了，不过对于一般的颜色风格转换任务，这个方法还算是有了较好的效果和速度。</p>\r\n<h2 id=\"超分辨率\">超分辨率</h2>\r\n<p>未完待续...</p>\r\n"},{"title":"凸优化学习笔记","date":"2020-04-23T07:52:30.000Z","mathjax":true,"_content":"\n# 仿射集、凸集、锥集、凸锥集、仿射组合、凸组合、凸锥组合、仿射包、凸包、凸锥包\n- 锥：$C = \\{x| \\theta x \\in C\\}, x \\in R^n,\\theta \\ge 0$\n- 仿射组合：$\\theta_1+\\theta_2+...=1$\n- 凸组合：$\\theta_1+\\theta_2+...=1,\\theta_1+\\theta_2+... \\ge 0$\n- 凸锥组合：$\\theta_1+\\theta_2+... \\ge 0$\n\n# 一些凸集：超平面、半空间、球、椭球、多面体、单纯形、对称（半）（正定）矩阵\n- 超平面：$\\{x|w^Tx=b\\},x \\in R^n,w \\in R^n, b \\in R, w \\ne \\mathbb{0}$\n- 半空间：$\\{x|w^Tx \\ge b\\},x \\in R^n,w \\in R^n, b \\in R, w \\ne \\mathbb{0}$或$\\{x|w^Tx \\le b\\},x \\in R^n,w \\in R^n, b \\in R, w \\ne \\mathbb{0}$\n- 球：$B(x_c, r) = \\{x |\\ ||x-x_c||_2 \\le r\\}, x_c \\in R^n, x \\in R^n, r \\ge 0$\n- 椭球：$\\epsilon(x_c, P) = \\{x |\\ (x-x_c)^T P^{-1} (x-x_c) \\le 1\\}, x_c \\in R^n, x \\in R^n, P \\in S_{++}^n$\n- 多面体：$P=\\{x|a_i^T x \\le b_i, c_j^T x = d_j, i = 1,2,...,m, j=1,2,...,n\\}$\n- 单纯形：$R^n$中的$k+1$个点$v_0, v_1, ..., v_k$，满足$v_1-v_0, v_2-v_0, ..., v_k-v_0$线性无关，则$v_0, v_1, ..., v_k$的单纯形为$v_0, v_1, ..., v_k$的凸包，单纯形是多面体的一种，证明的时候，可以借助\"秩为$k$的矩阵$B$,可以用一个非奇异矩阵$A$转换成$\\begin{bmatrix}I_k \\\\ \\mathbb{0} \\end{bmatrix}$的形式\"这个定理。\n\n# 仿射函数、透视函数、线性分数函数\n- 仿射函数：$f:R^n \\rightarrow R^m, f(x)=Ax+b, A \\in R^{(m \\times n)}, b \\in R^n$\n\n- 缩放：$\\alpha S = \\{\\alpha x| x \\in S\\}$、移位：$S+a=\\{x+a|x \\in S\\}$是仿射变换的一种。\n\n- 线性矩阵不等式的解集是凸集：$\\{x|A(x) \\preceq B\\}$\n\n- 透视函数：$P: R^{n+1} \\rightarrow R^n, P(x,y) = \\{\\frac{x}{y}\\}\\ x \\in R^n, y\\in R_{++}$\n\n# 保凸集运算：\n- 交集\n- 集合的和：$S_1 + S_2 = \\{x_1 + x_2 | x_1 \\in S_1, x_2 \\in S_2\\}$\n- 集合的笛卡尔积：$S_1 \\times S_2 = \\{(x_1 , x_2) | x_1 \\in S_1, x_2 \\in S_2\\}$\n- （逆）仿射函数\n- 透视函数\n- 线性分数函数：首先定义仿射函数$g:R^n \\rightarrow R^{m+1}, g(x)=\\begin{bmatrix} A \\\\ c^T \\end{bmatrix} x + \\begin{bmatrix} b \\\\ d \\end{bmatrix}, A \\in R^{m \\times n}, c \\in R^n, b \\in R^m, d \\in R$，再定义透视函数$p:R^{m+1} \\rightarrow R^m$，则线性分数函数$f = p \\circ g = \\frac{Ax + b}{c^T x + d},\\ dom f = \\{x| c^T x + d > 0\\}$\n\n# 凸函数\n\n## 凸函数定义\n- $f:R^n \\rightarrow R$为凸函数$\\Leftrightarrow$$domf$为凸集，且$f(\\theta x_1 + (1-\\theta)x_2) \\le \\theta f(x_1) + (1-\\theta)f(x_2)$，其中$\\theta \\in [0, 1]$\n\n- $f:R^n \\rightarrow R$为凸函数$\\Leftrightarrow$$domf$为凸集，且$\\forall x \\in dom f, \\forall v, g(t) = f(x + tv)$为凸，$dom g = \\{t | x + tv \\in dom f\\}$\n\n- $f:R^n \\rightarrow R$为凸函数且$f$可微$\\Leftrightarrow$$domf$为凸集，$\\forall x,y \\in dom f, f(y) \\ge f(x) + \\triangledown f^T(x)(y-x)$\n\n- $f:R^n \\rightarrow R$为凸函数且$f$二阶可微$\\Leftrightarrow$$domf$为凸集，$\\forall x \\in dom f,\\triangledown ^2 f(x) \\succeq 0$（二阶条件）\n\n如果将上述前三个定义中的$\\le$、$\\ge$, 改为$<$、$>$，那么$f$就变成了严格凸函数，严格凸函数的二阶导数不能保证$\\triangledown ^2 f(x) \\succ 0$，但$\\triangledown ^2 f(x) \\succ 0$可以保证严格凸，但是二次函数$f$严格凸$\\Leftrightarrow$Hession矩阵$\\triangledown ^2 f(x) \\in S^n_{++}$即$\\triangledown ^2 f(x) \\succeq 0$。\n\n## 凸函数的一个重要性质\n$f(x) \\ge f(x_0) + f' (x_0) ^ T(x-x_0)$\n\n## 一些常见的凸函数\n- 仿射函数：$f(x)=Ax+b, \\triangledown_2f=0$，是凸函数也是凹函数\n- 指数函数：$f(x) = e^{ax}, x \\in R$\n- 幂函数：$f(x) = x^a, x \\in R_{++}, a \\ge 1\\ or\\ a \\le 0$是凸函数，当$0 \\le a \\le 1$时，是凹函数\n- 绝对值的幂函数：$f(x)= |x|^p, x \\in R, x \\ne 0, P \\ge 1$\n- 对数函数：$f(x) = -log(x), x \\in R_{++}$\n- 负熵：$f(x) = xlog(x), x \\in R_{++}$\n- 范数：$p(x), x \\in R^n$，满足$p(a)=|a|p(x), p(x+y) \\le p(x) + p(y), p(x) = 0 \\Leftrightarrow x = 0$\n- 极大值函数：$f(x) = max(x_1, x_2, ..., x_n), x \\in R^n$\n- 极大值函数的逼近-log sum up：$f(x) = log(e^{x_1} + e^{x_2} + ... +e^{x_n}), x \\in R^n$其中$max(x_1, x_2, ..., x_n) \\le f(x) \\le max(x_1, x_2, ..., x_n) + log(n)$\n\n## 二次型\n二次型是一种二次齐次函数的统称，可以表示为$f(x_1, x_2, ..., x_n) = a_{11}x_1^2 + a_{22}x_2^2 + ... + a_{nn}x_n^2 + 2a_{12}x_1x_2 + 2a_{13}x_1x_3 + ... +  + 2a_{n-1, n}x_1x_n$\n\n二次型可以通过一个对称矩阵表示$f(x_1, x_2, ..., x_n)=X^T A X$，其中A因为是对称矩阵，有很多很好的分解方式，因此非常方便分析。\n\n## 一些常用性质\n- $V^TXV \\ge 0 \\Leftrightarrow X \\succeq 0$\n- $det(XY) = det(X)det(Y)$，$det$表示求行列式值。\n- $(a^Ta)(b^Tb) \\ge (a^Tb)^2$\n\n## 保凸函数运算\n- $f_1, f_2, ..., f_m$为凸，则$f = \\sum\\limits_{i=1}^m w_i f_i,\\ \\forall i,w_i \\ge 0$为凸\n- 若$f(x,y),\\ \\forall y \\in A,f(x,y)$为凸，对于$\\forall y \\in A,\\ w(y) \\ge 0, g(x) = \\int_{y \\in A}w(y)f(x,y)dy$为凸\n- 仿射映射$f:R^n \\rightarrow R, A\\in R^{n \\times m}, b\\in R^n,g(x) = f(Ax + b), dom g=\\{x|Ax+b \\in dom f\\}$是凸函数。\n- 对函数的仿射：$f_i: R^n \\rightarrow R,i=1,...,m$为凸函数，$A \\in R_+^n, b \\in R, g(x)=A^T\\begin{bmatrix}f_1(x)& \\cdots&f_n(x)\\end{bmatrix}^T + b$为凸函数\n- 极大值操作：$f_1,f_2$为凸，则$f(x) = max(f_1(x), f_2(x)), dom f = domf_1 \\bigcap domf_2$为凸。\n- 无限个凸函数的极大值：$f(x,y),\\forall y \\in A$对于$x$为凸，则$g = \\sup\\limits_{y \\in A}f(x,y)$为凸函数，其中$\\sup\\limits_{y\\in A}$表示$y\\in A$时的上确界（可以理解为最大值）。\n- 函数的组合也可以保证函数凸性：$h:R^k \\rightarrow R, g: R^n \\rightarrow R^k,f=h \\circ g, R^n \\rightarrow R, domf = \\{x|x\\in dom g,g(x)\\in dom h\\}$，1、若$h$为凸，$\\tilde{h}$不降，$g$为凸，则$f$为凸，2、若$h$为凸，$\\tilde{h}$不增，$g$为凹，则$f$为凸，3、若$h$为凹，$\\tilde{h}$不降，$g$为凹，则$f$为凹，4、若$h$为凹，$\\tilde{h}$不增，$g$为凸，则$f$为凹，其中$\\tilde{h}$为$h$的定义域扩展。\n\n向量的最大$r$个分量之和：$f(x) = max\\{x_{i1}+x_{i2}\\cdots+x_{ir}|i1,i2,...,ir互不相等\\}$是个凸函数。\n\n实对称矩阵的最大特征值函数$f(X) = \\lambda_{max}(X)$是个凸函数。\n\n## 函数的透视\n$f:R^n \\rightarrow R，g: R^n \\times R^n_{++} \\rightarrow R, g(u, v) = v^Tf(u \\odot \\frac{1}{v}), dom g = \\{(u,v)|v \\in R^n_{++}, u \\odot \\frac{1}{v} \\in dom f\\}$，$f$为凸$\\Rightarrow g$为凸，$f$为凹$\\Rightarrow g$为凹，且是联合凸和联合凹。\n\n## 函数的共轭\n\n$f:R^n \\rightarrow R, f^\\star:R^n\\rightarrow R, f^\\star(y)= \\sup\\limits_{x \\in domf}(y^Tx - f(x))$。若$f$可微，则$f^\\star(y)=\\sup\\limits_{x \\in domf}(y^Tx' - f(x')), f'(x') = y$。对任意情况，$f^\\star(y)$一定是凸函数。\n\n# 凸集和凸函数的关系\n\n## $\\alpha$-sublevel set\n对于函数$f:R^n \\rightarrow R$定义其$\\alpha$-sublevel set为$C_\\alpha=\\{x\\in domf|f(x)\\le \\alpha\\}$\n\n对于函数$f:R^n \\rightarrow R$定义其$\\alpha$-suplevel set为$C_\\alpha=\\{x\\in domf|f(x)\\ge \\alpha\\}$\n\n凸函数所有的$\\alpha$-sublevel set都是凸集。\n\n但是函数的$\\alpha$-sublevel set都是凸集，不能保证该函数是凸函数。\n\n# Quasi Convex（Concave） function（拟凸（凹）函数）\n$S_\\alpha' = \\{x|x in dom f,f(x) \\le \\alpha\\}$（$\\alpha$-sublevel set）都是凸集，则这样的函数$f(x)$称为拟凸函数（unimodal function，单模态函数）。\n\n$S_\\alpha' = \\{x|x in dom f,f(x) \\ge \\alpha\\}$（$\\alpha$-suplevel set）都是凸集，则这样的函数$f(x)$称为拟凹函数。\n\n$S_\\alpha' = \\{x|x in dom f,f(x) = \\alpha\\}$都是凸集，则这样的函数$f(x)$称为拟线性函数。\n\n类似于凸函数的第一个定义，拟凸函数也可以写成$f:R^n\\rightarrow R, dom f$为凸，$\\forall x,y \\in dom f, \\forall \\theta \\in [0,1]$，有$max(f(x), f(y)) \\ge f(\\theta x + (1-\\theta)y)$\n\n类似于凸函数的第三个定义，拟凸函数也可以写成若$f(x)$为拟凸函数，$\\forall x,y \\in domf, f(y) \\le f(x) \\Rightarrow \\triangledown^Tf(x)(y-x) \\le 0$\n\n类似于凸函数的二阶条件，拟凸函数可以写成若$f(x)$为拟凸函数，$y^T\\triangledown f(x) = 0 \\Rightarrow y^T \\triangledown^2f(x)y\\ge 0$\n\n# 凸优化问题（Convex Problems）\n目标函数是凸函数且约束集合是凸集的问题叫做凸优化问题。\n\n## 一般优化问题\n$$\n\\min f_0(x)\\\\\ns.t. f_i(x) \\le 0, i= 1, 2, ..., m\\\\\nh_j(x) = 0, j= 1,2,...,n\n$$\n其中$x\\in R^n$称为优化变量(Optimization variable)，$f_0$称为目标函数（损失函数）$f_i(x) \\le 0$称为不等式约束，$h_j(x) = 0$称为等式约束。如果$m=n=0$则称为无约束问题。\n\n## 优化问题的域：domain\n优化问题的定义域：$D = \\bigcap\\limits_{i=0}^m dom f_i \\cap \\bigcap\\limits_{j=1}^n dom h_j$\n\n## 可行解集：feasible set\n可行解集$X_f = \\{x|x \\in D, f_i(x) \\le 0,i=1,2,...,m,h_j(x) = 0, j= 1,2,...,n\\}$\n\n## 问题的最优值：optimal value\n$p^\\star = \\inf\\{f_0(x)|x \\in X_f\\}$，如果$X_f$是空集，则$p^\\star = +\\infty$\n\n## 最优解：optimal point/solution\n若$x^\\star$可行，且$f_0(x^\\star) = p^\\star$\n\n## 最优解集\n$X_{opt} = \\{x | x\\in X_f, f_0(x)=p^\\star\\}$\n\n## $\\epsilon$次优解集：$\\epsilon$-suboptimal set\n$X_\\epsilon = \\{x | x\\in X_f, f_0(x)) \\le p^\\star + \\epsilon\\}, \\epsilon \\ge 0$\n\n## 局部最优解：local optimal\n$\\exists R, f_0(x^\\star) = \\inf\\{f_0(z) | f_i(z) \\le 0, h_j(z) =0, ||z-x||\\le R\\}$则$x^\\star$是局部最优解\n\n## 可行性优化问题：feasibility Problems\n$\\min\\limits_x c, s.t. f_i(X) \\le 0, h_j(X) = 0$，其中$c$是任意一常数。\n\n## 凸优化问题\n$$\n\\min f_0(x)\\\\\ns.t. f_i(x) \\le 0, i= 1, 2, ..., m\\\\\na_j^Tx = b_j, j= 1,2,...,n\n$$\n其中$f_0$、$f_i$是凸函数，其可行解集是个凸集、。\n\n凸优化问题的局部最优解等于全局最优解（反证法）。\n\n## 可微目标函数情况下的最优解\n凸函数在可微的情况下：$f_0(y) \\ge f_0(x) + \\triangledown f_0^T(x)(y-x)$\n\n其最优解$x^\\star$满足$\\triangledown f_0^T(x^\\star)(y-x^\\star) \\ge 0, \\forall y \\in X_f$\n\n## 线性规划问题\n$$\n\\begin{aligned}\n\\min\\ &C^Tx +d\\\\\ns.t.\\ &Gx \\le h\\\\\n&Ax = b    \n\\end{aligned}\n$$\n\n其等式约束和不等式约束构成一个多面体集合。\n\n### 线性规划的等价变换\n上面的线性变换可以等价于：\n$$\n\\begin{aligned}\n    \\min &C^T x +d\\\\\n    s.t.\\ &Gx + S = h\\\\\n    &Ax = b\\\\\n    &S \\ge 0\n\\end{aligned}\n$$\n等价的判定条件：\n两个表达方式的可行解能够对应，且对应可行解的目标函数值相同。\n\n### 线性分数规划\n$$\n\\begin{aligned}\n    \\min\\ &f_0(x)\\\\\n    s.t.\\ &Gx \\le h\\\\\n    &Ax = b\n\\end{aligned}\n$$\n其中$f_0(x) = \\frac{C^Tx +d}{e^Tx + f}, domf = \\{x | e^T x + f \\ge 0\\}$\n\n这个问题是一个拟凸问题。\n\n如果该问题有可行解，则其有凸问题等价形式：\n$$\n\\begin{aligned}\n    \\min\\ &C^T y + dz\\\\\n    s.t.\\ &Gy-hz \\le 0\\\\\n    &Ay -bz = 0\\\\\n    &e^Ty+fz = 1\\\\\n    &z\\ge 0\n\\end{aligned}\n$$\n两个问题等价的证明：\n\n首先对于第一个问题任意可行的$x$，必须存在$y$和$z$对第二个问题可行，且$C^T y + dz = f_0(x)$，可以直接令$y = \\frac{x}{e^Tx +f}, z=\\frac{1}{e^Tx + f}$即证。\n\n齐次对于第二个问题，任意可行的$y$和$z$，必须存在$x$对第一个问题可行，且$C^T y + dz = f_0(x)$，如果$z>0$，则直接令$x=\\frac{y}{z}$, 若$z=0$，那么对于第一个问题的一个可行解$x_0$，$\\forall t\\ge 0, x = x_0 + ty$也对第一个问题可行，令$\\lim t \\rightarrow +\\infty$，则可以使两个目标函数值相等。\n\n## 二次规划(QP)\n$$\n\\begin{aligned}\n    \\min\\ &\\frac{1}{2} x^TPx + q^T x + r\\\\\n    s.t.\\ & Gx \\le h\\\\\n    &Ax = b\n\\end{aligned}\n$$\n其中$P\\in S_+^n$\n\n## 二次约束二次规划(QCQP)\n$$\n\\begin{aligned}\n    \\min\\ &\\frac{1}{2} x^TPx + q^T x + r\\\\\n    s.t.\\ & \\frac{1}{2} x^TP_ix + q_i^T x + r_i \\le 0,\\ i=1,2,...,m\\\\\n    &Ax = b\n\\end{aligned}\n$$\n其中$P\\in S_+^n$，$P_i \\in S_+^n, i =1,2,...,m$\n\n## 半正定规划（semi-Definite Programming）\n$$\n\\begin{aligned}\n    \\min\\ &tr(Cx)\\\\\n    s.t.\\ & tr(A_ix) = b_i, i=1,2,...,p\\\\\n    & x \\succeq 0\n\\end{aligned}\n$$\n其中$x\\in S_+^n, C \\in R^{n\\times n}, A_i \\in R^{n\\times n}, b_i \\in R$\n\n## 多目标优化问题\n\n$$\n\\begin{aligned}\n    \\min\\ &f_0(x):R^n \\rightarrow R^g\\\\\n    s.t.\\ &f_i(x) \\le 0, i= 1,2,...,m\\\\\n    &h_j(x) = 0, j= 1,2,...,p\\\\\n\\end{aligned}\n$$\n\n### Pareto optimal front\n一个点满足“如果找到其他可行解，可以使得在某些指标上比这个点更优，那么这个可行解必然在其他指标上比这个点更差”，则这个点属于Pareto optimal front。\n\n如果$f_0(x)$为凸，$f_i(x)$为凸，$h_j(x)$为仿射，则必可通过以下方式求得pareto optimal front中的一点：\n\n$$\n\\begin{aligned}\n    \\min\\ &\\sum\\limits_{i=1}^g \\lambda_i f_{0i}(x), \\lambda_i \\ge 0\\\\\n    s.t.\\ &f_i(x) \\le 0, i = 1,2,...,m\\\\\n    &h_j(x) = 0, j=1,2,...,p\n\\end{aligned}\n$$\n\n# 对偶性\n对于以下问题：\n$$\n\\begin{aligned}\n    \\min\\ &f_0(x)\\\\\n    s.t.\\ &f_i(x) \\le 0, i = 1,2,...,m\\\\\n    &h_j(x) = 0, j=1,2,...,p\n\\end{aligned}\n$$\n其定义域$D = \\{x| x \\in \\bigcap_{i=1}^m dom f_i \\cap \\bigcap_{j=1}^p dom h_j\\}$\n## 拉格朗日函数\n其拉格朗日函数定义为：$L(x, \\lambda, v) = f_0(x) + \\sum\\limits_{i=1}^m\\lambda_if_i(x) + \\sum\\limits_{j=1}^pv_jh_j(x)$，其中$\\lambda_i \\ge 0$和$v_j$称为拉格朗日乘子。\n\n## 拉格朗日对偶函数（对偶函数）\n$$\ng(\\lambda, v) = \\inf\\limits_{x\\in D}L(x, \\lambda ,v)\n$$\n\n- 对偶函数一定是个凹函数（因为函数对于$\\lambda$和$v$是线性的）。\n- $\\forall \\lambda \\ge 0, \\forall v, g(\\lambda,v) \\le p^\\star$,其中$p^\\star$表示原问题的最优值。\n\n## 对偶问题(Dual problem/lagrange Dual problem)\n对于原问题(Primal problem)（P）：\n$$\n\\begin{aligned}\n    \\min\\ &f_0(x)\\\\\n    s.t.\\ &f_i(x) \\le 0, i = 1,2,...,m\\\\\n    &h_j(x) = 0, j=1,2,...,p\n\\end{aligned}\n$$\n其最优解记为$p^\\star$\n\n其对偶问题（D）：\n$$\n\\begin{aligned}\n    \\max\\ &g(\\lambda, v)\\\\\n    s.t.\\ & \\lambda \\ge 0\n\\end{aligned}\n$$\n其中$g(\\lambda, v)$是原问题的拉格朗日对偶函数，对偶问题的最优解记为$d^\\star$\n\n有以下结论：\n- $d^\\star \\le p^\\star$（弱对偶）。\n- 对偶问题的最优解$\\lambda ^\\star, v^\\star$存在。\n- 一个凸问题的对偶问题的对偶问题就是自身。\n\n如果$d^\\star = p^\\star$，则称为强对偶。\n\n对偶间隙：$p^\\star - d^\\star$\n\n## 相对内部（Relative Interior）\n去掉集合的边缘，让集合变成一个开集，集合$D$的相对内部可以表示为：\n\n$Relint\\ D =\\{x| x \\in D, \\exists r\\>0, B(x,r) \\cap aff(D) \\le D\\}$\n\n其中$B(x,r)$是个球，$aff(D)$表示$D$的仿射包。\n\n## Slater's Condition\n对偶间隙为零的充分条件，但是不必要。\n\n如果凸问题：\n$$\n\\begin{aligned}\n    \\min\\ &f_0(x)\\\\\n    s.t.\\ &f_i(x) \\le 0, i=1,2,...,m\\\\\n    &Ax=b\n\\end{aligned}\n$$\n其定义域为$D$，对$\\forall i, f_i(x)$为凸，当$\\exists x \\in relint\\ D$使得$Ax=b, f_i(x) < 0, i =1,2,...,m$满足时，$d^\\star = p^\\star$\n\n## Weaken Slater's Condition\n如果不等式约束为仿射（$D = dom f_0$），只要可行域非空，则必有$d^\\star = p^\\star$\n\n## 鞍点（Saddle Point）\n对于函数$L(x, \\lambda)$，如果$\\inf\\limits_{x \\in D} \\sup\\limits_{\\lambda \\ge 0} L(x, \\lambda) = \\sup\\limits_{\\lambda \\ge 0} \\inf\\limits_{x \\in D} L(x, \\lambda)$，则称此时的$(x^\\star, \\lambda^\\star)$为鞍点。\n\n## 鞍点定理\n若$(\\tilde{x}, \\tilde{\\lambda})$为$L(x, \\lambda)$的鞍点$\\Leftrightarrow$强对偶存在，且$(\\tilde{x}, \\tilde{\\lambda})$为Primal与Dual的最优解。\n\n## KKT条件\n对于如下问题：\n$$\n\\begin{aligned}\n    \\min\\ &f_0(x)\\\\\n    s.t.\\ &f_i(x) \\le 0, i=1,2,...,m\\\\\n    &h_j(x) = 0, j=1,2,...,n\n\\end{aligned}\n$$\n其拉格朗日函数：$L(x, \\lambda, v) = f_0(x) + \\sum\\limits_{i=1}^m \\lambda_i f_i(x) + \\sum\\limits_{j=1}^n v_j h_j(x)$\n\n其对偶函数：$g(\\lambda, v) = \\inf\\limits_{x \\in D}L(x, \\lambda, v)$\n\n对偶问题：$\\sup\\limits_{\\lambda, v} g(\\lambda, v)$\n\n如果其满足对偶问题的最优解$d^\\star$和原问题的最优解$p^\\star$相等。\n即：\n$$\n\\begin{aligned}\n    f_0(x^\\star) &= g(\\lambda^\\star, v^\\star)\\\\\n    &=\\inf\\limits_{x \\in D} (f_0(x) + \\sum\\limits_{i=1}^m \\lambda^\\star_i f_i(x) + \\sum\\limits_{j=1}^n v^\\star_j h_j(x))\\\\\n    &\\le f_0(x^\\star) + \\sum\\limits_{i=1}^m \\lambda^\\star_i f_i(x^\\star) + \\sum\\limits_{j=1}^n v^\\star_j h_j(x^\\star)\\\\\n    &\\le f_0(x^\\star)\n\\end{aligned}\n$$\n\n这表示$f_0(x^\\star) = f_0(x^\\star) + \\sum\\limits_{i=1}^m \\lambda^\\star_i f_i(x^\\star) + \\sum\\limits_{j=1}^n v^\\star_j h_j(x^\\star) \\Rightarrow \\sum\\limits_{i=1}^m \\lambda^\\star_i f_i(x^\\star) = 0$，而$f_i(x^\\star) \\le 0，\\lambda_i \\ge 0, h_j(x^\\star) = 0$，因此可以得出$\\lambda^\\star_i f_i(x^\\star) = 0$，这就是互补松弛条件。\n\n又因为$x^\\star = \\mathop{\\arg\\inf}\\limits_{x \\in D} (f_0(x) + \\sum\\limits_{i=1}^m \\lambda^\\star_i f_i(x) + \\sum\\limits_{j=1}^n v^\\star_j h_j(x)) = \\mathop{\\arg\\inf}\\limits_{x \\in D} L(x, \\lambda^\\star, v^\\star)$（这里是因为上面的推导中，两个$\\le$可以改成$=$）,因此$x^\\star$是该问题的全局最优解。\n\n如果原问题中，所有函数都可微，则有：\n$$\n\\frac{\\partial L(x^\\star, \\lambda^\\star, v^\\star)}{\\partial x^\\star} = 0\n$$\n这就是稳定性条件。\n\n因此只要满足$p^\\star = d^\\star$，且所有函数可微，则有如下结论：\n- 互补松弛条件：$\\lambda^\\star_i f_i(x^\\star) = 0$\n- 稳定性条件：$\\frac{\\partial L(x^\\star, \\lambda^\\star, v^\\star)}{\\partial x^\\star} = 0$\n- 原问题可行：$f_i(x) \\le 0, i=1,2,...,m\\ \\ h_j(x) = 0, j=1,2,...,n$\n- 对偶问题可行：$\\lambda_i \\ge 0, i= 1,2,...,m$\n\n以上四个结论就是KKT条件。\n\n在对偶间隙为0且各个函数可微的情况下，KKT条件仅是最优解的必要条件，满足KKT条件的解，不一定是最优解。\n\n如果原问题为凸问题，各个函数可微，对偶间隙为0，则KKT条件是最优解的充分必要条件。\n\n充分性证明：如果$(x^\\star, \\lambda^\\star, v^\\star)$满足KKT条件，则必有$(x^\\star, \\lambda^\\star, v^\\star)$为最优解。证明$g(\\lambda ^\\star, v^\\star) = f_0(x^\\star)$即可。\n\n## 敏感性分析\n原问题$P$：\n$$\n\\begin{aligned}\n    \\min\\ &f_0(x)\\\\\n    s.t.\\ &f_i(x) \\le 0, i=1,2,...,m\\\\\n    &h_j(x) = 0,j=1,2,...,n\n\\end{aligned}\n$$\n干扰问题：\n$$\n\\begin{aligned}\n    \\min\\ &f_0(x)\\\\\n    s.t.\\ &f_i(x) \\le u_i, i=1,2,...,m\\\\\n    &h_j(x) = w_i,j=1,2,...,n\n\\end{aligned}\n$$\n\n将原问题的最优值$p^\\star$看做一个函数$p^\\star(u, w)$，原问题的最优值$p^\\star = p^\\star(0, 0)$\n\n有以下性质：\n\n- 若原问题为凸问题，则$p^\\star(u, w)$为关于$u$和$w$的凸函数。\n- 若原问题为凸，对偶间隙为0，$\\lambda^\\star$和$v^\\star$为对偶问题的最优解，则$p^\\star(u,w) \\le p^\\star(0, 0) -\\lambda^{\\star T}u - v^{\\star T}w$\n- 若原问题为凸，对偶间隙为0，且$p^\\star(u,w)$在$(0, 0)$点可微，则$\\lambda^\\star_i = -\\frac{\\partial p^\\star(0, 0)}{\\partial u_i}, v^\\star_j = -\\frac{\\partial p^\\star(0, 0)}{\\partial w_j}$, $p^\\star(u,w) = p^\\star(0, 0) -\\lambda^{\\star T}u - v^{\\star T}w$\n\n## 强凸性\n强凸性表示：$\\exists m > 0, \\forall x \\in dom f, \\triangledown^2f(x) \\succeq mI$\n\n强凸性也等价于：$\\exists m, \\forall x,y \\in dom f, f(y) \\ge f(x) + \\triangledown f^T(x)(y-x) + \\frac{1}{2}m||y-x||^2$\n\n如果$f(x)$二阶可微且有强凸性，则$\\min f(x)$的最优值$p^\\star$满足$f(x) - p^\\star \\le \\frac{1}{2m}||\\triangledown f(x)||_2^2$\n\n# 无约束优化问题的求解算法\n## 梯度下降法（Gradient Descent）\n每次沿着负梯度方向更新参数。\n\n### 梯度下降法的收敛性\n如果$\\exists M, m,MI \\succeq \\triangledown_2f\\succeq mI$，对于精确搜索的学习速率$\\alpha$，可以证明$f(x^{k+1}) -p^\\star \\le (1 - \\frac{m}{M})f(x^k - p^\\star)$（利用强凸性的性质证明）\n\n## 最速下降法（Steepest Descent）\n如果一阶可微，则可以找到一个方向$d = \\mathop{\\arg\\min}\\limits_v f(x+v) = \\mathop{\\arg\\min}\\limits_v \\triangledown f^T(x)v,\\ s.t.\\ ||v||_k = 1$（这里使用了一阶泰勒展开），其中的k-范数不一定是2-范数，所以这个方向不一定是梯度，梯度下降法是最速下降法在范数取2-范数情况下的一个特例。\n\n## 最速下降法的一些变种\n### 坐标轮换法（Coordinate Descent）\n每次选择一个基向量方向作为优化方向，但是需要在包含正值和负值的区间去搜索步长。\n### 分块坐标轮换法（Block Coordinate Descent）\n每次选择一组基向量的线性组合方向（一个子空间）作为优化方向。\n\n坐标轮换和分块坐标轮换适合于维度拆分之后损失函数比较简单的情况。\n\n## 次梯度方法\n如果$f(x)$在某些点不可微，那么定义次梯度的概念$\\frac{\\partial f}{\\partial x} = \\theta\\triangledown f(x)^+ + (1-\\theta)\\triangledown f(x)^-,\\ \\theta \\in [0, 1]$，即使用梯度的左极限和右极限的凸组合。如果在当前点，对于某个$\\theta$，次梯度可以为0，那么认为其到达极值点。\n\n## 牛顿法（Newton's Method）\n如果二阶可微，则使用二阶泰勒展开，找到一个方向$d = \\mathop{\\arg\\min}\\limits_v f(x+v) = \\mathop{\\arg\\min}\\limits_v \\triangledown f^T(x)v + \\frac{1}{2}v^T\\triangledown_2 f^T(x)v$，这里不需要关于$v$的约束项，是因为如果是个凸函数，则有唯一解的$v = -(\\triangledown_2 f^T(x))^{-1}\\triangledown f(x)$（也称为牛顿方向）。\n\n牛顿法在泰勒展开中的一次项接近于0的时候，就可以停止，即$\\triangledown f^T(x)v = -\\triangledown f^T(x)(\\triangledown_2 f^T(x))^{-1}\\triangledown f(x)$接近于0的时候就停止。\n\n### 牛顿法的收敛速度\n如果$||\\triangledown f(x)||_2$比较大，则泰勒展开的偏差较大，因此收敛速度比较慢，如果$||\\triangledown f(x)||_2$比较小，则收敛速度比梯度下降快很多。\n\n牛顿法的缺点在于求Hession矩阵可能非常难而且慢。\n\n## 拟牛顿法（Quasi-Newton Method）\n不求二阶偏导，根据拟牛顿条件，找到一个对Hession矩阵逆矩阵的近似矩阵来进行方向选择，例如BFGS、L-BFGS、DFP等方法。\n\n# 有约束优化问题的求解算法\n$$\n\\begin{aligned}\n    \\min\\ &f(x)\\\\\n    s.t.\\ &Ax = b\n\\end{aligned}\n$$\n在使用KKT条件的过程中，如果$\\triangledown f(x)$不是线性，那么存在求解困难。 \n\n首先变形原问题，使用迭代的方法，假设$x^k$满足约束$Ax^k = b$，那么下一次迭代问题变成：\n$$\n\\begin{aligned}\n    \\min\\limits_d\\ &f(x^k + d)\\\\\n    s.t.\\ &A(x^k +d) = b \\Rightarrow Ad = 0\n\\end{aligned}\n$$\n进行二阶泰勒展开，去掉高阶项，原问题近似等价于：\n$$\n\\begin{aligned}\n    \\min\\limits_d\\ &f(x^k) + \\triangledown f^T(x^k)d + \\frac{1}{2}d^T \\triangledown_2 f(x^k)d\\\\\n    s.t.\\ &Ad = 0\n\\end{aligned}\n$$\n\n## 拉格朗日法（Lagrangian Method）\n对于对偶间隙为零的问题，固定$v$去求$x$，然后固定$x$去求$v$，交替进行。\n$$\n\\begin{aligned}\n    x^{k+1} &= x^k - \\alpha^k(\\triangledown f(x^k) + A^Tv^k)\\\\\n    v^{k+1} &= v^k + \\alpha^k(Ax^k - b)\n\\end{aligned}\n$$\n\n## 增广拉格朗日法（Augmented Lagragian Method）\n$$\n\\begin{aligned}\n    \\min\\limits_x\\ &f(x) + \\frac{C}{2}||Ax -b||^2_2\\\\\n    s.t.\\ &Ax = b\n\\end{aligned}\n$$\n这个约束问题的拉格朗日函数（增广拉格朗日函数）：$L_C(x,v) = f(x) + v^T(Ax-b)+\\frac{C}{2}||Ax -b||^2_2$\n\n而且这个问题的原问题最优解和对偶问题最优解都和以下问题相同：\n$$\n\\begin{aligned}\n    \\min\\limits_x\\ &f(x)\\\\\n    s.t.\\ &Ax = b\n\\end{aligned}\n$$\n\n### 一些性质\n- 如果$v = v^\\star$，则$\\forall C > 0$，$x^\\star = \\mathop{\\arg\\min}\\limits_xL_C(x, v^\\star)$\n- 若$C \\rightarrow +\\infty$，则$\\forall v, x^\\star =\\mathop{\\arg\\min}\\limits_xL_C(x, v)$\n\n如果有$x^k$，和$v^k$，首先更新$x^{k+1} = \\mathop{\\arg\\min}\\limits_x L_C(x, v^k)$，然后更新$v^{k+1} = v^k + C \\triangledown_v L_C(x^{k+1}, v^k)$","source":"_posts/学习笔记/凸优化学习笔记.md","raw":"---\ntitle: 凸优化学习笔记\ndate: 2020-04-23 15:52:30\ntags: [凸优化]\nmathjax: true\n---\n\n# 仿射集、凸集、锥集、凸锥集、仿射组合、凸组合、凸锥组合、仿射包、凸包、凸锥包\n- 锥：$C = \\{x| \\theta x \\in C\\}, x \\in R^n,\\theta \\ge 0$\n- 仿射组合：$\\theta_1+\\theta_2+...=1$\n- 凸组合：$\\theta_1+\\theta_2+...=1,\\theta_1+\\theta_2+... \\ge 0$\n- 凸锥组合：$\\theta_1+\\theta_2+... \\ge 0$\n\n# 一些凸集：超平面、半空间、球、椭球、多面体、单纯形、对称（半）（正定）矩阵\n- 超平面：$\\{x|w^Tx=b\\},x \\in R^n,w \\in R^n, b \\in R, w \\ne \\mathbb{0}$\n- 半空间：$\\{x|w^Tx \\ge b\\},x \\in R^n,w \\in R^n, b \\in R, w \\ne \\mathbb{0}$或$\\{x|w^Tx \\le b\\},x \\in R^n,w \\in R^n, b \\in R, w \\ne \\mathbb{0}$\n- 球：$B(x_c, r) = \\{x |\\ ||x-x_c||_2 \\le r\\}, x_c \\in R^n, x \\in R^n, r \\ge 0$\n- 椭球：$\\epsilon(x_c, P) = \\{x |\\ (x-x_c)^T P^{-1} (x-x_c) \\le 1\\}, x_c \\in R^n, x \\in R^n, P \\in S_{++}^n$\n- 多面体：$P=\\{x|a_i^T x \\le b_i, c_j^T x = d_j, i = 1,2,...,m, j=1,2,...,n\\}$\n- 单纯形：$R^n$中的$k+1$个点$v_0, v_1, ..., v_k$，满足$v_1-v_0, v_2-v_0, ..., v_k-v_0$线性无关，则$v_0, v_1, ..., v_k$的单纯形为$v_0, v_1, ..., v_k$的凸包，单纯形是多面体的一种，证明的时候，可以借助\"秩为$k$的矩阵$B$,可以用一个非奇异矩阵$A$转换成$\\begin{bmatrix}I_k \\\\ \\mathbb{0} \\end{bmatrix}$的形式\"这个定理。\n\n# 仿射函数、透视函数、线性分数函数\n- 仿射函数：$f:R^n \\rightarrow R^m, f(x)=Ax+b, A \\in R^{(m \\times n)}, b \\in R^n$\n\n- 缩放：$\\alpha S = \\{\\alpha x| x \\in S\\}$、移位：$S+a=\\{x+a|x \\in S\\}$是仿射变换的一种。\n\n- 线性矩阵不等式的解集是凸集：$\\{x|A(x) \\preceq B\\}$\n\n- 透视函数：$P: R^{n+1} \\rightarrow R^n, P(x,y) = \\{\\frac{x}{y}\\}\\ x \\in R^n, y\\in R_{++}$\n\n# 保凸集运算：\n- 交集\n- 集合的和：$S_1 + S_2 = \\{x_1 + x_2 | x_1 \\in S_1, x_2 \\in S_2\\}$\n- 集合的笛卡尔积：$S_1 \\times S_2 = \\{(x_1 , x_2) | x_1 \\in S_1, x_2 \\in S_2\\}$\n- （逆）仿射函数\n- 透视函数\n- 线性分数函数：首先定义仿射函数$g:R^n \\rightarrow R^{m+1}, g(x)=\\begin{bmatrix} A \\\\ c^T \\end{bmatrix} x + \\begin{bmatrix} b \\\\ d \\end{bmatrix}, A \\in R^{m \\times n}, c \\in R^n, b \\in R^m, d \\in R$，再定义透视函数$p:R^{m+1} \\rightarrow R^m$，则线性分数函数$f = p \\circ g = \\frac{Ax + b}{c^T x + d},\\ dom f = \\{x| c^T x + d > 0\\}$\n\n# 凸函数\n\n## 凸函数定义\n- $f:R^n \\rightarrow R$为凸函数$\\Leftrightarrow$$domf$为凸集，且$f(\\theta x_1 + (1-\\theta)x_2) \\le \\theta f(x_1) + (1-\\theta)f(x_2)$，其中$\\theta \\in [0, 1]$\n\n- $f:R^n \\rightarrow R$为凸函数$\\Leftrightarrow$$domf$为凸集，且$\\forall x \\in dom f, \\forall v, g(t) = f(x + tv)$为凸，$dom g = \\{t | x + tv \\in dom f\\}$\n\n- $f:R^n \\rightarrow R$为凸函数且$f$可微$\\Leftrightarrow$$domf$为凸集，$\\forall x,y \\in dom f, f(y) \\ge f(x) + \\triangledown f^T(x)(y-x)$\n\n- $f:R^n \\rightarrow R$为凸函数且$f$二阶可微$\\Leftrightarrow$$domf$为凸集，$\\forall x \\in dom f,\\triangledown ^2 f(x) \\succeq 0$（二阶条件）\n\n如果将上述前三个定义中的$\\le$、$\\ge$, 改为$<$、$>$，那么$f$就变成了严格凸函数，严格凸函数的二阶导数不能保证$\\triangledown ^2 f(x) \\succ 0$，但$\\triangledown ^2 f(x) \\succ 0$可以保证严格凸，但是二次函数$f$严格凸$\\Leftrightarrow$Hession矩阵$\\triangledown ^2 f(x) \\in S^n_{++}$即$\\triangledown ^2 f(x) \\succeq 0$。\n\n## 凸函数的一个重要性质\n$f(x) \\ge f(x_0) + f' (x_0) ^ T(x-x_0)$\n\n## 一些常见的凸函数\n- 仿射函数：$f(x)=Ax+b, \\triangledown_2f=0$，是凸函数也是凹函数\n- 指数函数：$f(x) = e^{ax}, x \\in R$\n- 幂函数：$f(x) = x^a, x \\in R_{++}, a \\ge 1\\ or\\ a \\le 0$是凸函数，当$0 \\le a \\le 1$时，是凹函数\n- 绝对值的幂函数：$f(x)= |x|^p, x \\in R, x \\ne 0, P \\ge 1$\n- 对数函数：$f(x) = -log(x), x \\in R_{++}$\n- 负熵：$f(x) = xlog(x), x \\in R_{++}$\n- 范数：$p(x), x \\in R^n$，满足$p(a)=|a|p(x), p(x+y) \\le p(x) + p(y), p(x) = 0 \\Leftrightarrow x = 0$\n- 极大值函数：$f(x) = max(x_1, x_2, ..., x_n), x \\in R^n$\n- 极大值函数的逼近-log sum up：$f(x) = log(e^{x_1} + e^{x_2} + ... +e^{x_n}), x \\in R^n$其中$max(x_1, x_2, ..., x_n) \\le f(x) \\le max(x_1, x_2, ..., x_n) + log(n)$\n\n## 二次型\n二次型是一种二次齐次函数的统称，可以表示为$f(x_1, x_2, ..., x_n) = a_{11}x_1^2 + a_{22}x_2^2 + ... + a_{nn}x_n^2 + 2a_{12}x_1x_2 + 2a_{13}x_1x_3 + ... +  + 2a_{n-1, n}x_1x_n$\n\n二次型可以通过一个对称矩阵表示$f(x_1, x_2, ..., x_n)=X^T A X$，其中A因为是对称矩阵，有很多很好的分解方式，因此非常方便分析。\n\n## 一些常用性质\n- $V^TXV \\ge 0 \\Leftrightarrow X \\succeq 0$\n- $det(XY) = det(X)det(Y)$，$det$表示求行列式值。\n- $(a^Ta)(b^Tb) \\ge (a^Tb)^2$\n\n## 保凸函数运算\n- $f_1, f_2, ..., f_m$为凸，则$f = \\sum\\limits_{i=1}^m w_i f_i,\\ \\forall i,w_i \\ge 0$为凸\n- 若$f(x,y),\\ \\forall y \\in A,f(x,y)$为凸，对于$\\forall y \\in A,\\ w(y) \\ge 0, g(x) = \\int_{y \\in A}w(y)f(x,y)dy$为凸\n- 仿射映射$f:R^n \\rightarrow R, A\\in R^{n \\times m}, b\\in R^n,g(x) = f(Ax + b), dom g=\\{x|Ax+b \\in dom f\\}$是凸函数。\n- 对函数的仿射：$f_i: R^n \\rightarrow R,i=1,...,m$为凸函数，$A \\in R_+^n, b \\in R, g(x)=A^T\\begin{bmatrix}f_1(x)& \\cdots&f_n(x)\\end{bmatrix}^T + b$为凸函数\n- 极大值操作：$f_1,f_2$为凸，则$f(x) = max(f_1(x), f_2(x)), dom f = domf_1 \\bigcap domf_2$为凸。\n- 无限个凸函数的极大值：$f(x,y),\\forall y \\in A$对于$x$为凸，则$g = \\sup\\limits_{y \\in A}f(x,y)$为凸函数，其中$\\sup\\limits_{y\\in A}$表示$y\\in A$时的上确界（可以理解为最大值）。\n- 函数的组合也可以保证函数凸性：$h:R^k \\rightarrow R, g: R^n \\rightarrow R^k,f=h \\circ g, R^n \\rightarrow R, domf = \\{x|x\\in dom g,g(x)\\in dom h\\}$，1、若$h$为凸，$\\tilde{h}$不降，$g$为凸，则$f$为凸，2、若$h$为凸，$\\tilde{h}$不增，$g$为凹，则$f$为凸，3、若$h$为凹，$\\tilde{h}$不降，$g$为凹，则$f$为凹，4、若$h$为凹，$\\tilde{h}$不增，$g$为凸，则$f$为凹，其中$\\tilde{h}$为$h$的定义域扩展。\n\n向量的最大$r$个分量之和：$f(x) = max\\{x_{i1}+x_{i2}\\cdots+x_{ir}|i1,i2,...,ir互不相等\\}$是个凸函数。\n\n实对称矩阵的最大特征值函数$f(X) = \\lambda_{max}(X)$是个凸函数。\n\n## 函数的透视\n$f:R^n \\rightarrow R，g: R^n \\times R^n_{++} \\rightarrow R, g(u, v) = v^Tf(u \\odot \\frac{1}{v}), dom g = \\{(u,v)|v \\in R^n_{++}, u \\odot \\frac{1}{v} \\in dom f\\}$，$f$为凸$\\Rightarrow g$为凸，$f$为凹$\\Rightarrow g$为凹，且是联合凸和联合凹。\n\n## 函数的共轭\n\n$f:R^n \\rightarrow R, f^\\star:R^n\\rightarrow R, f^\\star(y)= \\sup\\limits_{x \\in domf}(y^Tx - f(x))$。若$f$可微，则$f^\\star(y)=\\sup\\limits_{x \\in domf}(y^Tx' - f(x')), f'(x') = y$。对任意情况，$f^\\star(y)$一定是凸函数。\n\n# 凸集和凸函数的关系\n\n## $\\alpha$-sublevel set\n对于函数$f:R^n \\rightarrow R$定义其$\\alpha$-sublevel set为$C_\\alpha=\\{x\\in domf|f(x)\\le \\alpha\\}$\n\n对于函数$f:R^n \\rightarrow R$定义其$\\alpha$-suplevel set为$C_\\alpha=\\{x\\in domf|f(x)\\ge \\alpha\\}$\n\n凸函数所有的$\\alpha$-sublevel set都是凸集。\n\n但是函数的$\\alpha$-sublevel set都是凸集，不能保证该函数是凸函数。\n\n# Quasi Convex（Concave） function（拟凸（凹）函数）\n$S_\\alpha' = \\{x|x in dom f,f(x) \\le \\alpha\\}$（$\\alpha$-sublevel set）都是凸集，则这样的函数$f(x)$称为拟凸函数（unimodal function，单模态函数）。\n\n$S_\\alpha' = \\{x|x in dom f,f(x) \\ge \\alpha\\}$（$\\alpha$-suplevel set）都是凸集，则这样的函数$f(x)$称为拟凹函数。\n\n$S_\\alpha' = \\{x|x in dom f,f(x) = \\alpha\\}$都是凸集，则这样的函数$f(x)$称为拟线性函数。\n\n类似于凸函数的第一个定义，拟凸函数也可以写成$f:R^n\\rightarrow R, dom f$为凸，$\\forall x,y \\in dom f, \\forall \\theta \\in [0,1]$，有$max(f(x), f(y)) \\ge f(\\theta x + (1-\\theta)y)$\n\n类似于凸函数的第三个定义，拟凸函数也可以写成若$f(x)$为拟凸函数，$\\forall x,y \\in domf, f(y) \\le f(x) \\Rightarrow \\triangledown^Tf(x)(y-x) \\le 0$\n\n类似于凸函数的二阶条件，拟凸函数可以写成若$f(x)$为拟凸函数，$y^T\\triangledown f(x) = 0 \\Rightarrow y^T \\triangledown^2f(x)y\\ge 0$\n\n# 凸优化问题（Convex Problems）\n目标函数是凸函数且约束集合是凸集的问题叫做凸优化问题。\n\n## 一般优化问题\n$$\n\\min f_0(x)\\\\\ns.t. f_i(x) \\le 0, i= 1, 2, ..., m\\\\\nh_j(x) = 0, j= 1,2,...,n\n$$\n其中$x\\in R^n$称为优化变量(Optimization variable)，$f_0$称为目标函数（损失函数）$f_i(x) \\le 0$称为不等式约束，$h_j(x) = 0$称为等式约束。如果$m=n=0$则称为无约束问题。\n\n## 优化问题的域：domain\n优化问题的定义域：$D = \\bigcap\\limits_{i=0}^m dom f_i \\cap \\bigcap\\limits_{j=1}^n dom h_j$\n\n## 可行解集：feasible set\n可行解集$X_f = \\{x|x \\in D, f_i(x) \\le 0,i=1,2,...,m,h_j(x) = 0, j= 1,2,...,n\\}$\n\n## 问题的最优值：optimal value\n$p^\\star = \\inf\\{f_0(x)|x \\in X_f\\}$，如果$X_f$是空集，则$p^\\star = +\\infty$\n\n## 最优解：optimal point/solution\n若$x^\\star$可行，且$f_0(x^\\star) = p^\\star$\n\n## 最优解集\n$X_{opt} = \\{x | x\\in X_f, f_0(x)=p^\\star\\}$\n\n## $\\epsilon$次优解集：$\\epsilon$-suboptimal set\n$X_\\epsilon = \\{x | x\\in X_f, f_0(x)) \\le p^\\star + \\epsilon\\}, \\epsilon \\ge 0$\n\n## 局部最优解：local optimal\n$\\exists R, f_0(x^\\star) = \\inf\\{f_0(z) | f_i(z) \\le 0, h_j(z) =0, ||z-x||\\le R\\}$则$x^\\star$是局部最优解\n\n## 可行性优化问题：feasibility Problems\n$\\min\\limits_x c, s.t. f_i(X) \\le 0, h_j(X) = 0$，其中$c$是任意一常数。\n\n## 凸优化问题\n$$\n\\min f_0(x)\\\\\ns.t. f_i(x) \\le 0, i= 1, 2, ..., m\\\\\na_j^Tx = b_j, j= 1,2,...,n\n$$\n其中$f_0$、$f_i$是凸函数，其可行解集是个凸集、。\n\n凸优化问题的局部最优解等于全局最优解（反证法）。\n\n## 可微目标函数情况下的最优解\n凸函数在可微的情况下：$f_0(y) \\ge f_0(x) + \\triangledown f_0^T(x)(y-x)$\n\n其最优解$x^\\star$满足$\\triangledown f_0^T(x^\\star)(y-x^\\star) \\ge 0, \\forall y \\in X_f$\n\n## 线性规划问题\n$$\n\\begin{aligned}\n\\min\\ &C^Tx +d\\\\\ns.t.\\ &Gx \\le h\\\\\n&Ax = b    \n\\end{aligned}\n$$\n\n其等式约束和不等式约束构成一个多面体集合。\n\n### 线性规划的等价变换\n上面的线性变换可以等价于：\n$$\n\\begin{aligned}\n    \\min &C^T x +d\\\\\n    s.t.\\ &Gx + S = h\\\\\n    &Ax = b\\\\\n    &S \\ge 0\n\\end{aligned}\n$$\n等价的判定条件：\n两个表达方式的可行解能够对应，且对应可行解的目标函数值相同。\n\n### 线性分数规划\n$$\n\\begin{aligned}\n    \\min\\ &f_0(x)\\\\\n    s.t.\\ &Gx \\le h\\\\\n    &Ax = b\n\\end{aligned}\n$$\n其中$f_0(x) = \\frac{C^Tx +d}{e^Tx + f}, domf = \\{x | e^T x + f \\ge 0\\}$\n\n这个问题是一个拟凸问题。\n\n如果该问题有可行解，则其有凸问题等价形式：\n$$\n\\begin{aligned}\n    \\min\\ &C^T y + dz\\\\\n    s.t.\\ &Gy-hz \\le 0\\\\\n    &Ay -bz = 0\\\\\n    &e^Ty+fz = 1\\\\\n    &z\\ge 0\n\\end{aligned}\n$$\n两个问题等价的证明：\n\n首先对于第一个问题任意可行的$x$，必须存在$y$和$z$对第二个问题可行，且$C^T y + dz = f_0(x)$，可以直接令$y = \\frac{x}{e^Tx +f}, z=\\frac{1}{e^Tx + f}$即证。\n\n齐次对于第二个问题，任意可行的$y$和$z$，必须存在$x$对第一个问题可行，且$C^T y + dz = f_0(x)$，如果$z>0$，则直接令$x=\\frac{y}{z}$, 若$z=0$，那么对于第一个问题的一个可行解$x_0$，$\\forall t\\ge 0, x = x_0 + ty$也对第一个问题可行，令$\\lim t \\rightarrow +\\infty$，则可以使两个目标函数值相等。\n\n## 二次规划(QP)\n$$\n\\begin{aligned}\n    \\min\\ &\\frac{1}{2} x^TPx + q^T x + r\\\\\n    s.t.\\ & Gx \\le h\\\\\n    &Ax = b\n\\end{aligned}\n$$\n其中$P\\in S_+^n$\n\n## 二次约束二次规划(QCQP)\n$$\n\\begin{aligned}\n    \\min\\ &\\frac{1}{2} x^TPx + q^T x + r\\\\\n    s.t.\\ & \\frac{1}{2} x^TP_ix + q_i^T x + r_i \\le 0,\\ i=1,2,...,m\\\\\n    &Ax = b\n\\end{aligned}\n$$\n其中$P\\in S_+^n$，$P_i \\in S_+^n, i =1,2,...,m$\n\n## 半正定规划（semi-Definite Programming）\n$$\n\\begin{aligned}\n    \\min\\ &tr(Cx)\\\\\n    s.t.\\ & tr(A_ix) = b_i, i=1,2,...,p\\\\\n    & x \\succeq 0\n\\end{aligned}\n$$\n其中$x\\in S_+^n, C \\in R^{n\\times n}, A_i \\in R^{n\\times n}, b_i \\in R$\n\n## 多目标优化问题\n\n$$\n\\begin{aligned}\n    \\min\\ &f_0(x):R^n \\rightarrow R^g\\\\\n    s.t.\\ &f_i(x) \\le 0, i= 1,2,...,m\\\\\n    &h_j(x) = 0, j= 1,2,...,p\\\\\n\\end{aligned}\n$$\n\n### Pareto optimal front\n一个点满足“如果找到其他可行解，可以使得在某些指标上比这个点更优，那么这个可行解必然在其他指标上比这个点更差”，则这个点属于Pareto optimal front。\n\n如果$f_0(x)$为凸，$f_i(x)$为凸，$h_j(x)$为仿射，则必可通过以下方式求得pareto optimal front中的一点：\n\n$$\n\\begin{aligned}\n    \\min\\ &\\sum\\limits_{i=1}^g \\lambda_i f_{0i}(x), \\lambda_i \\ge 0\\\\\n    s.t.\\ &f_i(x) \\le 0, i = 1,2,...,m\\\\\n    &h_j(x) = 0, j=1,2,...,p\n\\end{aligned}\n$$\n\n# 对偶性\n对于以下问题：\n$$\n\\begin{aligned}\n    \\min\\ &f_0(x)\\\\\n    s.t.\\ &f_i(x) \\le 0, i = 1,2,...,m\\\\\n    &h_j(x) = 0, j=1,2,...,p\n\\end{aligned}\n$$\n其定义域$D = \\{x| x \\in \\bigcap_{i=1}^m dom f_i \\cap \\bigcap_{j=1}^p dom h_j\\}$\n## 拉格朗日函数\n其拉格朗日函数定义为：$L(x, \\lambda, v) = f_0(x) + \\sum\\limits_{i=1}^m\\lambda_if_i(x) + \\sum\\limits_{j=1}^pv_jh_j(x)$，其中$\\lambda_i \\ge 0$和$v_j$称为拉格朗日乘子。\n\n## 拉格朗日对偶函数（对偶函数）\n$$\ng(\\lambda, v) = \\inf\\limits_{x\\in D}L(x, \\lambda ,v)\n$$\n\n- 对偶函数一定是个凹函数（因为函数对于$\\lambda$和$v$是线性的）。\n- $\\forall \\lambda \\ge 0, \\forall v, g(\\lambda,v) \\le p^\\star$,其中$p^\\star$表示原问题的最优值。\n\n## 对偶问题(Dual problem/lagrange Dual problem)\n对于原问题(Primal problem)（P）：\n$$\n\\begin{aligned}\n    \\min\\ &f_0(x)\\\\\n    s.t.\\ &f_i(x) \\le 0, i = 1,2,...,m\\\\\n    &h_j(x) = 0, j=1,2,...,p\n\\end{aligned}\n$$\n其最优解记为$p^\\star$\n\n其对偶问题（D）：\n$$\n\\begin{aligned}\n    \\max\\ &g(\\lambda, v)\\\\\n    s.t.\\ & \\lambda \\ge 0\n\\end{aligned}\n$$\n其中$g(\\lambda, v)$是原问题的拉格朗日对偶函数，对偶问题的最优解记为$d^\\star$\n\n有以下结论：\n- $d^\\star \\le p^\\star$（弱对偶）。\n- 对偶问题的最优解$\\lambda ^\\star, v^\\star$存在。\n- 一个凸问题的对偶问题的对偶问题就是自身。\n\n如果$d^\\star = p^\\star$，则称为强对偶。\n\n对偶间隙：$p^\\star - d^\\star$\n\n## 相对内部（Relative Interior）\n去掉集合的边缘，让集合变成一个开集，集合$D$的相对内部可以表示为：\n\n$Relint\\ D =\\{x| x \\in D, \\exists r\\>0, B(x,r) \\cap aff(D) \\le D\\}$\n\n其中$B(x,r)$是个球，$aff(D)$表示$D$的仿射包。\n\n## Slater's Condition\n对偶间隙为零的充分条件，但是不必要。\n\n如果凸问题：\n$$\n\\begin{aligned}\n    \\min\\ &f_0(x)\\\\\n    s.t.\\ &f_i(x) \\le 0, i=1,2,...,m\\\\\n    &Ax=b\n\\end{aligned}\n$$\n其定义域为$D$，对$\\forall i, f_i(x)$为凸，当$\\exists x \\in relint\\ D$使得$Ax=b, f_i(x) < 0, i =1,2,...,m$满足时，$d^\\star = p^\\star$\n\n## Weaken Slater's Condition\n如果不等式约束为仿射（$D = dom f_0$），只要可行域非空，则必有$d^\\star = p^\\star$\n\n## 鞍点（Saddle Point）\n对于函数$L(x, \\lambda)$，如果$\\inf\\limits_{x \\in D} \\sup\\limits_{\\lambda \\ge 0} L(x, \\lambda) = \\sup\\limits_{\\lambda \\ge 0} \\inf\\limits_{x \\in D} L(x, \\lambda)$，则称此时的$(x^\\star, \\lambda^\\star)$为鞍点。\n\n## 鞍点定理\n若$(\\tilde{x}, \\tilde{\\lambda})$为$L(x, \\lambda)$的鞍点$\\Leftrightarrow$强对偶存在，且$(\\tilde{x}, \\tilde{\\lambda})$为Primal与Dual的最优解。\n\n## KKT条件\n对于如下问题：\n$$\n\\begin{aligned}\n    \\min\\ &f_0(x)\\\\\n    s.t.\\ &f_i(x) \\le 0, i=1,2,...,m\\\\\n    &h_j(x) = 0, j=1,2,...,n\n\\end{aligned}\n$$\n其拉格朗日函数：$L(x, \\lambda, v) = f_0(x) + \\sum\\limits_{i=1}^m \\lambda_i f_i(x) + \\sum\\limits_{j=1}^n v_j h_j(x)$\n\n其对偶函数：$g(\\lambda, v) = \\inf\\limits_{x \\in D}L(x, \\lambda, v)$\n\n对偶问题：$\\sup\\limits_{\\lambda, v} g(\\lambda, v)$\n\n如果其满足对偶问题的最优解$d^\\star$和原问题的最优解$p^\\star$相等。\n即：\n$$\n\\begin{aligned}\n    f_0(x^\\star) &= g(\\lambda^\\star, v^\\star)\\\\\n    &=\\inf\\limits_{x \\in D} (f_0(x) + \\sum\\limits_{i=1}^m \\lambda^\\star_i f_i(x) + \\sum\\limits_{j=1}^n v^\\star_j h_j(x))\\\\\n    &\\le f_0(x^\\star) + \\sum\\limits_{i=1}^m \\lambda^\\star_i f_i(x^\\star) + \\sum\\limits_{j=1}^n v^\\star_j h_j(x^\\star)\\\\\n    &\\le f_0(x^\\star)\n\\end{aligned}\n$$\n\n这表示$f_0(x^\\star) = f_0(x^\\star) + \\sum\\limits_{i=1}^m \\lambda^\\star_i f_i(x^\\star) + \\sum\\limits_{j=1}^n v^\\star_j h_j(x^\\star) \\Rightarrow \\sum\\limits_{i=1}^m \\lambda^\\star_i f_i(x^\\star) = 0$，而$f_i(x^\\star) \\le 0，\\lambda_i \\ge 0, h_j(x^\\star) = 0$，因此可以得出$\\lambda^\\star_i f_i(x^\\star) = 0$，这就是互补松弛条件。\n\n又因为$x^\\star = \\mathop{\\arg\\inf}\\limits_{x \\in D} (f_0(x) + \\sum\\limits_{i=1}^m \\lambda^\\star_i f_i(x) + \\sum\\limits_{j=1}^n v^\\star_j h_j(x)) = \\mathop{\\arg\\inf}\\limits_{x \\in D} L(x, \\lambda^\\star, v^\\star)$（这里是因为上面的推导中，两个$\\le$可以改成$=$）,因此$x^\\star$是该问题的全局最优解。\n\n如果原问题中，所有函数都可微，则有：\n$$\n\\frac{\\partial L(x^\\star, \\lambda^\\star, v^\\star)}{\\partial x^\\star} = 0\n$$\n这就是稳定性条件。\n\n因此只要满足$p^\\star = d^\\star$，且所有函数可微，则有如下结论：\n- 互补松弛条件：$\\lambda^\\star_i f_i(x^\\star) = 0$\n- 稳定性条件：$\\frac{\\partial L(x^\\star, \\lambda^\\star, v^\\star)}{\\partial x^\\star} = 0$\n- 原问题可行：$f_i(x) \\le 0, i=1,2,...,m\\ \\ h_j(x) = 0, j=1,2,...,n$\n- 对偶问题可行：$\\lambda_i \\ge 0, i= 1,2,...,m$\n\n以上四个结论就是KKT条件。\n\n在对偶间隙为0且各个函数可微的情况下，KKT条件仅是最优解的必要条件，满足KKT条件的解，不一定是最优解。\n\n如果原问题为凸问题，各个函数可微，对偶间隙为0，则KKT条件是最优解的充分必要条件。\n\n充分性证明：如果$(x^\\star, \\lambda^\\star, v^\\star)$满足KKT条件，则必有$(x^\\star, \\lambda^\\star, v^\\star)$为最优解。证明$g(\\lambda ^\\star, v^\\star) = f_0(x^\\star)$即可。\n\n## 敏感性分析\n原问题$P$：\n$$\n\\begin{aligned}\n    \\min\\ &f_0(x)\\\\\n    s.t.\\ &f_i(x) \\le 0, i=1,2,...,m\\\\\n    &h_j(x) = 0,j=1,2,...,n\n\\end{aligned}\n$$\n干扰问题：\n$$\n\\begin{aligned}\n    \\min\\ &f_0(x)\\\\\n    s.t.\\ &f_i(x) \\le u_i, i=1,2,...,m\\\\\n    &h_j(x) = w_i,j=1,2,...,n\n\\end{aligned}\n$$\n\n将原问题的最优值$p^\\star$看做一个函数$p^\\star(u, w)$，原问题的最优值$p^\\star = p^\\star(0, 0)$\n\n有以下性质：\n\n- 若原问题为凸问题，则$p^\\star(u, w)$为关于$u$和$w$的凸函数。\n- 若原问题为凸，对偶间隙为0，$\\lambda^\\star$和$v^\\star$为对偶问题的最优解，则$p^\\star(u,w) \\le p^\\star(0, 0) -\\lambda^{\\star T}u - v^{\\star T}w$\n- 若原问题为凸，对偶间隙为0，且$p^\\star(u,w)$在$(0, 0)$点可微，则$\\lambda^\\star_i = -\\frac{\\partial p^\\star(0, 0)}{\\partial u_i}, v^\\star_j = -\\frac{\\partial p^\\star(0, 0)}{\\partial w_j}$, $p^\\star(u,w) = p^\\star(0, 0) -\\lambda^{\\star T}u - v^{\\star T}w$\n\n## 强凸性\n强凸性表示：$\\exists m > 0, \\forall x \\in dom f, \\triangledown^2f(x) \\succeq mI$\n\n强凸性也等价于：$\\exists m, \\forall x,y \\in dom f, f(y) \\ge f(x) + \\triangledown f^T(x)(y-x) + \\frac{1}{2}m||y-x||^2$\n\n如果$f(x)$二阶可微且有强凸性，则$\\min f(x)$的最优值$p^\\star$满足$f(x) - p^\\star \\le \\frac{1}{2m}||\\triangledown f(x)||_2^2$\n\n# 无约束优化问题的求解算法\n## 梯度下降法（Gradient Descent）\n每次沿着负梯度方向更新参数。\n\n### 梯度下降法的收敛性\n如果$\\exists M, m,MI \\succeq \\triangledown_2f\\succeq mI$，对于精确搜索的学习速率$\\alpha$，可以证明$f(x^{k+1}) -p^\\star \\le (1 - \\frac{m}{M})f(x^k - p^\\star)$（利用强凸性的性质证明）\n\n## 最速下降法（Steepest Descent）\n如果一阶可微，则可以找到一个方向$d = \\mathop{\\arg\\min}\\limits_v f(x+v) = \\mathop{\\arg\\min}\\limits_v \\triangledown f^T(x)v,\\ s.t.\\ ||v||_k = 1$（这里使用了一阶泰勒展开），其中的k-范数不一定是2-范数，所以这个方向不一定是梯度，梯度下降法是最速下降法在范数取2-范数情况下的一个特例。\n\n## 最速下降法的一些变种\n### 坐标轮换法（Coordinate Descent）\n每次选择一个基向量方向作为优化方向，但是需要在包含正值和负值的区间去搜索步长。\n### 分块坐标轮换法（Block Coordinate Descent）\n每次选择一组基向量的线性组合方向（一个子空间）作为优化方向。\n\n坐标轮换和分块坐标轮换适合于维度拆分之后损失函数比较简单的情况。\n\n## 次梯度方法\n如果$f(x)$在某些点不可微，那么定义次梯度的概念$\\frac{\\partial f}{\\partial x} = \\theta\\triangledown f(x)^+ + (1-\\theta)\\triangledown f(x)^-,\\ \\theta \\in [0, 1]$，即使用梯度的左极限和右极限的凸组合。如果在当前点，对于某个$\\theta$，次梯度可以为0，那么认为其到达极值点。\n\n## 牛顿法（Newton's Method）\n如果二阶可微，则使用二阶泰勒展开，找到一个方向$d = \\mathop{\\arg\\min}\\limits_v f(x+v) = \\mathop{\\arg\\min}\\limits_v \\triangledown f^T(x)v + \\frac{1}{2}v^T\\triangledown_2 f^T(x)v$，这里不需要关于$v$的约束项，是因为如果是个凸函数，则有唯一解的$v = -(\\triangledown_2 f^T(x))^{-1}\\triangledown f(x)$（也称为牛顿方向）。\n\n牛顿法在泰勒展开中的一次项接近于0的时候，就可以停止，即$\\triangledown f^T(x)v = -\\triangledown f^T(x)(\\triangledown_2 f^T(x))^{-1}\\triangledown f(x)$接近于0的时候就停止。\n\n### 牛顿法的收敛速度\n如果$||\\triangledown f(x)||_2$比较大，则泰勒展开的偏差较大，因此收敛速度比较慢，如果$||\\triangledown f(x)||_2$比较小，则收敛速度比梯度下降快很多。\n\n牛顿法的缺点在于求Hession矩阵可能非常难而且慢。\n\n## 拟牛顿法（Quasi-Newton Method）\n不求二阶偏导，根据拟牛顿条件，找到一个对Hession矩阵逆矩阵的近似矩阵来进行方向选择，例如BFGS、L-BFGS、DFP等方法。\n\n# 有约束优化问题的求解算法\n$$\n\\begin{aligned}\n    \\min\\ &f(x)\\\\\n    s.t.\\ &Ax = b\n\\end{aligned}\n$$\n在使用KKT条件的过程中，如果$\\triangledown f(x)$不是线性，那么存在求解困难。 \n\n首先变形原问题，使用迭代的方法，假设$x^k$满足约束$Ax^k = b$，那么下一次迭代问题变成：\n$$\n\\begin{aligned}\n    \\min\\limits_d\\ &f(x^k + d)\\\\\n    s.t.\\ &A(x^k +d) = b \\Rightarrow Ad = 0\n\\end{aligned}\n$$\n进行二阶泰勒展开，去掉高阶项，原问题近似等价于：\n$$\n\\begin{aligned}\n    \\min\\limits_d\\ &f(x^k) + \\triangledown f^T(x^k)d + \\frac{1}{2}d^T \\triangledown_2 f(x^k)d\\\\\n    s.t.\\ &Ad = 0\n\\end{aligned}\n$$\n\n## 拉格朗日法（Lagrangian Method）\n对于对偶间隙为零的问题，固定$v$去求$x$，然后固定$x$去求$v$，交替进行。\n$$\n\\begin{aligned}\n    x^{k+1} &= x^k - \\alpha^k(\\triangledown f(x^k) + A^Tv^k)\\\\\n    v^{k+1} &= v^k + \\alpha^k(Ax^k - b)\n\\end{aligned}\n$$\n\n## 增广拉格朗日法（Augmented Lagragian Method）\n$$\n\\begin{aligned}\n    \\min\\limits_x\\ &f(x) + \\frac{C}{2}||Ax -b||^2_2\\\\\n    s.t.\\ &Ax = b\n\\end{aligned}\n$$\n这个约束问题的拉格朗日函数（增广拉格朗日函数）：$L_C(x,v) = f(x) + v^T(Ax-b)+\\frac{C}{2}||Ax -b||^2_2$\n\n而且这个问题的原问题最优解和对偶问题最优解都和以下问题相同：\n$$\n\\begin{aligned}\n    \\min\\limits_x\\ &f(x)\\\\\n    s.t.\\ &Ax = b\n\\end{aligned}\n$$\n\n### 一些性质\n- 如果$v = v^\\star$，则$\\forall C > 0$，$x^\\star = \\mathop{\\arg\\min}\\limits_xL_C(x, v^\\star)$\n- 若$C \\rightarrow +\\infty$，则$\\forall v, x^\\star =\\mathop{\\arg\\min}\\limits_xL_C(x, v)$\n\n如果有$x^k$，和$v^k$，首先更新$x^{k+1} = \\mathop{\\arg\\min}\\limits_x L_C(x, v^k)$，然后更新$v^{k+1} = v^k + C \\triangledown_v L_C(x^{k+1}, v^k)$","slug":"学习笔记/凸优化学习笔记","published":1,"updated":"2020-08-31T06:39:20.763Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckjxpc3st003244mq00zk9xcn","content":"<h1 id=\"仿射集凸集锥集凸锥集仿射组合凸组合凸锥组合仿射包凸包凸锥包\">仿射集、凸集、锥集、凸锥集、仿射组合、凸组合、凸锥组合、仿射包、凸包、凸锥包</h1>\r\n<ul>\r\n<li>锥：<span class=\"math inline\">\\(C = \\{x| \\theta x \\in C\\}, x \\in R^n,\\theta \\ge 0\\)</span></li>\r\n<li>仿射组合：<span class=\"math inline\">\\(\\theta_1+\\theta_2+...=1\\)</span></li>\r\n<li>凸组合：<span class=\"math inline\">\\(\\theta_1+\\theta_2+...=1,\\theta_1+\\theta_2+... \\ge 0\\)</span></li>\r\n<li>凸锥组合：<span class=\"math inline\">\\(\\theta_1+\\theta_2+... \\ge 0\\)</span></li>\r\n</ul>\r\n<h1 id=\"一些凸集超平面半空间球椭球多面体单纯形对称半正定矩阵\">一些凸集：超平面、半空间、球、椭球、多面体、单纯形、对称（半）（正定）矩阵</h1>\r\n<ul>\r\n<li>超平面：<span class=\"math inline\">\\(\\{x|w^Tx=b\\},x \\in R^n,w \\in R^n, b \\in R, w \\ne \\mathbb{0}\\)</span></li>\r\n<li>半空间：<span class=\"math inline\">\\(\\{x|w^Tx \\ge b\\},x \\in R^n,w \\in R^n, b \\in R, w \\ne \\mathbb{0}\\)</span>或<span class=\"math inline\">\\(\\{x|w^Tx \\le b\\},x \\in R^n,w \\in R^n, b \\in R, w \\ne \\mathbb{0}\\)</span></li>\r\n<li>球：<span class=\"math inline\">\\(B(x_c, r) = \\{x |\\ ||x-x_c||_2 \\le r\\}, x_c \\in R^n, x \\in R^n, r \\ge 0\\)</span></li>\r\n<li>椭球：<span class=\"math inline\">\\(\\epsilon(x_c, P) = \\{x |\\ (x-x_c)^T P^{-1} (x-x_c) \\le 1\\}, x_c \\in R^n, x \\in R^n, P \\in S_{++}^n\\)</span></li>\r\n<li>多面体：<span class=\"math inline\">\\(P=\\{x|a_i^T x \\le b_i, c_j^T x = d_j, i = 1,2,...,m, j=1,2,...,n\\}\\)</span></li>\r\n<li>单纯形：<span class=\"math inline\">\\(R^n\\)</span>中的<span class=\"math inline\">\\(k+1\\)</span>个点<span class=\"math inline\">\\(v_0, v_1, ..., v_k\\)</span>，满足<span class=\"math inline\">\\(v_1-v_0, v_2-v_0, ..., v_k-v_0\\)</span>线性无关，则<span class=\"math inline\">\\(v_0, v_1, ..., v_k\\)</span>的单纯形为<span class=\"math inline\">\\(v_0, v_1, ..., v_k\\)</span>的凸包，单纯形是多面体的一种，证明的时候，可以借助\"秩为<span class=\"math inline\">\\(k\\)</span>的矩阵<span class=\"math inline\">\\(B\\)</span>,可以用一个非奇异矩阵<span class=\"math inline\">\\(A\\)</span>转换成<span class=\"math inline\">\\(\\begin{bmatrix}I_k \\\\ \\mathbb{0} \\end{bmatrix}\\)</span>的形式\"这个定理。</li>\r\n</ul>\r\n<h1 id=\"仿射函数透视函数线性分数函数\">仿射函数、透视函数、线性分数函数</h1>\r\n<ul>\r\n<li><p>仿射函数：<span class=\"math inline\">\\(f:R^n \\rightarrow R^m, f(x)=Ax+b, A \\in R^{(m \\times n)}, b \\in R^n\\)</span></p></li>\r\n<li><p>缩放：<span class=\"math inline\">\\(\\alpha S = \\{\\alpha x| x \\in S\\}\\)</span>、移位：<span class=\"math inline\">\\(S+a=\\{x+a|x \\in S\\}\\)</span>是仿射变换的一种。</p></li>\r\n<li><p>线性矩阵不等式的解集是凸集：<span class=\"math inline\">\\(\\{x|A(x) \\preceq B\\}\\)</span></p></li>\r\n<li><p>透视函数：<span class=\"math inline\">\\(P: R^{n+1} \\rightarrow R^n, P(x,y) = \\{\\frac{x}{y}\\}\\ x \\in R^n, y\\in R_{++}\\)</span></p></li>\r\n</ul>\r\n<h1 id=\"保凸集运算\">保凸集运算：</h1>\r\n<ul>\r\n<li>交集</li>\r\n<li>集合的和：<span class=\"math inline\">\\(S_1 + S_2 = \\{x_1 + x_2 | x_1 \\in S_1, x_2 \\in S_2\\}\\)</span></li>\r\n<li>集合的笛卡尔积：<span class=\"math inline\">\\(S_1 \\times S_2 = \\{(x_1 , x_2) | x_1 \\in S_1, x_2 \\in S_2\\}\\)</span></li>\r\n<li>（逆）仿射函数</li>\r\n<li>透视函数</li>\r\n<li>线性分数函数：首先定义仿射函数<span class=\"math inline\">\\(g:R^n \\rightarrow R^{m+1}, g(x)=\\begin{bmatrix} A \\\\ c^T \\end{bmatrix} x + \\begin{bmatrix} b \\\\ d \\end{bmatrix}, A \\in R^{m \\times n}, c \\in R^n, b \\in R^m, d \\in R\\)</span>，再定义透视函数<span class=\"math inline\">\\(p:R^{m+1} \\rightarrow R^m\\)</span>，则线性分数函数<span class=\"math inline\">\\(f = p \\circ g = \\frac{Ax + b}{c^T x + d},\\ dom f = \\{x| c^T x + d &gt; 0\\}\\)</span></li>\r\n</ul>\r\n<h1 id=\"凸函数\">凸函数</h1>\r\n<h2 id=\"凸函数定义\">凸函数定义</h2>\r\n<ul>\r\n<li><p><span class=\"math inline\">\\(f:R^n \\rightarrow R\\)</span>为凸函数<span class=\"math inline\">\\(\\Leftrightarrow\\)</span><span class=\"math inline\">\\(domf\\)</span>为凸集，且<span class=\"math inline\">\\(f(\\theta x_1 + (1-\\theta)x_2) \\le \\theta f(x_1) + (1-\\theta)f(x_2)\\)</span>，其中<span class=\"math inline\">\\(\\theta \\in [0, 1]\\)</span></p></li>\r\n<li><p><span class=\"math inline\">\\(f:R^n \\rightarrow R\\)</span>为凸函数<span class=\"math inline\">\\(\\Leftrightarrow\\)</span><span class=\"math inline\">\\(domf\\)</span>为凸集，且<span class=\"math inline\">\\(\\forall x \\in dom f, \\forall v, g(t) = f(x + tv)\\)</span>为凸，<span class=\"math inline\">\\(dom g = \\{t | x + tv \\in dom f\\}\\)</span></p></li>\r\n<li><p><span class=\"math inline\">\\(f:R^n \\rightarrow R\\)</span>为凸函数且<span class=\"math inline\">\\(f\\)</span>可微<span class=\"math inline\">\\(\\Leftrightarrow\\)</span><span class=\"math inline\">\\(domf\\)</span>为凸集，<span class=\"math inline\">\\(\\forall x,y \\in dom f, f(y) \\ge f(x) + \\triangledown f^T(x)(y-x)\\)</span></p></li>\r\n<li><p><span class=\"math inline\">\\(f:R^n \\rightarrow R\\)</span>为凸函数且<span class=\"math inline\">\\(f\\)</span>二阶可微<span class=\"math inline\">\\(\\Leftrightarrow\\)</span><span class=\"math inline\">\\(domf\\)</span>为凸集，<span class=\"math inline\">\\(\\forall x \\in dom f,\\triangledown ^2 f(x) \\succeq 0\\)</span>（二阶条件）</p></li>\r\n</ul>\r\n<p>如果将上述前三个定义中的<span class=\"math inline\">\\(\\le\\)</span>、<span class=\"math inline\">\\(\\ge\\)</span>, 改为<span class=\"math inline\">\\(&lt;\\)</span>、<span class=\"math inline\">\\(&gt;\\)</span>，那么<span class=\"math inline\">\\(f\\)</span>就变成了严格凸函数，严格凸函数的二阶导数不能保证<span class=\"math inline\">\\(\\triangledown ^2 f(x) \\succ 0\\)</span>，但<span class=\"math inline\">\\(\\triangledown ^2 f(x) \\succ 0\\)</span>可以保证严格凸，但是二次函数<span class=\"math inline\">\\(f\\)</span>严格凸<span class=\"math inline\">\\(\\Leftrightarrow\\)</span>Hession矩阵<span class=\"math inline\">\\(\\triangledown ^2 f(x) \\in S^n_{++}\\)</span>即<span class=\"math inline\">\\(\\triangledown ^2 f(x) \\succeq 0\\)</span>。</p>\r\n<h2 id=\"凸函数的一个重要性质\">凸函数的一个重要性质</h2>\r\n<p><span class=\"math inline\">\\(f(x) \\ge f(x_0) + f&#39; (x_0) ^ T(x-x_0)\\)</span></p>\r\n<h2 id=\"一些常见的凸函数\">一些常见的凸函数</h2>\r\n<ul>\r\n<li>仿射函数：<span class=\"math inline\">\\(f(x)=Ax+b, \\triangledown_2f=0\\)</span>，是凸函数也是凹函数</li>\r\n<li>指数函数：<span class=\"math inline\">\\(f(x) = e^{ax}, x \\in R\\)</span></li>\r\n<li>幂函数：<span class=\"math inline\">\\(f(x) = x^a, x \\in R_{++}, a \\ge 1\\ or\\ a \\le 0\\)</span>是凸函数，当<span class=\"math inline\">\\(0 \\le a \\le 1\\)</span>时，是凹函数</li>\r\n<li>绝对值的幂函数：<span class=\"math inline\">\\(f(x)= |x|^p, x \\in R, x \\ne 0, P \\ge 1\\)</span></li>\r\n<li>对数函数：<span class=\"math inline\">\\(f(x) = -log(x), x \\in R_{++}\\)</span></li>\r\n<li>负熵：<span class=\"math inline\">\\(f(x) = xlog(x), x \\in R_{++}\\)</span></li>\r\n<li>范数：<span class=\"math inline\">\\(p(x), x \\in R^n\\)</span>，满足<span class=\"math inline\">\\(p(a)=|a|p(x), p(x+y) \\le p(x) + p(y), p(x) = 0 \\Leftrightarrow x = 0\\)</span></li>\r\n<li>极大值函数：<span class=\"math inline\">\\(f(x) = max(x_1, x_2, ..., x_n), x \\in R^n\\)</span></li>\r\n<li>极大值函数的逼近-log sum up：<span class=\"math inline\">\\(f(x) = log(e^{x_1} + e^{x_2} + ... +e^{x_n}), x \\in R^n\\)</span>其中<span class=\"math inline\">\\(max(x_1, x_2, ..., x_n) \\le f(x) \\le max(x_1, x_2, ..., x_n) + log(n)\\)</span></li>\r\n</ul>\r\n<h2 id=\"二次型\">二次型</h2>\r\n<p>二次型是一种二次齐次函数的统称，可以表示为<span class=\"math inline\">\\(f(x_1, x_2, ..., x_n) = a_{11}x_1^2 + a_{22}x_2^2 + ... + a_{nn}x_n^2 + 2a_{12}x_1x_2 + 2a_{13}x_1x_3 + ... + + 2a_{n-1, n}x_1x_n\\)</span></p>\r\n<p>二次型可以通过一个对称矩阵表示<span class=\"math inline\">\\(f(x_1, x_2, ..., x_n)=X^T A X\\)</span>，其中A因为是对称矩阵，有很多很好的分解方式，因此非常方便分析。</p>\r\n<h2 id=\"一些常用性质\">一些常用性质</h2>\r\n<ul>\r\n<li><span class=\"math inline\">\\(V^TXV \\ge 0 \\Leftrightarrow X \\succeq 0\\)</span></li>\r\n<li><span class=\"math inline\">\\(det(XY) = det(X)det(Y)\\)</span>，<span class=\"math inline\">\\(det\\)</span>表示求行列式值。</li>\r\n<li><span class=\"math inline\">\\((a^Ta)(b^Tb) \\ge (a^Tb)^2\\)</span></li>\r\n</ul>\r\n<h2 id=\"保凸函数运算\">保凸函数运算</h2>\r\n<ul>\r\n<li><span class=\"math inline\">\\(f_1, f_2, ..., f_m\\)</span>为凸，则<span class=\"math inline\">\\(f = \\sum\\limits_{i=1}^m w_i f_i,\\ \\forall i,w_i \\ge 0\\)</span>为凸</li>\r\n<li>若<span class=\"math inline\">\\(f(x,y),\\ \\forall y \\in A,f(x,y)\\)</span>为凸，对于<span class=\"math inline\">\\(\\forall y \\in A,\\ w(y) \\ge 0, g(x) = \\int_{y \\in A}w(y)f(x,y)dy\\)</span>为凸</li>\r\n<li>仿射映射<span class=\"math inline\">\\(f:R^n \\rightarrow R, A\\in R^{n \\times m}, b\\in R^n,g(x) = f(Ax + b), dom g=\\{x|Ax+b \\in dom f\\}\\)</span>是凸函数。</li>\r\n<li>对函数的仿射：<span class=\"math inline\">\\(f_i: R^n \\rightarrow R,i=1,...,m\\)</span>为凸函数，<span class=\"math inline\">\\(A \\in R_+^n, b \\in R, g(x)=A^T\\begin{bmatrix}f_1(x)&amp; \\cdots&amp;f_n(x)\\end{bmatrix}^T + b\\)</span>为凸函数</li>\r\n<li>极大值操作：<span class=\"math inline\">\\(f_1,f_2\\)</span>为凸，则<span class=\"math inline\">\\(f(x) = max(f_1(x), f_2(x)), dom f = domf_1 \\bigcap domf_2\\)</span>为凸。</li>\r\n<li>无限个凸函数的极大值：<span class=\"math inline\">\\(f(x,y),\\forall y \\in A\\)</span>对于<span class=\"math inline\">\\(x\\)</span>为凸，则<span class=\"math inline\">\\(g = \\sup\\limits_{y \\in A}f(x,y)\\)</span>为凸函数，其中<span class=\"math inline\">\\(\\sup\\limits_{y\\in A}\\)</span>表示<span class=\"math inline\">\\(y\\in A\\)</span>时的上确界（可以理解为最大值）。</li>\r\n<li>函数的组合也可以保证函数凸性：<span class=\"math inline\">\\(h:R^k \\rightarrow R, g: R^n \\rightarrow R^k,f=h \\circ g, R^n \\rightarrow R, domf = \\{x|x\\in dom g,g(x)\\in dom h\\}\\)</span>，1、若<span class=\"math inline\">\\(h\\)</span>为凸，<span class=\"math inline\">\\(\\tilde{h}\\)</span>不降，<span class=\"math inline\">\\(g\\)</span>为凸，则<span class=\"math inline\">\\(f\\)</span>为凸，2、若<span class=\"math inline\">\\(h\\)</span>为凸，<span class=\"math inline\">\\(\\tilde{h}\\)</span>不增，<span class=\"math inline\">\\(g\\)</span>为凹，则<span class=\"math inline\">\\(f\\)</span>为凸，3、若<span class=\"math inline\">\\(h\\)</span>为凹，<span class=\"math inline\">\\(\\tilde{h}\\)</span>不降，<span class=\"math inline\">\\(g\\)</span>为凹，则<span class=\"math inline\">\\(f\\)</span>为凹，4、若<span class=\"math inline\">\\(h\\)</span>为凹，<span class=\"math inline\">\\(\\tilde{h}\\)</span>不增，<span class=\"math inline\">\\(g\\)</span>为凸，则<span class=\"math inline\">\\(f\\)</span>为凹，其中<span class=\"math inline\">\\(\\tilde{h}\\)</span>为<span class=\"math inline\">\\(h\\)</span>的定义域扩展。</li>\r\n</ul>\r\n<p>向量的最大<span class=\"math inline\">\\(r\\)</span>个分量之和：<span class=\"math inline\">\\(f(x) = max\\{x_{i1}+x_{i2}\\cdots+x_{ir}|i1,i2,...,ir互不相等\\}\\)</span>是个凸函数。</p>\r\n<p>实对称矩阵的最大特征值函数<span class=\"math inline\">\\(f(X) = \\lambda_{max}(X)\\)</span>是个凸函数。</p>\r\n<h2 id=\"函数的透视\">函数的透视</h2>\r\n<p><span class=\"math inline\">\\(f:R^n \\rightarrow R，g: R^n \\times R^n_{++} \\rightarrow R, g(u, v) = v^Tf(u \\odot \\frac{1}{v}), dom g = \\{(u,v)|v \\in R^n_{++}, u \\odot \\frac{1}{v} \\in dom f\\}\\)</span>，<span class=\"math inline\">\\(f\\)</span>为凸<span class=\"math inline\">\\(\\Rightarrow g\\)</span>为凸，<span class=\"math inline\">\\(f\\)</span>为凹<span class=\"math inline\">\\(\\Rightarrow g\\)</span>为凹，且是联合凸和联合凹。</p>\r\n<h2 id=\"函数的共轭\">函数的共轭</h2>\r\n<p><span class=\"math inline\">\\(f:R^n \\rightarrow R, f^\\star:R^n\\rightarrow R, f^\\star(y)= \\sup\\limits_{x \\in domf}(y^Tx - f(x))\\)</span>。若<span class=\"math inline\">\\(f\\)</span>可微，则<span class=\"math inline\">\\(f^\\star(y)=\\sup\\limits_{x \\in domf}(y^Tx&#39; - f(x&#39;)), f&#39;(x&#39;) = y\\)</span>。对任意情况，<span class=\"math inline\">\\(f^\\star(y)\\)</span>一定是凸函数。</p>\r\n<h1 id=\"凸集和凸函数的关系\">凸集和凸函数的关系</h1>\r\n<h2 id=\"alpha-sublevel-set\"><span class=\"math inline\">\\(\\alpha\\)</span>-sublevel set</h2>\r\n<p>对于函数<span class=\"math inline\">\\(f:R^n \\rightarrow R\\)</span>定义其<span class=\"math inline\">\\(\\alpha\\)</span>-sublevel set为<span class=\"math inline\">\\(C_\\alpha=\\{x\\in domf|f(x)\\le \\alpha\\}\\)</span></p>\r\n<p>对于函数<span class=\"math inline\">\\(f:R^n \\rightarrow R\\)</span>定义其<span class=\"math inline\">\\(\\alpha\\)</span>-suplevel set为<span class=\"math inline\">\\(C_\\alpha=\\{x\\in domf|f(x)\\ge \\alpha\\}\\)</span></p>\r\n<p>凸函数所有的<span class=\"math inline\">\\(\\alpha\\)</span>-sublevel set都是凸集。</p>\r\n<p>但是函数的<span class=\"math inline\">\\(\\alpha\\)</span>-sublevel set都是凸集，不能保证该函数是凸函数。</p>\r\n<h1 id=\"quasi-convexconcave-function拟凸凹函数\">Quasi Convex（Concave） function（拟凸（凹）函数）</h1>\r\n<p><span class=\"math inline\">\\(S_\\alpha&#39; = \\{x|x in dom f,f(x) \\le \\alpha\\}\\)</span>（<span class=\"math inline\">\\(\\alpha\\)</span>-sublevel set）都是凸集，则这样的函数<span class=\"math inline\">\\(f(x)\\)</span>称为拟凸函数（unimodal function，单模态函数）。</p>\r\n<p><span class=\"math inline\">\\(S_\\alpha&#39; = \\{x|x in dom f,f(x) \\ge \\alpha\\}\\)</span>（<span class=\"math inline\">\\(\\alpha\\)</span>-suplevel set）都是凸集，则这样的函数<span class=\"math inline\">\\(f(x)\\)</span>称为拟凹函数。</p>\r\n<p><span class=\"math inline\">\\(S_\\alpha&#39; = \\{x|x in dom f,f(x) = \\alpha\\}\\)</span>都是凸集，则这样的函数<span class=\"math inline\">\\(f(x)\\)</span>称为拟线性函数。</p>\r\n<p>类似于凸函数的第一个定义，拟凸函数也可以写成<span class=\"math inline\">\\(f:R^n\\rightarrow R, dom f\\)</span>为凸，<span class=\"math inline\">\\(\\forall x,y \\in dom f, \\forall \\theta \\in [0,1]\\)</span>，有<span class=\"math inline\">\\(max(f(x), f(y)) \\ge f(\\theta x + (1-\\theta)y)\\)</span></p>\r\n<p>类似于凸函数的第三个定义，拟凸函数也可以写成若<span class=\"math inline\">\\(f(x)\\)</span>为拟凸函数，<span class=\"math inline\">\\(\\forall x,y \\in domf, f(y) \\le f(x) \\Rightarrow \\triangledown^Tf(x)(y-x) \\le 0\\)</span></p>\r\n<p>类似于凸函数的二阶条件，拟凸函数可以写成若<span class=\"math inline\">\\(f(x)\\)</span>为拟凸函数，<span class=\"math inline\">\\(y^T\\triangledown f(x) = 0 \\Rightarrow y^T \\triangledown^2f(x)y\\ge 0\\)</span></p>\r\n<h1 id=\"凸优化问题convex-problems\">凸优化问题（Convex Problems）</h1>\r\n<p>目标函数是凸函数且约束集合是凸集的问题叫做凸优化问题。</p>\r\n<h2 id=\"一般优化问题\">一般优化问题</h2>\r\n<p><span class=\"math display\">\\[\r\n\\min f_0(x)\\\\\r\ns.t. f_i(x) \\le 0, i= 1, 2, ..., m\\\\\r\nh_j(x) = 0, j= 1,2,...,n\r\n\\]</span> 其中<span class=\"math inline\">\\(x\\in R^n\\)</span>称为优化变量(Optimization variable)，<span class=\"math inline\">\\(f_0\\)</span>称为目标函数（损失函数）<span class=\"math inline\">\\(f_i(x) \\le 0\\)</span>称为不等式约束，<span class=\"math inline\">\\(h_j(x) = 0\\)</span>称为等式约束。如果<span class=\"math inline\">\\(m=n=0\\)</span>则称为无约束问题。</p>\r\n<h2 id=\"优化问题的域domain\">优化问题的域：domain</h2>\r\n<p>优化问题的定义域：<span class=\"math inline\">\\(D = \\bigcap\\limits_{i=0}^m dom f_i \\cap \\bigcap\\limits_{j=1}^n dom h_j\\)</span></p>\r\n<h2 id=\"可行解集feasible-set\">可行解集：feasible set</h2>\r\n<p>可行解集<span class=\"math inline\">\\(X_f = \\{x|x \\in D, f_i(x) \\le 0,i=1,2,...,m,h_j(x) = 0, j= 1,2,...,n\\}\\)</span></p>\r\n<h2 id=\"问题的最优值optimal-value\">问题的最优值：optimal value</h2>\r\n<p><span class=\"math inline\">\\(p^\\star = \\inf\\{f_0(x)|x \\in X_f\\}\\)</span>，如果<span class=\"math inline\">\\(X_f\\)</span>是空集，则<span class=\"math inline\">\\(p^\\star = +\\infty\\)</span></p>\r\n<h2 id=\"最优解optimal-pointsolution\">最优解：optimal point/solution</h2>\r\n<p>若<span class=\"math inline\">\\(x^\\star\\)</span>可行，且<span class=\"math inline\">\\(f_0(x^\\star) = p^\\star\\)</span></p>\r\n<h2 id=\"最优解集\">最优解集</h2>\r\n<p><span class=\"math inline\">\\(X_{opt} = \\{x | x\\in X_f, f_0(x)=p^\\star\\}\\)</span></p>\r\n<h2 id=\"epsilon次优解集epsilon-suboptimal-set\"><span class=\"math inline\">\\(\\epsilon\\)</span>次优解集：<span class=\"math inline\">\\(\\epsilon\\)</span>-suboptimal set</h2>\r\n<p><span class=\"math inline\">\\(X_\\epsilon = \\{x | x\\in X_f, f_0(x)) \\le p^\\star + \\epsilon\\}, \\epsilon \\ge 0\\)</span></p>\r\n<h2 id=\"局部最优解local-optimal\">局部最优解：local optimal</h2>\r\n<p><span class=\"math inline\">\\(\\exists R, f_0(x^\\star) = \\inf\\{f_0(z) | f_i(z) \\le 0, h_j(z) =0, ||z-x||\\le R\\}\\)</span>则<span class=\"math inline\">\\(x^\\star\\)</span>是局部最优解</p>\r\n<h2 id=\"可行性优化问题feasibility-problems\">可行性优化问题：feasibility Problems</h2>\r\n<p><span class=\"math inline\">\\(\\min\\limits_x c, s.t. f_i(X) \\le 0, h_j(X) = 0\\)</span>，其中<span class=\"math inline\">\\(c\\)</span>是任意一常数。</p>\r\n<h2 id=\"凸优化问题\">凸优化问题</h2>\r\n<p><span class=\"math display\">\\[\r\n\\min f_0(x)\\\\\r\ns.t. f_i(x) \\le 0, i= 1, 2, ..., m\\\\\r\na_j^Tx = b_j, j= 1,2,...,n\r\n\\]</span> 其中<span class=\"math inline\">\\(f_0\\)</span>、<span class=\"math inline\">\\(f_i\\)</span>是凸函数，其可行解集是个凸集、。</p>\r\n<p>凸优化问题的局部最优解等于全局最优解（反证法）。</p>\r\n<h2 id=\"可微目标函数情况下的最优解\">可微目标函数情况下的最优解</h2>\r\n<p>凸函数在可微的情况下：<span class=\"math inline\">\\(f_0(y) \\ge f_0(x) + \\triangledown f_0^T(x)(y-x)\\)</span></p>\r\n<p>其最优解<span class=\"math inline\">\\(x^\\star\\)</span>满足<span class=\"math inline\">\\(\\triangledown f_0^T(x^\\star)(y-x^\\star) \\ge 0, \\forall y \\in X_f\\)</span></p>\r\n<h2 id=\"线性规划问题\">线性规划问题</h2>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n\\min\\ &amp;C^Tx +d\\\\\r\ns.t.\\ &amp;Gx \\le h\\\\\r\n&amp;Ax = b    \r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>其等式约束和不等式约束构成一个多面体集合。</p>\r\n<h3 id=\"线性规划的等价变换\">线性规划的等价变换</h3>\r\n<p>上面的线性变换可以等价于： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\min &amp;C^T x +d\\\\\r\n    s.t.\\ &amp;Gx + S = h\\\\\r\n    &amp;Ax = b\\\\\r\n    &amp;S \\ge 0\r\n\\end{aligned}\r\n\\]</span> 等价的判定条件： 两个表达方式的可行解能够对应，且对应可行解的目标函数值相同。</p>\r\n<h3 id=\"线性分数规划\">线性分数规划</h3>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\min\\ &amp;f_0(x)\\\\\r\n    s.t.\\ &amp;Gx \\le h\\\\\r\n    &amp;Ax = b\r\n\\end{aligned}\r\n\\]</span> 其中<span class=\"math inline\">\\(f_0(x) = \\frac{C^Tx +d}{e^Tx + f}, domf = \\{x | e^T x + f \\ge 0\\}\\)</span></p>\r\n<p>这个问题是一个拟凸问题。</p>\r\n<p>如果该问题有可行解，则其有凸问题等价形式： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\min\\ &amp;C^T y + dz\\\\\r\n    s.t.\\ &amp;Gy-hz \\le 0\\\\\r\n    &amp;Ay -bz = 0\\\\\r\n    &amp;e^Ty+fz = 1\\\\\r\n    &amp;z\\ge 0\r\n\\end{aligned}\r\n\\]</span> 两个问题等价的证明：</p>\r\n<p>首先对于第一个问题任意可行的<span class=\"math inline\">\\(x\\)</span>，必须存在<span class=\"math inline\">\\(y\\)</span>和<span class=\"math inline\">\\(z\\)</span>对第二个问题可行，且<span class=\"math inline\">\\(C^T y + dz = f_0(x)\\)</span>，可以直接令<span class=\"math inline\">\\(y = \\frac{x}{e^Tx +f}, z=\\frac{1}{e^Tx + f}\\)</span>即证。</p>\r\n<p>齐次对于第二个问题，任意可行的<span class=\"math inline\">\\(y\\)</span>和<span class=\"math inline\">\\(z\\)</span>，必须存在<span class=\"math inline\">\\(x\\)</span>对第一个问题可行，且<span class=\"math inline\">\\(C^T y + dz = f_0(x)\\)</span>，如果<span class=\"math inline\">\\(z&gt;0\\)</span>，则直接令<span class=\"math inline\">\\(x=\\frac{y}{z}\\)</span>, 若<span class=\"math inline\">\\(z=0\\)</span>，那么对于第一个问题的一个可行解<span class=\"math inline\">\\(x_0\\)</span>，<span class=\"math inline\">\\(\\forall t\\ge 0, x = x_0 + ty\\)</span>也对第一个问题可行，令<span class=\"math inline\">\\(\\lim t \\rightarrow +\\infty\\)</span>，则可以使两个目标函数值相等。</p>\r\n<h2 id=\"二次规划qp\">二次规划(QP)</h2>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\min\\ &amp;\\frac{1}{2} x^TPx + q^T x + r\\\\\r\n    s.t.\\ &amp; Gx \\le h\\\\\r\n    &amp;Ax = b\r\n\\end{aligned}\r\n\\]</span> 其中<span class=\"math inline\">\\(P\\in S_+^n\\)</span></p>\r\n<h2 id=\"二次约束二次规划qcqp\">二次约束二次规划(QCQP)</h2>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\min\\ &amp;\\frac{1}{2} x^TPx + q^T x + r\\\\\r\n    s.t.\\ &amp; \\frac{1}{2} x^TP_ix + q_i^T x + r_i \\le 0,\\ i=1,2,...,m\\\\\r\n    &amp;Ax = b\r\n\\end{aligned}\r\n\\]</span> 其中<span class=\"math inline\">\\(P\\in S_+^n\\)</span>，<span class=\"math inline\">\\(P_i \\in S_+^n, i =1,2,...,m\\)</span></p>\r\n<h2 id=\"半正定规划semi-definite-programming\">半正定规划（semi-Definite Programming）</h2>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\min\\ &amp;tr(Cx)\\\\\r\n    s.t.\\ &amp; tr(A_ix) = b_i, i=1,2,...,p\\\\\r\n    &amp; x \\succeq 0\r\n\\end{aligned}\r\n\\]</span> 其中<span class=\"math inline\">\\(x\\in S_+^n, C \\in R^{n\\times n}, A_i \\in R^{n\\times n}, b_i \\in R\\)</span></p>\r\n<h2 id=\"多目标优化问题\">多目标优化问题</h2>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\min\\ &amp;f_0(x):R^n \\rightarrow R^g\\\\\r\n    s.t.\\ &amp;f_i(x) \\le 0, i= 1,2,...,m\\\\\r\n    &amp;h_j(x) = 0, j= 1,2,...,p\\\\\r\n\\end{aligned}\r\n\\]</span></p>\r\n<h3 id=\"pareto-optimal-front\">Pareto optimal front</h3>\r\n<p>一个点满足“如果找到其他可行解，可以使得在某些指标上比这个点更优，那么这个可行解必然在其他指标上比这个点更差”，则这个点属于Pareto optimal front。</p>\r\n<p>如果<span class=\"math inline\">\\(f_0(x)\\)</span>为凸，<span class=\"math inline\">\\(f_i(x)\\)</span>为凸，<span class=\"math inline\">\\(h_j(x)\\)</span>为仿射，则必可通过以下方式求得pareto optimal front中的一点：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\min\\ &amp;\\sum\\limits_{i=1}^g \\lambda_i f_{0i}(x), \\lambda_i \\ge 0\\\\\r\n    s.t.\\ &amp;f_i(x) \\le 0, i = 1,2,...,m\\\\\r\n    &amp;h_j(x) = 0, j=1,2,...,p\r\n\\end{aligned}\r\n\\]</span></p>\r\n<h1 id=\"对偶性\">对偶性</h1>\r\n<p>对于以下问题： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\min\\ &amp;f_0(x)\\\\\r\n    s.t.\\ &amp;f_i(x) \\le 0, i = 1,2,...,m\\\\\r\n    &amp;h_j(x) = 0, j=1,2,...,p\r\n\\end{aligned}\r\n\\]</span> 其定义域<span class=\"math inline\">\\(D = \\{x| x \\in \\bigcap_{i=1}^m dom f_i \\cap \\bigcap_{j=1}^p dom h_j\\}\\)</span> ## 拉格朗日函数 其拉格朗日函数定义为：<span class=\"math inline\">\\(L(x, \\lambda, v) = f_0(x) + \\sum\\limits_{i=1}^m\\lambda_if_i(x) + \\sum\\limits_{j=1}^pv_jh_j(x)\\)</span>，其中<span class=\"math inline\">\\(\\lambda_i \\ge 0\\)</span>和<span class=\"math inline\">\\(v_j\\)</span>称为拉格朗日乘子。</p>\r\n<h2 id=\"拉格朗日对偶函数对偶函数\">拉格朗日对偶函数（对偶函数）</h2>\r\n<p><span class=\"math display\">\\[\r\ng(\\lambda, v) = \\inf\\limits_{x\\in D}L(x, \\lambda ,v)\r\n\\]</span></p>\r\n<ul>\r\n<li>对偶函数一定是个凹函数（因为函数对于<span class=\"math inline\">\\(\\lambda\\)</span>和<span class=\"math inline\">\\(v\\)</span>是线性的）。</li>\r\n<li><span class=\"math inline\">\\(\\forall \\lambda \\ge 0, \\forall v, g(\\lambda,v) \\le p^\\star\\)</span>,其中<span class=\"math inline\">\\(p^\\star\\)</span>表示原问题的最优值。</li>\r\n</ul>\r\n<h2 id=\"对偶问题dual-problemlagrange-dual-problem\">对偶问题(Dual problem/lagrange Dual problem)</h2>\r\n<p>对于原问题(Primal problem)（P）： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\min\\ &amp;f_0(x)\\\\\r\n    s.t.\\ &amp;f_i(x) \\le 0, i = 1,2,...,m\\\\\r\n    &amp;h_j(x) = 0, j=1,2,...,p\r\n\\end{aligned}\r\n\\]</span> 其最优解记为<span class=\"math inline\">\\(p^\\star\\)</span></p>\r\n<p>其对偶问题（D）： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\max\\ &amp;g(\\lambda, v)\\\\\r\n    s.t.\\ &amp; \\lambda \\ge 0\r\n\\end{aligned}\r\n\\]</span> 其中<span class=\"math inline\">\\(g(\\lambda, v)\\)</span>是原问题的拉格朗日对偶函数，对偶问题的最优解记为<span class=\"math inline\">\\(d^\\star\\)</span></p>\r\n<p>有以下结论： - <span class=\"math inline\">\\(d^\\star \\le p^\\star\\)</span>（弱对偶）。 - 对偶问题的最优解<span class=\"math inline\">\\(\\lambda ^\\star, v^\\star\\)</span>存在。 - 一个凸问题的对偶问题的对偶问题就是自身。</p>\r\n<p>如果<span class=\"math inline\">\\(d^\\star = p^\\star\\)</span>，则称为强对偶。</p>\r\n<p>对偶间隙：<span class=\"math inline\">\\(p^\\star - d^\\star\\)</span></p>\r\n<h2 id=\"相对内部relative-interior\">相对内部（Relative Interior）</h2>\r\n<p>去掉集合的边缘，让集合变成一个开集，集合<span class=\"math inline\">\\(D\\)</span>的相对内部可以表示为：</p>\r\n<p><span class=\"math inline\">\\(Relint\\ D =\\{x| x \\in D, \\exists r\\&gt;0, B(x,r) \\cap aff(D) \\le D\\}\\)</span></p>\r\n<p>其中<span class=\"math inline\">\\(B(x,r)\\)</span>是个球，<span class=\"math inline\">\\(aff(D)\\)</span>表示<span class=\"math inline\">\\(D\\)</span>的仿射包。</p>\r\n<h2 id=\"slaters-condition\">Slater's Condition</h2>\r\n<p>对偶间隙为零的充分条件，但是不必要。</p>\r\n<p>如果凸问题： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\min\\ &amp;f_0(x)\\\\\r\n    s.t.\\ &amp;f_i(x) \\le 0, i=1,2,...,m\\\\\r\n    &amp;Ax=b\r\n\\end{aligned}\r\n\\]</span> 其定义域为<span class=\"math inline\">\\(D\\)</span>，对<span class=\"math inline\">\\(\\forall i, f_i(x)\\)</span>为凸，当<span class=\"math inline\">\\(\\exists x \\in relint\\ D\\)</span>使得<span class=\"math inline\">\\(Ax=b, f_i(x) &lt; 0, i =1,2,...,m\\)</span>满足时，<span class=\"math inline\">\\(d^\\star = p^\\star\\)</span></p>\r\n<h2 id=\"weaken-slaters-condition\">Weaken Slater's Condition</h2>\r\n<p>如果不等式约束为仿射（<span class=\"math inline\">\\(D = dom f_0\\)</span>），只要可行域非空，则必有<span class=\"math inline\">\\(d^\\star = p^\\star\\)</span></p>\r\n<h2 id=\"鞍点saddle-point\">鞍点（Saddle Point）</h2>\r\n<p>对于函数<span class=\"math inline\">\\(L(x, \\lambda)\\)</span>，如果<span class=\"math inline\">\\(\\inf\\limits_{x \\in D} \\sup\\limits_{\\lambda \\ge 0} L(x, \\lambda) = \\sup\\limits_{\\lambda \\ge 0} \\inf\\limits_{x \\in D} L(x, \\lambda)\\)</span>，则称此时的<span class=\"math inline\">\\((x^\\star, \\lambda^\\star)\\)</span>为鞍点。</p>\r\n<h2 id=\"鞍点定理\">鞍点定理</h2>\r\n<p>若<span class=\"math inline\">\\((\\tilde{x}, \\tilde{\\lambda})\\)</span>为<span class=\"math inline\">\\(L(x, \\lambda)\\)</span>的鞍点<span class=\"math inline\">\\(\\Leftrightarrow\\)</span>强对偶存在，且<span class=\"math inline\">\\((\\tilde{x}, \\tilde{\\lambda})\\)</span>为Primal与Dual的最优解。</p>\r\n<h2 id=\"kkt条件\">KKT条件</h2>\r\n<p>对于如下问题： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\min\\ &amp;f_0(x)\\\\\r\n    s.t.\\ &amp;f_i(x) \\le 0, i=1,2,...,m\\\\\r\n    &amp;h_j(x) = 0, j=1,2,...,n\r\n\\end{aligned}\r\n\\]</span> 其拉格朗日函数：<span class=\"math inline\">\\(L(x, \\lambda, v) = f_0(x) + \\sum\\limits_{i=1}^m \\lambda_i f_i(x) + \\sum\\limits_{j=1}^n v_j h_j(x)\\)</span></p>\r\n<p>其对偶函数：<span class=\"math inline\">\\(g(\\lambda, v) = \\inf\\limits_{x \\in D}L(x, \\lambda, v)\\)</span></p>\r\n<p>对偶问题：<span class=\"math inline\">\\(\\sup\\limits_{\\lambda, v} g(\\lambda, v)\\)</span></p>\r\n<p>如果其满足对偶问题的最优解<span class=\"math inline\">\\(d^\\star\\)</span>和原问题的最优解<span class=\"math inline\">\\(p^\\star\\)</span>相等。 即： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    f_0(x^\\star) &amp;= g(\\lambda^\\star, v^\\star)\\\\\r\n    &amp;=\\inf\\limits_{x \\in D} (f_0(x) + \\sum\\limits_{i=1}^m \\lambda^\\star_i f_i(x) + \\sum\\limits_{j=1}^n v^\\star_j h_j(x))\\\\\r\n    &amp;\\le f_0(x^\\star) + \\sum\\limits_{i=1}^m \\lambda^\\star_i f_i(x^\\star) + \\sum\\limits_{j=1}^n v^\\star_j h_j(x^\\star)\\\\\r\n    &amp;\\le f_0(x^\\star)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>这表示<span class=\"math inline\">\\(f_0(x^\\star) = f_0(x^\\star) + \\sum\\limits_{i=1}^m \\lambda^\\star_i f_i(x^\\star) + \\sum\\limits_{j=1}^n v^\\star_j h_j(x^\\star) \\Rightarrow \\sum\\limits_{i=1}^m \\lambda^\\star_i f_i(x^\\star) = 0\\)</span>，而<span class=\"math inline\">\\(f_i(x^\\star) \\le 0，\\lambda_i \\ge 0, h_j(x^\\star) = 0\\)</span>，因此可以得出<span class=\"math inline\">\\(\\lambda^\\star_i f_i(x^\\star) = 0\\)</span>，这就是互补松弛条件。</p>\r\n<p>又因为<span class=\"math inline\">\\(x^\\star = \\mathop{\\arg\\inf}\\limits_{x \\in D} (f_0(x) + \\sum\\limits_{i=1}^m \\lambda^\\star_i f_i(x) + \\sum\\limits_{j=1}^n v^\\star_j h_j(x)) = \\mathop{\\arg\\inf}\\limits_{x \\in D} L(x, \\lambda^\\star, v^\\star)\\)</span>（这里是因为上面的推导中，两个<span class=\"math inline\">\\(\\le\\)</span>可以改成<span class=\"math inline\">\\(=\\)</span>）,因此<span class=\"math inline\">\\(x^\\star\\)</span>是该问题的全局最优解。</p>\r\n<p>如果原问题中，所有函数都可微，则有： <span class=\"math display\">\\[\r\n\\frac{\\partial L(x^\\star, \\lambda^\\star, v^\\star)}{\\partial x^\\star} = 0\r\n\\]</span> 这就是稳定性条件。</p>\r\n<p>因此只要满足<span class=\"math inline\">\\(p^\\star = d^\\star\\)</span>，且所有函数可微，则有如下结论： - 互补松弛条件：<span class=\"math inline\">\\(\\lambda^\\star_i f_i(x^\\star) = 0\\)</span> - 稳定性条件：<span class=\"math inline\">\\(\\frac{\\partial L(x^\\star, \\lambda^\\star, v^\\star)}{\\partial x^\\star} = 0\\)</span> - 原问题可行：<span class=\"math inline\">\\(f_i(x) \\le 0, i=1,2,...,m\\ \\ h_j(x) = 0, j=1,2,...,n\\)</span> - 对偶问题可行：<span class=\"math inline\">\\(\\lambda_i \\ge 0, i= 1,2,...,m\\)</span></p>\r\n<p>以上四个结论就是KKT条件。</p>\r\n<p>在对偶间隙为0且各个函数可微的情况下，KKT条件仅是最优解的必要条件，满足KKT条件的解，不一定是最优解。</p>\r\n<p>如果原问题为凸问题，各个函数可微，对偶间隙为0，则KKT条件是最优解的充分必要条件。</p>\r\n<p>充分性证明：如果<span class=\"math inline\">\\((x^\\star, \\lambda^\\star, v^\\star)\\)</span>满足KKT条件，则必有<span class=\"math inline\">\\((x^\\star, \\lambda^\\star, v^\\star)\\)</span>为最优解。证明<span class=\"math inline\">\\(g(\\lambda ^\\star, v^\\star) = f_0(x^\\star)\\)</span>即可。</p>\r\n<h2 id=\"敏感性分析\">敏感性分析</h2>\r\n<p>原问题<span class=\"math inline\">\\(P\\)</span>： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\min\\ &amp;f_0(x)\\\\\r\n    s.t.\\ &amp;f_i(x) \\le 0, i=1,2,...,m\\\\\r\n    &amp;h_j(x) = 0,j=1,2,...,n\r\n\\end{aligned}\r\n\\]</span> 干扰问题： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\min\\ &amp;f_0(x)\\\\\r\n    s.t.\\ &amp;f_i(x) \\le u_i, i=1,2,...,m\\\\\r\n    &amp;h_j(x) = w_i,j=1,2,...,n\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>将原问题的最优值<span class=\"math inline\">\\(p^\\star\\)</span>看做一个函数<span class=\"math inline\">\\(p^\\star(u, w)\\)</span>，原问题的最优值<span class=\"math inline\">\\(p^\\star = p^\\star(0, 0)\\)</span></p>\r\n<p>有以下性质：</p>\r\n<ul>\r\n<li>若原问题为凸问题，则<span class=\"math inline\">\\(p^\\star(u, w)\\)</span>为关于<span class=\"math inline\">\\(u\\)</span>和<span class=\"math inline\">\\(w\\)</span>的凸函数。</li>\r\n<li>若原问题为凸，对偶间隙为0，<span class=\"math inline\">\\(\\lambda^\\star\\)</span>和<span class=\"math inline\">\\(v^\\star\\)</span>为对偶问题的最优解，则<span class=\"math inline\">\\(p^\\star(u,w) \\le p^\\star(0, 0) -\\lambda^{\\star T}u - v^{\\star T}w\\)</span></li>\r\n<li>若原问题为凸，对偶间隙为0，且<span class=\"math inline\">\\(p^\\star(u,w)\\)</span>在<span class=\"math inline\">\\((0, 0)\\)</span>点可微，则<span class=\"math inline\">\\(\\lambda^\\star_i = -\\frac{\\partial p^\\star(0, 0)}{\\partial u_i}, v^\\star_j = -\\frac{\\partial p^\\star(0, 0)}{\\partial w_j}\\)</span>, <span class=\"math inline\">\\(p^\\star(u,w) = p^\\star(0, 0) -\\lambda^{\\star T}u - v^{\\star T}w\\)</span></li>\r\n</ul>\r\n<h2 id=\"强凸性\">强凸性</h2>\r\n<p>强凸性表示：<span class=\"math inline\">\\(\\exists m &gt; 0, \\forall x \\in dom f, \\triangledown^2f(x) \\succeq mI\\)</span></p>\r\n<p>强凸性也等价于：<span class=\"math inline\">\\(\\exists m, \\forall x,y \\in dom f, f(y) \\ge f(x) + \\triangledown f^T(x)(y-x) + \\frac{1}{2}m||y-x||^2\\)</span></p>\r\n<p>如果<span class=\"math inline\">\\(f(x)\\)</span>二阶可微且有强凸性，则<span class=\"math inline\">\\(\\min f(x)\\)</span>的最优值<span class=\"math inline\">\\(p^\\star\\)</span>满足<span class=\"math inline\">\\(f(x) - p^\\star \\le \\frac{1}{2m}||\\triangledown f(x)||_2^2\\)</span></p>\r\n<h1 id=\"无约束优化问题的求解算法\">无约束优化问题的求解算法</h1>\r\n<h2 id=\"梯度下降法gradient-descent\">梯度下降法（Gradient Descent）</h2>\r\n<p>每次沿着负梯度方向更新参数。</p>\r\n<h3 id=\"梯度下降法的收敛性\">梯度下降法的收敛性</h3>\r\n<p>如果<span class=\"math inline\">\\(\\exists M, m,MI \\succeq \\triangledown_2f\\succeq mI\\)</span>，对于精确搜索的学习速率<span class=\"math inline\">\\(\\alpha\\)</span>，可以证明<span class=\"math inline\">\\(f(x^{k+1}) -p^\\star \\le (1 - \\frac{m}{M})f(x^k - p^\\star)\\)</span>（利用强凸性的性质证明）</p>\r\n<h2 id=\"最速下降法steepest-descent\">最速下降法（Steepest Descent）</h2>\r\n<p>如果一阶可微，则可以找到一个方向<span class=\"math inline\">\\(d = \\mathop{\\arg\\min}\\limits_v f(x+v) = \\mathop{\\arg\\min}\\limits_v \\triangledown f^T(x)v,\\ s.t.\\ ||v||_k = 1\\)</span>（这里使用了一阶泰勒展开），其中的k-范数不一定是2-范数，所以这个方向不一定是梯度，梯度下降法是最速下降法在范数取2-范数情况下的一个特例。</p>\r\n<h2 id=\"最速下降法的一些变种\">最速下降法的一些变种</h2>\r\n<h3 id=\"坐标轮换法coordinate-descent\">坐标轮换法（Coordinate Descent）</h3>\r\n<p>每次选择一个基向量方向作为优化方向，但是需要在包含正值和负值的区间去搜索步长。 ### 分块坐标轮换法（Block Coordinate Descent） 每次选择一组基向量的线性组合方向（一个子空间）作为优化方向。</p>\r\n<p>坐标轮换和分块坐标轮换适合于维度拆分之后损失函数比较简单的情况。</p>\r\n<h2 id=\"次梯度方法\">次梯度方法</h2>\r\n<p>如果<span class=\"math inline\">\\(f(x)\\)</span>在某些点不可微，那么定义次梯度的概念<span class=\"math inline\">\\(\\frac{\\partial f}{\\partial x} = \\theta\\triangledown f(x)^+ + (1-\\theta)\\triangledown f(x)^-,\\ \\theta \\in [0, 1]\\)</span>，即使用梯度的左极限和右极限的凸组合。如果在当前点，对于某个<span class=\"math inline\">\\(\\theta\\)</span>，次梯度可以为0，那么认为其到达极值点。</p>\r\n<h2 id=\"牛顿法newtons-method\">牛顿法（Newton's Method）</h2>\r\n<p>如果二阶可微，则使用二阶泰勒展开，找到一个方向<span class=\"math inline\">\\(d = \\mathop{\\arg\\min}\\limits_v f(x+v) = \\mathop{\\arg\\min}\\limits_v \\triangledown f^T(x)v + \\frac{1}{2}v^T\\triangledown_2 f^T(x)v\\)</span>，这里不需要关于<span class=\"math inline\">\\(v\\)</span>的约束项，是因为如果是个凸函数，则有唯一解的<span class=\"math inline\">\\(v = -(\\triangledown_2 f^T(x))^{-1}\\triangledown f(x)\\)</span>（也称为牛顿方向）。</p>\r\n<p>牛顿法在泰勒展开中的一次项接近于0的时候，就可以停止，即<span class=\"math inline\">\\(\\triangledown f^T(x)v = -\\triangledown f^T(x)(\\triangledown_2 f^T(x))^{-1}\\triangledown f(x)\\)</span>接近于0的时候就停止。</p>\r\n<h3 id=\"牛顿法的收敛速度\">牛顿法的收敛速度</h3>\r\n<p>如果<span class=\"math inline\">\\(||\\triangledown f(x)||_2\\)</span>比较大，则泰勒展开的偏差较大，因此收敛速度比较慢，如果<span class=\"math inline\">\\(||\\triangledown f(x)||_2\\)</span>比较小，则收敛速度比梯度下降快很多。</p>\r\n<p>牛顿法的缺点在于求Hession矩阵可能非常难而且慢。</p>\r\n<h2 id=\"拟牛顿法quasi-newton-method\">拟牛顿法（Quasi-Newton Method）</h2>\r\n<p>不求二阶偏导，根据拟牛顿条件，找到一个对Hession矩阵逆矩阵的近似矩阵来进行方向选择，例如BFGS、L-BFGS、DFP等方法。</p>\r\n<h1 id=\"有约束优化问题的求解算法\">有约束优化问题的求解算法</h1>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\min\\ &amp;f(x)\\\\\r\n    s.t.\\ &amp;Ax = b\r\n\\end{aligned}\r\n\\]</span> 在使用KKT条件的过程中，如果<span class=\"math inline\">\\(\\triangledown f(x)\\)</span>不是线性，那么存在求解困难。</p>\r\n<p>首先变形原问题，使用迭代的方法，假设<span class=\"math inline\">\\(x^k\\)</span>满足约束<span class=\"math inline\">\\(Ax^k = b\\)</span>，那么下一次迭代问题变成： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\min\\limits_d\\ &amp;f(x^k + d)\\\\\r\n    s.t.\\ &amp;A(x^k +d) = b \\Rightarrow Ad = 0\r\n\\end{aligned}\r\n\\]</span> 进行二阶泰勒展开，去掉高阶项，原问题近似等价于： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\min\\limits_d\\ &amp;f(x^k) + \\triangledown f^T(x^k)d + \\frac{1}{2}d^T \\triangledown_2 f(x^k)d\\\\\r\n    s.t.\\ &amp;Ad = 0\r\n\\end{aligned}\r\n\\]</span></p>\r\n<h2 id=\"拉格朗日法lagrangian-method\">拉格朗日法（Lagrangian Method）</h2>\r\n<p>对于对偶间隙为零的问题，固定<span class=\"math inline\">\\(v\\)</span>去求<span class=\"math inline\">\\(x\\)</span>，然后固定<span class=\"math inline\">\\(x\\)</span>去求<span class=\"math inline\">\\(v\\)</span>，交替进行。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    x^{k+1} &amp;= x^k - \\alpha^k(\\triangledown f(x^k) + A^Tv^k)\\\\\r\n    v^{k+1} &amp;= v^k + \\alpha^k(Ax^k - b)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<h2 id=\"增广拉格朗日法augmented-lagragian-method\">增广拉格朗日法（Augmented Lagragian Method）</h2>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\min\\limits_x\\ &amp;f(x) + \\frac{C}{2}||Ax -b||^2_2\\\\\r\n    s.t.\\ &amp;Ax = b\r\n\\end{aligned}\r\n\\]</span> 这个约束问题的拉格朗日函数（增广拉格朗日函数）：<span class=\"math inline\">\\(L_C(x,v) = f(x) + v^T(Ax-b)+\\frac{C}{2}||Ax -b||^2_2\\)</span></p>\r\n<p>而且这个问题的原问题最优解和对偶问题最优解都和以下问题相同： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\min\\limits_x\\ &amp;f(x)\\\\\r\n    s.t.\\ &amp;Ax = b\r\n\\end{aligned}\r\n\\]</span></p>\r\n<h3 id=\"一些性质\">一些性质</h3>\r\n<ul>\r\n<li>如果<span class=\"math inline\">\\(v = v^\\star\\)</span>，则<span class=\"math inline\">\\(\\forall C &gt; 0\\)</span>，<span class=\"math inline\">\\(x^\\star = \\mathop{\\arg\\min}\\limits_xL_C(x, v^\\star)\\)</span></li>\r\n<li>若<span class=\"math inline\">\\(C \\rightarrow +\\infty\\)</span>，则<span class=\"math inline\">\\(\\forall v, x^\\star =\\mathop{\\arg\\min}\\limits_xL_C(x, v)\\)</span></li>\r\n</ul>\r\n<p>如果有<span class=\"math inline\">\\(x^k\\)</span>，和<span class=\"math inline\">\\(v^k\\)</span>，首先更新<span class=\"math inline\">\\(x^{k+1} = \\mathop{\\arg\\min}\\limits_x L_C(x, v^k)\\)</span>，然后更新<span class=\"math inline\">\\(v^{k+1} = v^k + C \\triangledown_v L_C(x^{k+1}, v^k)\\)</span></p>\r\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"仿射集凸集锥集凸锥集仿射组合凸组合凸锥组合仿射包凸包凸锥包\">仿射集、凸集、锥集、凸锥集、仿射组合、凸组合、凸锥组合、仿射包、凸包、凸锥包</h1>\r\n<ul>\r\n<li>锥：<span class=\"math inline\">\\(C = \\{x| \\theta x \\in C\\}, x \\in R^n,\\theta \\ge 0\\)</span></li>\r\n<li>仿射组合：<span class=\"math inline\">\\(\\theta_1+\\theta_2+...=1\\)</span></li>\r\n<li>凸组合：<span class=\"math inline\">\\(\\theta_1+\\theta_2+...=1,\\theta_1+\\theta_2+... \\ge 0\\)</span></li>\r\n<li>凸锥组合：<span class=\"math inline\">\\(\\theta_1+\\theta_2+... \\ge 0\\)</span></li>\r\n</ul>\r\n<h1 id=\"一些凸集超平面半空间球椭球多面体单纯形对称半正定矩阵\">一些凸集：超平面、半空间、球、椭球、多面体、单纯形、对称（半）（正定）矩阵</h1>\r\n<ul>\r\n<li>超平面：<span class=\"math inline\">\\(\\{x|w^Tx=b\\},x \\in R^n,w \\in R^n, b \\in R, w \\ne \\mathbb{0}\\)</span></li>\r\n<li>半空间：<span class=\"math inline\">\\(\\{x|w^Tx \\ge b\\},x \\in R^n,w \\in R^n, b \\in R, w \\ne \\mathbb{0}\\)</span>或<span class=\"math inline\">\\(\\{x|w^Tx \\le b\\},x \\in R^n,w \\in R^n, b \\in R, w \\ne \\mathbb{0}\\)</span></li>\r\n<li>球：<span class=\"math inline\">\\(B(x_c, r) = \\{x |\\ ||x-x_c||_2 \\le r\\}, x_c \\in R^n, x \\in R^n, r \\ge 0\\)</span></li>\r\n<li>椭球：<span class=\"math inline\">\\(\\epsilon(x_c, P) = \\{x |\\ (x-x_c)^T P^{-1} (x-x_c) \\le 1\\}, x_c \\in R^n, x \\in R^n, P \\in S_{++}^n\\)</span></li>\r\n<li>多面体：<span class=\"math inline\">\\(P=\\{x|a_i^T x \\le b_i, c_j^T x = d_j, i = 1,2,...,m, j=1,2,...,n\\}\\)</span></li>\r\n<li>单纯形：<span class=\"math inline\">\\(R^n\\)</span>中的<span class=\"math inline\">\\(k+1\\)</span>个点<span class=\"math inline\">\\(v_0, v_1, ..., v_k\\)</span>，满足<span class=\"math inline\">\\(v_1-v_0, v_2-v_0, ..., v_k-v_0\\)</span>线性无关，则<span class=\"math inline\">\\(v_0, v_1, ..., v_k\\)</span>的单纯形为<span class=\"math inline\">\\(v_0, v_1, ..., v_k\\)</span>的凸包，单纯形是多面体的一种，证明的时候，可以借助\"秩为<span class=\"math inline\">\\(k\\)</span>的矩阵<span class=\"math inline\">\\(B\\)</span>,可以用一个非奇异矩阵<span class=\"math inline\">\\(A\\)</span>转换成<span class=\"math inline\">\\(\\begin{bmatrix}I_k \\\\ \\mathbb{0} \\end{bmatrix}\\)</span>的形式\"这个定理。</li>\r\n</ul>\r\n<h1 id=\"仿射函数透视函数线性分数函数\">仿射函数、透视函数、线性分数函数</h1>\r\n<ul>\r\n<li><p>仿射函数：<span class=\"math inline\">\\(f:R^n \\rightarrow R^m, f(x)=Ax+b, A \\in R^{(m \\times n)}, b \\in R^n\\)</span></p></li>\r\n<li><p>缩放：<span class=\"math inline\">\\(\\alpha S = \\{\\alpha x| x \\in S\\}\\)</span>、移位：<span class=\"math inline\">\\(S+a=\\{x+a|x \\in S\\}\\)</span>是仿射变换的一种。</p></li>\r\n<li><p>线性矩阵不等式的解集是凸集：<span class=\"math inline\">\\(\\{x|A(x) \\preceq B\\}\\)</span></p></li>\r\n<li><p>透视函数：<span class=\"math inline\">\\(P: R^{n+1} \\rightarrow R^n, P(x,y) = \\{\\frac{x}{y}\\}\\ x \\in R^n, y\\in R_{++}\\)</span></p></li>\r\n</ul>\r\n<h1 id=\"保凸集运算\">保凸集运算：</h1>\r\n<ul>\r\n<li>交集</li>\r\n<li>集合的和：<span class=\"math inline\">\\(S_1 + S_2 = \\{x_1 + x_2 | x_1 \\in S_1, x_2 \\in S_2\\}\\)</span></li>\r\n<li>集合的笛卡尔积：<span class=\"math inline\">\\(S_1 \\times S_2 = \\{(x_1 , x_2) | x_1 \\in S_1, x_2 \\in S_2\\}\\)</span></li>\r\n<li>（逆）仿射函数</li>\r\n<li>透视函数</li>\r\n<li>线性分数函数：首先定义仿射函数<span class=\"math inline\">\\(g:R^n \\rightarrow R^{m+1}, g(x)=\\begin{bmatrix} A \\\\ c^T \\end{bmatrix} x + \\begin{bmatrix} b \\\\ d \\end{bmatrix}, A \\in R^{m \\times n}, c \\in R^n, b \\in R^m, d \\in R\\)</span>，再定义透视函数<span class=\"math inline\">\\(p:R^{m+1} \\rightarrow R^m\\)</span>，则线性分数函数<span class=\"math inline\">\\(f = p \\circ g = \\frac{Ax + b}{c^T x + d},\\ dom f = \\{x| c^T x + d &gt; 0\\}\\)</span></li>\r\n</ul>\r\n<h1 id=\"凸函数\">凸函数</h1>\r\n<h2 id=\"凸函数定义\">凸函数定义</h2>\r\n<ul>\r\n<li><p><span class=\"math inline\">\\(f:R^n \\rightarrow R\\)</span>为凸函数<span class=\"math inline\">\\(\\Leftrightarrow\\)</span><span class=\"math inline\">\\(domf\\)</span>为凸集，且<span class=\"math inline\">\\(f(\\theta x_1 + (1-\\theta)x_2) \\le \\theta f(x_1) + (1-\\theta)f(x_2)\\)</span>，其中<span class=\"math inline\">\\(\\theta \\in [0, 1]\\)</span></p></li>\r\n<li><p><span class=\"math inline\">\\(f:R^n \\rightarrow R\\)</span>为凸函数<span class=\"math inline\">\\(\\Leftrightarrow\\)</span><span class=\"math inline\">\\(domf\\)</span>为凸集，且<span class=\"math inline\">\\(\\forall x \\in dom f, \\forall v, g(t) = f(x + tv)\\)</span>为凸，<span class=\"math inline\">\\(dom g = \\{t | x + tv \\in dom f\\}\\)</span></p></li>\r\n<li><p><span class=\"math inline\">\\(f:R^n \\rightarrow R\\)</span>为凸函数且<span class=\"math inline\">\\(f\\)</span>可微<span class=\"math inline\">\\(\\Leftrightarrow\\)</span><span class=\"math inline\">\\(domf\\)</span>为凸集，<span class=\"math inline\">\\(\\forall x,y \\in dom f, f(y) \\ge f(x) + \\triangledown f^T(x)(y-x)\\)</span></p></li>\r\n<li><p><span class=\"math inline\">\\(f:R^n \\rightarrow R\\)</span>为凸函数且<span class=\"math inline\">\\(f\\)</span>二阶可微<span class=\"math inline\">\\(\\Leftrightarrow\\)</span><span class=\"math inline\">\\(domf\\)</span>为凸集，<span class=\"math inline\">\\(\\forall x \\in dom f,\\triangledown ^2 f(x) \\succeq 0\\)</span>（二阶条件）</p></li>\r\n</ul>\r\n<p>如果将上述前三个定义中的<span class=\"math inline\">\\(\\le\\)</span>、<span class=\"math inline\">\\(\\ge\\)</span>, 改为<span class=\"math inline\">\\(&lt;\\)</span>、<span class=\"math inline\">\\(&gt;\\)</span>，那么<span class=\"math inline\">\\(f\\)</span>就变成了严格凸函数，严格凸函数的二阶导数不能保证<span class=\"math inline\">\\(\\triangledown ^2 f(x) \\succ 0\\)</span>，但<span class=\"math inline\">\\(\\triangledown ^2 f(x) \\succ 0\\)</span>可以保证严格凸，但是二次函数<span class=\"math inline\">\\(f\\)</span>严格凸<span class=\"math inline\">\\(\\Leftrightarrow\\)</span>Hession矩阵<span class=\"math inline\">\\(\\triangledown ^2 f(x) \\in S^n_{++}\\)</span>即<span class=\"math inline\">\\(\\triangledown ^2 f(x) \\succeq 0\\)</span>。</p>\r\n<h2 id=\"凸函数的一个重要性质\">凸函数的一个重要性质</h2>\r\n<p><span class=\"math inline\">\\(f(x) \\ge f(x_0) + f&#39; (x_0) ^ T(x-x_0)\\)</span></p>\r\n<h2 id=\"一些常见的凸函数\">一些常见的凸函数</h2>\r\n<ul>\r\n<li>仿射函数：<span class=\"math inline\">\\(f(x)=Ax+b, \\triangledown_2f=0\\)</span>，是凸函数也是凹函数</li>\r\n<li>指数函数：<span class=\"math inline\">\\(f(x) = e^{ax}, x \\in R\\)</span></li>\r\n<li>幂函数：<span class=\"math inline\">\\(f(x) = x^a, x \\in R_{++}, a \\ge 1\\ or\\ a \\le 0\\)</span>是凸函数，当<span class=\"math inline\">\\(0 \\le a \\le 1\\)</span>时，是凹函数</li>\r\n<li>绝对值的幂函数：<span class=\"math inline\">\\(f(x)= |x|^p, x \\in R, x \\ne 0, P \\ge 1\\)</span></li>\r\n<li>对数函数：<span class=\"math inline\">\\(f(x) = -log(x), x \\in R_{++}\\)</span></li>\r\n<li>负熵：<span class=\"math inline\">\\(f(x) = xlog(x), x \\in R_{++}\\)</span></li>\r\n<li>范数：<span class=\"math inline\">\\(p(x), x \\in R^n\\)</span>，满足<span class=\"math inline\">\\(p(a)=|a|p(x), p(x+y) \\le p(x) + p(y), p(x) = 0 \\Leftrightarrow x = 0\\)</span></li>\r\n<li>极大值函数：<span class=\"math inline\">\\(f(x) = max(x_1, x_2, ..., x_n), x \\in R^n\\)</span></li>\r\n<li>极大值函数的逼近-log sum up：<span class=\"math inline\">\\(f(x) = log(e^{x_1} + e^{x_2} + ... +e^{x_n}), x \\in R^n\\)</span>其中<span class=\"math inline\">\\(max(x_1, x_2, ..., x_n) \\le f(x) \\le max(x_1, x_2, ..., x_n) + log(n)\\)</span></li>\r\n</ul>\r\n<h2 id=\"二次型\">二次型</h2>\r\n<p>二次型是一种二次齐次函数的统称，可以表示为<span class=\"math inline\">\\(f(x_1, x_2, ..., x_n) = a_{11}x_1^2 + a_{22}x_2^2 + ... + a_{nn}x_n^2 + 2a_{12}x_1x_2 + 2a_{13}x_1x_3 + ... + + 2a_{n-1, n}x_1x_n\\)</span></p>\r\n<p>二次型可以通过一个对称矩阵表示<span class=\"math inline\">\\(f(x_1, x_2, ..., x_n)=X^T A X\\)</span>，其中A因为是对称矩阵，有很多很好的分解方式，因此非常方便分析。</p>\r\n<h2 id=\"一些常用性质\">一些常用性质</h2>\r\n<ul>\r\n<li><span class=\"math inline\">\\(V^TXV \\ge 0 \\Leftrightarrow X \\succeq 0\\)</span></li>\r\n<li><span class=\"math inline\">\\(det(XY) = det(X)det(Y)\\)</span>，<span class=\"math inline\">\\(det\\)</span>表示求行列式值。</li>\r\n<li><span class=\"math inline\">\\((a^Ta)(b^Tb) \\ge (a^Tb)^2\\)</span></li>\r\n</ul>\r\n<h2 id=\"保凸函数运算\">保凸函数运算</h2>\r\n<ul>\r\n<li><span class=\"math inline\">\\(f_1, f_2, ..., f_m\\)</span>为凸，则<span class=\"math inline\">\\(f = \\sum\\limits_{i=1}^m w_i f_i,\\ \\forall i,w_i \\ge 0\\)</span>为凸</li>\r\n<li>若<span class=\"math inline\">\\(f(x,y),\\ \\forall y \\in A,f(x,y)\\)</span>为凸，对于<span class=\"math inline\">\\(\\forall y \\in A,\\ w(y) \\ge 0, g(x) = \\int_{y \\in A}w(y)f(x,y)dy\\)</span>为凸</li>\r\n<li>仿射映射<span class=\"math inline\">\\(f:R^n \\rightarrow R, A\\in R^{n \\times m}, b\\in R^n,g(x) = f(Ax + b), dom g=\\{x|Ax+b \\in dom f\\}\\)</span>是凸函数。</li>\r\n<li>对函数的仿射：<span class=\"math inline\">\\(f_i: R^n \\rightarrow R,i=1,...,m\\)</span>为凸函数，<span class=\"math inline\">\\(A \\in R_+^n, b \\in R, g(x)=A^T\\begin{bmatrix}f_1(x)&amp; \\cdots&amp;f_n(x)\\end{bmatrix}^T + b\\)</span>为凸函数</li>\r\n<li>极大值操作：<span class=\"math inline\">\\(f_1,f_2\\)</span>为凸，则<span class=\"math inline\">\\(f(x) = max(f_1(x), f_2(x)), dom f = domf_1 \\bigcap domf_2\\)</span>为凸。</li>\r\n<li>无限个凸函数的极大值：<span class=\"math inline\">\\(f(x,y),\\forall y \\in A\\)</span>对于<span class=\"math inline\">\\(x\\)</span>为凸，则<span class=\"math inline\">\\(g = \\sup\\limits_{y \\in A}f(x,y)\\)</span>为凸函数，其中<span class=\"math inline\">\\(\\sup\\limits_{y\\in A}\\)</span>表示<span class=\"math inline\">\\(y\\in A\\)</span>时的上确界（可以理解为最大值）。</li>\r\n<li>函数的组合也可以保证函数凸性：<span class=\"math inline\">\\(h:R^k \\rightarrow R, g: R^n \\rightarrow R^k,f=h \\circ g, R^n \\rightarrow R, domf = \\{x|x\\in dom g,g(x)\\in dom h\\}\\)</span>，1、若<span class=\"math inline\">\\(h\\)</span>为凸，<span class=\"math inline\">\\(\\tilde{h}\\)</span>不降，<span class=\"math inline\">\\(g\\)</span>为凸，则<span class=\"math inline\">\\(f\\)</span>为凸，2、若<span class=\"math inline\">\\(h\\)</span>为凸，<span class=\"math inline\">\\(\\tilde{h}\\)</span>不增，<span class=\"math inline\">\\(g\\)</span>为凹，则<span class=\"math inline\">\\(f\\)</span>为凸，3、若<span class=\"math inline\">\\(h\\)</span>为凹，<span class=\"math inline\">\\(\\tilde{h}\\)</span>不降，<span class=\"math inline\">\\(g\\)</span>为凹，则<span class=\"math inline\">\\(f\\)</span>为凹，4、若<span class=\"math inline\">\\(h\\)</span>为凹，<span class=\"math inline\">\\(\\tilde{h}\\)</span>不增，<span class=\"math inline\">\\(g\\)</span>为凸，则<span class=\"math inline\">\\(f\\)</span>为凹，其中<span class=\"math inline\">\\(\\tilde{h}\\)</span>为<span class=\"math inline\">\\(h\\)</span>的定义域扩展。</li>\r\n</ul>\r\n<p>向量的最大<span class=\"math inline\">\\(r\\)</span>个分量之和：<span class=\"math inline\">\\(f(x) = max\\{x_{i1}+x_{i2}\\cdots+x_{ir}|i1,i2,...,ir互不相等\\}\\)</span>是个凸函数。</p>\r\n<p>实对称矩阵的最大特征值函数<span class=\"math inline\">\\(f(X) = \\lambda_{max}(X)\\)</span>是个凸函数。</p>\r\n<h2 id=\"函数的透视\">函数的透视</h2>\r\n<p><span class=\"math inline\">\\(f:R^n \\rightarrow R，g: R^n \\times R^n_{++} \\rightarrow R, g(u, v) = v^Tf(u \\odot \\frac{1}{v}), dom g = \\{(u,v)|v \\in R^n_{++}, u \\odot \\frac{1}{v} \\in dom f\\}\\)</span>，<span class=\"math inline\">\\(f\\)</span>为凸<span class=\"math inline\">\\(\\Rightarrow g\\)</span>为凸，<span class=\"math inline\">\\(f\\)</span>为凹<span class=\"math inline\">\\(\\Rightarrow g\\)</span>为凹，且是联合凸和联合凹。</p>\r\n<h2 id=\"函数的共轭\">函数的共轭</h2>\r\n<p><span class=\"math inline\">\\(f:R^n \\rightarrow R, f^\\star:R^n\\rightarrow R, f^\\star(y)= \\sup\\limits_{x \\in domf}(y^Tx - f(x))\\)</span>。若<span class=\"math inline\">\\(f\\)</span>可微，则<span class=\"math inline\">\\(f^\\star(y)=\\sup\\limits_{x \\in domf}(y^Tx&#39; - f(x&#39;)), f&#39;(x&#39;) = y\\)</span>。对任意情况，<span class=\"math inline\">\\(f^\\star(y)\\)</span>一定是凸函数。</p>\r\n<h1 id=\"凸集和凸函数的关系\">凸集和凸函数的关系</h1>\r\n<h2 id=\"alpha-sublevel-set\"><span class=\"math inline\">\\(\\alpha\\)</span>-sublevel set</h2>\r\n<p>对于函数<span class=\"math inline\">\\(f:R^n \\rightarrow R\\)</span>定义其<span class=\"math inline\">\\(\\alpha\\)</span>-sublevel set为<span class=\"math inline\">\\(C_\\alpha=\\{x\\in domf|f(x)\\le \\alpha\\}\\)</span></p>\r\n<p>对于函数<span class=\"math inline\">\\(f:R^n \\rightarrow R\\)</span>定义其<span class=\"math inline\">\\(\\alpha\\)</span>-suplevel set为<span class=\"math inline\">\\(C_\\alpha=\\{x\\in domf|f(x)\\ge \\alpha\\}\\)</span></p>\r\n<p>凸函数所有的<span class=\"math inline\">\\(\\alpha\\)</span>-sublevel set都是凸集。</p>\r\n<p>但是函数的<span class=\"math inline\">\\(\\alpha\\)</span>-sublevel set都是凸集，不能保证该函数是凸函数。</p>\r\n<h1 id=\"quasi-convexconcave-function拟凸凹函数\">Quasi Convex（Concave） function（拟凸（凹）函数）</h1>\r\n<p><span class=\"math inline\">\\(S_\\alpha&#39; = \\{x|x in dom f,f(x) \\le \\alpha\\}\\)</span>（<span class=\"math inline\">\\(\\alpha\\)</span>-sublevel set）都是凸集，则这样的函数<span class=\"math inline\">\\(f(x)\\)</span>称为拟凸函数（unimodal function，单模态函数）。</p>\r\n<p><span class=\"math inline\">\\(S_\\alpha&#39; = \\{x|x in dom f,f(x) \\ge \\alpha\\}\\)</span>（<span class=\"math inline\">\\(\\alpha\\)</span>-suplevel set）都是凸集，则这样的函数<span class=\"math inline\">\\(f(x)\\)</span>称为拟凹函数。</p>\r\n<p><span class=\"math inline\">\\(S_\\alpha&#39; = \\{x|x in dom f,f(x) = \\alpha\\}\\)</span>都是凸集，则这样的函数<span class=\"math inline\">\\(f(x)\\)</span>称为拟线性函数。</p>\r\n<p>类似于凸函数的第一个定义，拟凸函数也可以写成<span class=\"math inline\">\\(f:R^n\\rightarrow R, dom f\\)</span>为凸，<span class=\"math inline\">\\(\\forall x,y \\in dom f, \\forall \\theta \\in [0,1]\\)</span>，有<span class=\"math inline\">\\(max(f(x), f(y)) \\ge f(\\theta x + (1-\\theta)y)\\)</span></p>\r\n<p>类似于凸函数的第三个定义，拟凸函数也可以写成若<span class=\"math inline\">\\(f(x)\\)</span>为拟凸函数，<span class=\"math inline\">\\(\\forall x,y \\in domf, f(y) \\le f(x) \\Rightarrow \\triangledown^Tf(x)(y-x) \\le 0\\)</span></p>\r\n<p>类似于凸函数的二阶条件，拟凸函数可以写成若<span class=\"math inline\">\\(f(x)\\)</span>为拟凸函数，<span class=\"math inline\">\\(y^T\\triangledown f(x) = 0 \\Rightarrow y^T \\triangledown^2f(x)y\\ge 0\\)</span></p>\r\n<h1 id=\"凸优化问题convex-problems\">凸优化问题（Convex Problems）</h1>\r\n<p>目标函数是凸函数且约束集合是凸集的问题叫做凸优化问题。</p>\r\n<h2 id=\"一般优化问题\">一般优化问题</h2>\r\n<p><span class=\"math display\">\\[\r\n\\min f_0(x)\\\\\r\ns.t. f_i(x) \\le 0, i= 1, 2, ..., m\\\\\r\nh_j(x) = 0, j= 1,2,...,n\r\n\\]</span> 其中<span class=\"math inline\">\\(x\\in R^n\\)</span>称为优化变量(Optimization variable)，<span class=\"math inline\">\\(f_0\\)</span>称为目标函数（损失函数）<span class=\"math inline\">\\(f_i(x) \\le 0\\)</span>称为不等式约束，<span class=\"math inline\">\\(h_j(x) = 0\\)</span>称为等式约束。如果<span class=\"math inline\">\\(m=n=0\\)</span>则称为无约束问题。</p>\r\n<h2 id=\"优化问题的域domain\">优化问题的域：domain</h2>\r\n<p>优化问题的定义域：<span class=\"math inline\">\\(D = \\bigcap\\limits_{i=0}^m dom f_i \\cap \\bigcap\\limits_{j=1}^n dom h_j\\)</span></p>\r\n<h2 id=\"可行解集feasible-set\">可行解集：feasible set</h2>\r\n<p>可行解集<span class=\"math inline\">\\(X_f = \\{x|x \\in D, f_i(x) \\le 0,i=1,2,...,m,h_j(x) = 0, j= 1,2,...,n\\}\\)</span></p>\r\n<h2 id=\"问题的最优值optimal-value\">问题的最优值：optimal value</h2>\r\n<p><span class=\"math inline\">\\(p^\\star = \\inf\\{f_0(x)|x \\in X_f\\}\\)</span>，如果<span class=\"math inline\">\\(X_f\\)</span>是空集，则<span class=\"math inline\">\\(p^\\star = +\\infty\\)</span></p>\r\n<h2 id=\"最优解optimal-pointsolution\">最优解：optimal point/solution</h2>\r\n<p>若<span class=\"math inline\">\\(x^\\star\\)</span>可行，且<span class=\"math inline\">\\(f_0(x^\\star) = p^\\star\\)</span></p>\r\n<h2 id=\"最优解集\">最优解集</h2>\r\n<p><span class=\"math inline\">\\(X_{opt} = \\{x | x\\in X_f, f_0(x)=p^\\star\\}\\)</span></p>\r\n<h2 id=\"epsilon次优解集epsilon-suboptimal-set\"><span class=\"math inline\">\\(\\epsilon\\)</span>次优解集：<span class=\"math inline\">\\(\\epsilon\\)</span>-suboptimal set</h2>\r\n<p><span class=\"math inline\">\\(X_\\epsilon = \\{x | x\\in X_f, f_0(x)) \\le p^\\star + \\epsilon\\}, \\epsilon \\ge 0\\)</span></p>\r\n<h2 id=\"局部最优解local-optimal\">局部最优解：local optimal</h2>\r\n<p><span class=\"math inline\">\\(\\exists R, f_0(x^\\star) = \\inf\\{f_0(z) | f_i(z) \\le 0, h_j(z) =0, ||z-x||\\le R\\}\\)</span>则<span class=\"math inline\">\\(x^\\star\\)</span>是局部最优解</p>\r\n<h2 id=\"可行性优化问题feasibility-problems\">可行性优化问题：feasibility Problems</h2>\r\n<p><span class=\"math inline\">\\(\\min\\limits_x c, s.t. f_i(X) \\le 0, h_j(X) = 0\\)</span>，其中<span class=\"math inline\">\\(c\\)</span>是任意一常数。</p>\r\n<h2 id=\"凸优化问题\">凸优化问题</h2>\r\n<p><span class=\"math display\">\\[\r\n\\min f_0(x)\\\\\r\ns.t. f_i(x) \\le 0, i= 1, 2, ..., m\\\\\r\na_j^Tx = b_j, j= 1,2,...,n\r\n\\]</span> 其中<span class=\"math inline\">\\(f_0\\)</span>、<span class=\"math inline\">\\(f_i\\)</span>是凸函数，其可行解集是个凸集、。</p>\r\n<p>凸优化问题的局部最优解等于全局最优解（反证法）。</p>\r\n<h2 id=\"可微目标函数情况下的最优解\">可微目标函数情况下的最优解</h2>\r\n<p>凸函数在可微的情况下：<span class=\"math inline\">\\(f_0(y) \\ge f_0(x) + \\triangledown f_0^T(x)(y-x)\\)</span></p>\r\n<p>其最优解<span class=\"math inline\">\\(x^\\star\\)</span>满足<span class=\"math inline\">\\(\\triangledown f_0^T(x^\\star)(y-x^\\star) \\ge 0, \\forall y \\in X_f\\)</span></p>\r\n<h2 id=\"线性规划问题\">线性规划问题</h2>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n\\min\\ &amp;C^Tx +d\\\\\r\ns.t.\\ &amp;Gx \\le h\\\\\r\n&amp;Ax = b    \r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>其等式约束和不等式约束构成一个多面体集合。</p>\r\n<h3 id=\"线性规划的等价变换\">线性规划的等价变换</h3>\r\n<p>上面的线性变换可以等价于： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\min &amp;C^T x +d\\\\\r\n    s.t.\\ &amp;Gx + S = h\\\\\r\n    &amp;Ax = b\\\\\r\n    &amp;S \\ge 0\r\n\\end{aligned}\r\n\\]</span> 等价的判定条件： 两个表达方式的可行解能够对应，且对应可行解的目标函数值相同。</p>\r\n<h3 id=\"线性分数规划\">线性分数规划</h3>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\min\\ &amp;f_0(x)\\\\\r\n    s.t.\\ &amp;Gx \\le h\\\\\r\n    &amp;Ax = b\r\n\\end{aligned}\r\n\\]</span> 其中<span class=\"math inline\">\\(f_0(x) = \\frac{C^Tx +d}{e^Tx + f}, domf = \\{x | e^T x + f \\ge 0\\}\\)</span></p>\r\n<p>这个问题是一个拟凸问题。</p>\r\n<p>如果该问题有可行解，则其有凸问题等价形式： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\min\\ &amp;C^T y + dz\\\\\r\n    s.t.\\ &amp;Gy-hz \\le 0\\\\\r\n    &amp;Ay -bz = 0\\\\\r\n    &amp;e^Ty+fz = 1\\\\\r\n    &amp;z\\ge 0\r\n\\end{aligned}\r\n\\]</span> 两个问题等价的证明：</p>\r\n<p>首先对于第一个问题任意可行的<span class=\"math inline\">\\(x\\)</span>，必须存在<span class=\"math inline\">\\(y\\)</span>和<span class=\"math inline\">\\(z\\)</span>对第二个问题可行，且<span class=\"math inline\">\\(C^T y + dz = f_0(x)\\)</span>，可以直接令<span class=\"math inline\">\\(y = \\frac{x}{e^Tx +f}, z=\\frac{1}{e^Tx + f}\\)</span>即证。</p>\r\n<p>齐次对于第二个问题，任意可行的<span class=\"math inline\">\\(y\\)</span>和<span class=\"math inline\">\\(z\\)</span>，必须存在<span class=\"math inline\">\\(x\\)</span>对第一个问题可行，且<span class=\"math inline\">\\(C^T y + dz = f_0(x)\\)</span>，如果<span class=\"math inline\">\\(z&gt;0\\)</span>，则直接令<span class=\"math inline\">\\(x=\\frac{y}{z}\\)</span>, 若<span class=\"math inline\">\\(z=0\\)</span>，那么对于第一个问题的一个可行解<span class=\"math inline\">\\(x_0\\)</span>，<span class=\"math inline\">\\(\\forall t\\ge 0, x = x_0 + ty\\)</span>也对第一个问题可行，令<span class=\"math inline\">\\(\\lim t \\rightarrow +\\infty\\)</span>，则可以使两个目标函数值相等。</p>\r\n<h2 id=\"二次规划qp\">二次规划(QP)</h2>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\min\\ &amp;\\frac{1}{2} x^TPx + q^T x + r\\\\\r\n    s.t.\\ &amp; Gx \\le h\\\\\r\n    &amp;Ax = b\r\n\\end{aligned}\r\n\\]</span> 其中<span class=\"math inline\">\\(P\\in S_+^n\\)</span></p>\r\n<h2 id=\"二次约束二次规划qcqp\">二次约束二次规划(QCQP)</h2>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\min\\ &amp;\\frac{1}{2} x^TPx + q^T x + r\\\\\r\n    s.t.\\ &amp; \\frac{1}{2} x^TP_ix + q_i^T x + r_i \\le 0,\\ i=1,2,...,m\\\\\r\n    &amp;Ax = b\r\n\\end{aligned}\r\n\\]</span> 其中<span class=\"math inline\">\\(P\\in S_+^n\\)</span>，<span class=\"math inline\">\\(P_i \\in S_+^n, i =1,2,...,m\\)</span></p>\r\n<h2 id=\"半正定规划semi-definite-programming\">半正定规划（semi-Definite Programming）</h2>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\min\\ &amp;tr(Cx)\\\\\r\n    s.t.\\ &amp; tr(A_ix) = b_i, i=1,2,...,p\\\\\r\n    &amp; x \\succeq 0\r\n\\end{aligned}\r\n\\]</span> 其中<span class=\"math inline\">\\(x\\in S_+^n, C \\in R^{n\\times n}, A_i \\in R^{n\\times n}, b_i \\in R\\)</span></p>\r\n<h2 id=\"多目标优化问题\">多目标优化问题</h2>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\min\\ &amp;f_0(x):R^n \\rightarrow R^g\\\\\r\n    s.t.\\ &amp;f_i(x) \\le 0, i= 1,2,...,m\\\\\r\n    &amp;h_j(x) = 0, j= 1,2,...,p\\\\\r\n\\end{aligned}\r\n\\]</span></p>\r\n<h3 id=\"pareto-optimal-front\">Pareto optimal front</h3>\r\n<p>一个点满足“如果找到其他可行解，可以使得在某些指标上比这个点更优，那么这个可行解必然在其他指标上比这个点更差”，则这个点属于Pareto optimal front。</p>\r\n<p>如果<span class=\"math inline\">\\(f_0(x)\\)</span>为凸，<span class=\"math inline\">\\(f_i(x)\\)</span>为凸，<span class=\"math inline\">\\(h_j(x)\\)</span>为仿射，则必可通过以下方式求得pareto optimal front中的一点：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\min\\ &amp;\\sum\\limits_{i=1}^g \\lambda_i f_{0i}(x), \\lambda_i \\ge 0\\\\\r\n    s.t.\\ &amp;f_i(x) \\le 0, i = 1,2,...,m\\\\\r\n    &amp;h_j(x) = 0, j=1,2,...,p\r\n\\end{aligned}\r\n\\]</span></p>\r\n<h1 id=\"对偶性\">对偶性</h1>\r\n<p>对于以下问题： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\min\\ &amp;f_0(x)\\\\\r\n    s.t.\\ &amp;f_i(x) \\le 0, i = 1,2,...,m\\\\\r\n    &amp;h_j(x) = 0, j=1,2,...,p\r\n\\end{aligned}\r\n\\]</span> 其定义域<span class=\"math inline\">\\(D = \\{x| x \\in \\bigcap_{i=1}^m dom f_i \\cap \\bigcap_{j=1}^p dom h_j\\}\\)</span> ## 拉格朗日函数 其拉格朗日函数定义为：<span class=\"math inline\">\\(L(x, \\lambda, v) = f_0(x) + \\sum\\limits_{i=1}^m\\lambda_if_i(x) + \\sum\\limits_{j=1}^pv_jh_j(x)\\)</span>，其中<span class=\"math inline\">\\(\\lambda_i \\ge 0\\)</span>和<span class=\"math inline\">\\(v_j\\)</span>称为拉格朗日乘子。</p>\r\n<h2 id=\"拉格朗日对偶函数对偶函数\">拉格朗日对偶函数（对偶函数）</h2>\r\n<p><span class=\"math display\">\\[\r\ng(\\lambda, v) = \\inf\\limits_{x\\in D}L(x, \\lambda ,v)\r\n\\]</span></p>\r\n<ul>\r\n<li>对偶函数一定是个凹函数（因为函数对于<span class=\"math inline\">\\(\\lambda\\)</span>和<span class=\"math inline\">\\(v\\)</span>是线性的）。</li>\r\n<li><span class=\"math inline\">\\(\\forall \\lambda \\ge 0, \\forall v, g(\\lambda,v) \\le p^\\star\\)</span>,其中<span class=\"math inline\">\\(p^\\star\\)</span>表示原问题的最优值。</li>\r\n</ul>\r\n<h2 id=\"对偶问题dual-problemlagrange-dual-problem\">对偶问题(Dual problem/lagrange Dual problem)</h2>\r\n<p>对于原问题(Primal problem)（P）： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\min\\ &amp;f_0(x)\\\\\r\n    s.t.\\ &amp;f_i(x) \\le 0, i = 1,2,...,m\\\\\r\n    &amp;h_j(x) = 0, j=1,2,...,p\r\n\\end{aligned}\r\n\\]</span> 其最优解记为<span class=\"math inline\">\\(p^\\star\\)</span></p>\r\n<p>其对偶问题（D）： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\max\\ &amp;g(\\lambda, v)\\\\\r\n    s.t.\\ &amp; \\lambda \\ge 0\r\n\\end{aligned}\r\n\\]</span> 其中<span class=\"math inline\">\\(g(\\lambda, v)\\)</span>是原问题的拉格朗日对偶函数，对偶问题的最优解记为<span class=\"math inline\">\\(d^\\star\\)</span></p>\r\n<p>有以下结论： - <span class=\"math inline\">\\(d^\\star \\le p^\\star\\)</span>（弱对偶）。 - 对偶问题的最优解<span class=\"math inline\">\\(\\lambda ^\\star, v^\\star\\)</span>存在。 - 一个凸问题的对偶问题的对偶问题就是自身。</p>\r\n<p>如果<span class=\"math inline\">\\(d^\\star = p^\\star\\)</span>，则称为强对偶。</p>\r\n<p>对偶间隙：<span class=\"math inline\">\\(p^\\star - d^\\star\\)</span></p>\r\n<h2 id=\"相对内部relative-interior\">相对内部（Relative Interior）</h2>\r\n<p>去掉集合的边缘，让集合变成一个开集，集合<span class=\"math inline\">\\(D\\)</span>的相对内部可以表示为：</p>\r\n<p><span class=\"math inline\">\\(Relint\\ D =\\{x| x \\in D, \\exists r\\&gt;0, B(x,r) \\cap aff(D) \\le D\\}\\)</span></p>\r\n<p>其中<span class=\"math inline\">\\(B(x,r)\\)</span>是个球，<span class=\"math inline\">\\(aff(D)\\)</span>表示<span class=\"math inline\">\\(D\\)</span>的仿射包。</p>\r\n<h2 id=\"slaters-condition\">Slater's Condition</h2>\r\n<p>对偶间隙为零的充分条件，但是不必要。</p>\r\n<p>如果凸问题： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\min\\ &amp;f_0(x)\\\\\r\n    s.t.\\ &amp;f_i(x) \\le 0, i=1,2,...,m\\\\\r\n    &amp;Ax=b\r\n\\end{aligned}\r\n\\]</span> 其定义域为<span class=\"math inline\">\\(D\\)</span>，对<span class=\"math inline\">\\(\\forall i, f_i(x)\\)</span>为凸，当<span class=\"math inline\">\\(\\exists x \\in relint\\ D\\)</span>使得<span class=\"math inline\">\\(Ax=b, f_i(x) &lt; 0, i =1,2,...,m\\)</span>满足时，<span class=\"math inline\">\\(d^\\star = p^\\star\\)</span></p>\r\n<h2 id=\"weaken-slaters-condition\">Weaken Slater's Condition</h2>\r\n<p>如果不等式约束为仿射（<span class=\"math inline\">\\(D = dom f_0\\)</span>），只要可行域非空，则必有<span class=\"math inline\">\\(d^\\star = p^\\star\\)</span></p>\r\n<h2 id=\"鞍点saddle-point\">鞍点（Saddle Point）</h2>\r\n<p>对于函数<span class=\"math inline\">\\(L(x, \\lambda)\\)</span>，如果<span class=\"math inline\">\\(\\inf\\limits_{x \\in D} \\sup\\limits_{\\lambda \\ge 0} L(x, \\lambda) = \\sup\\limits_{\\lambda \\ge 0} \\inf\\limits_{x \\in D} L(x, \\lambda)\\)</span>，则称此时的<span class=\"math inline\">\\((x^\\star, \\lambda^\\star)\\)</span>为鞍点。</p>\r\n<h2 id=\"鞍点定理\">鞍点定理</h2>\r\n<p>若<span class=\"math inline\">\\((\\tilde{x}, \\tilde{\\lambda})\\)</span>为<span class=\"math inline\">\\(L(x, \\lambda)\\)</span>的鞍点<span class=\"math inline\">\\(\\Leftrightarrow\\)</span>强对偶存在，且<span class=\"math inline\">\\((\\tilde{x}, \\tilde{\\lambda})\\)</span>为Primal与Dual的最优解。</p>\r\n<h2 id=\"kkt条件\">KKT条件</h2>\r\n<p>对于如下问题： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\min\\ &amp;f_0(x)\\\\\r\n    s.t.\\ &amp;f_i(x) \\le 0, i=1,2,...,m\\\\\r\n    &amp;h_j(x) = 0, j=1,2,...,n\r\n\\end{aligned}\r\n\\]</span> 其拉格朗日函数：<span class=\"math inline\">\\(L(x, \\lambda, v) = f_0(x) + \\sum\\limits_{i=1}^m \\lambda_i f_i(x) + \\sum\\limits_{j=1}^n v_j h_j(x)\\)</span></p>\r\n<p>其对偶函数：<span class=\"math inline\">\\(g(\\lambda, v) = \\inf\\limits_{x \\in D}L(x, \\lambda, v)\\)</span></p>\r\n<p>对偶问题：<span class=\"math inline\">\\(\\sup\\limits_{\\lambda, v} g(\\lambda, v)\\)</span></p>\r\n<p>如果其满足对偶问题的最优解<span class=\"math inline\">\\(d^\\star\\)</span>和原问题的最优解<span class=\"math inline\">\\(p^\\star\\)</span>相等。 即： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    f_0(x^\\star) &amp;= g(\\lambda^\\star, v^\\star)\\\\\r\n    &amp;=\\inf\\limits_{x \\in D} (f_0(x) + \\sum\\limits_{i=1}^m \\lambda^\\star_i f_i(x) + \\sum\\limits_{j=1}^n v^\\star_j h_j(x))\\\\\r\n    &amp;\\le f_0(x^\\star) + \\sum\\limits_{i=1}^m \\lambda^\\star_i f_i(x^\\star) + \\sum\\limits_{j=1}^n v^\\star_j h_j(x^\\star)\\\\\r\n    &amp;\\le f_0(x^\\star)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>这表示<span class=\"math inline\">\\(f_0(x^\\star) = f_0(x^\\star) + \\sum\\limits_{i=1}^m \\lambda^\\star_i f_i(x^\\star) + \\sum\\limits_{j=1}^n v^\\star_j h_j(x^\\star) \\Rightarrow \\sum\\limits_{i=1}^m \\lambda^\\star_i f_i(x^\\star) = 0\\)</span>，而<span class=\"math inline\">\\(f_i(x^\\star) \\le 0，\\lambda_i \\ge 0, h_j(x^\\star) = 0\\)</span>，因此可以得出<span class=\"math inline\">\\(\\lambda^\\star_i f_i(x^\\star) = 0\\)</span>，这就是互补松弛条件。</p>\r\n<p>又因为<span class=\"math inline\">\\(x^\\star = \\mathop{\\arg\\inf}\\limits_{x \\in D} (f_0(x) + \\sum\\limits_{i=1}^m \\lambda^\\star_i f_i(x) + \\sum\\limits_{j=1}^n v^\\star_j h_j(x)) = \\mathop{\\arg\\inf}\\limits_{x \\in D} L(x, \\lambda^\\star, v^\\star)\\)</span>（这里是因为上面的推导中，两个<span class=\"math inline\">\\(\\le\\)</span>可以改成<span class=\"math inline\">\\(=\\)</span>）,因此<span class=\"math inline\">\\(x^\\star\\)</span>是该问题的全局最优解。</p>\r\n<p>如果原问题中，所有函数都可微，则有： <span class=\"math display\">\\[\r\n\\frac{\\partial L(x^\\star, \\lambda^\\star, v^\\star)}{\\partial x^\\star} = 0\r\n\\]</span> 这就是稳定性条件。</p>\r\n<p>因此只要满足<span class=\"math inline\">\\(p^\\star = d^\\star\\)</span>，且所有函数可微，则有如下结论： - 互补松弛条件：<span class=\"math inline\">\\(\\lambda^\\star_i f_i(x^\\star) = 0\\)</span> - 稳定性条件：<span class=\"math inline\">\\(\\frac{\\partial L(x^\\star, \\lambda^\\star, v^\\star)}{\\partial x^\\star} = 0\\)</span> - 原问题可行：<span class=\"math inline\">\\(f_i(x) \\le 0, i=1,2,...,m\\ \\ h_j(x) = 0, j=1,2,...,n\\)</span> - 对偶问题可行：<span class=\"math inline\">\\(\\lambda_i \\ge 0, i= 1,2,...,m\\)</span></p>\r\n<p>以上四个结论就是KKT条件。</p>\r\n<p>在对偶间隙为0且各个函数可微的情况下，KKT条件仅是最优解的必要条件，满足KKT条件的解，不一定是最优解。</p>\r\n<p>如果原问题为凸问题，各个函数可微，对偶间隙为0，则KKT条件是最优解的充分必要条件。</p>\r\n<p>充分性证明：如果<span class=\"math inline\">\\((x^\\star, \\lambda^\\star, v^\\star)\\)</span>满足KKT条件，则必有<span class=\"math inline\">\\((x^\\star, \\lambda^\\star, v^\\star)\\)</span>为最优解。证明<span class=\"math inline\">\\(g(\\lambda ^\\star, v^\\star) = f_0(x^\\star)\\)</span>即可。</p>\r\n<h2 id=\"敏感性分析\">敏感性分析</h2>\r\n<p>原问题<span class=\"math inline\">\\(P\\)</span>： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\min\\ &amp;f_0(x)\\\\\r\n    s.t.\\ &amp;f_i(x) \\le 0, i=1,2,...,m\\\\\r\n    &amp;h_j(x) = 0,j=1,2,...,n\r\n\\end{aligned}\r\n\\]</span> 干扰问题： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\min\\ &amp;f_0(x)\\\\\r\n    s.t.\\ &amp;f_i(x) \\le u_i, i=1,2,...,m\\\\\r\n    &amp;h_j(x) = w_i,j=1,2,...,n\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>将原问题的最优值<span class=\"math inline\">\\(p^\\star\\)</span>看做一个函数<span class=\"math inline\">\\(p^\\star(u, w)\\)</span>，原问题的最优值<span class=\"math inline\">\\(p^\\star = p^\\star(0, 0)\\)</span></p>\r\n<p>有以下性质：</p>\r\n<ul>\r\n<li>若原问题为凸问题，则<span class=\"math inline\">\\(p^\\star(u, w)\\)</span>为关于<span class=\"math inline\">\\(u\\)</span>和<span class=\"math inline\">\\(w\\)</span>的凸函数。</li>\r\n<li>若原问题为凸，对偶间隙为0，<span class=\"math inline\">\\(\\lambda^\\star\\)</span>和<span class=\"math inline\">\\(v^\\star\\)</span>为对偶问题的最优解，则<span class=\"math inline\">\\(p^\\star(u,w) \\le p^\\star(0, 0) -\\lambda^{\\star T}u - v^{\\star T}w\\)</span></li>\r\n<li>若原问题为凸，对偶间隙为0，且<span class=\"math inline\">\\(p^\\star(u,w)\\)</span>在<span class=\"math inline\">\\((0, 0)\\)</span>点可微，则<span class=\"math inline\">\\(\\lambda^\\star_i = -\\frac{\\partial p^\\star(0, 0)}{\\partial u_i}, v^\\star_j = -\\frac{\\partial p^\\star(0, 0)}{\\partial w_j}\\)</span>, <span class=\"math inline\">\\(p^\\star(u,w) = p^\\star(0, 0) -\\lambda^{\\star T}u - v^{\\star T}w\\)</span></li>\r\n</ul>\r\n<h2 id=\"强凸性\">强凸性</h2>\r\n<p>强凸性表示：<span class=\"math inline\">\\(\\exists m &gt; 0, \\forall x \\in dom f, \\triangledown^2f(x) \\succeq mI\\)</span></p>\r\n<p>强凸性也等价于：<span class=\"math inline\">\\(\\exists m, \\forall x,y \\in dom f, f(y) \\ge f(x) + \\triangledown f^T(x)(y-x) + \\frac{1}{2}m||y-x||^2\\)</span></p>\r\n<p>如果<span class=\"math inline\">\\(f(x)\\)</span>二阶可微且有强凸性，则<span class=\"math inline\">\\(\\min f(x)\\)</span>的最优值<span class=\"math inline\">\\(p^\\star\\)</span>满足<span class=\"math inline\">\\(f(x) - p^\\star \\le \\frac{1}{2m}||\\triangledown f(x)||_2^2\\)</span></p>\r\n<h1 id=\"无约束优化问题的求解算法\">无约束优化问题的求解算法</h1>\r\n<h2 id=\"梯度下降法gradient-descent\">梯度下降法（Gradient Descent）</h2>\r\n<p>每次沿着负梯度方向更新参数。</p>\r\n<h3 id=\"梯度下降法的收敛性\">梯度下降法的收敛性</h3>\r\n<p>如果<span class=\"math inline\">\\(\\exists M, m,MI \\succeq \\triangledown_2f\\succeq mI\\)</span>，对于精确搜索的学习速率<span class=\"math inline\">\\(\\alpha\\)</span>，可以证明<span class=\"math inline\">\\(f(x^{k+1}) -p^\\star \\le (1 - \\frac{m}{M})f(x^k - p^\\star)\\)</span>（利用强凸性的性质证明）</p>\r\n<h2 id=\"最速下降法steepest-descent\">最速下降法（Steepest Descent）</h2>\r\n<p>如果一阶可微，则可以找到一个方向<span class=\"math inline\">\\(d = \\mathop{\\arg\\min}\\limits_v f(x+v) = \\mathop{\\arg\\min}\\limits_v \\triangledown f^T(x)v,\\ s.t.\\ ||v||_k = 1\\)</span>（这里使用了一阶泰勒展开），其中的k-范数不一定是2-范数，所以这个方向不一定是梯度，梯度下降法是最速下降法在范数取2-范数情况下的一个特例。</p>\r\n<h2 id=\"最速下降法的一些变种\">最速下降法的一些变种</h2>\r\n<h3 id=\"坐标轮换法coordinate-descent\">坐标轮换法（Coordinate Descent）</h3>\r\n<p>每次选择一个基向量方向作为优化方向，但是需要在包含正值和负值的区间去搜索步长。 ### 分块坐标轮换法（Block Coordinate Descent） 每次选择一组基向量的线性组合方向（一个子空间）作为优化方向。</p>\r\n<p>坐标轮换和分块坐标轮换适合于维度拆分之后损失函数比较简单的情况。</p>\r\n<h2 id=\"次梯度方法\">次梯度方法</h2>\r\n<p>如果<span class=\"math inline\">\\(f(x)\\)</span>在某些点不可微，那么定义次梯度的概念<span class=\"math inline\">\\(\\frac{\\partial f}{\\partial x} = \\theta\\triangledown f(x)^+ + (1-\\theta)\\triangledown f(x)^-,\\ \\theta \\in [0, 1]\\)</span>，即使用梯度的左极限和右极限的凸组合。如果在当前点，对于某个<span class=\"math inline\">\\(\\theta\\)</span>，次梯度可以为0，那么认为其到达极值点。</p>\r\n<h2 id=\"牛顿法newtons-method\">牛顿法（Newton's Method）</h2>\r\n<p>如果二阶可微，则使用二阶泰勒展开，找到一个方向<span class=\"math inline\">\\(d = \\mathop{\\arg\\min}\\limits_v f(x+v) = \\mathop{\\arg\\min}\\limits_v \\triangledown f^T(x)v + \\frac{1}{2}v^T\\triangledown_2 f^T(x)v\\)</span>，这里不需要关于<span class=\"math inline\">\\(v\\)</span>的约束项，是因为如果是个凸函数，则有唯一解的<span class=\"math inline\">\\(v = -(\\triangledown_2 f^T(x))^{-1}\\triangledown f(x)\\)</span>（也称为牛顿方向）。</p>\r\n<p>牛顿法在泰勒展开中的一次项接近于0的时候，就可以停止，即<span class=\"math inline\">\\(\\triangledown f^T(x)v = -\\triangledown f^T(x)(\\triangledown_2 f^T(x))^{-1}\\triangledown f(x)\\)</span>接近于0的时候就停止。</p>\r\n<h3 id=\"牛顿法的收敛速度\">牛顿法的收敛速度</h3>\r\n<p>如果<span class=\"math inline\">\\(||\\triangledown f(x)||_2\\)</span>比较大，则泰勒展开的偏差较大，因此收敛速度比较慢，如果<span class=\"math inline\">\\(||\\triangledown f(x)||_2\\)</span>比较小，则收敛速度比梯度下降快很多。</p>\r\n<p>牛顿法的缺点在于求Hession矩阵可能非常难而且慢。</p>\r\n<h2 id=\"拟牛顿法quasi-newton-method\">拟牛顿法（Quasi-Newton Method）</h2>\r\n<p>不求二阶偏导，根据拟牛顿条件，找到一个对Hession矩阵逆矩阵的近似矩阵来进行方向选择，例如BFGS、L-BFGS、DFP等方法。</p>\r\n<h1 id=\"有约束优化问题的求解算法\">有约束优化问题的求解算法</h1>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\min\\ &amp;f(x)\\\\\r\n    s.t.\\ &amp;Ax = b\r\n\\end{aligned}\r\n\\]</span> 在使用KKT条件的过程中，如果<span class=\"math inline\">\\(\\triangledown f(x)\\)</span>不是线性，那么存在求解困难。</p>\r\n<p>首先变形原问题，使用迭代的方法，假设<span class=\"math inline\">\\(x^k\\)</span>满足约束<span class=\"math inline\">\\(Ax^k = b\\)</span>，那么下一次迭代问题变成： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\min\\limits_d\\ &amp;f(x^k + d)\\\\\r\n    s.t.\\ &amp;A(x^k +d) = b \\Rightarrow Ad = 0\r\n\\end{aligned}\r\n\\]</span> 进行二阶泰勒展开，去掉高阶项，原问题近似等价于： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\min\\limits_d\\ &amp;f(x^k) + \\triangledown f^T(x^k)d + \\frac{1}{2}d^T \\triangledown_2 f(x^k)d\\\\\r\n    s.t.\\ &amp;Ad = 0\r\n\\end{aligned}\r\n\\]</span></p>\r\n<h2 id=\"拉格朗日法lagrangian-method\">拉格朗日法（Lagrangian Method）</h2>\r\n<p>对于对偶间隙为零的问题，固定<span class=\"math inline\">\\(v\\)</span>去求<span class=\"math inline\">\\(x\\)</span>，然后固定<span class=\"math inline\">\\(x\\)</span>去求<span class=\"math inline\">\\(v\\)</span>，交替进行。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    x^{k+1} &amp;= x^k - \\alpha^k(\\triangledown f(x^k) + A^Tv^k)\\\\\r\n    v^{k+1} &amp;= v^k + \\alpha^k(Ax^k - b)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<h2 id=\"增广拉格朗日法augmented-lagragian-method\">增广拉格朗日法（Augmented Lagragian Method）</h2>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\min\\limits_x\\ &amp;f(x) + \\frac{C}{2}||Ax -b||^2_2\\\\\r\n    s.t.\\ &amp;Ax = b\r\n\\end{aligned}\r\n\\]</span> 这个约束问题的拉格朗日函数（增广拉格朗日函数）：<span class=\"math inline\">\\(L_C(x,v) = f(x) + v^T(Ax-b)+\\frac{C}{2}||Ax -b||^2_2\\)</span></p>\r\n<p>而且这个问题的原问题最优解和对偶问题最优解都和以下问题相同： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\min\\limits_x\\ &amp;f(x)\\\\\r\n    s.t.\\ &amp;Ax = b\r\n\\end{aligned}\r\n\\]</span></p>\r\n<h3 id=\"一些性质\">一些性质</h3>\r\n<ul>\r\n<li>如果<span class=\"math inline\">\\(v = v^\\star\\)</span>，则<span class=\"math inline\">\\(\\forall C &gt; 0\\)</span>，<span class=\"math inline\">\\(x^\\star = \\mathop{\\arg\\min}\\limits_xL_C(x, v^\\star)\\)</span></li>\r\n<li>若<span class=\"math inline\">\\(C \\rightarrow +\\infty\\)</span>，则<span class=\"math inline\">\\(\\forall v, x^\\star =\\mathop{\\arg\\min}\\limits_xL_C(x, v)\\)</span></li>\r\n</ul>\r\n<p>如果有<span class=\"math inline\">\\(x^k\\)</span>，和<span class=\"math inline\">\\(v^k\\)</span>，首先更新<span class=\"math inline\">\\(x^{k+1} = \\mathop{\\arg\\min}\\limits_x L_C(x, v^k)\\)</span>，然后更新<span class=\"math inline\">\\(v^{k+1} = v^k + C \\triangledown_v L_C(x^{k+1}, v^k)\\)</span></p>\r\n"},{"title":"学习笔记_2019-01","date":"2019-02-13T07:06:42.000Z","mathjax":true,"_content":"# 2019-01-20\n\n## 学习了如何在hexo的markdown中使用数学公式\n首先修改next的配置文件如下：\n\n     mathjax:\n        enable: true\n        per_page: true\n\n由于开启了pre_page，因此首先需要在markdown页面中使用：\n\n     mathjax: true\n\n写法和latex中的公式差不多，如：\n     ```latex\n        $\\sum_{i=0}^{n}x_i$\n     ```\n显示效果：$\\sum_{i=0}^{n}x_i$\n\n但是在使用中碰到了数学公式渲染不正确的问题，网上说是hexo默认的渲染包有问题，因此需要修改。\n首先在博客根目录使用以下命令，卸载原来的渲染包，安装新的。\n\n     npm uninstall hexo-renderer-marked --save\n     npm install hexo-renderer-kramed --save\n\n这里需要注意的是一定要在博客根目录下打开控制台，才会安装到博客的node_modules目录中，否则不起作用。\n\n之后还需要修改kramed的rule文件\n\n     修改node_modules/kramed/lib/rules/inline.js\n     第11行: escape: /^\\\\([\\\\`*{}\\[\\]()#$+\\-.!_>])/,\n     替换为: escape: /^\\\\([`*\\[\\]()#$+\\-.!_>])/,\n     第20行: em: /^\\b_((?:__|[\\s\\S])+?)_\\b|^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/,\n     替换为: em: /^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/,\n\n修改之后，一切正常。语法与latex公式基本相同，详细参考[latex数学公式](https://www.luogu.org/blog/IowaBattleship/latex-gong-shi-tai-quan)\n\n## 《Deep learning》读书笔记\n### 极大似然与对数极大似然与KL散度\n极大似然估计表示如下：\n\n$$\\begin{aligned}\n\\theta_{ML}  & = \\mathop{\\arg\\max}_\\theta p_{model}(\\mathbb{X}; \\theta)\\\\\n& = \\mathop{\\arg\\max}_\\theta \\prod_{i=1}^m p_{model}(x^{(i)};\\theta)\n\\end{aligned}\n$$\n\n由于这样的连乘容易造成下溢，因此可以替换为对数极大似然估计：\n\n$$\n\\begin{aligned}\n\\theta_{ML} & = \\mathop{\\arg\\max}_\\theta\\sum_{i=1}^m\\log p_{model}(x^{(i)};\\theta)\n\\end{aligned}\n$$\n\n将训练数据看做一种经验分布$\\hat p_{data}$，且因整体缩放不会影响$\\mathop{\\arg\\max}$操作，因此后面的项可以用期望表示，对数似然估计可以用下面的式子表达：\n\n$$\n\\begin{aligned}\n\\theta_{ML} & = \\mathop{\\arg\\max}_\\theta \\mathbb{E}_{\\mathbf{x}\\sim \\hat p_{data}}\\log p_{model}(x^{(i)};\\theta)\n\\end{aligned}\n$$\n\n这其实就相当于最小化KL散度，KL散度的定义如下：\n\n$$\n\\begin{aligned}\n    D_{KL}(\\hat p_{data} \\| p_{model}) & = \\mathbb{E}_{\\mathbf{x} \\sim \\hat p_{data}}[\\log \\hat p_{data}(x) - \\log p_{model}(x)]\n\\end{aligned}\n$$\n\n其中有最后一项的期望:$-\\mathbb{E}_{\\mathbf{x} \\sim \\hat p_{data}} \\log p_{model}(x)$即是负的对数似然。\n\n# 2019-01-21\n\n## 在vscode预览markdown时渲染数学公式\n只需要安装'Markdown+Math'这个插件就OK了。\n\n## 尝试解析keras保存的参数hd5文件\n经过尝试，发现keras保存的参数文件结构如下：最上层有两个键:'optimizer_weights'和'model_weights'，其中'optimizer_weights'是优化器参数，这里不关心，第二个键有关于模型\n权重的信息。\n\n'model_weights'包含attrs属性，其下又会有三个键:'layer_names','backend','keras_version'。 \n重要的是其中的'layer_names',这个下面需要包含所有层名，字节数组的形式。\n\n'model_weights'下所有层名作为键值，每个键值都有attrs属性，attrs属性下有键值'weight_names'，包括所有的权重参数名，字节数组形式。\n\n# 2019-01-22\n\n## 空洞卷积(也叫膨胀卷积,Dilated Convolution)\n空洞卷积的数学定义如下：\n如果$F:\\mathbb{Z}^2\\rightarrow\\mathbb{R}$是一个离散函数，定义一个变量域$\\Omega_r = [-r, r]^2 \\cap\\mathbb{Z}^2$再定义一个大小为$(2r+1)^2$的离散卷积$k:\\Omega_r\\rightarrow \\mathbb{R}$,那么卷积操作可以表示为：\n\n$$\n\\begin{aligned}\n(F \\ast k)(p) = \\sum_{s+t=p}F(s)k(t)\n\\end{aligned}\n$$\n\n空洞卷积可以表示为：\n\n$$\n\\begin{aligned}\n(F \\ast_l k)(p) = \\sum_{s+lt=p}F(s)k(t)\n\\end{aligned}\n$$\n\n可见，当$l$为1时，空洞卷积就是普通的卷积。\n\n空洞卷积可以增加感受野，空洞卷积感受野示意图如下，其中(a)图为普通卷积产生的感受野示意,记为$F1$，$3 \\times 3$的普通卷积感受野和卷积核大小相同，(b)图为在(a)中的$F1$基础上进行$l$等于2的空洞卷积操作，结果记为$F2$，其感受野变为$7 \\times 7$，(c)图为在(b)中$F2$的基础上进行$l$等于4的空洞卷积，其感受野计算为$(4 \\ast 2 + 1) \\times (4 \\ast 2 + 1) = (9 \\times 9)$，注意这里的感受野计算是基于逐层卷积的结果，很多博客中没有说明，我看了原文才知道。\n{% asset_img 空洞卷积感受野示意图.png 空洞卷积感受野示意图%}\n\n# 2019-01-23\n## DenseNet论文阅读\n论文地址：[Densely Connected Convolutional Networks](https://arxiv.org/pdf/1608.06993.pdf)\n\nDenseNet模型结构如下。\n\n{%asset_img DenseNet模型结构.png DenseNet模型结构%}\n\n其中$1 \\times 1$卷积层称为bottleneck层，用于减少通道个数，DenseBlock由BN-ReLU-Conv($1 \\times 1$)-BN-ReLU-Conv($3 \\times 3$)这样的结构重复而组成，如果一个DenseBlock中每一个$3 \\times 3$的卷积输出通道个数是$k$，那么作者建议设置的bottleneck层输出通道个数为$4k$，使用了bottleneck层的DenseNet称为DenseNet-B。\n\n之后的TransitionLayer会进一步压缩模型的通道个数，其输出通道个数为$\\theta m$，其中m为DenseBlock的输出通道数。而$0 < \\theta \\le 1$，如果$0 < \\theta < 1$那么称为DenseNet-C。\n\n作者的实验中，最前面的一个卷积层输出通道数为$2k$，设置$\\theta=0.5$并且使用了bottleneck层，因此称其模型为DenseNet-BC。训练过程中，使用SGD，初始学习率为0.1在30代和60代的时候分别除以10，训练batch_size：256，一共训练90代。\n\n论文中解释说Densenet的提出是希望解决深层网络带来的梯度消失和梯度爆炸问题，并提对深度学习模型提出了一种新的解释：传统的前向传播模型就像是一种具有状态的算法，每一层读取其前一层的状态(输入)并对其进行处理，修改并保存了认为需要保存的状态，之后传到下一层，而Resnet通过相加处理，显式的保存了前一层的状态，Densenet通过通道连接，不仅保存了前一层的状态，而且还可以加以区分，虽然连接更密集，但是Densenet的模型可以参数相比于Resnet少，因为Densenet在DenseBlock中每一层的卷积核个数可以很少，通过$k$来指定。\n\n## ROI Pooling\nROI Pooling可以根据提供的区域位置信息，将特征图上的位置pooling到一个固定大小的输出。\n\n以一个输出为$2 \\times 2$的ROI Pooling为例。\n\n输入为一张特征图。\n\n{%asset_img ROI_Pooling输入.png ROI Pooling输入%}\n\n由区域建议网络给出区域位置。\n\n{%asset_img 区域建议网络给出的位置.png 区域建议网络给出的位置%}\n\n将建议区域划分为$2 \\times 2$的区域。\n\n{%asset_img 按照设定的输出大小进行划分.png 按照设定的输出大小进行划分%}\n\n在各个区域内进行Pooling操作(这里是Max Pooling)，得到最终输出。\n\n{%asset_img ROI_Pooling输出.png ROI Pooling输出%}\n\n## 卷积层输出的尺寸计算\n$n_{out} = [\\dfrac{n_{in} + 2p - k}{s}] + 1$\n\n其中$n_{out}$表示输出的特征图的大小，$n_{in}$表示输入的特征图的大小，$p$表示padding大小，$k$表示卷积核大小，$s$表示stride大小。\n\n## Inception Net\nInception使用了NIN(Network in Network)的思想，网络中的一层不再是单一的卷积，而是几种大小的卷积分支或者几个不同深度的分支进行同时计算，最后在通道维度连接到一起。\n\nInception在V1版本中(也就是GoogLeNet)使用了$5 \\times 5$和$7 \\times 7$大小的卷积核，以适应不同尺度的目标识别。\n\nInception V2版本将V1中的$5 \\times 5$和$7 \\times 7$卷积拆开成了小卷积的堆叠，减少了计算量的同时，增加了层数。在卷积层之后使用了BN层，添加了辅助分类层，论文中说，辅助分类层在最开始的训练过程中看不出效果，在模型收敛的时候才显现出效果。\n\nInception V3将V2中的辅助分类层也加上了BN。\n\nInception V4修改了之前inception模型中最前面的卷积部分(进入Inception Block之前)，将其中的下采样变成了两个分支，一个是步长为2的卷积，一个是池化，最终再Concatenate。\n\ninception v2/v3论文中还提到了设计模型的一些经验原则，我的理解如下：\n\n      1、慎用非常窄的瓶颈层，前馈神经网络中的瓶颈层会减少传播的信息量，如果瓶颈层非常窄，会导致有用信息的丢失，特征图尺寸应该从输入到输出逐渐减小，直到用来完成当前的任务(识别、分类等)。\n      2、增加卷积层的卷积核个数可以丰富特征的组合，让特征图通道之间耦合程度更低，使网络加速收敛。\n      3、网络开始的几层特征关联性很高，对其进行降维导致的信息损失较小，降维甚至可以加速学习。\n      4、平衡网络的宽度和深度。优化网络性能可以认为是平衡每个阶段(层)的卷积核数目和网络深度。同时增加宽度和深度能够提升网络性能。\n\n# 2019-01-25 ~ 2019-01-26\n## Pandas库的排序问题\n注意到了一个非常坑的地方，关于Pandas库的，例如以下代码。\n\n```python\nimport pandas as pd\ndict1 = {'c': [1, 4, 5, 2, 3]}\ndict2 = {'c': [5, 4, 1, 2, 3]}\ndf1 = pd.DataFrame(dict1)\ndf2 = pd.DataFrame(dict2)\ndf1['c'] += df2['c']\nprint(list(df1['c']))\n```\n代码中创建了两个DataFrame，然后进行列求和。\n这个输出如下，一切正常。\n\n          c\n      0   6\n      1   8\n      2   6\n      3   4\n      4   6\n\n但是，如果首先对两个DataFrame排序，如下：\n\n```python\nimport pandas as pd\ndict1 = {'c': [1, 4, 5, 2, 3]}\ndict2 = {'c': [5, 4, 1, 2, 3]}\ndf1 = pd.DataFrame(dict1)\ndf2 = pd.DataFrame(dict2)\n\ndf1 = df1.sort_values(by='c')\ndf2 = df2.sort_values(by='c')\nprint(df1)\nprint(df2)\ndf1['c'] += df2['c']\nprint(list(df1['c']))\n```\n\n这个时候输出就很奇怪了，如下。\n\n         c\n      0  1\n      3  2\n      4  3\n      1  4\n      2  5\n         c\n      2  1\n      3  2\n      4  3\n      1  4\n      0  5\n      [6, 4, 6, 8, 6]\n\n首先是打印的两个DataFrame，常规操作没有任何问题，之后进行了两列的求和，但是求和结果的顺序看不懂了，按照代码里的意思，我希望得到的结果是\n\n      [2, 4, 6, 8, 10]\n\n但是输出的结果是，乍一看，好像还挺顺口！！！\n\n      [6, 4, 6, 8, 6]\n这个结果，既不是排序之后相加，也不是原顺序相加(注意这个输出和不排序版本的输出也有差别)。\n\n最后找到了正确解释：先按照对应的index相加，之后再按照df1的index顺序进行输出。\n\n## 全连接层的反向传播算法\n之前看过深度神经网络(Deep Neural Network)反向传播的推导，但是没怎么用，现在感觉快忘光了，再来详细的推导一遍。\n首先假设损失函数:\n$$\nloss = J(a^L, y)\n$$\n其中$a^L$为第$L$层的输出值，且有\n$$\n\\begin{aligned}\na^L & = \\sigma(z^L)\\\\\nz^L & = W^L a^{L-1} + b^L\\\\\nz^L & = W^L \\sigma(z^{L-1}) + b^L\n\\end{aligned}\n$$\n那么损失函数对第$L$层的权重和偏置的偏导为：\n$$\n\\begin{aligned}\n\\frac{\\partial J}{\\partial W^L} & = \\frac{\\partial J}{\\partial z^L}\\frac{\\partial z^L}{\\partial W^L}\\\\\n\\frac{\\partial J}{\\partial b^L} & = \\frac{\\partial J}{\\partial z^L}\\frac{\\partial z^L}{\\partial b^L}\n\\end{aligned}\n$$\n首先要计算的是$\\frac{\\partial J}{\\partial z^L}$，这一项与损失函数和最后一层的激活函数有关，这里不具体讨论，直接计算即可，并将其结果记为$\\delta^L$，之后的$\\frac{\\partial z^L}{\\partial W^L}$项和$\\frac{\\partial z^L}{\\partial b^L}$的计算非常简单，最后计算出的两个偏导结果分别为$\\delta^L(a^{L-1})^T$和$\\delta^L$。\n\n到这里，第$L$层的偏导就计算完了，那么$L-1$层同理可以如下计算。\n\n$$\n\\begin{aligned}\n\\frac{\\partial J}{\\partial W^{L-1}} & = \\frac{\\partial J}{\\partial z^L}\\frac{\\partial z^L}{\\partial z^{L-1}}\\frac{\\partial z^{L-1}}{\\partial W^{L-1}}\\\\\n& = \\delta^L \\frac{\\partial z^L}{\\partial z^{L-1}}(a^{L-2})^T\\\\\n\\frac{\\partial J}{\\partial b^{L-1}} & = \\frac{\\partial J}{\\partial z^L}\\frac{\\partial z^L}{\\partial z^{L-1}}\\frac{\\partial z^{L-1}}{\\partial b^{L-1}}\\\\\n& = \\delta^L \\frac{\\partial z^L}{\\partial z^{L-1}}\n\\end{aligned}\n$$\n\n推广开来，若一共有$L$层，为方便表达，定义任意层的$\\delta^l$\n$$\n\\delta^l = \\begin{cases}\n     \\frac{\\partial J}{\\partial z^l}&l=L\\\\\n     \\\\\n     \\delta^{l+1} \\frac{\\partial z^l}{\\partial z^{l-1}}&l < L\n\\end{cases}\n$$\n\n则$L-n$层的计算如下：\n$$\n\\begin{aligned}\n     \\frac{\\partial J}{\\partial W^{L-n}} & = \\delta^{L-n} (a^{L-n-1})^T\\\\\n     \\frac{\\partial J}{\\partial b^{L-n}} & = \\delta^{L-n}\\\\\n\\end{aligned}\n$$\n\n## 卷积层的反向传播\n首先要明确数学上的离散卷积和卷积网络中的卷积操作(网上有人也称作互相关)有区别。\n\n对于数学中的二维卷积$Z = K*B$，若$K$的宽度和高度分别为$W_K, H_K$，若$B$的宽度和高度分别为$W_B, H_B$，那么其表达式可以写为:\n$$\n\\begin{aligned}\n     Z &= K \\ast B\\\\\n     Z_{s,t} &= \\sum_{h=0}^{H_K-1}\\sum_{w=0}^{W_K-1}K_{h,w}B_{s-h,t-w}\n\\end{aligned}\n$$\n\n而同样条件下，卷积网络卷积操作表示如下：\n$$\n\\begin{aligned}\n     Z &= K \\ast_{Conv} B\\\\\n     Z_{s,t} &= \\sum_{h=0}^{H_K-1}\\sum_{w=0}^{W_K-1}K_{h,w}B_{s+h,t+w}\n\\end{aligned}\n$$\n数学中的卷积$\\ast$可以看做是把$\\ast_{Conv}$中的卷积核$K$旋转180度后再进行$\\ast_{Conv}$操作。\n\n一般的卷积网络中，卷积层的操作可以表示如下\n$$\n\\begin{aligned}\n     z^l_{s,t} &= b^l + \\sum_{h=0}^{H_K-1}\\sum_{w=0}^{W_K-1}K^l_{h,w}a^{l-1}_{s+h,t+w}\\\\\n     a^l &= \\sigma(z^l)\n\\end{aligned}\n$$\n其中$a^l$表示第$l$层输出的特征图，是一个三维张量，$W_K$和$H_K$意义同上，$K^l$表示第$l$层的卷积核，二维卷积的卷积核是一个四维张量，前面两维表示位置，后两维是一个用于映射前一层特征向量到下一层特征向量的矩阵。\n\n如果不考虑卷积核的最后两维和特征图的最后一维，第$l$层的卷积核的偏导表示如下，其中$S$和$T$分别为第$z^l$层的高和宽。\n$$\n\\begin{aligned}\n     \\frac{\\partial J}{\\partial K^l} &= \\frac{\\partial J}{\\partial z^l}\\frac{\\partial z^l}{\\partial K^l}\\\\\n\\end{aligned}\n$$\n写成逐像素计算的形式，如下:\n$$\n\\begin{aligned}\n     \\frac{\\partial J}{\\partial K^l_{h,w}} &= \\sum_{s=0}^{S}\\sum_{t=0}^{T}\\frac{\\partial J}{\\partial z^l_{s,t}}\\frac{\\partial z^l_{s,t}}{\\partial K^l_{h,w}}\n\\end{aligned}\n$$\n其中的第二项如下：\n$$\n\\begin{aligned}\n     \\frac{\\partial z^l_{s,t}}{\\partial K^l_{h,w}} &= \\frac{b^l + \\sum_{h^{'}=0}^{H_K-1}\\sum_{w^{'}=0}^{W_K-1}K^l_{h^{'},w^{'}}a^{l-1}_{s+h^{'},t+w^{'}}}{\\partial K^l_{h,w}}\\\\\n     &=a^{l-1}_{s+h,t+w}\n\\end{aligned}\n$$\n则有：\n$$\n\\begin{aligned}\n     \\frac{\\partial J}{\\partial K^l_{h,w}} &= \\sum_{s=0}^{S}\\sum_{t=0}^{T}\\frac{\\partial J}{\\partial z^l_{s,t}}a^{l-1}_{s+h,t+w}\n\\end{aligned}\n$$\n\n要进一步计算，为了简化，这里和DNN中的反向传播相同，先给出$\\delta^l$定义和逐层计算规则如下：\n\n$$\n\\begin{aligned}\n     \\delta^l &= \\frac{\\partial J}{\\partial z^l}\\\\\n     \\delta^l_{s,t} &= \\frac{\\partial J}{\\partial z^l_{s,t}}\n\\end{aligned}\n$$\n\n因此可以将上面的偏导等式写成如下表示：\n\n$$\n\\begin{aligned}\n     \\frac{\\partial J}{\\partial K^l_{h,w}} &= \\sum_{s=0}^{S}\\sum_{t=0}^{T}\\delta^l_{s,t}a^{l-1}_{s+h,t+w}\\\\\n     &=\\delta^l\\ast_{Conv}a^{l-1}\n\\end{aligned}\n$$\n这里的卷积操作是上面提到的卷积层的卷积(互相关)。\n\n最后的问题就是，如何计算$\\delta^l$，推导如下：\n\n$$\n\\begin{aligned}\n     \\delta^l &= \\frac{\\partial J}{\\partial z^l}\\\\\n     \\delta^{l-1} &= \\frac{\\partial J}{\\partial z^l}\\frac{\\partial z^l}{\\partial z^{l-1}}\\\\\n     &=\\delta^l\\frac{\\partial z^l}{\\partial z^{l-1}}\\\\\n     \\delta^{l-1}_{s,t} &= \\frac{\\partial J}{\\partial z^l_{s,t}}\\\\\n     &=\\sum_{s^{'} = 0}^{S^{'}}\\sum_{t^{'} = 0}^{T^{'}}\\frac{\\partial J}{\\partial z^l_{s{'}, t^{'}}}\\frac{\\partial z^l_{s{'}, t^{'}}}{\\partial z^{l-1}_{s,t}}\\\\\n     &=\\sum_{s^{'} = 0}^{S^{'}}\\sum_{t^{'} = 0}^{T^{'}}\\delta^l_{s^{'},t^{'}}\\frac{\\partial z^l_{s{'}, t^{'}}}{\\partial z^{l-1}_{s,t}}\\\\\n     z^l_{s^{'},t^{'}} &= b^l + \\sum_{h=0}^{H_K-1}\\sum_{w=0}^{W_K-1}K^l_{h,w}a^{l-1}_{s^{'}+h,t^{'}+w}\\\\\n     &=b^l + \\sum_{h=0}^{H_K-1}\\sum_{w=0}^{W_K-1}K^l_{h,w}\\sigma(z^{l-1}_{s^{'}+h,t^{'}+w})\\\\\n     \\frac{\\partial z^l_{s{'}, t^{'}}}{\\partial z^{l-1}_{s,t}} &= K^l_{s-s^{'},t-t^{'}}\\sigma^{'}(z^l_{s, t})\\\\\n     \\delta^{l-1}_{s,t} &= \\sum_{s^{'} = 0}^{S^{'}}\\sum_{t^{'} = 0}^{T^{'}}\\delta^l_{s^{'},t^{'}}(K^l_{s-s^{'},t-t^{'}}\\sigma^{'}(z^l_{s, t}))\\\\\n     &=\\sigma^{'}(z^l_{s, t})(\\delta^l\\ast K^l)\n\\end{aligned}\n$$\n\n如果考虑卷积核的最后两维的话，卷积操作中的乘法应该是向量乘法，这里不详细讨论。\n\n## 池化层的反向传播\n池化层的反向传播算法非常简单，因为池化层没有可学习的参数，所以只需要传播$\\delta^l$，以便于前面的层计算梯度。\n\n# 2019-01-27\n\n## 无偏性\n估计值的均值等于被估计的随机变量:$E(\\hat{\\alpha}) = \\alpha$\n\n## 渐进无偏性\n渐进无偏性相比于无偏性的要求要弱一些，是指在样本数趋于无穷大的时候，估计值的期望等于被估计值:$\\lim_{n\\rightarrow\\infty}E(\\hat{\\alpha}) = \\alpha$\n\n## 依概率收敛\n\n$\\lim_{n\\rightarrow\\infty}\\mathbb{P}(|X-X_n| \\ge\\epsilon) = 0$\n\n## 条件极大似然与均方误差\n首先回顾下极大似然估计：极大似然估计用于估计数据的分布参数，其对数似然形式的定义为：$\\sum_i\\log P(x^{(i)};\\theta)$。\n\n而在线性回归问题中，给定一个输入$x$，要预测一个$y$。\n\n即需要求出一个函数$\\hat{y}(x;w)$，其参数为$w$，给定输入$x$，输出预测值。\n\n假设训练集中样本为$x^{(i)}$，符合独立同分布(i.i.d.)条件，观测标签为$y^{(i)}$。\n\n最小二乘法的思想是直接对$\\hat{y}(x;w)$建模并学习参数$w$。\n如果使用最小二乘法，学习的过程可以表示如下：\n$$\n\\begin{aligned}\n     \\mathop{\\arg\\min}_w\\sum_{i}{||y^{(i)} - \\hat{y}(x^{(i)};w)||}_2\n\\end{aligned}\n$$\n\n从似然估计的角度，可以对$P(y|x)$建模，借助高斯分布，可以做如下定义：\n\n$$\n\\begin{aligned}\n     P(y|x) = \\mathcal{N}(y;\\hat{y}(x;w),\\sigma^2)\n\\end{aligned}\n$$\n\n这里使用$\\hat{y}(x;w)$作为均值，方差则考虑了观测标签中的噪声。\n\n因此在线性回归问题中，使用条件极大似然的方法，给出条件对数似然的形式定义如下：\n\n$$\n\\begin{aligned}\n     \\sum_i\\log P(y^{(i)}|x^{(i)};w)\n\\end{aligned}\n$$\n可以进一步写成：\n$$\n\\begin{aligned}\n     \\sum_i\\log \\mathcal{N}(y;\\hat{y}(x;w),\\sigma^2)\n\\end{aligned}\n$$\n\n学习的过程可以表示成如下：\n\n$$\n\\begin{aligned}\n     \\mathop{\\arg\\max}_{w}\\sum_i\\log \\mathcal{N}(y^{(i)};\\hat{y}(x^{(i)};w),\\sigma^2)\\\\\n\\end{aligned}\n$$\n\n展开表示：\n$$\n\\begin{aligned}\n     &\\mathop{\\arg\\max}_{w} \\sum_i\\log (\\frac{1}{\\sqrt{2\\pi}\\sigma} e^{ - \\frac{(y^{(i)} - \\hat{y}(x^{(i)};w))^2}{2\\sigma^2}})\\\\\n     &=\\mathop{\\arg\\max}_{w}\\sum_i(-\\frac{1}{2}\\log2\\pi - \\log \\sigma - \\frac{(y^{(i)} - \\hat{y}(x^{(i)};w))^2}{2\\sigma^2})\n\\end{aligned}\n$$\n\n可见这里和最小二乘估计相同，依旧需要小化$\\sum_i{(y^{(i)}-\\hat{y}(x^{(i)};w))}^2$\n\n因此条件似然估计和最小二乘估计其实最终得到的结果是相同的。\n\n## hexo的一个问题\n在latex公式中，如果出现如下的写法，会导致报错(虽然在vscode中预览渲染没有问题)\n```latex\n{(...)}^2\n```\n报错如下，说是表达式后面需要逗号，明显是hexo解析有问题，暂时没有解决方案，只能换一种写法，将大括号去掉(不影响公式形式)。\n\n      INFO  Start processing\n      FATAL Something's wrong. Maybe you can find the solution here: http://hexo.io/docs/troubleshooting.html\n      Template render error: (unknown path) [Line 201, Column 78]parseAggregate: expected comma after expression\n\n# 2019-01-28\n## 贝叶斯统计方法和频率统计方法的理解\n贝叶斯统计的视角将参数也看做一个随机变量，而频率统计的视角是将参数看做一个固定量。","source":"_posts/学习笔记/学习笔记-2019-01.md","raw":"---\ntitle: 学习笔记_2019-01\ndate: 2019-02-13 15:06:42\ntags: [学习笔记，杂项]\nmathjax: true\n---\n# 2019-01-20\n\n## 学习了如何在hexo的markdown中使用数学公式\n首先修改next的配置文件如下：\n\n     mathjax:\n        enable: true\n        per_page: true\n\n由于开启了pre_page，因此首先需要在markdown页面中使用：\n\n     mathjax: true\n\n写法和latex中的公式差不多，如：\n     ```latex\n        $\\sum_{i=0}^{n}x_i$\n     ```\n显示效果：$\\sum_{i=0}^{n}x_i$\n\n但是在使用中碰到了数学公式渲染不正确的问题，网上说是hexo默认的渲染包有问题，因此需要修改。\n首先在博客根目录使用以下命令，卸载原来的渲染包，安装新的。\n\n     npm uninstall hexo-renderer-marked --save\n     npm install hexo-renderer-kramed --save\n\n这里需要注意的是一定要在博客根目录下打开控制台，才会安装到博客的node_modules目录中，否则不起作用。\n\n之后还需要修改kramed的rule文件\n\n     修改node_modules/kramed/lib/rules/inline.js\n     第11行: escape: /^\\\\([\\\\`*{}\\[\\]()#$+\\-.!_>])/,\n     替换为: escape: /^\\\\([`*\\[\\]()#$+\\-.!_>])/,\n     第20行: em: /^\\b_((?:__|[\\s\\S])+?)_\\b|^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/,\n     替换为: em: /^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/,\n\n修改之后，一切正常。语法与latex公式基本相同，详细参考[latex数学公式](https://www.luogu.org/blog/IowaBattleship/latex-gong-shi-tai-quan)\n\n## 《Deep learning》读书笔记\n### 极大似然与对数极大似然与KL散度\n极大似然估计表示如下：\n\n$$\\begin{aligned}\n\\theta_{ML}  & = \\mathop{\\arg\\max}_\\theta p_{model}(\\mathbb{X}; \\theta)\\\\\n& = \\mathop{\\arg\\max}_\\theta \\prod_{i=1}^m p_{model}(x^{(i)};\\theta)\n\\end{aligned}\n$$\n\n由于这样的连乘容易造成下溢，因此可以替换为对数极大似然估计：\n\n$$\n\\begin{aligned}\n\\theta_{ML} & = \\mathop{\\arg\\max}_\\theta\\sum_{i=1}^m\\log p_{model}(x^{(i)};\\theta)\n\\end{aligned}\n$$\n\n将训练数据看做一种经验分布$\\hat p_{data}$，且因整体缩放不会影响$\\mathop{\\arg\\max}$操作，因此后面的项可以用期望表示，对数似然估计可以用下面的式子表达：\n\n$$\n\\begin{aligned}\n\\theta_{ML} & = \\mathop{\\arg\\max}_\\theta \\mathbb{E}_{\\mathbf{x}\\sim \\hat p_{data}}\\log p_{model}(x^{(i)};\\theta)\n\\end{aligned}\n$$\n\n这其实就相当于最小化KL散度，KL散度的定义如下：\n\n$$\n\\begin{aligned}\n    D_{KL}(\\hat p_{data} \\| p_{model}) & = \\mathbb{E}_{\\mathbf{x} \\sim \\hat p_{data}}[\\log \\hat p_{data}(x) - \\log p_{model}(x)]\n\\end{aligned}\n$$\n\n其中有最后一项的期望:$-\\mathbb{E}_{\\mathbf{x} \\sim \\hat p_{data}} \\log p_{model}(x)$即是负的对数似然。\n\n# 2019-01-21\n\n## 在vscode预览markdown时渲染数学公式\n只需要安装'Markdown+Math'这个插件就OK了。\n\n## 尝试解析keras保存的参数hd5文件\n经过尝试，发现keras保存的参数文件结构如下：最上层有两个键:'optimizer_weights'和'model_weights'，其中'optimizer_weights'是优化器参数，这里不关心，第二个键有关于模型\n权重的信息。\n\n'model_weights'包含attrs属性，其下又会有三个键:'layer_names','backend','keras_version'。 \n重要的是其中的'layer_names',这个下面需要包含所有层名，字节数组的形式。\n\n'model_weights'下所有层名作为键值，每个键值都有attrs属性，attrs属性下有键值'weight_names'，包括所有的权重参数名，字节数组形式。\n\n# 2019-01-22\n\n## 空洞卷积(也叫膨胀卷积,Dilated Convolution)\n空洞卷积的数学定义如下：\n如果$F:\\mathbb{Z}^2\\rightarrow\\mathbb{R}$是一个离散函数，定义一个变量域$\\Omega_r = [-r, r]^2 \\cap\\mathbb{Z}^2$再定义一个大小为$(2r+1)^2$的离散卷积$k:\\Omega_r\\rightarrow \\mathbb{R}$,那么卷积操作可以表示为：\n\n$$\n\\begin{aligned}\n(F \\ast k)(p) = \\sum_{s+t=p}F(s)k(t)\n\\end{aligned}\n$$\n\n空洞卷积可以表示为：\n\n$$\n\\begin{aligned}\n(F \\ast_l k)(p) = \\sum_{s+lt=p}F(s)k(t)\n\\end{aligned}\n$$\n\n可见，当$l$为1时，空洞卷积就是普通的卷积。\n\n空洞卷积可以增加感受野，空洞卷积感受野示意图如下，其中(a)图为普通卷积产生的感受野示意,记为$F1$，$3 \\times 3$的普通卷积感受野和卷积核大小相同，(b)图为在(a)中的$F1$基础上进行$l$等于2的空洞卷积操作，结果记为$F2$，其感受野变为$7 \\times 7$，(c)图为在(b)中$F2$的基础上进行$l$等于4的空洞卷积，其感受野计算为$(4 \\ast 2 + 1) \\times (4 \\ast 2 + 1) = (9 \\times 9)$，注意这里的感受野计算是基于逐层卷积的结果，很多博客中没有说明，我看了原文才知道。\n{% asset_img 空洞卷积感受野示意图.png 空洞卷积感受野示意图%}\n\n# 2019-01-23\n## DenseNet论文阅读\n论文地址：[Densely Connected Convolutional Networks](https://arxiv.org/pdf/1608.06993.pdf)\n\nDenseNet模型结构如下。\n\n{%asset_img DenseNet模型结构.png DenseNet模型结构%}\n\n其中$1 \\times 1$卷积层称为bottleneck层，用于减少通道个数，DenseBlock由BN-ReLU-Conv($1 \\times 1$)-BN-ReLU-Conv($3 \\times 3$)这样的结构重复而组成，如果一个DenseBlock中每一个$3 \\times 3$的卷积输出通道个数是$k$，那么作者建议设置的bottleneck层输出通道个数为$4k$，使用了bottleneck层的DenseNet称为DenseNet-B。\n\n之后的TransitionLayer会进一步压缩模型的通道个数，其输出通道个数为$\\theta m$，其中m为DenseBlock的输出通道数。而$0 < \\theta \\le 1$，如果$0 < \\theta < 1$那么称为DenseNet-C。\n\n作者的实验中，最前面的一个卷积层输出通道数为$2k$，设置$\\theta=0.5$并且使用了bottleneck层，因此称其模型为DenseNet-BC。训练过程中，使用SGD，初始学习率为0.1在30代和60代的时候分别除以10，训练batch_size：256，一共训练90代。\n\n论文中解释说Densenet的提出是希望解决深层网络带来的梯度消失和梯度爆炸问题，并提对深度学习模型提出了一种新的解释：传统的前向传播模型就像是一种具有状态的算法，每一层读取其前一层的状态(输入)并对其进行处理，修改并保存了认为需要保存的状态，之后传到下一层，而Resnet通过相加处理，显式的保存了前一层的状态，Densenet通过通道连接，不仅保存了前一层的状态，而且还可以加以区分，虽然连接更密集，但是Densenet的模型可以参数相比于Resnet少，因为Densenet在DenseBlock中每一层的卷积核个数可以很少，通过$k$来指定。\n\n## ROI Pooling\nROI Pooling可以根据提供的区域位置信息，将特征图上的位置pooling到一个固定大小的输出。\n\n以一个输出为$2 \\times 2$的ROI Pooling为例。\n\n输入为一张特征图。\n\n{%asset_img ROI_Pooling输入.png ROI Pooling输入%}\n\n由区域建议网络给出区域位置。\n\n{%asset_img 区域建议网络给出的位置.png 区域建议网络给出的位置%}\n\n将建议区域划分为$2 \\times 2$的区域。\n\n{%asset_img 按照设定的输出大小进行划分.png 按照设定的输出大小进行划分%}\n\n在各个区域内进行Pooling操作(这里是Max Pooling)，得到最终输出。\n\n{%asset_img ROI_Pooling输出.png ROI Pooling输出%}\n\n## 卷积层输出的尺寸计算\n$n_{out} = [\\dfrac{n_{in} + 2p - k}{s}] + 1$\n\n其中$n_{out}$表示输出的特征图的大小，$n_{in}$表示输入的特征图的大小，$p$表示padding大小，$k$表示卷积核大小，$s$表示stride大小。\n\n## Inception Net\nInception使用了NIN(Network in Network)的思想，网络中的一层不再是单一的卷积，而是几种大小的卷积分支或者几个不同深度的分支进行同时计算，最后在通道维度连接到一起。\n\nInception在V1版本中(也就是GoogLeNet)使用了$5 \\times 5$和$7 \\times 7$大小的卷积核，以适应不同尺度的目标识别。\n\nInception V2版本将V1中的$5 \\times 5$和$7 \\times 7$卷积拆开成了小卷积的堆叠，减少了计算量的同时，增加了层数。在卷积层之后使用了BN层，添加了辅助分类层，论文中说，辅助分类层在最开始的训练过程中看不出效果，在模型收敛的时候才显现出效果。\n\nInception V3将V2中的辅助分类层也加上了BN。\n\nInception V4修改了之前inception模型中最前面的卷积部分(进入Inception Block之前)，将其中的下采样变成了两个分支，一个是步长为2的卷积，一个是池化，最终再Concatenate。\n\ninception v2/v3论文中还提到了设计模型的一些经验原则，我的理解如下：\n\n      1、慎用非常窄的瓶颈层，前馈神经网络中的瓶颈层会减少传播的信息量，如果瓶颈层非常窄，会导致有用信息的丢失，特征图尺寸应该从输入到输出逐渐减小，直到用来完成当前的任务(识别、分类等)。\n      2、增加卷积层的卷积核个数可以丰富特征的组合，让特征图通道之间耦合程度更低，使网络加速收敛。\n      3、网络开始的几层特征关联性很高，对其进行降维导致的信息损失较小，降维甚至可以加速学习。\n      4、平衡网络的宽度和深度。优化网络性能可以认为是平衡每个阶段(层)的卷积核数目和网络深度。同时增加宽度和深度能够提升网络性能。\n\n# 2019-01-25 ~ 2019-01-26\n## Pandas库的排序问题\n注意到了一个非常坑的地方，关于Pandas库的，例如以下代码。\n\n```python\nimport pandas as pd\ndict1 = {'c': [1, 4, 5, 2, 3]}\ndict2 = {'c': [5, 4, 1, 2, 3]}\ndf1 = pd.DataFrame(dict1)\ndf2 = pd.DataFrame(dict2)\ndf1['c'] += df2['c']\nprint(list(df1['c']))\n```\n代码中创建了两个DataFrame，然后进行列求和。\n这个输出如下，一切正常。\n\n          c\n      0   6\n      1   8\n      2   6\n      3   4\n      4   6\n\n但是，如果首先对两个DataFrame排序，如下：\n\n```python\nimport pandas as pd\ndict1 = {'c': [1, 4, 5, 2, 3]}\ndict2 = {'c': [5, 4, 1, 2, 3]}\ndf1 = pd.DataFrame(dict1)\ndf2 = pd.DataFrame(dict2)\n\ndf1 = df1.sort_values(by='c')\ndf2 = df2.sort_values(by='c')\nprint(df1)\nprint(df2)\ndf1['c'] += df2['c']\nprint(list(df1['c']))\n```\n\n这个时候输出就很奇怪了，如下。\n\n         c\n      0  1\n      3  2\n      4  3\n      1  4\n      2  5\n         c\n      2  1\n      3  2\n      4  3\n      1  4\n      0  5\n      [6, 4, 6, 8, 6]\n\n首先是打印的两个DataFrame，常规操作没有任何问题，之后进行了两列的求和，但是求和结果的顺序看不懂了，按照代码里的意思，我希望得到的结果是\n\n      [2, 4, 6, 8, 10]\n\n但是输出的结果是，乍一看，好像还挺顺口！！！\n\n      [6, 4, 6, 8, 6]\n这个结果，既不是排序之后相加，也不是原顺序相加(注意这个输出和不排序版本的输出也有差别)。\n\n最后找到了正确解释：先按照对应的index相加，之后再按照df1的index顺序进行输出。\n\n## 全连接层的反向传播算法\n之前看过深度神经网络(Deep Neural Network)反向传播的推导，但是没怎么用，现在感觉快忘光了，再来详细的推导一遍。\n首先假设损失函数:\n$$\nloss = J(a^L, y)\n$$\n其中$a^L$为第$L$层的输出值，且有\n$$\n\\begin{aligned}\na^L & = \\sigma(z^L)\\\\\nz^L & = W^L a^{L-1} + b^L\\\\\nz^L & = W^L \\sigma(z^{L-1}) + b^L\n\\end{aligned}\n$$\n那么损失函数对第$L$层的权重和偏置的偏导为：\n$$\n\\begin{aligned}\n\\frac{\\partial J}{\\partial W^L} & = \\frac{\\partial J}{\\partial z^L}\\frac{\\partial z^L}{\\partial W^L}\\\\\n\\frac{\\partial J}{\\partial b^L} & = \\frac{\\partial J}{\\partial z^L}\\frac{\\partial z^L}{\\partial b^L}\n\\end{aligned}\n$$\n首先要计算的是$\\frac{\\partial J}{\\partial z^L}$，这一项与损失函数和最后一层的激活函数有关，这里不具体讨论，直接计算即可，并将其结果记为$\\delta^L$，之后的$\\frac{\\partial z^L}{\\partial W^L}$项和$\\frac{\\partial z^L}{\\partial b^L}$的计算非常简单，最后计算出的两个偏导结果分别为$\\delta^L(a^{L-1})^T$和$\\delta^L$。\n\n到这里，第$L$层的偏导就计算完了，那么$L-1$层同理可以如下计算。\n\n$$\n\\begin{aligned}\n\\frac{\\partial J}{\\partial W^{L-1}} & = \\frac{\\partial J}{\\partial z^L}\\frac{\\partial z^L}{\\partial z^{L-1}}\\frac{\\partial z^{L-1}}{\\partial W^{L-1}}\\\\\n& = \\delta^L \\frac{\\partial z^L}{\\partial z^{L-1}}(a^{L-2})^T\\\\\n\\frac{\\partial J}{\\partial b^{L-1}} & = \\frac{\\partial J}{\\partial z^L}\\frac{\\partial z^L}{\\partial z^{L-1}}\\frac{\\partial z^{L-1}}{\\partial b^{L-1}}\\\\\n& = \\delta^L \\frac{\\partial z^L}{\\partial z^{L-1}}\n\\end{aligned}\n$$\n\n推广开来，若一共有$L$层，为方便表达，定义任意层的$\\delta^l$\n$$\n\\delta^l = \\begin{cases}\n     \\frac{\\partial J}{\\partial z^l}&l=L\\\\\n     \\\\\n     \\delta^{l+1} \\frac{\\partial z^l}{\\partial z^{l-1}}&l < L\n\\end{cases}\n$$\n\n则$L-n$层的计算如下：\n$$\n\\begin{aligned}\n     \\frac{\\partial J}{\\partial W^{L-n}} & = \\delta^{L-n} (a^{L-n-1})^T\\\\\n     \\frac{\\partial J}{\\partial b^{L-n}} & = \\delta^{L-n}\\\\\n\\end{aligned}\n$$\n\n## 卷积层的反向传播\n首先要明确数学上的离散卷积和卷积网络中的卷积操作(网上有人也称作互相关)有区别。\n\n对于数学中的二维卷积$Z = K*B$，若$K$的宽度和高度分别为$W_K, H_K$，若$B$的宽度和高度分别为$W_B, H_B$，那么其表达式可以写为:\n$$\n\\begin{aligned}\n     Z &= K \\ast B\\\\\n     Z_{s,t} &= \\sum_{h=0}^{H_K-1}\\sum_{w=0}^{W_K-1}K_{h,w}B_{s-h,t-w}\n\\end{aligned}\n$$\n\n而同样条件下，卷积网络卷积操作表示如下：\n$$\n\\begin{aligned}\n     Z &= K \\ast_{Conv} B\\\\\n     Z_{s,t} &= \\sum_{h=0}^{H_K-1}\\sum_{w=0}^{W_K-1}K_{h,w}B_{s+h,t+w}\n\\end{aligned}\n$$\n数学中的卷积$\\ast$可以看做是把$\\ast_{Conv}$中的卷积核$K$旋转180度后再进行$\\ast_{Conv}$操作。\n\n一般的卷积网络中，卷积层的操作可以表示如下\n$$\n\\begin{aligned}\n     z^l_{s,t} &= b^l + \\sum_{h=0}^{H_K-1}\\sum_{w=0}^{W_K-1}K^l_{h,w}a^{l-1}_{s+h,t+w}\\\\\n     a^l &= \\sigma(z^l)\n\\end{aligned}\n$$\n其中$a^l$表示第$l$层输出的特征图，是一个三维张量，$W_K$和$H_K$意义同上，$K^l$表示第$l$层的卷积核，二维卷积的卷积核是一个四维张量，前面两维表示位置，后两维是一个用于映射前一层特征向量到下一层特征向量的矩阵。\n\n如果不考虑卷积核的最后两维和特征图的最后一维，第$l$层的卷积核的偏导表示如下，其中$S$和$T$分别为第$z^l$层的高和宽。\n$$\n\\begin{aligned}\n     \\frac{\\partial J}{\\partial K^l} &= \\frac{\\partial J}{\\partial z^l}\\frac{\\partial z^l}{\\partial K^l}\\\\\n\\end{aligned}\n$$\n写成逐像素计算的形式，如下:\n$$\n\\begin{aligned}\n     \\frac{\\partial J}{\\partial K^l_{h,w}} &= \\sum_{s=0}^{S}\\sum_{t=0}^{T}\\frac{\\partial J}{\\partial z^l_{s,t}}\\frac{\\partial z^l_{s,t}}{\\partial K^l_{h,w}}\n\\end{aligned}\n$$\n其中的第二项如下：\n$$\n\\begin{aligned}\n     \\frac{\\partial z^l_{s,t}}{\\partial K^l_{h,w}} &= \\frac{b^l + \\sum_{h^{'}=0}^{H_K-1}\\sum_{w^{'}=0}^{W_K-1}K^l_{h^{'},w^{'}}a^{l-1}_{s+h^{'},t+w^{'}}}{\\partial K^l_{h,w}}\\\\\n     &=a^{l-1}_{s+h,t+w}\n\\end{aligned}\n$$\n则有：\n$$\n\\begin{aligned}\n     \\frac{\\partial J}{\\partial K^l_{h,w}} &= \\sum_{s=0}^{S}\\sum_{t=0}^{T}\\frac{\\partial J}{\\partial z^l_{s,t}}a^{l-1}_{s+h,t+w}\n\\end{aligned}\n$$\n\n要进一步计算，为了简化，这里和DNN中的反向传播相同，先给出$\\delta^l$定义和逐层计算规则如下：\n\n$$\n\\begin{aligned}\n     \\delta^l &= \\frac{\\partial J}{\\partial z^l}\\\\\n     \\delta^l_{s,t} &= \\frac{\\partial J}{\\partial z^l_{s,t}}\n\\end{aligned}\n$$\n\n因此可以将上面的偏导等式写成如下表示：\n\n$$\n\\begin{aligned}\n     \\frac{\\partial J}{\\partial K^l_{h,w}} &= \\sum_{s=0}^{S}\\sum_{t=0}^{T}\\delta^l_{s,t}a^{l-1}_{s+h,t+w}\\\\\n     &=\\delta^l\\ast_{Conv}a^{l-1}\n\\end{aligned}\n$$\n这里的卷积操作是上面提到的卷积层的卷积(互相关)。\n\n最后的问题就是，如何计算$\\delta^l$，推导如下：\n\n$$\n\\begin{aligned}\n     \\delta^l &= \\frac{\\partial J}{\\partial z^l}\\\\\n     \\delta^{l-1} &= \\frac{\\partial J}{\\partial z^l}\\frac{\\partial z^l}{\\partial z^{l-1}}\\\\\n     &=\\delta^l\\frac{\\partial z^l}{\\partial z^{l-1}}\\\\\n     \\delta^{l-1}_{s,t} &= \\frac{\\partial J}{\\partial z^l_{s,t}}\\\\\n     &=\\sum_{s^{'} = 0}^{S^{'}}\\sum_{t^{'} = 0}^{T^{'}}\\frac{\\partial J}{\\partial z^l_{s{'}, t^{'}}}\\frac{\\partial z^l_{s{'}, t^{'}}}{\\partial z^{l-1}_{s,t}}\\\\\n     &=\\sum_{s^{'} = 0}^{S^{'}}\\sum_{t^{'} = 0}^{T^{'}}\\delta^l_{s^{'},t^{'}}\\frac{\\partial z^l_{s{'}, t^{'}}}{\\partial z^{l-1}_{s,t}}\\\\\n     z^l_{s^{'},t^{'}} &= b^l + \\sum_{h=0}^{H_K-1}\\sum_{w=0}^{W_K-1}K^l_{h,w}a^{l-1}_{s^{'}+h,t^{'}+w}\\\\\n     &=b^l + \\sum_{h=0}^{H_K-1}\\sum_{w=0}^{W_K-1}K^l_{h,w}\\sigma(z^{l-1}_{s^{'}+h,t^{'}+w})\\\\\n     \\frac{\\partial z^l_{s{'}, t^{'}}}{\\partial z^{l-1}_{s,t}} &= K^l_{s-s^{'},t-t^{'}}\\sigma^{'}(z^l_{s, t})\\\\\n     \\delta^{l-1}_{s,t} &= \\sum_{s^{'} = 0}^{S^{'}}\\sum_{t^{'} = 0}^{T^{'}}\\delta^l_{s^{'},t^{'}}(K^l_{s-s^{'},t-t^{'}}\\sigma^{'}(z^l_{s, t}))\\\\\n     &=\\sigma^{'}(z^l_{s, t})(\\delta^l\\ast K^l)\n\\end{aligned}\n$$\n\n如果考虑卷积核的最后两维的话，卷积操作中的乘法应该是向量乘法，这里不详细讨论。\n\n## 池化层的反向传播\n池化层的反向传播算法非常简单，因为池化层没有可学习的参数，所以只需要传播$\\delta^l$，以便于前面的层计算梯度。\n\n# 2019-01-27\n\n## 无偏性\n估计值的均值等于被估计的随机变量:$E(\\hat{\\alpha}) = \\alpha$\n\n## 渐进无偏性\n渐进无偏性相比于无偏性的要求要弱一些，是指在样本数趋于无穷大的时候，估计值的期望等于被估计值:$\\lim_{n\\rightarrow\\infty}E(\\hat{\\alpha}) = \\alpha$\n\n## 依概率收敛\n\n$\\lim_{n\\rightarrow\\infty}\\mathbb{P}(|X-X_n| \\ge\\epsilon) = 0$\n\n## 条件极大似然与均方误差\n首先回顾下极大似然估计：极大似然估计用于估计数据的分布参数，其对数似然形式的定义为：$\\sum_i\\log P(x^{(i)};\\theta)$。\n\n而在线性回归问题中，给定一个输入$x$，要预测一个$y$。\n\n即需要求出一个函数$\\hat{y}(x;w)$，其参数为$w$，给定输入$x$，输出预测值。\n\n假设训练集中样本为$x^{(i)}$，符合独立同分布(i.i.d.)条件，观测标签为$y^{(i)}$。\n\n最小二乘法的思想是直接对$\\hat{y}(x;w)$建模并学习参数$w$。\n如果使用最小二乘法，学习的过程可以表示如下：\n$$\n\\begin{aligned}\n     \\mathop{\\arg\\min}_w\\sum_{i}{||y^{(i)} - \\hat{y}(x^{(i)};w)||}_2\n\\end{aligned}\n$$\n\n从似然估计的角度，可以对$P(y|x)$建模，借助高斯分布，可以做如下定义：\n\n$$\n\\begin{aligned}\n     P(y|x) = \\mathcal{N}(y;\\hat{y}(x;w),\\sigma^2)\n\\end{aligned}\n$$\n\n这里使用$\\hat{y}(x;w)$作为均值，方差则考虑了观测标签中的噪声。\n\n因此在线性回归问题中，使用条件极大似然的方法，给出条件对数似然的形式定义如下：\n\n$$\n\\begin{aligned}\n     \\sum_i\\log P(y^{(i)}|x^{(i)};w)\n\\end{aligned}\n$$\n可以进一步写成：\n$$\n\\begin{aligned}\n     \\sum_i\\log \\mathcal{N}(y;\\hat{y}(x;w),\\sigma^2)\n\\end{aligned}\n$$\n\n学习的过程可以表示成如下：\n\n$$\n\\begin{aligned}\n     \\mathop{\\arg\\max}_{w}\\sum_i\\log \\mathcal{N}(y^{(i)};\\hat{y}(x^{(i)};w),\\sigma^2)\\\\\n\\end{aligned}\n$$\n\n展开表示：\n$$\n\\begin{aligned}\n     &\\mathop{\\arg\\max}_{w} \\sum_i\\log (\\frac{1}{\\sqrt{2\\pi}\\sigma} e^{ - \\frac{(y^{(i)} - \\hat{y}(x^{(i)};w))^2}{2\\sigma^2}})\\\\\n     &=\\mathop{\\arg\\max}_{w}\\sum_i(-\\frac{1}{2}\\log2\\pi - \\log \\sigma - \\frac{(y^{(i)} - \\hat{y}(x^{(i)};w))^2}{2\\sigma^2})\n\\end{aligned}\n$$\n\n可见这里和最小二乘估计相同，依旧需要小化$\\sum_i{(y^{(i)}-\\hat{y}(x^{(i)};w))}^2$\n\n因此条件似然估计和最小二乘估计其实最终得到的结果是相同的。\n\n## hexo的一个问题\n在latex公式中，如果出现如下的写法，会导致报错(虽然在vscode中预览渲染没有问题)\n```latex\n{(...)}^2\n```\n报错如下，说是表达式后面需要逗号，明显是hexo解析有问题，暂时没有解决方案，只能换一种写法，将大括号去掉(不影响公式形式)。\n\n      INFO  Start processing\n      FATAL Something's wrong. Maybe you can find the solution here: http://hexo.io/docs/troubleshooting.html\n      Template render error: (unknown path) [Line 201, Column 78]parseAggregate: expected comma after expression\n\n# 2019-01-28\n## 贝叶斯统计方法和频率统计方法的理解\n贝叶斯统计的视角将参数也看做一个随机变量，而频率统计的视角是将参数看做一个固定量。","slug":"学习笔记/学习笔记-2019-01","published":1,"updated":"2019-07-13T02:51:42.778Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckjxpc3su003444mqethl3gbe","content":"<h1 id=\"section\">2019-01-20</h1>\r\n<h2 id=\"学习了如何在hexo的markdown中使用数学公式\">学习了如何在hexo的markdown中使用数学公式</h2>\r\n<p>首先修改next的配置文件如下：</p>\r\n<pre><code> mathjax:\r\n    enable: true\r\n    per_page: true</code></pre>\r\n<p>由于开启了pre_page，因此首先需要在markdown页面中使用：</p>\r\n<pre><code> mathjax: true</code></pre>\r\n<p>写法和latex中的公式差不多，如： <figure class=\"highlight latex\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">$</span><span class=\"keyword\">\\sum</span><span class=\"built_in\">_</span>&#123;i=0&#125;<span class=\"built_in\">^</span>&#123;n&#125;x<span class=\"built_in\">_</span>i<span class=\"built_in\">$</span></span><br></pre></td></tr></table></figure> 显示效果：<span class=\"math inline\">\\(\\sum_{i=0}^{n}x_i\\)</span></p>\r\n<p>但是在使用中碰到了数学公式渲染不正确的问题，网上说是hexo默认的渲染包有问题，因此需要修改。 首先在博客根目录使用以下命令，卸载原来的渲染包，安装新的。</p>\r\n<pre><code> npm uninstall hexo-renderer-marked --save\r\n npm install hexo-renderer-kramed --save</code></pre>\r\n<p>这里需要注意的是一定要在博客根目录下打开控制台，才会安装到博客的node_modules目录中，否则不起作用。</p>\r\n<p>之后还需要修改kramed的rule文件</p>\r\n<pre><code> 修改node_modules/kramed/lib/rules/inline.js\r\n 第11行: escape: /^\\\\([\\\\`*&#123;&#125;\\[\\]()#$+\\-.!_&gt;])/,\r\n 替换为: escape: /^\\\\([`*\\[\\]()#$+\\-.!_&gt;])/,\r\n 第20行: em: /^\\b_((?:__|[\\s\\S])+?)_\\b|^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/,\r\n 替换为: em: /^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/,</code></pre>\r\n<p>修改之后，一切正常。语法与latex公式基本相同，详细参考<a href=\"https://www.luogu.org/blog/IowaBattleship/latex-gong-shi-tai-quan\">latex数学公式</a></p>\r\n<h2 id=\"deep-learning读书笔记\">《Deep learning》读书笔记</h2>\r\n<h3 id=\"极大似然与对数极大似然与kl散度\">极大似然与对数极大似然与KL散度</h3>\r\n<p>极大似然估计表示如下：</p>\r\n<p><span class=\"math display\">\\[\\begin{aligned}\r\n\\theta_{ML}  &amp; = \\mathop{\\arg\\max}_\\theta p_{model}(\\mathbb{X}; \\theta)\\\\\r\n&amp; = \\mathop{\\arg\\max}_\\theta \\prod_{i=1}^m p_{model}(x^{(i)};\\theta)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>由于这样的连乘容易造成下溢，因此可以替换为对数极大似然估计：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n\\theta_{ML} &amp; = \\mathop{\\arg\\max}_\\theta\\sum_{i=1}^m\\log p_{model}(x^{(i)};\\theta)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>将训练数据看做一种经验分布<span class=\"math inline\">\\(\\hat p_{data}\\)</span>，且因整体缩放不会影响<span class=\"math inline\">\\(\\mathop{\\arg\\max}\\)</span>操作，因此后面的项可以用期望表示，对数似然估计可以用下面的式子表达：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n\\theta_{ML} &amp; = \\mathop{\\arg\\max}_\\theta \\mathbb{E}_{\\mathbf{x}\\sim \\hat p_{data}}\\log p_{model}(x^{(i)};\\theta)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>这其实就相当于最小化KL散度，KL散度的定义如下：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    D_{KL}(\\hat p_{data} \\| p_{model}) &amp; = \\mathbb{E}_{\\mathbf{x} \\sim \\hat p_{data}}[\\log \\hat p_{data}(x) - \\log p_{model}(x)]\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>其中有最后一项的期望:<span class=\"math inline\">\\(-\\mathbb{E}_{\\mathbf{x} \\sim \\hat p_{data}} \\log p_{model}(x)\\)</span>即是负的对数似然。</p>\r\n<h1 id=\"section-1\">2019-01-21</h1>\r\n<h2 id=\"在vscode预览markdown时渲染数学公式\">在vscode预览markdown时渲染数学公式</h2>\r\n<p>只需要安装'Markdown+Math'这个插件就OK了。</p>\r\n<h2 id=\"尝试解析keras保存的参数hd5文件\">尝试解析keras保存的参数hd5文件</h2>\r\n<p>经过尝试，发现keras保存的参数文件结构如下：最上层有两个键:'optimizer_weights'和'model_weights'，其中'optimizer_weights'是优化器参数，这里不关心，第二个键有关于模型 权重的信息。</p>\r\n<p>'model_weights'包含attrs属性，其下又会有三个键:'layer_names','backend','keras_version'。 重要的是其中的'layer_names',这个下面需要包含所有层名，字节数组的形式。</p>\r\n<p>'model_weights'下所有层名作为键值，每个键值都有attrs属性，attrs属性下有键值'weight_names'，包括所有的权重参数名，字节数组形式。</p>\r\n<h1 id=\"section-2\">2019-01-22</h1>\r\n<h2 id=\"空洞卷积也叫膨胀卷积dilated-convolution\">空洞卷积(也叫膨胀卷积,Dilated Convolution)</h2>\r\n<p>空洞卷积的数学定义如下： 如果<span class=\"math inline\">\\(F:\\mathbb{Z}^2\\rightarrow\\mathbb{R}\\)</span>是一个离散函数，定义一个变量域<span class=\"math inline\">\\(\\Omega_r = [-r, r]^2 \\cap\\mathbb{Z}^2\\)</span>再定义一个大小为<span class=\"math inline\">\\((2r+1)^2\\)</span>的离散卷积<span class=\"math inline\">\\(k:\\Omega_r\\rightarrow \\mathbb{R}\\)</span>,那么卷积操作可以表示为：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n(F \\ast k)(p) = \\sum_{s+t=p}F(s)k(t)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>空洞卷积可以表示为：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n(F \\ast_l k)(p) = \\sum_{s+lt=p}F(s)k(t)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>可见，当<span class=\"math inline\">\\(l\\)</span>为1时，空洞卷积就是普通的卷积。</p>\r\n<p>空洞卷积可以增加感受野，空洞卷积感受野示意图如下，其中(a)图为普通卷积产生的感受野示意,记为<span class=\"math inline\">\\(F1\\)</span>，<span class=\"math inline\">\\(3 \\times 3\\)</span>的普通卷积感受野和卷积核大小相同，(b)图为在(a)中的<span class=\"math inline\">\\(F1\\)</span>基础上进行<span class=\"math inline\">\\(l\\)</span>等于2的空洞卷积操作，结果记为<span class=\"math inline\">\\(F2\\)</span>，其感受野变为<span class=\"math inline\">\\(7 \\times 7\\)</span>，(c)图为在(b)中<span class=\"math inline\">\\(F2\\)</span>的基础上进行<span class=\"math inline\">\\(l\\)</span>等于4的空洞卷积，其感受野计算为<span class=\"math inline\">\\((4 \\ast 2 + 1) \\times (4 \\ast 2 + 1) = (9 \\times 9)\\)</span>，注意这里的感受野计算是基于逐层卷积的结果，很多博客中没有说明，我看了原文才知道。 <img src=\"/2019/02/13/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2019-01/%E7%A9%BA%E6%B4%9E%E5%8D%B7%E7%A7%AF%E6%84%9F%E5%8F%97%E9%87%8E%E7%A4%BA%E6%84%8F%E5%9B%BE.png\" class=\"\" title=\"空洞卷积感受野示意图\"></p>\r\n<h1 id=\"section-3\">2019-01-23</h1>\r\n<h2 id=\"densenet论文阅读\">DenseNet论文阅读</h2>\r\n<p>论文地址：<a href=\"https://arxiv.org/pdf/1608.06993.pdf\">Densely Connected Convolutional Networks</a></p>\r\n<p>DenseNet模型结构如下。</p>\r\n<img src=\"/2019/02/13/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2019-01/DenseNet%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84.png\" class=\"\" title=\"DenseNet模型结构\">\r\n<p>其中<span class=\"math inline\">\\(1 \\times 1\\)</span>卷积层称为bottleneck层，用于减少通道个数，DenseBlock由BN-ReLU-Conv(<span class=\"math inline\">\\(1 \\times 1\\)</span>)-BN-ReLU-Conv(<span class=\"math inline\">\\(3 \\times 3\\)</span>)这样的结构重复而组成，如果一个DenseBlock中每一个<span class=\"math inline\">\\(3 \\times 3\\)</span>的卷积输出通道个数是<span class=\"math inline\">\\(k\\)</span>，那么作者建议设置的bottleneck层输出通道个数为<span class=\"math inline\">\\(4k\\)</span>，使用了bottleneck层的DenseNet称为DenseNet-B。</p>\r\n<p>之后的TransitionLayer会进一步压缩模型的通道个数，其输出通道个数为<span class=\"math inline\">\\(\\theta m\\)</span>，其中m为DenseBlock的输出通道数。而<span class=\"math inline\">\\(0 &lt; \\theta \\le 1\\)</span>，如果<span class=\"math inline\">\\(0 &lt; \\theta &lt; 1\\)</span>那么称为DenseNet-C。</p>\r\n<p>作者的实验中，最前面的一个卷积层输出通道数为<span class=\"math inline\">\\(2k\\)</span>，设置<span class=\"math inline\">\\(\\theta=0.5\\)</span>并且使用了bottleneck层，因此称其模型为DenseNet-BC。训练过程中，使用SGD，初始学习率为0.1在30代和60代的时候分别除以10，训练batch_size：256，一共训练90代。</p>\r\n<p>论文中解释说Densenet的提出是希望解决深层网络带来的梯度消失和梯度爆炸问题，并提对深度学习模型提出了一种新的解释：传统的前向传播模型就像是一种具有状态的算法，每一层读取其前一层的状态(输入)并对其进行处理，修改并保存了认为需要保存的状态，之后传到下一层，而Resnet通过相加处理，显式的保存了前一层的状态，Densenet通过通道连接，不仅保存了前一层的状态，而且还可以加以区分，虽然连接更密集，但是Densenet的模型可以参数相比于Resnet少，因为Densenet在DenseBlock中每一层的卷积核个数可以很少，通过<span class=\"math inline\">\\(k\\)</span>来指定。</p>\r\n<h2 id=\"roi-pooling\">ROI Pooling</h2>\r\n<p>ROI Pooling可以根据提供的区域位置信息，将特征图上的位置pooling到一个固定大小的输出。</p>\r\n<p>以一个输出为<span class=\"math inline\">\\(2 \\times 2\\)</span>的ROI Pooling为例。</p>\r\n<p>输入为一张特征图。</p>\r\n<img src=\"/2019/02/13/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2019-01/ROI_Pooling%E8%BE%93%E5%85%A5.png\" class=\"\" title=\"ROI Pooling输入\">\r\n<p>由区域建议网络给出区域位置。</p>\r\n<img src=\"/2019/02/13/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2019-01/%E5%8C%BA%E5%9F%9F%E5%BB%BA%E8%AE%AE%E7%BD%91%E7%BB%9C%E7%BB%99%E5%87%BA%E7%9A%84%E4%BD%8D%E7%BD%AE.png\" class=\"\" title=\"区域建议网络给出的位置\">\r\n<p>将建议区域划分为<span class=\"math inline\">\\(2 \\times 2\\)</span>的区域。</p>\r\n<img src=\"/2019/02/13/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2019-01/%E6%8C%89%E7%85%A7%E8%AE%BE%E5%AE%9A%E7%9A%84%E8%BE%93%E5%87%BA%E5%A4%A7%E5%B0%8F%E8%BF%9B%E8%A1%8C%E5%88%92%E5%88%86.png\" class=\"\" title=\"按照设定的输出大小进行划分\">\r\n<p>在各个区域内进行Pooling操作(这里是Max Pooling)，得到最终输出。</p>\r\n<img src=\"/2019/02/13/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2019-01/ROI_Pooling%E8%BE%93%E5%87%BA.png\" class=\"\" title=\"ROI Pooling输出\">\r\n<h2 id=\"卷积层输出的尺寸计算\">卷积层输出的尺寸计算</h2>\r\n<p><span class=\"math inline\">\\(n_{out} = [\\dfrac{n_{in} + 2p - k}{s}] + 1\\)</span></p>\r\n<p>其中<span class=\"math inline\">\\(n_{out}\\)</span>表示输出的特征图的大小，<span class=\"math inline\">\\(n_{in}\\)</span>表示输入的特征图的大小，<span class=\"math inline\">\\(p\\)</span>表示padding大小，<span class=\"math inline\">\\(k\\)</span>表示卷积核大小，<span class=\"math inline\">\\(s\\)</span>表示stride大小。</p>\r\n<h2 id=\"inception-net\">Inception Net</h2>\r\n<p>Inception使用了NIN(Network in Network)的思想，网络中的一层不再是单一的卷积，而是几种大小的卷积分支或者几个不同深度的分支进行同时计算，最后在通道维度连接到一起。</p>\r\n<p>Inception在V1版本中(也就是GoogLeNet)使用了<span class=\"math inline\">\\(5 \\times 5\\)</span>和<span class=\"math inline\">\\(7 \\times 7\\)</span>大小的卷积核，以适应不同尺度的目标识别。</p>\r\n<p>Inception V2版本将V1中的<span class=\"math inline\">\\(5 \\times 5\\)</span>和<span class=\"math inline\">\\(7 \\times 7\\)</span>卷积拆开成了小卷积的堆叠，减少了计算量的同时，增加了层数。在卷积层之后使用了BN层，添加了辅助分类层，论文中说，辅助分类层在最开始的训练过程中看不出效果，在模型收敛的时候才显现出效果。</p>\r\n<p>Inception V3将V2中的辅助分类层也加上了BN。</p>\r\n<p>Inception V4修改了之前inception模型中最前面的卷积部分(进入Inception Block之前)，将其中的下采样变成了两个分支，一个是步长为2的卷积，一个是池化，最终再Concatenate。</p>\r\n<p>inception v2/v3论文中还提到了设计模型的一些经验原则，我的理解如下：</p>\r\n<pre><code>  1、慎用非常窄的瓶颈层，前馈神经网络中的瓶颈层会减少传播的信息量，如果瓶颈层非常窄，会导致有用信息的丢失，特征图尺寸应该从输入到输出逐渐减小，直到用来完成当前的任务(识别、分类等)。\r\n  2、增加卷积层的卷积核个数可以丰富特征的组合，让特征图通道之间耦合程度更低，使网络加速收敛。\r\n  3、网络开始的几层特征关联性很高，对其进行降维导致的信息损失较小，降维甚至可以加速学习。\r\n  4、平衡网络的宽度和深度。优化网络性能可以认为是平衡每个阶段(层)的卷积核数目和网络深度。同时增加宽度和深度能够提升网络性能。</code></pre>\r\n<h1 id=\"section-4\">2019-01-25 ~ 2019-01-26</h1>\r\n<h2 id=\"pandas库的排序问题\">Pandas库的排序问题</h2>\r\n<p>注意到了一个非常坑的地方，关于Pandas库的，例如以下代码。</p>\r\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\">dict1 = &#123;<span class=\"string\">&#x27;c&#x27;</span>: [<span class=\"number\">1</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>]&#125;</span><br><span class=\"line\">dict2 = &#123;<span class=\"string\">&#x27;c&#x27;</span>: [<span class=\"number\">5</span>, <span class=\"number\">4</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>]&#125;</span><br><span class=\"line\">df1 = pd.DataFrame(dict1)</span><br><span class=\"line\">df2 = pd.DataFrame(dict2)</span><br><span class=\"line\">df1[<span class=\"string\">&#x27;c&#x27;</span>] += df2[<span class=\"string\">&#x27;c&#x27;</span>]</span><br><span class=\"line\">print(<span class=\"built_in\">list</span>(df1[<span class=\"string\">&#x27;c&#x27;</span>]))</span><br></pre></td></tr></table></figure>\r\n<p>代码中创建了两个DataFrame，然后进行列求和。 这个输出如下，一切正常。</p>\r\n<pre><code>      c\r\n  0   6\r\n  1   8\r\n  2   6\r\n  3   4\r\n  4   6</code></pre>\r\n<p>但是，如果首先对两个DataFrame排序，如下：</p>\r\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\">dict1 = &#123;<span class=\"string\">&#x27;c&#x27;</span>: [<span class=\"number\">1</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>]&#125;</span><br><span class=\"line\">dict2 = &#123;<span class=\"string\">&#x27;c&#x27;</span>: [<span class=\"number\">5</span>, <span class=\"number\">4</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>]&#125;</span><br><span class=\"line\">df1 = pd.DataFrame(dict1)</span><br><span class=\"line\">df2 = pd.DataFrame(dict2)</span><br><span class=\"line\"></span><br><span class=\"line\">df1 = df1.sort_values(by=<span class=\"string\">&#x27;c&#x27;</span>)</span><br><span class=\"line\">df2 = df2.sort_values(by=<span class=\"string\">&#x27;c&#x27;</span>)</span><br><span class=\"line\">print(df1)</span><br><span class=\"line\">print(df2)</span><br><span class=\"line\">df1[<span class=\"string\">&#x27;c&#x27;</span>] += df2[<span class=\"string\">&#x27;c&#x27;</span>]</span><br><span class=\"line\">print(<span class=\"built_in\">list</span>(df1[<span class=\"string\">&#x27;c&#x27;</span>]))</span><br></pre></td></tr></table></figure>\r\n<p>这个时候输出就很奇怪了，如下。</p>\r\n<pre><code>     c\r\n  0  1\r\n  3  2\r\n  4  3\r\n  1  4\r\n  2  5\r\n     c\r\n  2  1\r\n  3  2\r\n  4  3\r\n  1  4\r\n  0  5\r\n  [6, 4, 6, 8, 6]</code></pre>\r\n<p>首先是打印的两个DataFrame，常规操作没有任何问题，之后进行了两列的求和，但是求和结果的顺序看不懂了，按照代码里的意思，我希望得到的结果是</p>\r\n<pre><code>  [2, 4, 6, 8, 10]</code></pre>\r\n<p>但是输出的结果是，乍一看，好像还挺顺口！！！</p>\r\n<pre><code>  [6, 4, 6, 8, 6]</code></pre>\r\n<p>这个结果，既不是排序之后相加，也不是原顺序相加(注意这个输出和不排序版本的输出也有差别)。</p>\r\n<p>最后找到了正确解释：先按照对应的index相加，之后再按照df1的index顺序进行输出。</p>\r\n<h2 id=\"全连接层的反向传播算法\">全连接层的反向传播算法</h2>\r\n<p>之前看过深度神经网络(Deep Neural Network)反向传播的推导，但是没怎么用，现在感觉快忘光了，再来详细的推导一遍。 首先假设损失函数: <span class=\"math display\">\\[\r\nloss = J(a^L, y)\r\n\\]</span> 其中<span class=\"math inline\">\\(a^L\\)</span>为第<span class=\"math inline\">\\(L\\)</span>层的输出值，且有 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\na^L &amp; = \\sigma(z^L)\\\\\r\nz^L &amp; = W^L a^{L-1} + b^L\\\\\r\nz^L &amp; = W^L \\sigma(z^{L-1}) + b^L\r\n\\end{aligned}\r\n\\]</span> 那么损失函数对第<span class=\"math inline\">\\(L\\)</span>层的权重和偏置的偏导为： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n\\frac{\\partial J}{\\partial W^L} &amp; = \\frac{\\partial J}{\\partial z^L}\\frac{\\partial z^L}{\\partial W^L}\\\\\r\n\\frac{\\partial J}{\\partial b^L} &amp; = \\frac{\\partial J}{\\partial z^L}\\frac{\\partial z^L}{\\partial b^L}\r\n\\end{aligned}\r\n\\]</span> 首先要计算的是<span class=\"math inline\">\\(\\frac{\\partial J}{\\partial z^L}\\)</span>，这一项与损失函数和最后一层的激活函数有关，这里不具体讨论，直接计算即可，并将其结果记为<span class=\"math inline\">\\(\\delta^L\\)</span>，之后的<span class=\"math inline\">\\(\\frac{\\partial z^L}{\\partial W^L}\\)</span>项和<span class=\"math inline\">\\(\\frac{\\partial z^L}{\\partial b^L}\\)</span>的计算非常简单，最后计算出的两个偏导结果分别为<span class=\"math inline\">\\(\\delta^L(a^{L-1})^T\\)</span>和<span class=\"math inline\">\\(\\delta^L\\)</span>。</p>\r\n<p>到这里，第<span class=\"math inline\">\\(L\\)</span>层的偏导就计算完了，那么<span class=\"math inline\">\\(L-1\\)</span>层同理可以如下计算。</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n\\frac{\\partial J}{\\partial W^{L-1}} &amp; = \\frac{\\partial J}{\\partial z^L}\\frac{\\partial z^L}{\\partial z^{L-1}}\\frac{\\partial z^{L-1}}{\\partial W^{L-1}}\\\\\r\n&amp; = \\delta^L \\frac{\\partial z^L}{\\partial z^{L-1}}(a^{L-2})^T\\\\\r\n\\frac{\\partial J}{\\partial b^{L-1}} &amp; = \\frac{\\partial J}{\\partial z^L}\\frac{\\partial z^L}{\\partial z^{L-1}}\\frac{\\partial z^{L-1}}{\\partial b^{L-1}}\\\\\r\n&amp; = \\delta^L \\frac{\\partial z^L}{\\partial z^{L-1}}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>推广开来，若一共有<span class=\"math inline\">\\(L\\)</span>层，为方便表达，定义任意层的<span class=\"math inline\">\\(\\delta^l\\)</span> <span class=\"math display\">\\[\r\n\\delta^l = \\begin{cases}\r\n     \\frac{\\partial J}{\\partial z^l}&amp;l=L\\\\\r\n     \\\\\r\n     \\delta^{l+1} \\frac{\\partial z^l}{\\partial z^{l-1}}&amp;l &lt; L\r\n\\end{cases}\r\n\\]</span></p>\r\n<p>则<span class=\"math inline\">\\(L-n\\)</span>层的计算如下： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n     \\frac{\\partial J}{\\partial W^{L-n}} &amp; = \\delta^{L-n} (a^{L-n-1})^T\\\\\r\n     \\frac{\\partial J}{\\partial b^{L-n}} &amp; = \\delta^{L-n}\\\\\r\n\\end{aligned}\r\n\\]</span></p>\r\n<h2 id=\"卷积层的反向传播\">卷积层的反向传播</h2>\r\n<p>首先要明确数学上的离散卷积和卷积网络中的卷积操作(网上有人也称作互相关)有区别。</p>\r\n<p>对于数学中的二维卷积<span class=\"math inline\">\\(Z = K*B\\)</span>，若<span class=\"math inline\">\\(K\\)</span>的宽度和高度分别为<span class=\"math inline\">\\(W_K, H_K\\)</span>，若<span class=\"math inline\">\\(B\\)</span>的宽度和高度分别为<span class=\"math inline\">\\(W_B, H_B\\)</span>，那么其表达式可以写为: <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n     Z &amp;= K \\ast B\\\\\r\n     Z_{s,t} &amp;= \\sum_{h=0}^{H_K-1}\\sum_{w=0}^{W_K-1}K_{h,w}B_{s-h,t-w}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>而同样条件下，卷积网络卷积操作表示如下： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n     Z &amp;= K \\ast_{Conv} B\\\\\r\n     Z_{s,t} &amp;= \\sum_{h=0}^{H_K-1}\\sum_{w=0}^{W_K-1}K_{h,w}B_{s+h,t+w}\r\n\\end{aligned}\r\n\\]</span> 数学中的卷积<span class=\"math inline\">\\(\\ast\\)</span>可以看做是把<span class=\"math inline\">\\(\\ast_{Conv}\\)</span>中的卷积核<span class=\"math inline\">\\(K\\)</span>旋转180度后再进行<span class=\"math inline\">\\(\\ast_{Conv}\\)</span>操作。</p>\r\n<p>一般的卷积网络中，卷积层的操作可以表示如下 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n     z^l_{s,t} &amp;= b^l + \\sum_{h=0}^{H_K-1}\\sum_{w=0}^{W_K-1}K^l_{h,w}a^{l-1}_{s+h,t+w}\\\\\r\n     a^l &amp;= \\sigma(z^l)\r\n\\end{aligned}\r\n\\]</span> 其中<span class=\"math inline\">\\(a^l\\)</span>表示第<span class=\"math inline\">\\(l\\)</span>层输出的特征图，是一个三维张量，<span class=\"math inline\">\\(W_K\\)</span>和<span class=\"math inline\">\\(H_K\\)</span>意义同上，<span class=\"math inline\">\\(K^l\\)</span>表示第<span class=\"math inline\">\\(l\\)</span>层的卷积核，二维卷积的卷积核是一个四维张量，前面两维表示位置，后两维是一个用于映射前一层特征向量到下一层特征向量的矩阵。</p>\r\n<p>如果不考虑卷积核的最后两维和特征图的最后一维，第<span class=\"math inline\">\\(l\\)</span>层的卷积核的偏导表示如下，其中<span class=\"math inline\">\\(S\\)</span>和<span class=\"math inline\">\\(T\\)</span>分别为第<span class=\"math inline\">\\(z^l\\)</span>层的高和宽。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n     \\frac{\\partial J}{\\partial K^l} &amp;= \\frac{\\partial J}{\\partial z^l}\\frac{\\partial z^l}{\\partial K^l}\\\\\r\n\\end{aligned}\r\n\\]</span> 写成逐像素计算的形式，如下: <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n     \\frac{\\partial J}{\\partial K^l_{h,w}} &amp;= \\sum_{s=0}^{S}\\sum_{t=0}^{T}\\frac{\\partial J}{\\partial z^l_{s,t}}\\frac{\\partial z^l_{s,t}}{\\partial K^l_{h,w}}\r\n\\end{aligned}\r\n\\]</span> 其中的第二项如下： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n     \\frac{\\partial z^l_{s,t}}{\\partial K^l_{h,w}} &amp;= \\frac{b^l + \\sum_{h^{&#39;}=0}^{H_K-1}\\sum_{w^{&#39;}=0}^{W_K-1}K^l_{h^{&#39;},w^{&#39;}}a^{l-1}_{s+h^{&#39;},t+w^{&#39;}}}{\\partial K^l_{h,w}}\\\\\r\n     &amp;=a^{l-1}_{s+h,t+w}\r\n\\end{aligned}\r\n\\]</span> 则有： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n     \\frac{\\partial J}{\\partial K^l_{h,w}} &amp;= \\sum_{s=0}^{S}\\sum_{t=0}^{T}\\frac{\\partial J}{\\partial z^l_{s,t}}a^{l-1}_{s+h,t+w}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>要进一步计算，为了简化，这里和DNN中的反向传播相同，先给出<span class=\"math inline\">\\(\\delta^l\\)</span>定义和逐层计算规则如下：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n     \\delta^l &amp;= \\frac{\\partial J}{\\partial z^l}\\\\\r\n     \\delta^l_{s,t} &amp;= \\frac{\\partial J}{\\partial z^l_{s,t}}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>因此可以将上面的偏导等式写成如下表示：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n     \\frac{\\partial J}{\\partial K^l_{h,w}} &amp;= \\sum_{s=0}^{S}\\sum_{t=0}^{T}\\delta^l_{s,t}a^{l-1}_{s+h,t+w}\\\\\r\n     &amp;=\\delta^l\\ast_{Conv}a^{l-1}\r\n\\end{aligned}\r\n\\]</span> 这里的卷积操作是上面提到的卷积层的卷积(互相关)。</p>\r\n<p>最后的问题就是，如何计算<span class=\"math inline\">\\(\\delta^l\\)</span>，推导如下：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n     \\delta^l &amp;= \\frac{\\partial J}{\\partial z^l}\\\\\r\n     \\delta^{l-1} &amp;= \\frac{\\partial J}{\\partial z^l}\\frac{\\partial z^l}{\\partial z^{l-1}}\\\\\r\n     &amp;=\\delta^l\\frac{\\partial z^l}{\\partial z^{l-1}}\\\\\r\n     \\delta^{l-1}_{s,t} &amp;= \\frac{\\partial J}{\\partial z^l_{s,t}}\\\\\r\n     &amp;=\\sum_{s^{&#39;} = 0}^{S^{&#39;}}\\sum_{t^{&#39;} = 0}^{T^{&#39;}}\\frac{\\partial J}{\\partial z^l_{s{&#39;}, t^{&#39;}}}\\frac{\\partial z^l_{s{&#39;}, t^{&#39;}}}{\\partial z^{l-1}_{s,t}}\\\\\r\n     &amp;=\\sum_{s^{&#39;} = 0}^{S^{&#39;}}\\sum_{t^{&#39;} = 0}^{T^{&#39;}}\\delta^l_{s^{&#39;},t^{&#39;}}\\frac{\\partial z^l_{s{&#39;}, t^{&#39;}}}{\\partial z^{l-1}_{s,t}}\\\\\r\n     z^l_{s^{&#39;},t^{&#39;}} &amp;= b^l + \\sum_{h=0}^{H_K-1}\\sum_{w=0}^{W_K-1}K^l_{h,w}a^{l-1}_{s^{&#39;}+h,t^{&#39;}+w}\\\\\r\n     &amp;=b^l + \\sum_{h=0}^{H_K-1}\\sum_{w=0}^{W_K-1}K^l_{h,w}\\sigma(z^{l-1}_{s^{&#39;}+h,t^{&#39;}+w})\\\\\r\n     \\frac{\\partial z^l_{s{&#39;}, t^{&#39;}}}{\\partial z^{l-1}_{s,t}} &amp;= K^l_{s-s^{&#39;},t-t^{&#39;}}\\sigma^{&#39;}(z^l_{s, t})\\\\\r\n     \\delta^{l-1}_{s,t} &amp;= \\sum_{s^{&#39;} = 0}^{S^{&#39;}}\\sum_{t^{&#39;} = 0}^{T^{&#39;}}\\delta^l_{s^{&#39;},t^{&#39;}}(K^l_{s-s^{&#39;},t-t^{&#39;}}\\sigma^{&#39;}(z^l_{s, t}))\\\\\r\n     &amp;=\\sigma^{&#39;}(z^l_{s, t})(\\delta^l\\ast K^l)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>如果考虑卷积核的最后两维的话，卷积操作中的乘法应该是向量乘法，这里不详细讨论。</p>\r\n<h2 id=\"池化层的反向传播\">池化层的反向传播</h2>\r\n<p>池化层的反向传播算法非常简单，因为池化层没有可学习的参数，所以只需要传播<span class=\"math inline\">\\(\\delta^l\\)</span>，以便于前面的层计算梯度。</p>\r\n<h1 id=\"section-5\">2019-01-27</h1>\r\n<h2 id=\"无偏性\">无偏性</h2>\r\n<p>估计值的均值等于被估计的随机变量:<span class=\"math inline\">\\(E(\\hat{\\alpha}) = \\alpha\\)</span></p>\r\n<h2 id=\"渐进无偏性\">渐进无偏性</h2>\r\n<p>渐进无偏性相比于无偏性的要求要弱一些，是指在样本数趋于无穷大的时候，估计值的期望等于被估计值:<span class=\"math inline\">\\(\\lim_{n\\rightarrow\\infty}E(\\hat{\\alpha}) = \\alpha\\)</span></p>\r\n<h2 id=\"依概率收敛\">依概率收敛</h2>\r\n<p><span class=\"math inline\">\\(\\lim_{n\\rightarrow\\infty}\\mathbb{P}(|X-X_n| \\ge\\epsilon) = 0\\)</span></p>\r\n<h2 id=\"条件极大似然与均方误差\">条件极大似然与均方误差</h2>\r\n<p>首先回顾下极大似然估计：极大似然估计用于估计数据的分布参数，其对数似然形式的定义为：<span class=\"math inline\">\\(\\sum_i\\log P(x^{(i)};\\theta)\\)</span>。</p>\r\n<p>而在线性回归问题中，给定一个输入<span class=\"math inline\">\\(x\\)</span>，要预测一个<span class=\"math inline\">\\(y\\)</span>。</p>\r\n<p>即需要求出一个函数<span class=\"math inline\">\\(\\hat{y}(x;w)\\)</span>，其参数为<span class=\"math inline\">\\(w\\)</span>，给定输入<span class=\"math inline\">\\(x\\)</span>，输出预测值。</p>\r\n<p>假设训练集中样本为<span class=\"math inline\">\\(x^{(i)}\\)</span>，符合独立同分布(i.i.d.)条件，观测标签为<span class=\"math inline\">\\(y^{(i)}\\)</span>。</p>\r\n<p>最小二乘法的思想是直接对<span class=\"math inline\">\\(\\hat{y}(x;w)\\)</span>建模并学习参数<span class=\"math inline\">\\(w\\)</span>。 如果使用最小二乘法，学习的过程可以表示如下： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n     \\mathop{\\arg\\min}_w\\sum_{i}{||y^{(i)} - \\hat{y}(x^{(i)};w)||}_2\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>从似然估计的角度，可以对<span class=\"math inline\">\\(P(y|x)\\)</span>建模，借助高斯分布，可以做如下定义：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n     P(y|x) = \\mathcal{N}(y;\\hat{y}(x;w),\\sigma^2)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>这里使用<span class=\"math inline\">\\(\\hat{y}(x;w)\\)</span>作为均值，方差则考虑了观测标签中的噪声。</p>\r\n<p>因此在线性回归问题中，使用条件极大似然的方法，给出条件对数似然的形式定义如下：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n     \\sum_i\\log P(y^{(i)}|x^{(i)};w)\r\n\\end{aligned}\r\n\\]</span> 可以进一步写成： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n     \\sum_i\\log \\mathcal{N}(y;\\hat{y}(x;w),\\sigma^2)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>学习的过程可以表示成如下：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n     \\mathop{\\arg\\max}_{w}\\sum_i\\log \\mathcal{N}(y^{(i)};\\hat{y}(x^{(i)};w),\\sigma^2)\\\\\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>展开表示： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n     &amp;\\mathop{\\arg\\max}_{w} \\sum_i\\log (\\frac{1}{\\sqrt{2\\pi}\\sigma} e^{ - \\frac{(y^{(i)} - \\hat{y}(x^{(i)};w))^2}{2\\sigma^2}})\\\\\r\n     &amp;=\\mathop{\\arg\\max}_{w}\\sum_i(-\\frac{1}{2}\\log2\\pi - \\log \\sigma - \\frac{(y^{(i)} - \\hat{y}(x^{(i)};w))^2}{2\\sigma^2})\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>可见这里和最小二乘估计相同，依旧需要小化<span class=\"math inline\">\\(\\sum_i{(y^{(i)}-\\hat{y}(x^{(i)};w))}^2\\)</span></p>\r\n<p>因此条件似然估计和最小二乘估计其实最终得到的结果是相同的。</p>\r\n<h2 id=\"hexo的一个问题\">hexo的一个问题</h2>\r\n<p>在latex公式中，如果出现如下的写法，会导致报错(虽然在vscode中预览渲染没有问题) <figure class=\"highlight latex\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;(...)&#125;<span class=\"built_in\">^</span>2</span><br></pre></td></tr></table></figure> 报错如下，说是表达式后面需要逗号，明显是hexo解析有问题，暂时没有解决方案，只能换一种写法，将大括号去掉(不影响公式形式)。</p>\r\n<pre><code>  INFO  Start processing\r\n  FATAL Something&#39;s wrong. Maybe you can find the solution here: http://hexo.io/docs/troubleshooting.html\r\n  Template render error: (unknown path) [Line 201, Column 78]parseAggregate: expected comma after expression</code></pre>\r\n<h1 id=\"section-6\">2019-01-28</h1>\r\n<h2 id=\"贝叶斯统计方法和频率统计方法的理解\">贝叶斯统计方法和频率统计方法的理解</h2>\r\n<p>贝叶斯统计的视角将参数也看做一个随机变量，而频率统计的视角是将参数看做一个固定量。</p>\r\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"section\">2019-01-20</h1>\r\n<h2 id=\"学习了如何在hexo的markdown中使用数学公式\">学习了如何在hexo的markdown中使用数学公式</h2>\r\n<p>首先修改next的配置文件如下：</p>\r\n<pre><code> mathjax:\r\n    enable: true\r\n    per_page: true</code></pre>\r\n<p>由于开启了pre_page，因此首先需要在markdown页面中使用：</p>\r\n<pre><code> mathjax: true</code></pre>\r\n<p>写法和latex中的公式差不多，如： <figure class=\"highlight latex\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">$</span><span class=\"keyword\">\\sum</span><span class=\"built_in\">_</span>&#123;i=0&#125;<span class=\"built_in\">^</span>&#123;n&#125;x<span class=\"built_in\">_</span>i<span class=\"built_in\">$</span></span><br></pre></td></tr></table></figure> 显示效果：<span class=\"math inline\">\\(\\sum_{i=0}^{n}x_i\\)</span></p>\r\n<p>但是在使用中碰到了数学公式渲染不正确的问题，网上说是hexo默认的渲染包有问题，因此需要修改。 首先在博客根目录使用以下命令，卸载原来的渲染包，安装新的。</p>\r\n<pre><code> npm uninstall hexo-renderer-marked --save\r\n npm install hexo-renderer-kramed --save</code></pre>\r\n<p>这里需要注意的是一定要在博客根目录下打开控制台，才会安装到博客的node_modules目录中，否则不起作用。</p>\r\n<p>之后还需要修改kramed的rule文件</p>\r\n<pre><code> 修改node_modules/kramed/lib/rules/inline.js\r\n 第11行: escape: /^\\\\([\\\\`*&#123;&#125;\\[\\]()#$+\\-.!_&gt;])/,\r\n 替换为: escape: /^\\\\([`*\\[\\]()#$+\\-.!_&gt;])/,\r\n 第20行: em: /^\\b_((?:__|[\\s\\S])+?)_\\b|^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/,\r\n 替换为: em: /^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/,</code></pre>\r\n<p>修改之后，一切正常。语法与latex公式基本相同，详细参考<a href=\"https://www.luogu.org/blog/IowaBattleship/latex-gong-shi-tai-quan\">latex数学公式</a></p>\r\n<h2 id=\"deep-learning读书笔记\">《Deep learning》读书笔记</h2>\r\n<h3 id=\"极大似然与对数极大似然与kl散度\">极大似然与对数极大似然与KL散度</h3>\r\n<p>极大似然估计表示如下：</p>\r\n<p><span class=\"math display\">\\[\\begin{aligned}\r\n\\theta_{ML}  &amp; = \\mathop{\\arg\\max}_\\theta p_{model}(\\mathbb{X}; \\theta)\\\\\r\n&amp; = \\mathop{\\arg\\max}_\\theta \\prod_{i=1}^m p_{model}(x^{(i)};\\theta)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>由于这样的连乘容易造成下溢，因此可以替换为对数极大似然估计：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n\\theta_{ML} &amp; = \\mathop{\\arg\\max}_\\theta\\sum_{i=1}^m\\log p_{model}(x^{(i)};\\theta)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>将训练数据看做一种经验分布<span class=\"math inline\">\\(\\hat p_{data}\\)</span>，且因整体缩放不会影响<span class=\"math inline\">\\(\\mathop{\\arg\\max}\\)</span>操作，因此后面的项可以用期望表示，对数似然估计可以用下面的式子表达：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n\\theta_{ML} &amp; = \\mathop{\\arg\\max}_\\theta \\mathbb{E}_{\\mathbf{x}\\sim \\hat p_{data}}\\log p_{model}(x^{(i)};\\theta)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>这其实就相当于最小化KL散度，KL散度的定义如下：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    D_{KL}(\\hat p_{data} \\| p_{model}) &amp; = \\mathbb{E}_{\\mathbf{x} \\sim \\hat p_{data}}[\\log \\hat p_{data}(x) - \\log p_{model}(x)]\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>其中有最后一项的期望:<span class=\"math inline\">\\(-\\mathbb{E}_{\\mathbf{x} \\sim \\hat p_{data}} \\log p_{model}(x)\\)</span>即是负的对数似然。</p>\r\n<h1 id=\"section-1\">2019-01-21</h1>\r\n<h2 id=\"在vscode预览markdown时渲染数学公式\">在vscode预览markdown时渲染数学公式</h2>\r\n<p>只需要安装'Markdown+Math'这个插件就OK了。</p>\r\n<h2 id=\"尝试解析keras保存的参数hd5文件\">尝试解析keras保存的参数hd5文件</h2>\r\n<p>经过尝试，发现keras保存的参数文件结构如下：最上层有两个键:'optimizer_weights'和'model_weights'，其中'optimizer_weights'是优化器参数，这里不关心，第二个键有关于模型 权重的信息。</p>\r\n<p>'model_weights'包含attrs属性，其下又会有三个键:'layer_names','backend','keras_version'。 重要的是其中的'layer_names',这个下面需要包含所有层名，字节数组的形式。</p>\r\n<p>'model_weights'下所有层名作为键值，每个键值都有attrs属性，attrs属性下有键值'weight_names'，包括所有的权重参数名，字节数组形式。</p>\r\n<h1 id=\"section-2\">2019-01-22</h1>\r\n<h2 id=\"空洞卷积也叫膨胀卷积dilated-convolution\">空洞卷积(也叫膨胀卷积,Dilated Convolution)</h2>\r\n<p>空洞卷积的数学定义如下： 如果<span class=\"math inline\">\\(F:\\mathbb{Z}^2\\rightarrow\\mathbb{R}\\)</span>是一个离散函数，定义一个变量域<span class=\"math inline\">\\(\\Omega_r = [-r, r]^2 \\cap\\mathbb{Z}^2\\)</span>再定义一个大小为<span class=\"math inline\">\\((2r+1)^2\\)</span>的离散卷积<span class=\"math inline\">\\(k:\\Omega_r\\rightarrow \\mathbb{R}\\)</span>,那么卷积操作可以表示为：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n(F \\ast k)(p) = \\sum_{s+t=p}F(s)k(t)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>空洞卷积可以表示为：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n(F \\ast_l k)(p) = \\sum_{s+lt=p}F(s)k(t)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>可见，当<span class=\"math inline\">\\(l\\)</span>为1时，空洞卷积就是普通的卷积。</p>\r\n<p>空洞卷积可以增加感受野，空洞卷积感受野示意图如下，其中(a)图为普通卷积产生的感受野示意,记为<span class=\"math inline\">\\(F1\\)</span>，<span class=\"math inline\">\\(3 \\times 3\\)</span>的普通卷积感受野和卷积核大小相同，(b)图为在(a)中的<span class=\"math inline\">\\(F1\\)</span>基础上进行<span class=\"math inline\">\\(l\\)</span>等于2的空洞卷积操作，结果记为<span class=\"math inline\">\\(F2\\)</span>，其感受野变为<span class=\"math inline\">\\(7 \\times 7\\)</span>，(c)图为在(b)中<span class=\"math inline\">\\(F2\\)</span>的基础上进行<span class=\"math inline\">\\(l\\)</span>等于4的空洞卷积，其感受野计算为<span class=\"math inline\">\\((4 \\ast 2 + 1) \\times (4 \\ast 2 + 1) = (9 \\times 9)\\)</span>，注意这里的感受野计算是基于逐层卷积的结果，很多博客中没有说明，我看了原文才知道。 <img src=\"/2019/02/13/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2019-01/%E7%A9%BA%E6%B4%9E%E5%8D%B7%E7%A7%AF%E6%84%9F%E5%8F%97%E9%87%8E%E7%A4%BA%E6%84%8F%E5%9B%BE.png\" class=\"\" title=\"空洞卷积感受野示意图\"></p>\r\n<h1 id=\"section-3\">2019-01-23</h1>\r\n<h2 id=\"densenet论文阅读\">DenseNet论文阅读</h2>\r\n<p>论文地址：<a href=\"https://arxiv.org/pdf/1608.06993.pdf\">Densely Connected Convolutional Networks</a></p>\r\n<p>DenseNet模型结构如下。</p>\r\n<img src=\"/2019/02/13/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2019-01/DenseNet%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84.png\" class=\"\" title=\"DenseNet模型结构\">\r\n<p>其中<span class=\"math inline\">\\(1 \\times 1\\)</span>卷积层称为bottleneck层，用于减少通道个数，DenseBlock由BN-ReLU-Conv(<span class=\"math inline\">\\(1 \\times 1\\)</span>)-BN-ReLU-Conv(<span class=\"math inline\">\\(3 \\times 3\\)</span>)这样的结构重复而组成，如果一个DenseBlock中每一个<span class=\"math inline\">\\(3 \\times 3\\)</span>的卷积输出通道个数是<span class=\"math inline\">\\(k\\)</span>，那么作者建议设置的bottleneck层输出通道个数为<span class=\"math inline\">\\(4k\\)</span>，使用了bottleneck层的DenseNet称为DenseNet-B。</p>\r\n<p>之后的TransitionLayer会进一步压缩模型的通道个数，其输出通道个数为<span class=\"math inline\">\\(\\theta m\\)</span>，其中m为DenseBlock的输出通道数。而<span class=\"math inline\">\\(0 &lt; \\theta \\le 1\\)</span>，如果<span class=\"math inline\">\\(0 &lt; \\theta &lt; 1\\)</span>那么称为DenseNet-C。</p>\r\n<p>作者的实验中，最前面的一个卷积层输出通道数为<span class=\"math inline\">\\(2k\\)</span>，设置<span class=\"math inline\">\\(\\theta=0.5\\)</span>并且使用了bottleneck层，因此称其模型为DenseNet-BC。训练过程中，使用SGD，初始学习率为0.1在30代和60代的时候分别除以10，训练batch_size：256，一共训练90代。</p>\r\n<p>论文中解释说Densenet的提出是希望解决深层网络带来的梯度消失和梯度爆炸问题，并提对深度学习模型提出了一种新的解释：传统的前向传播模型就像是一种具有状态的算法，每一层读取其前一层的状态(输入)并对其进行处理，修改并保存了认为需要保存的状态，之后传到下一层，而Resnet通过相加处理，显式的保存了前一层的状态，Densenet通过通道连接，不仅保存了前一层的状态，而且还可以加以区分，虽然连接更密集，但是Densenet的模型可以参数相比于Resnet少，因为Densenet在DenseBlock中每一层的卷积核个数可以很少，通过<span class=\"math inline\">\\(k\\)</span>来指定。</p>\r\n<h2 id=\"roi-pooling\">ROI Pooling</h2>\r\n<p>ROI Pooling可以根据提供的区域位置信息，将特征图上的位置pooling到一个固定大小的输出。</p>\r\n<p>以一个输出为<span class=\"math inline\">\\(2 \\times 2\\)</span>的ROI Pooling为例。</p>\r\n<p>输入为一张特征图。</p>\r\n<img src=\"/2019/02/13/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2019-01/ROI_Pooling%E8%BE%93%E5%85%A5.png\" class=\"\" title=\"ROI Pooling输入\">\r\n<p>由区域建议网络给出区域位置。</p>\r\n<img src=\"/2019/02/13/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2019-01/%E5%8C%BA%E5%9F%9F%E5%BB%BA%E8%AE%AE%E7%BD%91%E7%BB%9C%E7%BB%99%E5%87%BA%E7%9A%84%E4%BD%8D%E7%BD%AE.png\" class=\"\" title=\"区域建议网络给出的位置\">\r\n<p>将建议区域划分为<span class=\"math inline\">\\(2 \\times 2\\)</span>的区域。</p>\r\n<img src=\"/2019/02/13/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2019-01/%E6%8C%89%E7%85%A7%E8%AE%BE%E5%AE%9A%E7%9A%84%E8%BE%93%E5%87%BA%E5%A4%A7%E5%B0%8F%E8%BF%9B%E8%A1%8C%E5%88%92%E5%88%86.png\" class=\"\" title=\"按照设定的输出大小进行划分\">\r\n<p>在各个区域内进行Pooling操作(这里是Max Pooling)，得到最终输出。</p>\r\n<img src=\"/2019/02/13/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2019-01/ROI_Pooling%E8%BE%93%E5%87%BA.png\" class=\"\" title=\"ROI Pooling输出\">\r\n<h2 id=\"卷积层输出的尺寸计算\">卷积层输出的尺寸计算</h2>\r\n<p><span class=\"math inline\">\\(n_{out} = [\\dfrac{n_{in} + 2p - k}{s}] + 1\\)</span></p>\r\n<p>其中<span class=\"math inline\">\\(n_{out}\\)</span>表示输出的特征图的大小，<span class=\"math inline\">\\(n_{in}\\)</span>表示输入的特征图的大小，<span class=\"math inline\">\\(p\\)</span>表示padding大小，<span class=\"math inline\">\\(k\\)</span>表示卷积核大小，<span class=\"math inline\">\\(s\\)</span>表示stride大小。</p>\r\n<h2 id=\"inception-net\">Inception Net</h2>\r\n<p>Inception使用了NIN(Network in Network)的思想，网络中的一层不再是单一的卷积，而是几种大小的卷积分支或者几个不同深度的分支进行同时计算，最后在通道维度连接到一起。</p>\r\n<p>Inception在V1版本中(也就是GoogLeNet)使用了<span class=\"math inline\">\\(5 \\times 5\\)</span>和<span class=\"math inline\">\\(7 \\times 7\\)</span>大小的卷积核，以适应不同尺度的目标识别。</p>\r\n<p>Inception V2版本将V1中的<span class=\"math inline\">\\(5 \\times 5\\)</span>和<span class=\"math inline\">\\(7 \\times 7\\)</span>卷积拆开成了小卷积的堆叠，减少了计算量的同时，增加了层数。在卷积层之后使用了BN层，添加了辅助分类层，论文中说，辅助分类层在最开始的训练过程中看不出效果，在模型收敛的时候才显现出效果。</p>\r\n<p>Inception V3将V2中的辅助分类层也加上了BN。</p>\r\n<p>Inception V4修改了之前inception模型中最前面的卷积部分(进入Inception Block之前)，将其中的下采样变成了两个分支，一个是步长为2的卷积，一个是池化，最终再Concatenate。</p>\r\n<p>inception v2/v3论文中还提到了设计模型的一些经验原则，我的理解如下：</p>\r\n<pre><code>  1、慎用非常窄的瓶颈层，前馈神经网络中的瓶颈层会减少传播的信息量，如果瓶颈层非常窄，会导致有用信息的丢失，特征图尺寸应该从输入到输出逐渐减小，直到用来完成当前的任务(识别、分类等)。\r\n  2、增加卷积层的卷积核个数可以丰富特征的组合，让特征图通道之间耦合程度更低，使网络加速收敛。\r\n  3、网络开始的几层特征关联性很高，对其进行降维导致的信息损失较小，降维甚至可以加速学习。\r\n  4、平衡网络的宽度和深度。优化网络性能可以认为是平衡每个阶段(层)的卷积核数目和网络深度。同时增加宽度和深度能够提升网络性能。</code></pre>\r\n<h1 id=\"section-4\">2019-01-25 ~ 2019-01-26</h1>\r\n<h2 id=\"pandas库的排序问题\">Pandas库的排序问题</h2>\r\n<p>注意到了一个非常坑的地方，关于Pandas库的，例如以下代码。</p>\r\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\">dict1 = &#123;<span class=\"string\">&#x27;c&#x27;</span>: [<span class=\"number\">1</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>]&#125;</span><br><span class=\"line\">dict2 = &#123;<span class=\"string\">&#x27;c&#x27;</span>: [<span class=\"number\">5</span>, <span class=\"number\">4</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>]&#125;</span><br><span class=\"line\">df1 = pd.DataFrame(dict1)</span><br><span class=\"line\">df2 = pd.DataFrame(dict2)</span><br><span class=\"line\">df1[<span class=\"string\">&#x27;c&#x27;</span>] += df2[<span class=\"string\">&#x27;c&#x27;</span>]</span><br><span class=\"line\">print(<span class=\"built_in\">list</span>(df1[<span class=\"string\">&#x27;c&#x27;</span>]))</span><br></pre></td></tr></table></figure>\r\n<p>代码中创建了两个DataFrame，然后进行列求和。 这个输出如下，一切正常。</p>\r\n<pre><code>      c\r\n  0   6\r\n  1   8\r\n  2   6\r\n  3   4\r\n  4   6</code></pre>\r\n<p>但是，如果首先对两个DataFrame排序，如下：</p>\r\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\">dict1 = &#123;<span class=\"string\">&#x27;c&#x27;</span>: [<span class=\"number\">1</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>]&#125;</span><br><span class=\"line\">dict2 = &#123;<span class=\"string\">&#x27;c&#x27;</span>: [<span class=\"number\">5</span>, <span class=\"number\">4</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>]&#125;</span><br><span class=\"line\">df1 = pd.DataFrame(dict1)</span><br><span class=\"line\">df2 = pd.DataFrame(dict2)</span><br><span class=\"line\"></span><br><span class=\"line\">df1 = df1.sort_values(by=<span class=\"string\">&#x27;c&#x27;</span>)</span><br><span class=\"line\">df2 = df2.sort_values(by=<span class=\"string\">&#x27;c&#x27;</span>)</span><br><span class=\"line\">print(df1)</span><br><span class=\"line\">print(df2)</span><br><span class=\"line\">df1[<span class=\"string\">&#x27;c&#x27;</span>] += df2[<span class=\"string\">&#x27;c&#x27;</span>]</span><br><span class=\"line\">print(<span class=\"built_in\">list</span>(df1[<span class=\"string\">&#x27;c&#x27;</span>]))</span><br></pre></td></tr></table></figure>\r\n<p>这个时候输出就很奇怪了，如下。</p>\r\n<pre><code>     c\r\n  0  1\r\n  3  2\r\n  4  3\r\n  1  4\r\n  2  5\r\n     c\r\n  2  1\r\n  3  2\r\n  4  3\r\n  1  4\r\n  0  5\r\n  [6, 4, 6, 8, 6]</code></pre>\r\n<p>首先是打印的两个DataFrame，常规操作没有任何问题，之后进行了两列的求和，但是求和结果的顺序看不懂了，按照代码里的意思，我希望得到的结果是</p>\r\n<pre><code>  [2, 4, 6, 8, 10]</code></pre>\r\n<p>但是输出的结果是，乍一看，好像还挺顺口！！！</p>\r\n<pre><code>  [6, 4, 6, 8, 6]</code></pre>\r\n<p>这个结果，既不是排序之后相加，也不是原顺序相加(注意这个输出和不排序版本的输出也有差别)。</p>\r\n<p>最后找到了正确解释：先按照对应的index相加，之后再按照df1的index顺序进行输出。</p>\r\n<h2 id=\"全连接层的反向传播算法\">全连接层的反向传播算法</h2>\r\n<p>之前看过深度神经网络(Deep Neural Network)反向传播的推导，但是没怎么用，现在感觉快忘光了，再来详细的推导一遍。 首先假设损失函数: <span class=\"math display\">\\[\r\nloss = J(a^L, y)\r\n\\]</span> 其中<span class=\"math inline\">\\(a^L\\)</span>为第<span class=\"math inline\">\\(L\\)</span>层的输出值，且有 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\na^L &amp; = \\sigma(z^L)\\\\\r\nz^L &amp; = W^L a^{L-1} + b^L\\\\\r\nz^L &amp; = W^L \\sigma(z^{L-1}) + b^L\r\n\\end{aligned}\r\n\\]</span> 那么损失函数对第<span class=\"math inline\">\\(L\\)</span>层的权重和偏置的偏导为： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n\\frac{\\partial J}{\\partial W^L} &amp; = \\frac{\\partial J}{\\partial z^L}\\frac{\\partial z^L}{\\partial W^L}\\\\\r\n\\frac{\\partial J}{\\partial b^L} &amp; = \\frac{\\partial J}{\\partial z^L}\\frac{\\partial z^L}{\\partial b^L}\r\n\\end{aligned}\r\n\\]</span> 首先要计算的是<span class=\"math inline\">\\(\\frac{\\partial J}{\\partial z^L}\\)</span>，这一项与损失函数和最后一层的激活函数有关，这里不具体讨论，直接计算即可，并将其结果记为<span class=\"math inline\">\\(\\delta^L\\)</span>，之后的<span class=\"math inline\">\\(\\frac{\\partial z^L}{\\partial W^L}\\)</span>项和<span class=\"math inline\">\\(\\frac{\\partial z^L}{\\partial b^L}\\)</span>的计算非常简单，最后计算出的两个偏导结果分别为<span class=\"math inline\">\\(\\delta^L(a^{L-1})^T\\)</span>和<span class=\"math inline\">\\(\\delta^L\\)</span>。</p>\r\n<p>到这里，第<span class=\"math inline\">\\(L\\)</span>层的偏导就计算完了，那么<span class=\"math inline\">\\(L-1\\)</span>层同理可以如下计算。</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n\\frac{\\partial J}{\\partial W^{L-1}} &amp; = \\frac{\\partial J}{\\partial z^L}\\frac{\\partial z^L}{\\partial z^{L-1}}\\frac{\\partial z^{L-1}}{\\partial W^{L-1}}\\\\\r\n&amp; = \\delta^L \\frac{\\partial z^L}{\\partial z^{L-1}}(a^{L-2})^T\\\\\r\n\\frac{\\partial J}{\\partial b^{L-1}} &amp; = \\frac{\\partial J}{\\partial z^L}\\frac{\\partial z^L}{\\partial z^{L-1}}\\frac{\\partial z^{L-1}}{\\partial b^{L-1}}\\\\\r\n&amp; = \\delta^L \\frac{\\partial z^L}{\\partial z^{L-1}}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>推广开来，若一共有<span class=\"math inline\">\\(L\\)</span>层，为方便表达，定义任意层的<span class=\"math inline\">\\(\\delta^l\\)</span> <span class=\"math display\">\\[\r\n\\delta^l = \\begin{cases}\r\n     \\frac{\\partial J}{\\partial z^l}&amp;l=L\\\\\r\n     \\\\\r\n     \\delta^{l+1} \\frac{\\partial z^l}{\\partial z^{l-1}}&amp;l &lt; L\r\n\\end{cases}\r\n\\]</span></p>\r\n<p>则<span class=\"math inline\">\\(L-n\\)</span>层的计算如下： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n     \\frac{\\partial J}{\\partial W^{L-n}} &amp; = \\delta^{L-n} (a^{L-n-1})^T\\\\\r\n     \\frac{\\partial J}{\\partial b^{L-n}} &amp; = \\delta^{L-n}\\\\\r\n\\end{aligned}\r\n\\]</span></p>\r\n<h2 id=\"卷积层的反向传播\">卷积层的反向传播</h2>\r\n<p>首先要明确数学上的离散卷积和卷积网络中的卷积操作(网上有人也称作互相关)有区别。</p>\r\n<p>对于数学中的二维卷积<span class=\"math inline\">\\(Z = K*B\\)</span>，若<span class=\"math inline\">\\(K\\)</span>的宽度和高度分别为<span class=\"math inline\">\\(W_K, H_K\\)</span>，若<span class=\"math inline\">\\(B\\)</span>的宽度和高度分别为<span class=\"math inline\">\\(W_B, H_B\\)</span>，那么其表达式可以写为: <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n     Z &amp;= K \\ast B\\\\\r\n     Z_{s,t} &amp;= \\sum_{h=0}^{H_K-1}\\sum_{w=0}^{W_K-1}K_{h,w}B_{s-h,t-w}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>而同样条件下，卷积网络卷积操作表示如下： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n     Z &amp;= K \\ast_{Conv} B\\\\\r\n     Z_{s,t} &amp;= \\sum_{h=0}^{H_K-1}\\sum_{w=0}^{W_K-1}K_{h,w}B_{s+h,t+w}\r\n\\end{aligned}\r\n\\]</span> 数学中的卷积<span class=\"math inline\">\\(\\ast\\)</span>可以看做是把<span class=\"math inline\">\\(\\ast_{Conv}\\)</span>中的卷积核<span class=\"math inline\">\\(K\\)</span>旋转180度后再进行<span class=\"math inline\">\\(\\ast_{Conv}\\)</span>操作。</p>\r\n<p>一般的卷积网络中，卷积层的操作可以表示如下 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n     z^l_{s,t} &amp;= b^l + \\sum_{h=0}^{H_K-1}\\sum_{w=0}^{W_K-1}K^l_{h,w}a^{l-1}_{s+h,t+w}\\\\\r\n     a^l &amp;= \\sigma(z^l)\r\n\\end{aligned}\r\n\\]</span> 其中<span class=\"math inline\">\\(a^l\\)</span>表示第<span class=\"math inline\">\\(l\\)</span>层输出的特征图，是一个三维张量，<span class=\"math inline\">\\(W_K\\)</span>和<span class=\"math inline\">\\(H_K\\)</span>意义同上，<span class=\"math inline\">\\(K^l\\)</span>表示第<span class=\"math inline\">\\(l\\)</span>层的卷积核，二维卷积的卷积核是一个四维张量，前面两维表示位置，后两维是一个用于映射前一层特征向量到下一层特征向量的矩阵。</p>\r\n<p>如果不考虑卷积核的最后两维和特征图的最后一维，第<span class=\"math inline\">\\(l\\)</span>层的卷积核的偏导表示如下，其中<span class=\"math inline\">\\(S\\)</span>和<span class=\"math inline\">\\(T\\)</span>分别为第<span class=\"math inline\">\\(z^l\\)</span>层的高和宽。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n     \\frac{\\partial J}{\\partial K^l} &amp;= \\frac{\\partial J}{\\partial z^l}\\frac{\\partial z^l}{\\partial K^l}\\\\\r\n\\end{aligned}\r\n\\]</span> 写成逐像素计算的形式，如下: <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n     \\frac{\\partial J}{\\partial K^l_{h,w}} &amp;= \\sum_{s=0}^{S}\\sum_{t=0}^{T}\\frac{\\partial J}{\\partial z^l_{s,t}}\\frac{\\partial z^l_{s,t}}{\\partial K^l_{h,w}}\r\n\\end{aligned}\r\n\\]</span> 其中的第二项如下： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n     \\frac{\\partial z^l_{s,t}}{\\partial K^l_{h,w}} &amp;= \\frac{b^l + \\sum_{h^{&#39;}=0}^{H_K-1}\\sum_{w^{&#39;}=0}^{W_K-1}K^l_{h^{&#39;},w^{&#39;}}a^{l-1}_{s+h^{&#39;},t+w^{&#39;}}}{\\partial K^l_{h,w}}\\\\\r\n     &amp;=a^{l-1}_{s+h,t+w}\r\n\\end{aligned}\r\n\\]</span> 则有： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n     \\frac{\\partial J}{\\partial K^l_{h,w}} &amp;= \\sum_{s=0}^{S}\\sum_{t=0}^{T}\\frac{\\partial J}{\\partial z^l_{s,t}}a^{l-1}_{s+h,t+w}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>要进一步计算，为了简化，这里和DNN中的反向传播相同，先给出<span class=\"math inline\">\\(\\delta^l\\)</span>定义和逐层计算规则如下：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n     \\delta^l &amp;= \\frac{\\partial J}{\\partial z^l}\\\\\r\n     \\delta^l_{s,t} &amp;= \\frac{\\partial J}{\\partial z^l_{s,t}}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>因此可以将上面的偏导等式写成如下表示：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n     \\frac{\\partial J}{\\partial K^l_{h,w}} &amp;= \\sum_{s=0}^{S}\\sum_{t=0}^{T}\\delta^l_{s,t}a^{l-1}_{s+h,t+w}\\\\\r\n     &amp;=\\delta^l\\ast_{Conv}a^{l-1}\r\n\\end{aligned}\r\n\\]</span> 这里的卷积操作是上面提到的卷积层的卷积(互相关)。</p>\r\n<p>最后的问题就是，如何计算<span class=\"math inline\">\\(\\delta^l\\)</span>，推导如下：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n     \\delta^l &amp;= \\frac{\\partial J}{\\partial z^l}\\\\\r\n     \\delta^{l-1} &amp;= \\frac{\\partial J}{\\partial z^l}\\frac{\\partial z^l}{\\partial z^{l-1}}\\\\\r\n     &amp;=\\delta^l\\frac{\\partial z^l}{\\partial z^{l-1}}\\\\\r\n     \\delta^{l-1}_{s,t} &amp;= \\frac{\\partial J}{\\partial z^l_{s,t}}\\\\\r\n     &amp;=\\sum_{s^{&#39;} = 0}^{S^{&#39;}}\\sum_{t^{&#39;} = 0}^{T^{&#39;}}\\frac{\\partial J}{\\partial z^l_{s{&#39;}, t^{&#39;}}}\\frac{\\partial z^l_{s{&#39;}, t^{&#39;}}}{\\partial z^{l-1}_{s,t}}\\\\\r\n     &amp;=\\sum_{s^{&#39;} = 0}^{S^{&#39;}}\\sum_{t^{&#39;} = 0}^{T^{&#39;}}\\delta^l_{s^{&#39;},t^{&#39;}}\\frac{\\partial z^l_{s{&#39;}, t^{&#39;}}}{\\partial z^{l-1}_{s,t}}\\\\\r\n     z^l_{s^{&#39;},t^{&#39;}} &amp;= b^l + \\sum_{h=0}^{H_K-1}\\sum_{w=0}^{W_K-1}K^l_{h,w}a^{l-1}_{s^{&#39;}+h,t^{&#39;}+w}\\\\\r\n     &amp;=b^l + \\sum_{h=0}^{H_K-1}\\sum_{w=0}^{W_K-1}K^l_{h,w}\\sigma(z^{l-1}_{s^{&#39;}+h,t^{&#39;}+w})\\\\\r\n     \\frac{\\partial z^l_{s{&#39;}, t^{&#39;}}}{\\partial z^{l-1}_{s,t}} &amp;= K^l_{s-s^{&#39;},t-t^{&#39;}}\\sigma^{&#39;}(z^l_{s, t})\\\\\r\n     \\delta^{l-1}_{s,t} &amp;= \\sum_{s^{&#39;} = 0}^{S^{&#39;}}\\sum_{t^{&#39;} = 0}^{T^{&#39;}}\\delta^l_{s^{&#39;},t^{&#39;}}(K^l_{s-s^{&#39;},t-t^{&#39;}}\\sigma^{&#39;}(z^l_{s, t}))\\\\\r\n     &amp;=\\sigma^{&#39;}(z^l_{s, t})(\\delta^l\\ast K^l)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>如果考虑卷积核的最后两维的话，卷积操作中的乘法应该是向量乘法，这里不详细讨论。</p>\r\n<h2 id=\"池化层的反向传播\">池化层的反向传播</h2>\r\n<p>池化层的反向传播算法非常简单，因为池化层没有可学习的参数，所以只需要传播<span class=\"math inline\">\\(\\delta^l\\)</span>，以便于前面的层计算梯度。</p>\r\n<h1 id=\"section-5\">2019-01-27</h1>\r\n<h2 id=\"无偏性\">无偏性</h2>\r\n<p>估计值的均值等于被估计的随机变量:<span class=\"math inline\">\\(E(\\hat{\\alpha}) = \\alpha\\)</span></p>\r\n<h2 id=\"渐进无偏性\">渐进无偏性</h2>\r\n<p>渐进无偏性相比于无偏性的要求要弱一些，是指在样本数趋于无穷大的时候，估计值的期望等于被估计值:<span class=\"math inline\">\\(\\lim_{n\\rightarrow\\infty}E(\\hat{\\alpha}) = \\alpha\\)</span></p>\r\n<h2 id=\"依概率收敛\">依概率收敛</h2>\r\n<p><span class=\"math inline\">\\(\\lim_{n\\rightarrow\\infty}\\mathbb{P}(|X-X_n| \\ge\\epsilon) = 0\\)</span></p>\r\n<h2 id=\"条件极大似然与均方误差\">条件极大似然与均方误差</h2>\r\n<p>首先回顾下极大似然估计：极大似然估计用于估计数据的分布参数，其对数似然形式的定义为：<span class=\"math inline\">\\(\\sum_i\\log P(x^{(i)};\\theta)\\)</span>。</p>\r\n<p>而在线性回归问题中，给定一个输入<span class=\"math inline\">\\(x\\)</span>，要预测一个<span class=\"math inline\">\\(y\\)</span>。</p>\r\n<p>即需要求出一个函数<span class=\"math inline\">\\(\\hat{y}(x;w)\\)</span>，其参数为<span class=\"math inline\">\\(w\\)</span>，给定输入<span class=\"math inline\">\\(x\\)</span>，输出预测值。</p>\r\n<p>假设训练集中样本为<span class=\"math inline\">\\(x^{(i)}\\)</span>，符合独立同分布(i.i.d.)条件，观测标签为<span class=\"math inline\">\\(y^{(i)}\\)</span>。</p>\r\n<p>最小二乘法的思想是直接对<span class=\"math inline\">\\(\\hat{y}(x;w)\\)</span>建模并学习参数<span class=\"math inline\">\\(w\\)</span>。 如果使用最小二乘法，学习的过程可以表示如下： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n     \\mathop{\\arg\\min}_w\\sum_{i}{||y^{(i)} - \\hat{y}(x^{(i)};w)||}_2\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>从似然估计的角度，可以对<span class=\"math inline\">\\(P(y|x)\\)</span>建模，借助高斯分布，可以做如下定义：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n     P(y|x) = \\mathcal{N}(y;\\hat{y}(x;w),\\sigma^2)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>这里使用<span class=\"math inline\">\\(\\hat{y}(x;w)\\)</span>作为均值，方差则考虑了观测标签中的噪声。</p>\r\n<p>因此在线性回归问题中，使用条件极大似然的方法，给出条件对数似然的形式定义如下：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n     \\sum_i\\log P(y^{(i)}|x^{(i)};w)\r\n\\end{aligned}\r\n\\]</span> 可以进一步写成： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n     \\sum_i\\log \\mathcal{N}(y;\\hat{y}(x;w),\\sigma^2)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>学习的过程可以表示成如下：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n     \\mathop{\\arg\\max}_{w}\\sum_i\\log \\mathcal{N}(y^{(i)};\\hat{y}(x^{(i)};w),\\sigma^2)\\\\\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>展开表示： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n     &amp;\\mathop{\\arg\\max}_{w} \\sum_i\\log (\\frac{1}{\\sqrt{2\\pi}\\sigma} e^{ - \\frac{(y^{(i)} - \\hat{y}(x^{(i)};w))^2}{2\\sigma^2}})\\\\\r\n     &amp;=\\mathop{\\arg\\max}_{w}\\sum_i(-\\frac{1}{2}\\log2\\pi - \\log \\sigma - \\frac{(y^{(i)} - \\hat{y}(x^{(i)};w))^2}{2\\sigma^2})\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>可见这里和最小二乘估计相同，依旧需要小化<span class=\"math inline\">\\(\\sum_i{(y^{(i)}-\\hat{y}(x^{(i)};w))}^2\\)</span></p>\r\n<p>因此条件似然估计和最小二乘估计其实最终得到的结果是相同的。</p>\r\n<h2 id=\"hexo的一个问题\">hexo的一个问题</h2>\r\n<p>在latex公式中，如果出现如下的写法，会导致报错(虽然在vscode中预览渲染没有问题) <figure class=\"highlight latex\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;(...)&#125;<span class=\"built_in\">^</span>2</span><br></pre></td></tr></table></figure> 报错如下，说是表达式后面需要逗号，明显是hexo解析有问题，暂时没有解决方案，只能换一种写法，将大括号去掉(不影响公式形式)。</p>\r\n<pre><code>  INFO  Start processing\r\n  FATAL Something&#39;s wrong. Maybe you can find the solution here: http://hexo.io/docs/troubleshooting.html\r\n  Template render error: (unknown path) [Line 201, Column 78]parseAggregate: expected comma after expression</code></pre>\r\n<h1 id=\"section-6\">2019-01-28</h1>\r\n<h2 id=\"贝叶斯统计方法和频率统计方法的理解\">贝叶斯统计方法和频率统计方法的理解</h2>\r\n<p>贝叶斯统计的视角将参数也看做一个随机变量，而频率统计的视角是将参数看做一个固定量。</p>\r\n"},{"title":"学习笔记_2019-02","date":"2019-02-13T07:16:58.000Z","mathjax":true,"_content":"\n# 2019-02-13\n\n## 支持向量机\n首先复习了数学上的一个小知识，如果有一个超平面可以表达为如下线性方程：\n$$\n\\begin{aligned}\n    w^Tx+b = 0\n\\end{aligned}\n$$\n则空间中任意一点$x_0$到该超平面的距离可以表示为如下：\n$$\n\\begin{aligned}\n    r = \\frac{|w^Tx_0+b|}{||w||}\n\\end{aligned}\n$$\n\n在分类问题的样本空间中，令$x_i$和$y_i$分别表示某个样本和其对应的标签，支持向量机的基本思想是求出一个满足如下条件的超平面：\n$$\n\\begin{cases}\n    w^Tx_i + b \\ge 1 & y_i = 1\\\\\n    w^Tx_i + b \\le 1 & y_i = -1\n\\end{cases}\n$$\n示意图如下,这里z展示的样本的特征空间为2维，图中被圈出来的满足上式等号条件一个正样本和两个负样本被称为支持向量，$\\gamma=\\frac{2}{||w||}$称为间隔。\n\n{% asset_img 支持向量机超平面示意.png 支持向量机超平面示意%}\n\n找分类超平面的问题就可以转化为如下：\n$$\n\\begin{aligned}\n    &\\max_{w,b}\\frac{2}{||w||}\\\\\n    &s.t.\\quad y_i(w^Tx_i + b) \\ge 1, i = 1,2,...,m\\\\\n    等价为\\\\\n    &\\min_{w,b}\\frac{||w||^2}{2}\\\\\n    &s.t.\\quad y_i(w^Tx_i + b) \\ge 1, i = 1,2,...,m\n\\end{aligned}\n$$\n这里可以使用不等式约束下的拉格朗日乘子法转换为对偶问题进行求解。\n\n支持向量机的基本型就是希望找到一个间隔最大的超平面，但是这里有个问题，在原始的特征空间中，样本不一定线性可分，因此引入一个函数，将原始特征空间映射到一个高维的特征空间，使得在这个高维空间中线性可分，这样即可将任何分类问题转换成线性可分的形式，但是这样也有一个问题，设映射后的特征向量为$\\phi(x)$，求解超平面的过程中，会遇到${\\phi(x)}^T\\phi(x)$的情况，因为$\\phi(x)$的维数可以为无穷，所以这里的计算无法进行，因此需要设想一个这样的函数：$k(x_i,x_j)={\\phi(x_i)}^T\\phi(x_j)$使得不需要通过$\\phi(x)$来计算${\\phi(x)}^T\\phi(x)$，而是直接通过$k(x_i,x_j)$来表示，这个函数即称为核函数。\n\n核函数可以用一个核矩阵来表示如下，任何一个对称的半正定矩阵，都可以作为核矩阵。\n$$\nk=\\begin{bmatrix}\n    k(x_1,x_1) &\\cdots &k(x_1,x_j) &\\cdots &k(x_1,x_n)\\\\\n    \\vdots &\\ddots &\\vdots &\\ddots &\\vdots\\\\\n    k(x_j,x_1) &\\cdots &k(x_j,x_j) &\\cdots &k(x_j,x_n)\\\\\n    \\vdots &\\ddots &\\vdots &\\ddots &\\vdots\\\\\n    k(x_n,x_1) &\\cdots &k(x_n,x_j) &\\cdots &k(x_n,x_n)\\\\\n\\end{bmatrix}\n$$\n\n选择一个核函数，就隐式的确定了映射的特征空间，因此核函数的选择非常重要。\n\n但是在实际使用中，选出一个合适的特征空间恰好让这个样本线性可分非常不容易，因此将优化目标重写为：\n$$\n\\min_{w,b}\\frac{1}{2}{||w||}^2+C\\sum_{i=1}^ml(y_i(w^Tx_i+b) - 1)\n$$\n这里的$C$为一个设定好的常数，决定模型的偏好，$l$为一种损失函数。这样的支持向量机被称为软间隔支持向量机。\n\n# 2019-02-19\n## 对拉格朗日乘子法的理解\n前置知识：梯度的方向是当前等值平面的法线方向。约束函数和目标函数相切时，约束函数和目标函数的梯度方向相同。\n\n如下图所示，其中$f(x,y)$是目标函数，$f(x,y) = d_1,f(x,y) = d_2,f(x,y) = d_3$都是目标函数的等值线，\n$g(x,y) = c$是约束条件，在约束条件和等值线相切的地方，有$\\nabla f = \\lambda \\nabla g$,这里假设$d_1 < d_2 < d_3$且所求问题为\n$$\n\\min f(x,y) \\\\\ns.t. g(x,y)=c\n$$\n那么其最优解一定出现在相切的地方，即只要求如下联立方程即可得到最优解的可能位置。\n\n$$\n\\begin{cases}\n    \\nabla f = \\lambda \\nabla g \\\\\n    g(x,y) = 0\n\\end{cases}\n$$\n\n但是一般看到的拉格朗日乘子法是下面的形式：\n$$\n\\begin{aligned}\n    &首先定义F(x,y,\\lambda) = f(x,y) + \\lambda g(x,y)\\\\\n    &之后求解\\nabla F = \\mathbf{0}\n\\end{aligned}\n$$\n\n其实是一样的，展开$\\nabla F$即可，如下：\n$$\n\\begin{aligned}\n    \\begin{pmatrix}\n        \\frac{\\partial F}{\\partial x}\\\\\n        \\\\\n        \\frac{\\partial F}{\\partial y}\\\\\n        \\\\\n        \\frac{\\partial F}{\\partial \\lambda}\\\\\n    \\end{pmatrix} &= \\begin{pmatrix}\n        0\\\\\n        \\\\\n        0\\\\\n        \\\\\n        0\\\\\n    \\end{pmatrix}\\\\\n    &即\\\\\n    \\nabla f &= -\\lambda \\nabla g \\\\\n    \\frac{\\partial F}{\\partial \\lambda} &= g(x,y)\\\\\n    &= 0\\\\\n\\end{aligned}\n$$\n\n{%asset_img 约束条件和目标函数的等值线.png 约束条件和目标函数的等值线%}\n\n上面只是单个约束时且约束为等式的情况，如果是多个等式约束，则可以写成\n\n$F(x,y,\\lambda_1, \\lambda_2,...,\\lambda_m) = f(x,y) + \\lambda_i g_i(x,y);i = 1, 2, ..., m$\n\n同理求解如下：\n$$\n\\begin{cases}\n    \\nabla f = \\lambda_i \\nabla g_i &i=1,2,...m\\\\\n    g_i(x,y) = 0&i=1,2,...m\n\\end{cases}\n$$\n也可以写成\n$$\n\\begin{aligned}\n    \\begin{pmatrix}\n        \\frac{\\partial F}{\\partial x}\\\\\n        \\\\\n        \\frac{\\partial F}{\\partial y}\\\\\n        \\\\\n        \\frac{\\partial F}{\\partial \\lambda_1}\\\\\n        \\vdots\\\\\n        \\frac{\\partial F}{\\partial \\lambda_m}\\\\\n    \\end{pmatrix} = \\begin{pmatrix}\n        0\\\\\n        \\\\\n        0\\\\\n        \\\\\n        0\\\\\n        \\vdots\\\\\n        0\\\\\n    \\end{pmatrix}\n\\end{aligned}\n$$\n\n如果是不等式约束，如下描述：\n$$\n\\begin{aligned}\n    \\min f(x,y)\\\\\n    s.t.\\quad h(x,y) \\le 0\n\\end{aligned}$$\n可以分成两种情况讨论，第一种情况是内部解，第二种情况是边界解，如果最优解在约束条件内部，那么约束条件不起作用， 退化为无约束问题，$\\nabla f = 0 且 \\lambda = 0$，如果在边界上，则满足$h(x,y)=0$因此不论哪种情况，$\\lambda h(x,y) = 0$恒成立。\n\n这里依旧定义一个如下形式的函数：\n$$\n\\begin{aligned}\n    F(x,y,\\mu) = f(x,y) + \\mu h(x,y)\n\\end{aligned}\n$$\n\n因此可以提出KKT条件的基本形式：\n$$\n\\begin{aligned}\n    \\begin{cases}\n        \\begin{pmatrix}\n        \\frac{\\partial F}{\\partial x}\\\\\n        \\\\\n        \\frac{\\partial F}{\\partial y}\n    \\end{pmatrix} &= \\begin{pmatrix}\n        0\\\\\n        \\\\\n        0\n    \\end{pmatrix}\\\\\n    h(x,y) &\\le 0\\\\\n    \\lambda &\\ge 0 \\\\\n    \\lambda h(x,y) &= 0\n    \\end{cases}\n\\end{aligned}\n$$\n这里的$\\lambda \\ge 0$是因为要求的是$\\min f(x,y)$，如果要求的是$\\max f(x,y)$,则使用$\\lambda \\le 0$\n\n对于多个等式约束和多个不等式约束的情况：\n$$\n\\begin{aligned}\n    &\\min f(\\mathbf{x})\\\\\n    s.t.\\quad &g_j(\\mathbf{x}) = 0, j=1,2,...m\\\\\n    s.t.\\quad &h_k(\\mathbf{x}) \\le 0, k=1,2,...n\n\\end{aligned}\n$$\n首先定义拉格朗日函数：\n$$\nL(\\mathbf{x},\\lambda_1,\\lambda_2,...,\\lambda_m,\\mu_1,\\mu_2,...,\\mu_n) = f(\\mathbf{x}) + \\sum_{j=1}^m \\lambda_j g_j(\\mathbf{x}) + \\sum_{k=1}^n \\mu_k h_k(\\mathbf{x})\n$$\n关于对偶问题的分析略过，结论是如果满足KKT条件，则原问题的最优解和对偶问题最优解相同，计算KKT条件，即可得到候选解，然后对比不同的候选解，即可得出最优解，这里直接给出完整的KKT条件：\n$$\n\\begin{cases}\n    \\nabla_{\\mathbf{x}}L = \\mathbf{0}\\\\\n    g_j(\\mathbf{x})=0 \\quad j=1,2,...,m\\\\\n    h_k(\\mathbf{x})\\le 0 \\quad k=1,2,...,n\\\\\n    \\mu_k \\ge 0 \\quad k=1,2,...,n\\\\\n    \\mu_k h_k(\\mathbf{x})=0 \\quad k=1,2,...,n\n\\end{cases}\n$$\n求解KKT条件，即可找到约束优化问题的解。\n\n# 2019-02-20\n## git的常用操作复习\n\n查看仓库当前状态\n\n     git status \n\n查看修改内容\n\n     git diff <file>\n\n查看最近的提交\n\n     git log\n\n恢复最近提交\n\n     git reset --[soft|mixed|hard] HEAD~\n     soft:仅仅修改HEAD指向\n     mixed:在修改HEAD之后，修改暂存区内容\n     hard:在修改暂存区内容之后，修改工作区内容\n     HEAD~:等效于HEAD^，代表HEAD的上一次提交，可以使用HEAD~100代表HEAD之前100次提交\n     HEAD~也可替换为commit id\n\n如果指定恢复文件，则只从最后一次提交恢复暂存区文件。\n\n     git reset file\n     等价于 git reset --mixed HEAD file\n\n查看版本库历史和commit id，可以用\n\n     git log\n\n修改最后一次commit的信息\n\n     git commit --amend\n\n查看版本库所有的改动，可以用\n\n     git reflog\n\n创建和切换分支\n\n     git branch <branch_name> #创建分支\n     git chechout <branch_name> #切换分支\n     git checkout -b <branch_name> #切换分支，不存在则创建\n\n将当前的暂存区改动压栈并隐藏\n\n     git stash\n\n查看当前的栈区\n\n     git stash list\n\n从栈顶恢复\n\n     git stash apply # 只恢复不删除栈内容\n     git stash pop # 恢复并删除栈内容\n\n查看当前分支\n\n     git branch\n\n合并分支\n\n     git merge <branch_name> #合并指定分支到当前分支\n\n如果合并出现冲突，修改冲突文件之后，可以重新add、commit进行提交\n\n删除分支\n\n     git branch -d <branch_name>\n\n关联远程分支和本地分支\n\n     git branch --set-upstream <branch-name> <origin/branch-name>\n\n之后就可以从远程分支pull或者push到远程分支了\n\n关联远程服务器\n\n     git remote add <remote-name> <remote-url>\n\n创建远程分支\n\n     git push <remote-name> <local-branch-name>:<remote-branch-name>\n\n删除远程分支\n\n     git push <remote-name> :<remote-branch-name>\n\n# 2019-02-21\n\n## 对l2正则化的理解\n\n普通的损失函数可以写成如下：\n$$\nJ(w;X,y)\n$$\n\n在权重参数上加上l2正则化项之后，损失函数如下：\n\n$$\n\\begin{aligned}\n    &\\tilde{J} = \\frac{\\alpha}{2}w^Tw + J(w;X,y)\\\\\n    &对w的求导：\\\\\n    &\\nabla_w\\tilde{J} = \\alpha w + \\nabla_w J(w;X,y)\n\\end{aligned}\n$$\n\n每次的参数更新可以写成如下：\n\n$$\n\\begin{aligned}\n    &w \\leftarrow w - \\epsilon(\\alpha w + \\nabla_w J(w;X,y))\\\\\n    &等价于：\\\\\n    &w \\leftarrow (1 - \\epsilon\\alpha) w + \\epsilon\\nabla_w J(w;X,y)\n\\end{aligned}\n$$\n\n为了分析l2正则化的作用，假定$J(w ^ \\ast ; X , y)$是损失函数的极小值。\n$w^\\ast$是使损失函数取得极小值的参数，先简单的对损失函数进行二次近似如下，这里没有一次项是因为在函数极小值的地方，一次导数应该为0。\n\n$$\n\\begin{aligned}\n    J(w;X,y) = J(w^\\ast;X,y) + (w-w^\\ast)^TH(w-w^\\ast)\n\\end{aligned}\n$$\n\n对加上l2正则化项的损失函数近似形式如下：\n\n$$\n\\begin{aligned}\n    \\hat{J}(w;X,y) = J(w^\\ast;X,y) + (w-w^\\ast)^TH(w-w^\\ast) + \\alpha\\frac{1}{2}w^Tw\n\\end{aligned}\n$$\n\n对损失函数进行求导如下。\n\n$$\n\\begin{aligned}\n    \\nabla_w \\hat{J}(w;X,y) = \\alpha w + H(w - w^\\ast)\n\\end{aligned}\n$$\n\n当$\\nabla_w\\hat{J}(w;X,y) = 0$时，有如下推导：\n\n$$\n\\begin{aligned}\n    \\alpha w + H(w - w^\\ast) = 0\\\\\n    (H + \\alpha I)w = Hw^\\ast\\\\\n    w = (H + \\alpha I)^{-1}Hw^\\ast\n\\end{aligned}\n$$\n\n可见当$\\alpha \\rightarrow 0$时，$w \\rightarrow w^\\ast$，进一步，因为$H$是实对称矩阵，因此必定可以正交对角化，$H=Q \\Lambda Q^T$，因此进一步推导如下：\n\n$$\n\\begin{aligned}\n    w &= (Q \\Lambda Q^T+ \\alpha I)^{-1}Q \\Lambda Q^Tw^\\ast\\\\\n    &=[Q (\\Lambda+ \\alpha I) Q^T]^{-1}Q \\Lambda Q^Tw^\\ast\\\\\n    &=Q (\\Lambda+ \\alpha I)^{-1}\\Lambda Q^Tw^\\ast\n\\end{aligned}\n$$\n\n其中，假设$\\Lambda$如下：\n\n$$\n\\begin{bmatrix}\n    &\\lambda_1 &0 &0 &\\cdots &0\\\\\n    &0 &\\lambda_2 &0 &\\cdots &0\\\\\n    &0 &0 &\\lambda_3 &\\cdots &0\\\\\n    &\\vdots &\\vdots &\\vdots &\\ddots &\\vdots\\\\\n    &0 &0 &0 &\\cdots &\\lambda_n\\\\\n\\end{bmatrix}\n$$\n\n则$(\\Lambda+ \\alpha I)^{-1}\\Lambda$如下：\n$$\n\\begin{bmatrix}\n    &\\frac{\\lambda_1}{\\lambda_1 + \\alpha} &0 &0 &\\cdots &0\\\\\n    &0 &\\frac{\\lambda_2}{\\lambda_2 + \\alpha} &0 &\\cdots &0\\\\\n    &0 &0 &\\frac{\\lambda_3}{\\lambda_3 + \\alpha} &\\cdots &0\\\\\n    &\\vdots &\\vdots &\\vdots &\\ddots &\\vdots\\\\\n    &0 &0 &0 &\\cdots &\\frac{\\lambda_n}{\\lambda_n + \\alpha}\\\\\n\\end{bmatrix}\n$$\n这就相当于在原损失函数极小值点的Hession矩阵$H$的特征向量方向上，将$w^\\ast$进行了缩放，而且特征值$\\lambda_i$越小的方向，$\\alpha$对其影响越大，缩小得越大，即加入l2正则化项之后，模型参数倾向于对损失函数影响显著的方向偏移。\n\n以最小二乘线性回归为例，其损失函数如下：\n\n$$\n(Xw-y)^T(Xw-y)\n$$\n\n如果加上l2正则化，则损失函数变成如下形式：\n\n$$\n(Xw-y)^T(Xw-y) + \\frac{1}{2}\\alpha w^Tw\n$$\n\n那么线性回归的解就从：\n\n$$\nw = (X^TX)^{-1}X^Ty\n$$\n\n变成了：\n$$\nw = (X^TX + \\alpha I)^{-1}X^Ty\n$$\n\n其中$X$可以写成如下,其中$x_{ij}$表示第i个样本$x_i$的第j维：\n$$\nX=\\begin{bmatrix}\n    &x_{11} &x_{12} &\\cdots &x_{1m}\\\\\n    &x_{21} &x_{22} &\\cdots &x_{2m}\\\\\n    &\\vdots &\\vdots &\\ddots &\\vdots\\\\\n    &x_{n1} &x_{n1} &\\cdots &x_{nm}\\\\\n\\end{bmatrix}\n$$\n\n则$X^TX$可以表示如下：\n$$\nX^TX=\\begin{bmatrix}\n    &\\sum_i^nx_{i1}x_{i1} &\\sum_i^nx_{i1}x_{i2} &\\cdots &\\sum_i^nx_{i1}x_{im}\\\\\n    &\\sum_i^nx_{i2}x_{i1} &\\sum_i^nx_{i2}x_{i2} &\\cdots &\\sum_i^nx_{i2}x_{im}\\\\\n    &\\vdots &\\vdots &\\ddots &\\vdots\\\\\n    &\\sum_i^nx_{im}x_{i1} &\\sum_i^nx_{im}x_{i2} &\\cdots &\\sum_i^nx_{im}x_{im}\\\\\n\\end{bmatrix}\n$$\n\n$X^TX$同样可以正交对角化，$X^TX = Q \\Lambda Q^T$，这里的$\\Lambda$对角线上的值是$X$奇异值的平方，之后的推导和上面相同，可见，加入l2正则化项之后，模型参数倾向于对损失函数影响显著的方向偏移，从而忽略掉数据中的一些干扰，增强模型泛化能力。\n\n## 记录几个简单的矩阵求导公式\n$$\n\\frac{\\partial \\beta^T X}{\\partial X} = \\beta\\\\\n\\quad\\\\\n\\frac{\\partial X^T X}{\\partial X} = X\\\\\n\\quad\\\\\n\\frac{\\partial X^T A X}{\\partial X} = (A+A^T)X\\\\\n$$\n\n# 2019-02-25\n## 使用tensorflow后端的keras中的session和graph\n\n这两天在进行将keras模型集成到项目系统的工作，这个系统需要加载不同的模型和训练不同的模型，而且顺序不固定，多个模型可能同时存在，一些在训练，一些在预测，因此直接加载模型，使用keras创建的session和graph的时候，遇到了很多错误，最后的解决方案是为模型手动创建session，并在加载或者使用模型之前使用Keras.backend.set_session方法设置当前使用的session。\n\n还有一个问题是模型使用完成之后，已经使用del model的形式删除，但是windows任务管理器上显存依旧占用，尝试过Keras.backend.clear_session的方式，显存依旧占用，但是其它模型也可以继续跑，不知道是不是tensorflow自动分配了显存的原因。\n\n## python multiprocessing的问题\n前两天遇到一个问题，在使用multiprocessing进行多进程任务时，发现所调用的函数所在的文件，多次执行import操作，导致已经被修改的import变量出现不一致的问题，现在还没有找到解决办法，只能把多进程改为了单进程执行。\n\n# 2019-02-26\n## 对l1正则化的理解\n进行l1正则化之后的损失函数如下：\n\n$$\n\\tilde{J} = \\alpha ||w||_1 + J(w;X,y)\n$$\n\n对其进行求导：\n$$\n\\nabla_w\\tilde{J} = \\alpha sign(w) + \\nabla_wJ(w;X,y)\n$$\n从这里可以看出l1正则化项对导数的影响是一个固定值，和l2有很大区别。\n\n使用在l2正则化分析中的损失函数近似方法，将原本的损失函数二次近似为$\\hat{J}(w)$，其导数如下：\n$$\n\\nabla_w\\hat{J} = H(w - w^\\ast)\n$$\n\n加上正则化项之后，其损失函数的二次近似可以表示为：\n\n$$\n\\hat{J}(w;X,y) = J(w^\\ast;X,y) + \\sum_i[\\frac{1}{2}H_{i,i}(w_i-w_i^\\ast)^2 + \\alpha|w_i|]\n$$\n\n这里将Hessian矩阵简化成了对角阵，这个类似对数据进行PCA之类的操作，将数据之间的相关性去掉了，因此Hessian矩阵变成对角阵，这样分析要简单一些。\n\n如果对$w_i$进行求导，如下：\n$$\n\\nabla_{w_i}\\hat{J}(w;X,y) = H_{i,i}(w_i - w_i^\\ast) + \\alpha sign(w_i)\n$$\n\n可以看出，加了l1正则化之后最优解变成了如下：\n$$\nw_i = sign(w_i^\\ast)max\\{|w_i^\\ast| - \\frac{\\alpha}{H_{i,i}}, 0\\}\n$$\n\n描述成图像大概如下：\n\n{%asset_img l1正则化效果示意.png l1正则化效果示意%}\n\n可以看出，l1使得权重变得更加稀疏，这在特征选择方面非常有用。\n\n# 2019-02-27\n## 数据增强之噪声\n\n### 1.直接在数据上添加噪声\n普遍来说，在数据上添加噪声比简单的收缩权重更加有效。\n\n### 2.在权重上添加噪声\n类似贝叶斯方法中将权重参数也作为一个随机变量的做法，在权重上添加噪声类似于在损失函数上加上预测结果对权重的导数的l2正则化项:$\\eta E_{p(x,y)}[||\\nabla_W \\hat{y}(x)||^2]$，其中$\\hat{y}$表示模型的输出。这会使得模型偏好于一个对于参数来说更加平缓的区域。\n\n### 3.在标签上添加噪声\n对于二分类问题，可以使用$\\epsilon$和$1 - \\epsilon$来代替0、1标签，对于$k$类多分类问题，则可以使用$\\frac{\\epsilon}{k}$ 和 $1 - \\frac{k - 1}{k}\\epsilon$来代替，这对交叉熵损失函数可能没什么区别，因为交叉熵损失函数中为了避免$log0$，已经使用了这种标签平滑，但是对于最大似然学习并在最后一层使用softmax的算法，标签平滑可能会很有用，因为使用softmax之后，输出不可能为绝对的1和0，因此模型会尝试学习越来越大的权重，让输出更加极端，容易导致数值溢出和模型不稳定的问题，在标签上添加噪声可以解决这个问题，虽然通过权重衰减策略也可以解决这个问题，但是盲目的权重衰减可能影响到分类正确性，而在标签上添加噪声没有这个问题。","source":"_posts/学习笔记/学习笔记-2019-02.md","raw":"---\ntitle: 学习笔记_2019-02\ndate: 2019-02-13 15:16:58\ntags: [学习笔记，杂项]\nmathjax: true\n---\n\n# 2019-02-13\n\n## 支持向量机\n首先复习了数学上的一个小知识，如果有一个超平面可以表达为如下线性方程：\n$$\n\\begin{aligned}\n    w^Tx+b = 0\n\\end{aligned}\n$$\n则空间中任意一点$x_0$到该超平面的距离可以表示为如下：\n$$\n\\begin{aligned}\n    r = \\frac{|w^Tx_0+b|}{||w||}\n\\end{aligned}\n$$\n\n在分类问题的样本空间中，令$x_i$和$y_i$分别表示某个样本和其对应的标签，支持向量机的基本思想是求出一个满足如下条件的超平面：\n$$\n\\begin{cases}\n    w^Tx_i + b \\ge 1 & y_i = 1\\\\\n    w^Tx_i + b \\le 1 & y_i = -1\n\\end{cases}\n$$\n示意图如下,这里z展示的样本的特征空间为2维，图中被圈出来的满足上式等号条件一个正样本和两个负样本被称为支持向量，$\\gamma=\\frac{2}{||w||}$称为间隔。\n\n{% asset_img 支持向量机超平面示意.png 支持向量机超平面示意%}\n\n找分类超平面的问题就可以转化为如下：\n$$\n\\begin{aligned}\n    &\\max_{w,b}\\frac{2}{||w||}\\\\\n    &s.t.\\quad y_i(w^Tx_i + b) \\ge 1, i = 1,2,...,m\\\\\n    等价为\\\\\n    &\\min_{w,b}\\frac{||w||^2}{2}\\\\\n    &s.t.\\quad y_i(w^Tx_i + b) \\ge 1, i = 1,2,...,m\n\\end{aligned}\n$$\n这里可以使用不等式约束下的拉格朗日乘子法转换为对偶问题进行求解。\n\n支持向量机的基本型就是希望找到一个间隔最大的超平面，但是这里有个问题，在原始的特征空间中，样本不一定线性可分，因此引入一个函数，将原始特征空间映射到一个高维的特征空间，使得在这个高维空间中线性可分，这样即可将任何分类问题转换成线性可分的形式，但是这样也有一个问题，设映射后的特征向量为$\\phi(x)$，求解超平面的过程中，会遇到${\\phi(x)}^T\\phi(x)$的情况，因为$\\phi(x)$的维数可以为无穷，所以这里的计算无法进行，因此需要设想一个这样的函数：$k(x_i,x_j)={\\phi(x_i)}^T\\phi(x_j)$使得不需要通过$\\phi(x)$来计算${\\phi(x)}^T\\phi(x)$，而是直接通过$k(x_i,x_j)$来表示，这个函数即称为核函数。\n\n核函数可以用一个核矩阵来表示如下，任何一个对称的半正定矩阵，都可以作为核矩阵。\n$$\nk=\\begin{bmatrix}\n    k(x_1,x_1) &\\cdots &k(x_1,x_j) &\\cdots &k(x_1,x_n)\\\\\n    \\vdots &\\ddots &\\vdots &\\ddots &\\vdots\\\\\n    k(x_j,x_1) &\\cdots &k(x_j,x_j) &\\cdots &k(x_j,x_n)\\\\\n    \\vdots &\\ddots &\\vdots &\\ddots &\\vdots\\\\\n    k(x_n,x_1) &\\cdots &k(x_n,x_j) &\\cdots &k(x_n,x_n)\\\\\n\\end{bmatrix}\n$$\n\n选择一个核函数，就隐式的确定了映射的特征空间，因此核函数的选择非常重要。\n\n但是在实际使用中，选出一个合适的特征空间恰好让这个样本线性可分非常不容易，因此将优化目标重写为：\n$$\n\\min_{w,b}\\frac{1}{2}{||w||}^2+C\\sum_{i=1}^ml(y_i(w^Tx_i+b) - 1)\n$$\n这里的$C$为一个设定好的常数，决定模型的偏好，$l$为一种损失函数。这样的支持向量机被称为软间隔支持向量机。\n\n# 2019-02-19\n## 对拉格朗日乘子法的理解\n前置知识：梯度的方向是当前等值平面的法线方向。约束函数和目标函数相切时，约束函数和目标函数的梯度方向相同。\n\n如下图所示，其中$f(x,y)$是目标函数，$f(x,y) = d_1,f(x,y) = d_2,f(x,y) = d_3$都是目标函数的等值线，\n$g(x,y) = c$是约束条件，在约束条件和等值线相切的地方，有$\\nabla f = \\lambda \\nabla g$,这里假设$d_1 < d_2 < d_3$且所求问题为\n$$\n\\min f(x,y) \\\\\ns.t. g(x,y)=c\n$$\n那么其最优解一定出现在相切的地方，即只要求如下联立方程即可得到最优解的可能位置。\n\n$$\n\\begin{cases}\n    \\nabla f = \\lambda \\nabla g \\\\\n    g(x,y) = 0\n\\end{cases}\n$$\n\n但是一般看到的拉格朗日乘子法是下面的形式：\n$$\n\\begin{aligned}\n    &首先定义F(x,y,\\lambda) = f(x,y) + \\lambda g(x,y)\\\\\n    &之后求解\\nabla F = \\mathbf{0}\n\\end{aligned}\n$$\n\n其实是一样的，展开$\\nabla F$即可，如下：\n$$\n\\begin{aligned}\n    \\begin{pmatrix}\n        \\frac{\\partial F}{\\partial x}\\\\\n        \\\\\n        \\frac{\\partial F}{\\partial y}\\\\\n        \\\\\n        \\frac{\\partial F}{\\partial \\lambda}\\\\\n    \\end{pmatrix} &= \\begin{pmatrix}\n        0\\\\\n        \\\\\n        0\\\\\n        \\\\\n        0\\\\\n    \\end{pmatrix}\\\\\n    &即\\\\\n    \\nabla f &= -\\lambda \\nabla g \\\\\n    \\frac{\\partial F}{\\partial \\lambda} &= g(x,y)\\\\\n    &= 0\\\\\n\\end{aligned}\n$$\n\n{%asset_img 约束条件和目标函数的等值线.png 约束条件和目标函数的等值线%}\n\n上面只是单个约束时且约束为等式的情况，如果是多个等式约束，则可以写成\n\n$F(x,y,\\lambda_1, \\lambda_2,...,\\lambda_m) = f(x,y) + \\lambda_i g_i(x,y);i = 1, 2, ..., m$\n\n同理求解如下：\n$$\n\\begin{cases}\n    \\nabla f = \\lambda_i \\nabla g_i &i=1,2,...m\\\\\n    g_i(x,y) = 0&i=1,2,...m\n\\end{cases}\n$$\n也可以写成\n$$\n\\begin{aligned}\n    \\begin{pmatrix}\n        \\frac{\\partial F}{\\partial x}\\\\\n        \\\\\n        \\frac{\\partial F}{\\partial y}\\\\\n        \\\\\n        \\frac{\\partial F}{\\partial \\lambda_1}\\\\\n        \\vdots\\\\\n        \\frac{\\partial F}{\\partial \\lambda_m}\\\\\n    \\end{pmatrix} = \\begin{pmatrix}\n        0\\\\\n        \\\\\n        0\\\\\n        \\\\\n        0\\\\\n        \\vdots\\\\\n        0\\\\\n    \\end{pmatrix}\n\\end{aligned}\n$$\n\n如果是不等式约束，如下描述：\n$$\n\\begin{aligned}\n    \\min f(x,y)\\\\\n    s.t.\\quad h(x,y) \\le 0\n\\end{aligned}$$\n可以分成两种情况讨论，第一种情况是内部解，第二种情况是边界解，如果最优解在约束条件内部，那么约束条件不起作用， 退化为无约束问题，$\\nabla f = 0 且 \\lambda = 0$，如果在边界上，则满足$h(x,y)=0$因此不论哪种情况，$\\lambda h(x,y) = 0$恒成立。\n\n这里依旧定义一个如下形式的函数：\n$$\n\\begin{aligned}\n    F(x,y,\\mu) = f(x,y) + \\mu h(x,y)\n\\end{aligned}\n$$\n\n因此可以提出KKT条件的基本形式：\n$$\n\\begin{aligned}\n    \\begin{cases}\n        \\begin{pmatrix}\n        \\frac{\\partial F}{\\partial x}\\\\\n        \\\\\n        \\frac{\\partial F}{\\partial y}\n    \\end{pmatrix} &= \\begin{pmatrix}\n        0\\\\\n        \\\\\n        0\n    \\end{pmatrix}\\\\\n    h(x,y) &\\le 0\\\\\n    \\lambda &\\ge 0 \\\\\n    \\lambda h(x,y) &= 0\n    \\end{cases}\n\\end{aligned}\n$$\n这里的$\\lambda \\ge 0$是因为要求的是$\\min f(x,y)$，如果要求的是$\\max f(x,y)$,则使用$\\lambda \\le 0$\n\n对于多个等式约束和多个不等式约束的情况：\n$$\n\\begin{aligned}\n    &\\min f(\\mathbf{x})\\\\\n    s.t.\\quad &g_j(\\mathbf{x}) = 0, j=1,2,...m\\\\\n    s.t.\\quad &h_k(\\mathbf{x}) \\le 0, k=1,2,...n\n\\end{aligned}\n$$\n首先定义拉格朗日函数：\n$$\nL(\\mathbf{x},\\lambda_1,\\lambda_2,...,\\lambda_m,\\mu_1,\\mu_2,...,\\mu_n) = f(\\mathbf{x}) + \\sum_{j=1}^m \\lambda_j g_j(\\mathbf{x}) + \\sum_{k=1}^n \\mu_k h_k(\\mathbf{x})\n$$\n关于对偶问题的分析略过，结论是如果满足KKT条件，则原问题的最优解和对偶问题最优解相同，计算KKT条件，即可得到候选解，然后对比不同的候选解，即可得出最优解，这里直接给出完整的KKT条件：\n$$\n\\begin{cases}\n    \\nabla_{\\mathbf{x}}L = \\mathbf{0}\\\\\n    g_j(\\mathbf{x})=0 \\quad j=1,2,...,m\\\\\n    h_k(\\mathbf{x})\\le 0 \\quad k=1,2,...,n\\\\\n    \\mu_k \\ge 0 \\quad k=1,2,...,n\\\\\n    \\mu_k h_k(\\mathbf{x})=0 \\quad k=1,2,...,n\n\\end{cases}\n$$\n求解KKT条件，即可找到约束优化问题的解。\n\n# 2019-02-20\n## git的常用操作复习\n\n查看仓库当前状态\n\n     git status \n\n查看修改内容\n\n     git diff <file>\n\n查看最近的提交\n\n     git log\n\n恢复最近提交\n\n     git reset --[soft|mixed|hard] HEAD~\n     soft:仅仅修改HEAD指向\n     mixed:在修改HEAD之后，修改暂存区内容\n     hard:在修改暂存区内容之后，修改工作区内容\n     HEAD~:等效于HEAD^，代表HEAD的上一次提交，可以使用HEAD~100代表HEAD之前100次提交\n     HEAD~也可替换为commit id\n\n如果指定恢复文件，则只从最后一次提交恢复暂存区文件。\n\n     git reset file\n     等价于 git reset --mixed HEAD file\n\n查看版本库历史和commit id，可以用\n\n     git log\n\n修改最后一次commit的信息\n\n     git commit --amend\n\n查看版本库所有的改动，可以用\n\n     git reflog\n\n创建和切换分支\n\n     git branch <branch_name> #创建分支\n     git chechout <branch_name> #切换分支\n     git checkout -b <branch_name> #切换分支，不存在则创建\n\n将当前的暂存区改动压栈并隐藏\n\n     git stash\n\n查看当前的栈区\n\n     git stash list\n\n从栈顶恢复\n\n     git stash apply # 只恢复不删除栈内容\n     git stash pop # 恢复并删除栈内容\n\n查看当前分支\n\n     git branch\n\n合并分支\n\n     git merge <branch_name> #合并指定分支到当前分支\n\n如果合并出现冲突，修改冲突文件之后，可以重新add、commit进行提交\n\n删除分支\n\n     git branch -d <branch_name>\n\n关联远程分支和本地分支\n\n     git branch --set-upstream <branch-name> <origin/branch-name>\n\n之后就可以从远程分支pull或者push到远程分支了\n\n关联远程服务器\n\n     git remote add <remote-name> <remote-url>\n\n创建远程分支\n\n     git push <remote-name> <local-branch-name>:<remote-branch-name>\n\n删除远程分支\n\n     git push <remote-name> :<remote-branch-name>\n\n# 2019-02-21\n\n## 对l2正则化的理解\n\n普通的损失函数可以写成如下：\n$$\nJ(w;X,y)\n$$\n\n在权重参数上加上l2正则化项之后，损失函数如下：\n\n$$\n\\begin{aligned}\n    &\\tilde{J} = \\frac{\\alpha}{2}w^Tw + J(w;X,y)\\\\\n    &对w的求导：\\\\\n    &\\nabla_w\\tilde{J} = \\alpha w + \\nabla_w J(w;X,y)\n\\end{aligned}\n$$\n\n每次的参数更新可以写成如下：\n\n$$\n\\begin{aligned}\n    &w \\leftarrow w - \\epsilon(\\alpha w + \\nabla_w J(w;X,y))\\\\\n    &等价于：\\\\\n    &w \\leftarrow (1 - \\epsilon\\alpha) w + \\epsilon\\nabla_w J(w;X,y)\n\\end{aligned}\n$$\n\n为了分析l2正则化的作用，假定$J(w ^ \\ast ; X , y)$是损失函数的极小值。\n$w^\\ast$是使损失函数取得极小值的参数，先简单的对损失函数进行二次近似如下，这里没有一次项是因为在函数极小值的地方，一次导数应该为0。\n\n$$\n\\begin{aligned}\n    J(w;X,y) = J(w^\\ast;X,y) + (w-w^\\ast)^TH(w-w^\\ast)\n\\end{aligned}\n$$\n\n对加上l2正则化项的损失函数近似形式如下：\n\n$$\n\\begin{aligned}\n    \\hat{J}(w;X,y) = J(w^\\ast;X,y) + (w-w^\\ast)^TH(w-w^\\ast) + \\alpha\\frac{1}{2}w^Tw\n\\end{aligned}\n$$\n\n对损失函数进行求导如下。\n\n$$\n\\begin{aligned}\n    \\nabla_w \\hat{J}(w;X,y) = \\alpha w + H(w - w^\\ast)\n\\end{aligned}\n$$\n\n当$\\nabla_w\\hat{J}(w;X,y) = 0$时，有如下推导：\n\n$$\n\\begin{aligned}\n    \\alpha w + H(w - w^\\ast) = 0\\\\\n    (H + \\alpha I)w = Hw^\\ast\\\\\n    w = (H + \\alpha I)^{-1}Hw^\\ast\n\\end{aligned}\n$$\n\n可见当$\\alpha \\rightarrow 0$时，$w \\rightarrow w^\\ast$，进一步，因为$H$是实对称矩阵，因此必定可以正交对角化，$H=Q \\Lambda Q^T$，因此进一步推导如下：\n\n$$\n\\begin{aligned}\n    w &= (Q \\Lambda Q^T+ \\alpha I)^{-1}Q \\Lambda Q^Tw^\\ast\\\\\n    &=[Q (\\Lambda+ \\alpha I) Q^T]^{-1}Q \\Lambda Q^Tw^\\ast\\\\\n    &=Q (\\Lambda+ \\alpha I)^{-1}\\Lambda Q^Tw^\\ast\n\\end{aligned}\n$$\n\n其中，假设$\\Lambda$如下：\n\n$$\n\\begin{bmatrix}\n    &\\lambda_1 &0 &0 &\\cdots &0\\\\\n    &0 &\\lambda_2 &0 &\\cdots &0\\\\\n    &0 &0 &\\lambda_3 &\\cdots &0\\\\\n    &\\vdots &\\vdots &\\vdots &\\ddots &\\vdots\\\\\n    &0 &0 &0 &\\cdots &\\lambda_n\\\\\n\\end{bmatrix}\n$$\n\n则$(\\Lambda+ \\alpha I)^{-1}\\Lambda$如下：\n$$\n\\begin{bmatrix}\n    &\\frac{\\lambda_1}{\\lambda_1 + \\alpha} &0 &0 &\\cdots &0\\\\\n    &0 &\\frac{\\lambda_2}{\\lambda_2 + \\alpha} &0 &\\cdots &0\\\\\n    &0 &0 &\\frac{\\lambda_3}{\\lambda_3 + \\alpha} &\\cdots &0\\\\\n    &\\vdots &\\vdots &\\vdots &\\ddots &\\vdots\\\\\n    &0 &0 &0 &\\cdots &\\frac{\\lambda_n}{\\lambda_n + \\alpha}\\\\\n\\end{bmatrix}\n$$\n这就相当于在原损失函数极小值点的Hession矩阵$H$的特征向量方向上，将$w^\\ast$进行了缩放，而且特征值$\\lambda_i$越小的方向，$\\alpha$对其影响越大，缩小得越大，即加入l2正则化项之后，模型参数倾向于对损失函数影响显著的方向偏移。\n\n以最小二乘线性回归为例，其损失函数如下：\n\n$$\n(Xw-y)^T(Xw-y)\n$$\n\n如果加上l2正则化，则损失函数变成如下形式：\n\n$$\n(Xw-y)^T(Xw-y) + \\frac{1}{2}\\alpha w^Tw\n$$\n\n那么线性回归的解就从：\n\n$$\nw = (X^TX)^{-1}X^Ty\n$$\n\n变成了：\n$$\nw = (X^TX + \\alpha I)^{-1}X^Ty\n$$\n\n其中$X$可以写成如下,其中$x_{ij}$表示第i个样本$x_i$的第j维：\n$$\nX=\\begin{bmatrix}\n    &x_{11} &x_{12} &\\cdots &x_{1m}\\\\\n    &x_{21} &x_{22} &\\cdots &x_{2m}\\\\\n    &\\vdots &\\vdots &\\ddots &\\vdots\\\\\n    &x_{n1} &x_{n1} &\\cdots &x_{nm}\\\\\n\\end{bmatrix}\n$$\n\n则$X^TX$可以表示如下：\n$$\nX^TX=\\begin{bmatrix}\n    &\\sum_i^nx_{i1}x_{i1} &\\sum_i^nx_{i1}x_{i2} &\\cdots &\\sum_i^nx_{i1}x_{im}\\\\\n    &\\sum_i^nx_{i2}x_{i1} &\\sum_i^nx_{i2}x_{i2} &\\cdots &\\sum_i^nx_{i2}x_{im}\\\\\n    &\\vdots &\\vdots &\\ddots &\\vdots\\\\\n    &\\sum_i^nx_{im}x_{i1} &\\sum_i^nx_{im}x_{i2} &\\cdots &\\sum_i^nx_{im}x_{im}\\\\\n\\end{bmatrix}\n$$\n\n$X^TX$同样可以正交对角化，$X^TX = Q \\Lambda Q^T$，这里的$\\Lambda$对角线上的值是$X$奇异值的平方，之后的推导和上面相同，可见，加入l2正则化项之后，模型参数倾向于对损失函数影响显著的方向偏移，从而忽略掉数据中的一些干扰，增强模型泛化能力。\n\n## 记录几个简单的矩阵求导公式\n$$\n\\frac{\\partial \\beta^T X}{\\partial X} = \\beta\\\\\n\\quad\\\\\n\\frac{\\partial X^T X}{\\partial X} = X\\\\\n\\quad\\\\\n\\frac{\\partial X^T A X}{\\partial X} = (A+A^T)X\\\\\n$$\n\n# 2019-02-25\n## 使用tensorflow后端的keras中的session和graph\n\n这两天在进行将keras模型集成到项目系统的工作，这个系统需要加载不同的模型和训练不同的模型，而且顺序不固定，多个模型可能同时存在，一些在训练，一些在预测，因此直接加载模型，使用keras创建的session和graph的时候，遇到了很多错误，最后的解决方案是为模型手动创建session，并在加载或者使用模型之前使用Keras.backend.set_session方法设置当前使用的session。\n\n还有一个问题是模型使用完成之后，已经使用del model的形式删除，但是windows任务管理器上显存依旧占用，尝试过Keras.backend.clear_session的方式，显存依旧占用，但是其它模型也可以继续跑，不知道是不是tensorflow自动分配了显存的原因。\n\n## python multiprocessing的问题\n前两天遇到一个问题，在使用multiprocessing进行多进程任务时，发现所调用的函数所在的文件，多次执行import操作，导致已经被修改的import变量出现不一致的问题，现在还没有找到解决办法，只能把多进程改为了单进程执行。\n\n# 2019-02-26\n## 对l1正则化的理解\n进行l1正则化之后的损失函数如下：\n\n$$\n\\tilde{J} = \\alpha ||w||_1 + J(w;X,y)\n$$\n\n对其进行求导：\n$$\n\\nabla_w\\tilde{J} = \\alpha sign(w) + \\nabla_wJ(w;X,y)\n$$\n从这里可以看出l1正则化项对导数的影响是一个固定值，和l2有很大区别。\n\n使用在l2正则化分析中的损失函数近似方法，将原本的损失函数二次近似为$\\hat{J}(w)$，其导数如下：\n$$\n\\nabla_w\\hat{J} = H(w - w^\\ast)\n$$\n\n加上正则化项之后，其损失函数的二次近似可以表示为：\n\n$$\n\\hat{J}(w;X,y) = J(w^\\ast;X,y) + \\sum_i[\\frac{1}{2}H_{i,i}(w_i-w_i^\\ast)^2 + \\alpha|w_i|]\n$$\n\n这里将Hessian矩阵简化成了对角阵，这个类似对数据进行PCA之类的操作，将数据之间的相关性去掉了，因此Hessian矩阵变成对角阵，这样分析要简单一些。\n\n如果对$w_i$进行求导，如下：\n$$\n\\nabla_{w_i}\\hat{J}(w;X,y) = H_{i,i}(w_i - w_i^\\ast) + \\alpha sign(w_i)\n$$\n\n可以看出，加了l1正则化之后最优解变成了如下：\n$$\nw_i = sign(w_i^\\ast)max\\{|w_i^\\ast| - \\frac{\\alpha}{H_{i,i}}, 0\\}\n$$\n\n描述成图像大概如下：\n\n{%asset_img l1正则化效果示意.png l1正则化效果示意%}\n\n可以看出，l1使得权重变得更加稀疏，这在特征选择方面非常有用。\n\n# 2019-02-27\n## 数据增强之噪声\n\n### 1.直接在数据上添加噪声\n普遍来说，在数据上添加噪声比简单的收缩权重更加有效。\n\n### 2.在权重上添加噪声\n类似贝叶斯方法中将权重参数也作为一个随机变量的做法，在权重上添加噪声类似于在损失函数上加上预测结果对权重的导数的l2正则化项:$\\eta E_{p(x,y)}[||\\nabla_W \\hat{y}(x)||^2]$，其中$\\hat{y}$表示模型的输出。这会使得模型偏好于一个对于参数来说更加平缓的区域。\n\n### 3.在标签上添加噪声\n对于二分类问题，可以使用$\\epsilon$和$1 - \\epsilon$来代替0、1标签，对于$k$类多分类问题，则可以使用$\\frac{\\epsilon}{k}$ 和 $1 - \\frac{k - 1}{k}\\epsilon$来代替，这对交叉熵损失函数可能没什么区别，因为交叉熵损失函数中为了避免$log0$，已经使用了这种标签平滑，但是对于最大似然学习并在最后一层使用softmax的算法，标签平滑可能会很有用，因为使用softmax之后，输出不可能为绝对的1和0，因此模型会尝试学习越来越大的权重，让输出更加极端，容易导致数值溢出和模型不稳定的问题，在标签上添加噪声可以解决这个问题，虽然通过权重衰减策略也可以解决这个问题，但是盲目的权重衰减可能影响到分类正确性，而在标签上添加噪声没有这个问题。","slug":"学习笔记/学习笔记-2019-02","published":1,"updated":"2020-08-31T06:39:20.768Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckjxpc3sv003644mqh1ed3kwn","content":"<h1 id=\"section\">2019-02-13</h1>\r\n<h2 id=\"支持向量机\">支持向量机</h2>\r\n<p>首先复习了数学上的一个小知识，如果有一个超平面可以表达为如下线性方程： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    w^Tx+b = 0\r\n\\end{aligned}\r\n\\]</span> 则空间中任意一点<span class=\"math inline\">\\(x_0\\)</span>到该超平面的距离可以表示为如下： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    r = \\frac{|w^Tx_0+b|}{||w||}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>在分类问题的样本空间中，令<span class=\"math inline\">\\(x_i\\)</span>和<span class=\"math inline\">\\(y_i\\)</span>分别表示某个样本和其对应的标签，支持向量机的基本思想是求出一个满足如下条件的超平面： <span class=\"math display\">\\[\r\n\\begin{cases}\r\n    w^Tx_i + b \\ge 1 &amp; y_i = 1\\\\\r\n    w^Tx_i + b \\le 1 &amp; y_i = -1\r\n\\end{cases}\r\n\\]</span> 示意图如下,这里z展示的样本的特征空间为2维，图中被圈出来的满足上式等号条件一个正样本和两个负样本被称为支持向量，<span class=\"math inline\">\\(\\gamma=\\frac{2}{||w||}\\)</span>称为间隔。</p>\r\n<img src=\"/2019/02/13/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2019-02/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E8%B6%85%E5%B9%B3%E9%9D%A2%E7%A4%BA%E6%84%8F.png\" class=\"\" title=\"支持向量机超平面示意\">\r\n<p>找分类超平面的问题就可以转化为如下： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    &amp;\\max_{w,b}\\frac{2}{||w||}\\\\\r\n    &amp;s.t.\\quad y_i(w^Tx_i + b) \\ge 1, i = 1,2,...,m\\\\\r\n    等价为\\\\\r\n    &amp;\\min_{w,b}\\frac{||w||^2}{2}\\\\\r\n    &amp;s.t.\\quad y_i(w^Tx_i + b) \\ge 1, i = 1,2,...,m\r\n\\end{aligned}\r\n\\]</span> 这里可以使用不等式约束下的拉格朗日乘子法转换为对偶问题进行求解。</p>\r\n<p>支持向量机的基本型就是希望找到一个间隔最大的超平面，但是这里有个问题，在原始的特征空间中，样本不一定线性可分，因此引入一个函数，将原始特征空间映射到一个高维的特征空间，使得在这个高维空间中线性可分，这样即可将任何分类问题转换成线性可分的形式，但是这样也有一个问题，设映射后的特征向量为<span class=\"math inline\">\\(\\phi(x)\\)</span>，求解超平面的过程中，会遇到<span class=\"math inline\">\\({\\phi(x)}^T\\phi(x)\\)</span>的情况，因为<span class=\"math inline\">\\(\\phi(x)\\)</span>的维数可以为无穷，所以这里的计算无法进行，因此需要设想一个这样的函数：<span class=\"math inline\">\\(k(x_i,x_j)={\\phi(x_i)}^T\\phi(x_j)\\)</span>使得不需要通过<span class=\"math inline\">\\(\\phi(x)\\)</span>来计算<span class=\"math inline\">\\({\\phi(x)}^T\\phi(x)\\)</span>，而是直接通过<span class=\"math inline\">\\(k(x_i,x_j)\\)</span>来表示，这个函数即称为核函数。</p>\r\n<p>核函数可以用一个核矩阵来表示如下，任何一个对称的半正定矩阵，都可以作为核矩阵。 <span class=\"math display\">\\[\r\nk=\\begin{bmatrix}\r\n    k(x_1,x_1) &amp;\\cdots &amp;k(x_1,x_j) &amp;\\cdots &amp;k(x_1,x_n)\\\\\r\n    \\vdots &amp;\\ddots &amp;\\vdots &amp;\\ddots &amp;\\vdots\\\\\r\n    k(x_j,x_1) &amp;\\cdots &amp;k(x_j,x_j) &amp;\\cdots &amp;k(x_j,x_n)\\\\\r\n    \\vdots &amp;\\ddots &amp;\\vdots &amp;\\ddots &amp;\\vdots\\\\\r\n    k(x_n,x_1) &amp;\\cdots &amp;k(x_n,x_j) &amp;\\cdots &amp;k(x_n,x_n)\\\\\r\n\\end{bmatrix}\r\n\\]</span></p>\r\n<p>选择一个核函数，就隐式的确定了映射的特征空间，因此核函数的选择非常重要。</p>\r\n<p>但是在实际使用中，选出一个合适的特征空间恰好让这个样本线性可分非常不容易，因此将优化目标重写为： <span class=\"math display\">\\[\r\n\\min_{w,b}\\frac{1}{2}{||w||}^2+C\\sum_{i=1}^ml(y_i(w^Tx_i+b) - 1)\r\n\\]</span> 这里的<span class=\"math inline\">\\(C\\)</span>为一个设定好的常数，决定模型的偏好，<span class=\"math inline\">\\(l\\)</span>为一种损失函数。这样的支持向量机被称为软间隔支持向量机。</p>\r\n<h1 id=\"section-1\">2019-02-19</h1>\r\n<h2 id=\"对拉格朗日乘子法的理解\">对拉格朗日乘子法的理解</h2>\r\n<p>前置知识：梯度的方向是当前等值平面的法线方向。约束函数和目标函数相切时，约束函数和目标函数的梯度方向相同。</p>\r\n<p>如下图所示，其中<span class=\"math inline\">\\(f(x,y)\\)</span>是目标函数，<span class=\"math inline\">\\(f(x,y) = d_1,f(x,y) = d_2,f(x,y) = d_3\\)</span>都是目标函数的等值线， <span class=\"math inline\">\\(g(x,y) = c\\)</span>是约束条件，在约束条件和等值线相切的地方，有<span class=\"math inline\">\\(\\nabla f = \\lambda \\nabla g\\)</span>,这里假设<span class=\"math inline\">\\(d_1 &lt; d_2 &lt; d_3\\)</span>且所求问题为 <span class=\"math display\">\\[\r\n\\min f(x,y) \\\\\r\ns.t. g(x,y)=c\r\n\\]</span> 那么其最优解一定出现在相切的地方，即只要求如下联立方程即可得到最优解的可能位置。</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{cases}\r\n    \\nabla f = \\lambda \\nabla g \\\\\r\n    g(x,y) = 0\r\n\\end{cases}\r\n\\]</span></p>\r\n<p>但是一般看到的拉格朗日乘子法是下面的形式： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    &amp;首先定义F(x,y,\\lambda) = f(x,y) + \\lambda g(x,y)\\\\\r\n    &amp;之后求解\\nabla F = \\mathbf{0}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>其实是一样的，展开<span class=\"math inline\">\\(\\nabla F\\)</span>即可，如下： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\begin{pmatrix}\r\n        \\frac{\\partial F}{\\partial x}\\\\\r\n        \\\\\r\n        \\frac{\\partial F}{\\partial y}\\\\\r\n        \\\\\r\n        \\frac{\\partial F}{\\partial \\lambda}\\\\\r\n    \\end{pmatrix} &amp;= \\begin{pmatrix}\r\n        0\\\\\r\n        \\\\\r\n        0\\\\\r\n        \\\\\r\n        0\\\\\r\n    \\end{pmatrix}\\\\\r\n    &amp;即\\\\\r\n    \\nabla f &amp;= -\\lambda \\nabla g \\\\\r\n    \\frac{\\partial F}{\\partial \\lambda} &amp;= g(x,y)\\\\\r\n    &amp;= 0\\\\\r\n\\end{aligned}\r\n\\]</span></p>\r\n<img src=\"/2019/02/13/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2019-02/%E7%BA%A6%E6%9D%9F%E6%9D%A1%E4%BB%B6%E5%92%8C%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E7%9A%84%E7%AD%89%E5%80%BC%E7%BA%BF.png\" class=\"\" title=\"约束条件和目标函数的等值线\">\r\n<p>上面只是单个约束时且约束为等式的情况，如果是多个等式约束，则可以写成</p>\r\n<p><span class=\"math inline\">\\(F(x,y,\\lambda_1, \\lambda_2,...,\\lambda_m) = f(x,y) + \\lambda_i g_i(x,y);i = 1, 2, ..., m\\)</span></p>\r\n<p>同理求解如下： <span class=\"math display\">\\[\r\n\\begin{cases}\r\n    \\nabla f = \\lambda_i \\nabla g_i &amp;i=1,2,...m\\\\\r\n    g_i(x,y) = 0&amp;i=1,2,...m\r\n\\end{cases}\r\n\\]</span> 也可以写成 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\begin{pmatrix}\r\n        \\frac{\\partial F}{\\partial x}\\\\\r\n        \\\\\r\n        \\frac{\\partial F}{\\partial y}\\\\\r\n        \\\\\r\n        \\frac{\\partial F}{\\partial \\lambda_1}\\\\\r\n        \\vdots\\\\\r\n        \\frac{\\partial F}{\\partial \\lambda_m}\\\\\r\n    \\end{pmatrix} = \\begin{pmatrix}\r\n        0\\\\\r\n        \\\\\r\n        0\\\\\r\n        \\\\\r\n        0\\\\\r\n        \\vdots\\\\\r\n        0\\\\\r\n    \\end{pmatrix}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>如果是不等式约束，如下描述： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\min f(x,y)\\\\\r\n    s.t.\\quad h(x,y) \\le 0\r\n\\end{aligned}\\]</span> 可以分成两种情况讨论，第一种情况是内部解，第二种情况是边界解，如果最优解在约束条件内部，那么约束条件不起作用， 退化为无约束问题，<span class=\"math inline\">\\(\\nabla f = 0 且 \\lambda = 0\\)</span>，如果在边界上，则满足<span class=\"math inline\">\\(h(x,y)=0\\)</span>因此不论哪种情况，<span class=\"math inline\">\\(\\lambda h(x,y) = 0\\)</span>恒成立。</p>\r\n<p>这里依旧定义一个如下形式的函数： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    F(x,y,\\mu) = f(x,y) + \\mu h(x,y)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>因此可以提出KKT条件的基本形式： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\begin{cases}\r\n        \\begin{pmatrix}\r\n        \\frac{\\partial F}{\\partial x}\\\\\r\n        \\\\\r\n        \\frac{\\partial F}{\\partial y}\r\n    \\end{pmatrix} &amp;= \\begin{pmatrix}\r\n        0\\\\\r\n        \\\\\r\n        0\r\n    \\end{pmatrix}\\\\\r\n    h(x,y) &amp;\\le 0\\\\\r\n    \\lambda &amp;\\ge 0 \\\\\r\n    \\lambda h(x,y) &amp;= 0\r\n    \\end{cases}\r\n\\end{aligned}\r\n\\]</span> 这里的<span class=\"math inline\">\\(\\lambda \\ge 0\\)</span>是因为要求的是<span class=\"math inline\">\\(\\min f(x,y)\\)</span>，如果要求的是<span class=\"math inline\">\\(\\max f(x,y)\\)</span>,则使用<span class=\"math inline\">\\(\\lambda \\le 0\\)</span></p>\r\n<p>对于多个等式约束和多个不等式约束的情况： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    &amp;\\min f(\\mathbf{x})\\\\\r\n    s.t.\\quad &amp;g_j(\\mathbf{x}) = 0, j=1,2,...m\\\\\r\n    s.t.\\quad &amp;h_k(\\mathbf{x}) \\le 0, k=1,2,...n\r\n\\end{aligned}\r\n\\]</span> 首先定义拉格朗日函数： <span class=\"math display\">\\[\r\nL(\\mathbf{x},\\lambda_1,\\lambda_2,...,\\lambda_m,\\mu_1,\\mu_2,...,\\mu_n) = f(\\mathbf{x}) + \\sum_{j=1}^m \\lambda_j g_j(\\mathbf{x}) + \\sum_{k=1}^n \\mu_k h_k(\\mathbf{x})\r\n\\]</span> 关于对偶问题的分析略过，结论是如果满足KKT条件，则原问题的最优解和对偶问题最优解相同，计算KKT条件，即可得到候选解，然后对比不同的候选解，即可得出最优解，这里直接给出完整的KKT条件： <span class=\"math display\">\\[\r\n\\begin{cases}\r\n    \\nabla_{\\mathbf{x}}L = \\mathbf{0}\\\\\r\n    g_j(\\mathbf{x})=0 \\quad j=1,2,...,m\\\\\r\n    h_k(\\mathbf{x})\\le 0 \\quad k=1,2,...,n\\\\\r\n    \\mu_k \\ge 0 \\quad k=1,2,...,n\\\\\r\n    \\mu_k h_k(\\mathbf{x})=0 \\quad k=1,2,...,n\r\n\\end{cases}\r\n\\]</span> 求解KKT条件，即可找到约束优化问题的解。</p>\r\n<h1 id=\"section-2\">2019-02-20</h1>\r\n<h2 id=\"git的常用操作复习\">git的常用操作复习</h2>\r\n<p>查看仓库当前状态</p>\r\n<pre><code> git status </code></pre>\r\n<p>查看修改内容</p>\r\n<pre><code> git diff &lt;file&gt;</code></pre>\r\n<p>查看最近的提交</p>\r\n<pre><code> git log</code></pre>\r\n<p>恢复最近提交</p>\r\n<pre><code> git reset --[soft|mixed|hard] HEAD~\r\n soft:仅仅修改HEAD指向\r\n mixed:在修改HEAD之后，修改暂存区内容\r\n hard:在修改暂存区内容之后，修改工作区内容\r\n HEAD~:等效于HEAD^，代表HEAD的上一次提交，可以使用HEAD~100代表HEAD之前100次提交\r\n HEAD~也可替换为commit id</code></pre>\r\n<p>如果指定恢复文件，则只从最后一次提交恢复暂存区文件。</p>\r\n<pre><code> git reset file\r\n 等价于 git reset --mixed HEAD file</code></pre>\r\n<p>查看版本库历史和commit id，可以用</p>\r\n<pre><code> git log</code></pre>\r\n<p>修改最后一次commit的信息</p>\r\n<pre><code> git commit --amend</code></pre>\r\n<p>查看版本库所有的改动，可以用</p>\r\n<pre><code> git reflog</code></pre>\r\n<p>创建和切换分支</p>\r\n<pre><code> git branch &lt;branch_name&gt; #创建分支\r\n git chechout &lt;branch_name&gt; #切换分支\r\n git checkout -b &lt;branch_name&gt; #切换分支，不存在则创建</code></pre>\r\n<p>将当前的暂存区改动压栈并隐藏</p>\r\n<pre><code> git stash</code></pre>\r\n<p>查看当前的栈区</p>\r\n<pre><code> git stash list</code></pre>\r\n<p>从栈顶恢复</p>\r\n<pre><code> git stash apply # 只恢复不删除栈内容\r\n git stash pop # 恢复并删除栈内容</code></pre>\r\n<p>查看当前分支</p>\r\n<pre><code> git branch</code></pre>\r\n<p>合并分支</p>\r\n<pre><code> git merge &lt;branch_name&gt; #合并指定分支到当前分支</code></pre>\r\n<p>如果合并出现冲突，修改冲突文件之后，可以重新add、commit进行提交</p>\r\n<p>删除分支</p>\r\n<pre><code> git branch -d &lt;branch_name&gt;</code></pre>\r\n<p>关联远程分支和本地分支</p>\r\n<pre><code> git branch --set-upstream &lt;branch-name&gt; &lt;origin/branch-name&gt;</code></pre>\r\n<p>之后就可以从远程分支pull或者push到远程分支了</p>\r\n<p>关联远程服务器</p>\r\n<pre><code> git remote add &lt;remote-name&gt; &lt;remote-url&gt;</code></pre>\r\n<p>创建远程分支</p>\r\n<pre><code> git push &lt;remote-name&gt; &lt;local-branch-name&gt;:&lt;remote-branch-name&gt;</code></pre>\r\n<p>删除远程分支</p>\r\n<pre><code> git push &lt;remote-name&gt; :&lt;remote-branch-name&gt;</code></pre>\r\n<h1 id=\"section-3\">2019-02-21</h1>\r\n<h2 id=\"对l2正则化的理解\">对l2正则化的理解</h2>\r\n<p>普通的损失函数可以写成如下： <span class=\"math display\">\\[\r\nJ(w;X,y)\r\n\\]</span></p>\r\n<p>在权重参数上加上l2正则化项之后，损失函数如下：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    &amp;\\tilde{J} = \\frac{\\alpha}{2}w^Tw + J(w;X,y)\\\\\r\n    &amp;对w的求导：\\\\\r\n    &amp;\\nabla_w\\tilde{J} = \\alpha w + \\nabla_w J(w;X,y)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>每次的参数更新可以写成如下：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    &amp;w \\leftarrow w - \\epsilon(\\alpha w + \\nabla_w J(w;X,y))\\\\\r\n    &amp;等价于：\\\\\r\n    &amp;w \\leftarrow (1 - \\epsilon\\alpha) w + \\epsilon\\nabla_w J(w;X,y)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>为了分析l2正则化的作用，假定<span class=\"math inline\">\\(J(w ^ \\ast ; X , y)\\)</span>是损失函数的极小值。 <span class=\"math inline\">\\(w^\\ast\\)</span>是使损失函数取得极小值的参数，先简单的对损失函数进行二次近似如下，这里没有一次项是因为在函数极小值的地方，一次导数应该为0。</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    J(w;X,y) = J(w^\\ast;X,y) + (w-w^\\ast)^TH(w-w^\\ast)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>对加上l2正则化项的损失函数近似形式如下：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\hat{J}(w;X,y) = J(w^\\ast;X,y) + (w-w^\\ast)^TH(w-w^\\ast) + \\alpha\\frac{1}{2}w^Tw\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>对损失函数进行求导如下。</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\nabla_w \\hat{J}(w;X,y) = \\alpha w + H(w - w^\\ast)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>当<span class=\"math inline\">\\(\\nabla_w\\hat{J}(w;X,y) = 0\\)</span>时，有如下推导：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\alpha w + H(w - w^\\ast) = 0\\\\\r\n    (H + \\alpha I)w = Hw^\\ast\\\\\r\n    w = (H + \\alpha I)^{-1}Hw^\\ast\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>可见当<span class=\"math inline\">\\(\\alpha \\rightarrow 0\\)</span>时，<span class=\"math inline\">\\(w \\rightarrow w^\\ast\\)</span>，进一步，因为<span class=\"math inline\">\\(H\\)</span>是实对称矩阵，因此必定可以正交对角化，<span class=\"math inline\">\\(H=Q \\Lambda Q^T\\)</span>，因此进一步推导如下：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    w &amp;= (Q \\Lambda Q^T+ \\alpha I)^{-1}Q \\Lambda Q^Tw^\\ast\\\\\r\n    &amp;=[Q (\\Lambda+ \\alpha I) Q^T]^{-1}Q \\Lambda Q^Tw^\\ast\\\\\r\n    &amp;=Q (\\Lambda+ \\alpha I)^{-1}\\Lambda Q^Tw^\\ast\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>其中，假设<span class=\"math inline\">\\(\\Lambda\\)</span>如下：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{bmatrix}\r\n    &amp;\\lambda_1 &amp;0 &amp;0 &amp;\\cdots &amp;0\\\\\r\n    &amp;0 &amp;\\lambda_2 &amp;0 &amp;\\cdots &amp;0\\\\\r\n    &amp;0 &amp;0 &amp;\\lambda_3 &amp;\\cdots &amp;0\\\\\r\n    &amp;\\vdots &amp;\\vdots &amp;\\vdots &amp;\\ddots &amp;\\vdots\\\\\r\n    &amp;0 &amp;0 &amp;0 &amp;\\cdots &amp;\\lambda_n\\\\\r\n\\end{bmatrix}\r\n\\]</span></p>\r\n<p>则<span class=\"math inline\">\\((\\Lambda+ \\alpha I)^{-1}\\Lambda\\)</span>如下： <span class=\"math display\">\\[\r\n\\begin{bmatrix}\r\n    &amp;\\frac{\\lambda_1}{\\lambda_1 + \\alpha} &amp;0 &amp;0 &amp;\\cdots &amp;0\\\\\r\n    &amp;0 &amp;\\frac{\\lambda_2}{\\lambda_2 + \\alpha} &amp;0 &amp;\\cdots &amp;0\\\\\r\n    &amp;0 &amp;0 &amp;\\frac{\\lambda_3}{\\lambda_3 + \\alpha} &amp;\\cdots &amp;0\\\\\r\n    &amp;\\vdots &amp;\\vdots &amp;\\vdots &amp;\\ddots &amp;\\vdots\\\\\r\n    &amp;0 &amp;0 &amp;0 &amp;\\cdots &amp;\\frac{\\lambda_n}{\\lambda_n + \\alpha}\\\\\r\n\\end{bmatrix}\r\n\\]</span> 这就相当于在原损失函数极小值点的Hession矩阵<span class=\"math inline\">\\(H\\)</span>的特征向量方向上，将<span class=\"math inline\">\\(w^\\ast\\)</span>进行了缩放，而且特征值<span class=\"math inline\">\\(\\lambda_i\\)</span>越小的方向，<span class=\"math inline\">\\(\\alpha\\)</span>对其影响越大，缩小得越大，即加入l2正则化项之后，模型参数倾向于对损失函数影响显著的方向偏移。</p>\r\n<p>以最小二乘线性回归为例，其损失函数如下：</p>\r\n<p><span class=\"math display\">\\[\r\n(Xw-y)^T(Xw-y)\r\n\\]</span></p>\r\n<p>如果加上l2正则化，则损失函数变成如下形式：</p>\r\n<p><span class=\"math display\">\\[\r\n(Xw-y)^T(Xw-y) + \\frac{1}{2}\\alpha w^Tw\r\n\\]</span></p>\r\n<p>那么线性回归的解就从：</p>\r\n<p><span class=\"math display\">\\[\r\nw = (X^TX)^{-1}X^Ty\r\n\\]</span></p>\r\n<p>变成了： <span class=\"math display\">\\[\r\nw = (X^TX + \\alpha I)^{-1}X^Ty\r\n\\]</span></p>\r\n<p>其中<span class=\"math inline\">\\(X\\)</span>可以写成如下,其中<span class=\"math inline\">\\(x_{ij}\\)</span>表示第i个样本<span class=\"math inline\">\\(x_i\\)</span>的第j维： <span class=\"math display\">\\[\r\nX=\\begin{bmatrix}\r\n    &amp;x_{11} &amp;x_{12} &amp;\\cdots &amp;x_{1m}\\\\\r\n    &amp;x_{21} &amp;x_{22} &amp;\\cdots &amp;x_{2m}\\\\\r\n    &amp;\\vdots &amp;\\vdots &amp;\\ddots &amp;\\vdots\\\\\r\n    &amp;x_{n1} &amp;x_{n1} &amp;\\cdots &amp;x_{nm}\\\\\r\n\\end{bmatrix}\r\n\\]</span></p>\r\n<p>则<span class=\"math inline\">\\(X^TX\\)</span>可以表示如下： <span class=\"math display\">\\[\r\nX^TX=\\begin{bmatrix}\r\n    &amp;\\sum_i^nx_{i1}x_{i1} &amp;\\sum_i^nx_{i1}x_{i2} &amp;\\cdots &amp;\\sum_i^nx_{i1}x_{im}\\\\\r\n    &amp;\\sum_i^nx_{i2}x_{i1} &amp;\\sum_i^nx_{i2}x_{i2} &amp;\\cdots &amp;\\sum_i^nx_{i2}x_{im}\\\\\r\n    &amp;\\vdots &amp;\\vdots &amp;\\ddots &amp;\\vdots\\\\\r\n    &amp;\\sum_i^nx_{im}x_{i1} &amp;\\sum_i^nx_{im}x_{i2} &amp;\\cdots &amp;\\sum_i^nx_{im}x_{im}\\\\\r\n\\end{bmatrix}\r\n\\]</span></p>\r\n<p><span class=\"math inline\">\\(X^TX\\)</span>同样可以正交对角化，<span class=\"math inline\">\\(X^TX = Q \\Lambda Q^T\\)</span>，这里的<span class=\"math inline\">\\(\\Lambda\\)</span>对角线上的值是<span class=\"math inline\">\\(X\\)</span>奇异值的平方，之后的推导和上面相同，可见，加入l2正则化项之后，模型参数倾向于对损失函数影响显著的方向偏移，从而忽略掉数据中的一些干扰，增强模型泛化能力。</p>\r\n<h2 id=\"记录几个简单的矩阵求导公式\">记录几个简单的矩阵求导公式</h2>\r\n<p><span class=\"math display\">\\[\r\n\\frac{\\partial \\beta^T X}{\\partial X} = \\beta\\\\\r\n\\quad\\\\\r\n\\frac{\\partial X^T X}{\\partial X} = X\\\\\r\n\\quad\\\\\r\n\\frac{\\partial X^T A X}{\\partial X} = (A+A^T)X\\\\\r\n\\]</span></p>\r\n<h1 id=\"section-4\">2019-02-25</h1>\r\n<h2 id=\"使用tensorflow后端的keras中的session和graph\">使用tensorflow后端的keras中的session和graph</h2>\r\n<p>这两天在进行将keras模型集成到项目系统的工作，这个系统需要加载不同的模型和训练不同的模型，而且顺序不固定，多个模型可能同时存在，一些在训练，一些在预测，因此直接加载模型，使用keras创建的session和graph的时候，遇到了很多错误，最后的解决方案是为模型手动创建session，并在加载或者使用模型之前使用Keras.backend.set_session方法设置当前使用的session。</p>\r\n<p>还有一个问题是模型使用完成之后，已经使用del model的形式删除，但是windows任务管理器上显存依旧占用，尝试过Keras.backend.clear_session的方式，显存依旧占用，但是其它模型也可以继续跑，不知道是不是tensorflow自动分配了显存的原因。</p>\r\n<h2 id=\"python-multiprocessing的问题\">python multiprocessing的问题</h2>\r\n<p>前两天遇到一个问题，在使用multiprocessing进行多进程任务时，发现所调用的函数所在的文件，多次执行import操作，导致已经被修改的import变量出现不一致的问题，现在还没有找到解决办法，只能把多进程改为了单进程执行。</p>\r\n<h1 id=\"section-5\">2019-02-26</h1>\r\n<h2 id=\"对l1正则化的理解\">对l1正则化的理解</h2>\r\n<p>进行l1正则化之后的损失函数如下：</p>\r\n<p><span class=\"math display\">\\[\r\n\\tilde{J} = \\alpha ||w||_1 + J(w;X,y)\r\n\\]</span></p>\r\n<p>对其进行求导： <span class=\"math display\">\\[\r\n\\nabla_w\\tilde{J} = \\alpha sign(w) + \\nabla_wJ(w;X,y)\r\n\\]</span> 从这里可以看出l1正则化项对导数的影响是一个固定值，和l2有很大区别。</p>\r\n<p>使用在l2正则化分析中的损失函数近似方法，将原本的损失函数二次近似为<span class=\"math inline\">\\(\\hat{J}(w)\\)</span>，其导数如下： <span class=\"math display\">\\[\r\n\\nabla_w\\hat{J} = H(w - w^\\ast)\r\n\\]</span></p>\r\n<p>加上正则化项之后，其损失函数的二次近似可以表示为：</p>\r\n<p><span class=\"math display\">\\[\r\n\\hat{J}(w;X,y) = J(w^\\ast;X,y) + \\sum_i[\\frac{1}{2}H_{i,i}(w_i-w_i^\\ast)^2 + \\alpha|w_i|]\r\n\\]</span></p>\r\n<p>这里将Hessian矩阵简化成了对角阵，这个类似对数据进行PCA之类的操作，将数据之间的相关性去掉了，因此Hessian矩阵变成对角阵，这样分析要简单一些。</p>\r\n<p>如果对<span class=\"math inline\">\\(w_i\\)</span>进行求导，如下： <span class=\"math display\">\\[\r\n\\nabla_{w_i}\\hat{J}(w;X,y) = H_{i,i}(w_i - w_i^\\ast) + \\alpha sign(w_i)\r\n\\]</span></p>\r\n<p>可以看出，加了l1正则化之后最优解变成了如下： <span class=\"math display\">\\[\r\nw_i = sign(w_i^\\ast)max\\{|w_i^\\ast| - \\frac{\\alpha}{H_{i,i}}, 0\\}\r\n\\]</span></p>\r\n<p>描述成图像大概如下：</p>\r\n<img src=\"/2019/02/13/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2019-02/l1%E6%AD%A3%E5%88%99%E5%8C%96%E6%95%88%E6%9E%9C%E7%A4%BA%E6%84%8F.png\" class=\"\" title=\"l1正则化效果示意\">\r\n<p>可以看出，l1使得权重变得更加稀疏，这在特征选择方面非常有用。</p>\r\n<h1 id=\"section-6\">2019-02-27</h1>\r\n<h2 id=\"数据增强之噪声\">数据增强之噪声</h2>\r\n<h3 id=\"直接在数据上添加噪声\">1.直接在数据上添加噪声</h3>\r\n<p>普遍来说，在数据上添加噪声比简单的收缩权重更加有效。</p>\r\n<h3 id=\"在权重上添加噪声\">2.在权重上添加噪声</h3>\r\n<p>类似贝叶斯方法中将权重参数也作为一个随机变量的做法，在权重上添加噪声类似于在损失函数上加上预测结果对权重的导数的l2正则化项:<span class=\"math inline\">\\(\\eta E_{p(x,y)}[||\\nabla_W \\hat{y}(x)||^2]\\)</span>，其中<span class=\"math inline\">\\(\\hat{y}\\)</span>表示模型的输出。这会使得模型偏好于一个对于参数来说更加平缓的区域。</p>\r\n<h3 id=\"在标签上添加噪声\">3.在标签上添加噪声</h3>\r\n<p>对于二分类问题，可以使用<span class=\"math inline\">\\(\\epsilon\\)</span>和<span class=\"math inline\">\\(1 - \\epsilon\\)</span>来代替0、1标签，对于<span class=\"math inline\">\\(k\\)</span>类多分类问题，则可以使用<span class=\"math inline\">\\(\\frac{\\epsilon}{k}\\)</span> 和 <span class=\"math inline\">\\(1 - \\frac{k - 1}{k}\\epsilon\\)</span>来代替，这对交叉熵损失函数可能没什么区别，因为交叉熵损失函数中为了避免<span class=\"math inline\">\\(log0\\)</span>，已经使用了这种标签平滑，但是对于最大似然学习并在最后一层使用softmax的算法，标签平滑可能会很有用，因为使用softmax之后，输出不可能为绝对的1和0，因此模型会尝试学习越来越大的权重，让输出更加极端，容易导致数值溢出和模型不稳定的问题，在标签上添加噪声可以解决这个问题，虽然通过权重衰减策略也可以解决这个问题，但是盲目的权重衰减可能影响到分类正确性，而在标签上添加噪声没有这个问题。</p>\r\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"section\">2019-02-13</h1>\r\n<h2 id=\"支持向量机\">支持向量机</h2>\r\n<p>首先复习了数学上的一个小知识，如果有一个超平面可以表达为如下线性方程： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    w^Tx+b = 0\r\n\\end{aligned}\r\n\\]</span> 则空间中任意一点<span class=\"math inline\">\\(x_0\\)</span>到该超平面的距离可以表示为如下： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    r = \\frac{|w^Tx_0+b|}{||w||}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>在分类问题的样本空间中，令<span class=\"math inline\">\\(x_i\\)</span>和<span class=\"math inline\">\\(y_i\\)</span>分别表示某个样本和其对应的标签，支持向量机的基本思想是求出一个满足如下条件的超平面： <span class=\"math display\">\\[\r\n\\begin{cases}\r\n    w^Tx_i + b \\ge 1 &amp; y_i = 1\\\\\r\n    w^Tx_i + b \\le 1 &amp; y_i = -1\r\n\\end{cases}\r\n\\]</span> 示意图如下,这里z展示的样本的特征空间为2维，图中被圈出来的满足上式等号条件一个正样本和两个负样本被称为支持向量，<span class=\"math inline\">\\(\\gamma=\\frac{2}{||w||}\\)</span>称为间隔。</p>\r\n<img src=\"/2019/02/13/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2019-02/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E8%B6%85%E5%B9%B3%E9%9D%A2%E7%A4%BA%E6%84%8F.png\" class=\"\" title=\"支持向量机超平面示意\">\r\n<p>找分类超平面的问题就可以转化为如下： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    &amp;\\max_{w,b}\\frac{2}{||w||}\\\\\r\n    &amp;s.t.\\quad y_i(w^Tx_i + b) \\ge 1, i = 1,2,...,m\\\\\r\n    等价为\\\\\r\n    &amp;\\min_{w,b}\\frac{||w||^2}{2}\\\\\r\n    &amp;s.t.\\quad y_i(w^Tx_i + b) \\ge 1, i = 1,2,...,m\r\n\\end{aligned}\r\n\\]</span> 这里可以使用不等式约束下的拉格朗日乘子法转换为对偶问题进行求解。</p>\r\n<p>支持向量机的基本型就是希望找到一个间隔最大的超平面，但是这里有个问题，在原始的特征空间中，样本不一定线性可分，因此引入一个函数，将原始特征空间映射到一个高维的特征空间，使得在这个高维空间中线性可分，这样即可将任何分类问题转换成线性可分的形式，但是这样也有一个问题，设映射后的特征向量为<span class=\"math inline\">\\(\\phi(x)\\)</span>，求解超平面的过程中，会遇到<span class=\"math inline\">\\({\\phi(x)}^T\\phi(x)\\)</span>的情况，因为<span class=\"math inline\">\\(\\phi(x)\\)</span>的维数可以为无穷，所以这里的计算无法进行，因此需要设想一个这样的函数：<span class=\"math inline\">\\(k(x_i,x_j)={\\phi(x_i)}^T\\phi(x_j)\\)</span>使得不需要通过<span class=\"math inline\">\\(\\phi(x)\\)</span>来计算<span class=\"math inline\">\\({\\phi(x)}^T\\phi(x)\\)</span>，而是直接通过<span class=\"math inline\">\\(k(x_i,x_j)\\)</span>来表示，这个函数即称为核函数。</p>\r\n<p>核函数可以用一个核矩阵来表示如下，任何一个对称的半正定矩阵，都可以作为核矩阵。 <span class=\"math display\">\\[\r\nk=\\begin{bmatrix}\r\n    k(x_1,x_1) &amp;\\cdots &amp;k(x_1,x_j) &amp;\\cdots &amp;k(x_1,x_n)\\\\\r\n    \\vdots &amp;\\ddots &amp;\\vdots &amp;\\ddots &amp;\\vdots\\\\\r\n    k(x_j,x_1) &amp;\\cdots &amp;k(x_j,x_j) &amp;\\cdots &amp;k(x_j,x_n)\\\\\r\n    \\vdots &amp;\\ddots &amp;\\vdots &amp;\\ddots &amp;\\vdots\\\\\r\n    k(x_n,x_1) &amp;\\cdots &amp;k(x_n,x_j) &amp;\\cdots &amp;k(x_n,x_n)\\\\\r\n\\end{bmatrix}\r\n\\]</span></p>\r\n<p>选择一个核函数，就隐式的确定了映射的特征空间，因此核函数的选择非常重要。</p>\r\n<p>但是在实际使用中，选出一个合适的特征空间恰好让这个样本线性可分非常不容易，因此将优化目标重写为： <span class=\"math display\">\\[\r\n\\min_{w,b}\\frac{1}{2}{||w||}^2+C\\sum_{i=1}^ml(y_i(w^Tx_i+b) - 1)\r\n\\]</span> 这里的<span class=\"math inline\">\\(C\\)</span>为一个设定好的常数，决定模型的偏好，<span class=\"math inline\">\\(l\\)</span>为一种损失函数。这样的支持向量机被称为软间隔支持向量机。</p>\r\n<h1 id=\"section-1\">2019-02-19</h1>\r\n<h2 id=\"对拉格朗日乘子法的理解\">对拉格朗日乘子法的理解</h2>\r\n<p>前置知识：梯度的方向是当前等值平面的法线方向。约束函数和目标函数相切时，约束函数和目标函数的梯度方向相同。</p>\r\n<p>如下图所示，其中<span class=\"math inline\">\\(f(x,y)\\)</span>是目标函数，<span class=\"math inline\">\\(f(x,y) = d_1,f(x,y) = d_2,f(x,y) = d_3\\)</span>都是目标函数的等值线， <span class=\"math inline\">\\(g(x,y) = c\\)</span>是约束条件，在约束条件和等值线相切的地方，有<span class=\"math inline\">\\(\\nabla f = \\lambda \\nabla g\\)</span>,这里假设<span class=\"math inline\">\\(d_1 &lt; d_2 &lt; d_3\\)</span>且所求问题为 <span class=\"math display\">\\[\r\n\\min f(x,y) \\\\\r\ns.t. g(x,y)=c\r\n\\]</span> 那么其最优解一定出现在相切的地方，即只要求如下联立方程即可得到最优解的可能位置。</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{cases}\r\n    \\nabla f = \\lambda \\nabla g \\\\\r\n    g(x,y) = 0\r\n\\end{cases}\r\n\\]</span></p>\r\n<p>但是一般看到的拉格朗日乘子法是下面的形式： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    &amp;首先定义F(x,y,\\lambda) = f(x,y) + \\lambda g(x,y)\\\\\r\n    &amp;之后求解\\nabla F = \\mathbf{0}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>其实是一样的，展开<span class=\"math inline\">\\(\\nabla F\\)</span>即可，如下： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\begin{pmatrix}\r\n        \\frac{\\partial F}{\\partial x}\\\\\r\n        \\\\\r\n        \\frac{\\partial F}{\\partial y}\\\\\r\n        \\\\\r\n        \\frac{\\partial F}{\\partial \\lambda}\\\\\r\n    \\end{pmatrix} &amp;= \\begin{pmatrix}\r\n        0\\\\\r\n        \\\\\r\n        0\\\\\r\n        \\\\\r\n        0\\\\\r\n    \\end{pmatrix}\\\\\r\n    &amp;即\\\\\r\n    \\nabla f &amp;= -\\lambda \\nabla g \\\\\r\n    \\frac{\\partial F}{\\partial \\lambda} &amp;= g(x,y)\\\\\r\n    &amp;= 0\\\\\r\n\\end{aligned}\r\n\\]</span></p>\r\n<img src=\"/2019/02/13/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2019-02/%E7%BA%A6%E6%9D%9F%E6%9D%A1%E4%BB%B6%E5%92%8C%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E7%9A%84%E7%AD%89%E5%80%BC%E7%BA%BF.png\" class=\"\" title=\"约束条件和目标函数的等值线\">\r\n<p>上面只是单个约束时且约束为等式的情况，如果是多个等式约束，则可以写成</p>\r\n<p><span class=\"math inline\">\\(F(x,y,\\lambda_1, \\lambda_2,...,\\lambda_m) = f(x,y) + \\lambda_i g_i(x,y);i = 1, 2, ..., m\\)</span></p>\r\n<p>同理求解如下： <span class=\"math display\">\\[\r\n\\begin{cases}\r\n    \\nabla f = \\lambda_i \\nabla g_i &amp;i=1,2,...m\\\\\r\n    g_i(x,y) = 0&amp;i=1,2,...m\r\n\\end{cases}\r\n\\]</span> 也可以写成 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\begin{pmatrix}\r\n        \\frac{\\partial F}{\\partial x}\\\\\r\n        \\\\\r\n        \\frac{\\partial F}{\\partial y}\\\\\r\n        \\\\\r\n        \\frac{\\partial F}{\\partial \\lambda_1}\\\\\r\n        \\vdots\\\\\r\n        \\frac{\\partial F}{\\partial \\lambda_m}\\\\\r\n    \\end{pmatrix} = \\begin{pmatrix}\r\n        0\\\\\r\n        \\\\\r\n        0\\\\\r\n        \\\\\r\n        0\\\\\r\n        \\vdots\\\\\r\n        0\\\\\r\n    \\end{pmatrix}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>如果是不等式约束，如下描述： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\min f(x,y)\\\\\r\n    s.t.\\quad h(x,y) \\le 0\r\n\\end{aligned}\\]</span> 可以分成两种情况讨论，第一种情况是内部解，第二种情况是边界解，如果最优解在约束条件内部，那么约束条件不起作用， 退化为无约束问题，<span class=\"math inline\">\\(\\nabla f = 0 且 \\lambda = 0\\)</span>，如果在边界上，则满足<span class=\"math inline\">\\(h(x,y)=0\\)</span>因此不论哪种情况，<span class=\"math inline\">\\(\\lambda h(x,y) = 0\\)</span>恒成立。</p>\r\n<p>这里依旧定义一个如下形式的函数： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    F(x,y,\\mu) = f(x,y) + \\mu h(x,y)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>因此可以提出KKT条件的基本形式： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\begin{cases}\r\n        \\begin{pmatrix}\r\n        \\frac{\\partial F}{\\partial x}\\\\\r\n        \\\\\r\n        \\frac{\\partial F}{\\partial y}\r\n    \\end{pmatrix} &amp;= \\begin{pmatrix}\r\n        0\\\\\r\n        \\\\\r\n        0\r\n    \\end{pmatrix}\\\\\r\n    h(x,y) &amp;\\le 0\\\\\r\n    \\lambda &amp;\\ge 0 \\\\\r\n    \\lambda h(x,y) &amp;= 0\r\n    \\end{cases}\r\n\\end{aligned}\r\n\\]</span> 这里的<span class=\"math inline\">\\(\\lambda \\ge 0\\)</span>是因为要求的是<span class=\"math inline\">\\(\\min f(x,y)\\)</span>，如果要求的是<span class=\"math inline\">\\(\\max f(x,y)\\)</span>,则使用<span class=\"math inline\">\\(\\lambda \\le 0\\)</span></p>\r\n<p>对于多个等式约束和多个不等式约束的情况： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    &amp;\\min f(\\mathbf{x})\\\\\r\n    s.t.\\quad &amp;g_j(\\mathbf{x}) = 0, j=1,2,...m\\\\\r\n    s.t.\\quad &amp;h_k(\\mathbf{x}) \\le 0, k=1,2,...n\r\n\\end{aligned}\r\n\\]</span> 首先定义拉格朗日函数： <span class=\"math display\">\\[\r\nL(\\mathbf{x},\\lambda_1,\\lambda_2,...,\\lambda_m,\\mu_1,\\mu_2,...,\\mu_n) = f(\\mathbf{x}) + \\sum_{j=1}^m \\lambda_j g_j(\\mathbf{x}) + \\sum_{k=1}^n \\mu_k h_k(\\mathbf{x})\r\n\\]</span> 关于对偶问题的分析略过，结论是如果满足KKT条件，则原问题的最优解和对偶问题最优解相同，计算KKT条件，即可得到候选解，然后对比不同的候选解，即可得出最优解，这里直接给出完整的KKT条件： <span class=\"math display\">\\[\r\n\\begin{cases}\r\n    \\nabla_{\\mathbf{x}}L = \\mathbf{0}\\\\\r\n    g_j(\\mathbf{x})=0 \\quad j=1,2,...,m\\\\\r\n    h_k(\\mathbf{x})\\le 0 \\quad k=1,2,...,n\\\\\r\n    \\mu_k \\ge 0 \\quad k=1,2,...,n\\\\\r\n    \\mu_k h_k(\\mathbf{x})=0 \\quad k=1,2,...,n\r\n\\end{cases}\r\n\\]</span> 求解KKT条件，即可找到约束优化问题的解。</p>\r\n<h1 id=\"section-2\">2019-02-20</h1>\r\n<h2 id=\"git的常用操作复习\">git的常用操作复习</h2>\r\n<p>查看仓库当前状态</p>\r\n<pre><code> git status </code></pre>\r\n<p>查看修改内容</p>\r\n<pre><code> git diff &lt;file&gt;</code></pre>\r\n<p>查看最近的提交</p>\r\n<pre><code> git log</code></pre>\r\n<p>恢复最近提交</p>\r\n<pre><code> git reset --[soft|mixed|hard] HEAD~\r\n soft:仅仅修改HEAD指向\r\n mixed:在修改HEAD之后，修改暂存区内容\r\n hard:在修改暂存区内容之后，修改工作区内容\r\n HEAD~:等效于HEAD^，代表HEAD的上一次提交，可以使用HEAD~100代表HEAD之前100次提交\r\n HEAD~也可替换为commit id</code></pre>\r\n<p>如果指定恢复文件，则只从最后一次提交恢复暂存区文件。</p>\r\n<pre><code> git reset file\r\n 等价于 git reset --mixed HEAD file</code></pre>\r\n<p>查看版本库历史和commit id，可以用</p>\r\n<pre><code> git log</code></pre>\r\n<p>修改最后一次commit的信息</p>\r\n<pre><code> git commit --amend</code></pre>\r\n<p>查看版本库所有的改动，可以用</p>\r\n<pre><code> git reflog</code></pre>\r\n<p>创建和切换分支</p>\r\n<pre><code> git branch &lt;branch_name&gt; #创建分支\r\n git chechout &lt;branch_name&gt; #切换分支\r\n git checkout -b &lt;branch_name&gt; #切换分支，不存在则创建</code></pre>\r\n<p>将当前的暂存区改动压栈并隐藏</p>\r\n<pre><code> git stash</code></pre>\r\n<p>查看当前的栈区</p>\r\n<pre><code> git stash list</code></pre>\r\n<p>从栈顶恢复</p>\r\n<pre><code> git stash apply # 只恢复不删除栈内容\r\n git stash pop # 恢复并删除栈内容</code></pre>\r\n<p>查看当前分支</p>\r\n<pre><code> git branch</code></pre>\r\n<p>合并分支</p>\r\n<pre><code> git merge &lt;branch_name&gt; #合并指定分支到当前分支</code></pre>\r\n<p>如果合并出现冲突，修改冲突文件之后，可以重新add、commit进行提交</p>\r\n<p>删除分支</p>\r\n<pre><code> git branch -d &lt;branch_name&gt;</code></pre>\r\n<p>关联远程分支和本地分支</p>\r\n<pre><code> git branch --set-upstream &lt;branch-name&gt; &lt;origin/branch-name&gt;</code></pre>\r\n<p>之后就可以从远程分支pull或者push到远程分支了</p>\r\n<p>关联远程服务器</p>\r\n<pre><code> git remote add &lt;remote-name&gt; &lt;remote-url&gt;</code></pre>\r\n<p>创建远程分支</p>\r\n<pre><code> git push &lt;remote-name&gt; &lt;local-branch-name&gt;:&lt;remote-branch-name&gt;</code></pre>\r\n<p>删除远程分支</p>\r\n<pre><code> git push &lt;remote-name&gt; :&lt;remote-branch-name&gt;</code></pre>\r\n<h1 id=\"section-3\">2019-02-21</h1>\r\n<h2 id=\"对l2正则化的理解\">对l2正则化的理解</h2>\r\n<p>普通的损失函数可以写成如下： <span class=\"math display\">\\[\r\nJ(w;X,y)\r\n\\]</span></p>\r\n<p>在权重参数上加上l2正则化项之后，损失函数如下：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    &amp;\\tilde{J} = \\frac{\\alpha}{2}w^Tw + J(w;X,y)\\\\\r\n    &amp;对w的求导：\\\\\r\n    &amp;\\nabla_w\\tilde{J} = \\alpha w + \\nabla_w J(w;X,y)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>每次的参数更新可以写成如下：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    &amp;w \\leftarrow w - \\epsilon(\\alpha w + \\nabla_w J(w;X,y))\\\\\r\n    &amp;等价于：\\\\\r\n    &amp;w \\leftarrow (1 - \\epsilon\\alpha) w + \\epsilon\\nabla_w J(w;X,y)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>为了分析l2正则化的作用，假定<span class=\"math inline\">\\(J(w ^ \\ast ; X , y)\\)</span>是损失函数的极小值。 <span class=\"math inline\">\\(w^\\ast\\)</span>是使损失函数取得极小值的参数，先简单的对损失函数进行二次近似如下，这里没有一次项是因为在函数极小值的地方，一次导数应该为0。</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    J(w;X,y) = J(w^\\ast;X,y) + (w-w^\\ast)^TH(w-w^\\ast)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>对加上l2正则化项的损失函数近似形式如下：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\hat{J}(w;X,y) = J(w^\\ast;X,y) + (w-w^\\ast)^TH(w-w^\\ast) + \\alpha\\frac{1}{2}w^Tw\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>对损失函数进行求导如下。</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\nabla_w \\hat{J}(w;X,y) = \\alpha w + H(w - w^\\ast)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>当<span class=\"math inline\">\\(\\nabla_w\\hat{J}(w;X,y) = 0\\)</span>时，有如下推导：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\alpha w + H(w - w^\\ast) = 0\\\\\r\n    (H + \\alpha I)w = Hw^\\ast\\\\\r\n    w = (H + \\alpha I)^{-1}Hw^\\ast\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>可见当<span class=\"math inline\">\\(\\alpha \\rightarrow 0\\)</span>时，<span class=\"math inline\">\\(w \\rightarrow w^\\ast\\)</span>，进一步，因为<span class=\"math inline\">\\(H\\)</span>是实对称矩阵，因此必定可以正交对角化，<span class=\"math inline\">\\(H=Q \\Lambda Q^T\\)</span>，因此进一步推导如下：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    w &amp;= (Q \\Lambda Q^T+ \\alpha I)^{-1}Q \\Lambda Q^Tw^\\ast\\\\\r\n    &amp;=[Q (\\Lambda+ \\alpha I) Q^T]^{-1}Q \\Lambda Q^Tw^\\ast\\\\\r\n    &amp;=Q (\\Lambda+ \\alpha I)^{-1}\\Lambda Q^Tw^\\ast\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>其中，假设<span class=\"math inline\">\\(\\Lambda\\)</span>如下：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{bmatrix}\r\n    &amp;\\lambda_1 &amp;0 &amp;0 &amp;\\cdots &amp;0\\\\\r\n    &amp;0 &amp;\\lambda_2 &amp;0 &amp;\\cdots &amp;0\\\\\r\n    &amp;0 &amp;0 &amp;\\lambda_3 &amp;\\cdots &amp;0\\\\\r\n    &amp;\\vdots &amp;\\vdots &amp;\\vdots &amp;\\ddots &amp;\\vdots\\\\\r\n    &amp;0 &amp;0 &amp;0 &amp;\\cdots &amp;\\lambda_n\\\\\r\n\\end{bmatrix}\r\n\\]</span></p>\r\n<p>则<span class=\"math inline\">\\((\\Lambda+ \\alpha I)^{-1}\\Lambda\\)</span>如下： <span class=\"math display\">\\[\r\n\\begin{bmatrix}\r\n    &amp;\\frac{\\lambda_1}{\\lambda_1 + \\alpha} &amp;0 &amp;0 &amp;\\cdots &amp;0\\\\\r\n    &amp;0 &amp;\\frac{\\lambda_2}{\\lambda_2 + \\alpha} &amp;0 &amp;\\cdots &amp;0\\\\\r\n    &amp;0 &amp;0 &amp;\\frac{\\lambda_3}{\\lambda_3 + \\alpha} &amp;\\cdots &amp;0\\\\\r\n    &amp;\\vdots &amp;\\vdots &amp;\\vdots &amp;\\ddots &amp;\\vdots\\\\\r\n    &amp;0 &amp;0 &amp;0 &amp;\\cdots &amp;\\frac{\\lambda_n}{\\lambda_n + \\alpha}\\\\\r\n\\end{bmatrix}\r\n\\]</span> 这就相当于在原损失函数极小值点的Hession矩阵<span class=\"math inline\">\\(H\\)</span>的特征向量方向上，将<span class=\"math inline\">\\(w^\\ast\\)</span>进行了缩放，而且特征值<span class=\"math inline\">\\(\\lambda_i\\)</span>越小的方向，<span class=\"math inline\">\\(\\alpha\\)</span>对其影响越大，缩小得越大，即加入l2正则化项之后，模型参数倾向于对损失函数影响显著的方向偏移。</p>\r\n<p>以最小二乘线性回归为例，其损失函数如下：</p>\r\n<p><span class=\"math display\">\\[\r\n(Xw-y)^T(Xw-y)\r\n\\]</span></p>\r\n<p>如果加上l2正则化，则损失函数变成如下形式：</p>\r\n<p><span class=\"math display\">\\[\r\n(Xw-y)^T(Xw-y) + \\frac{1}{2}\\alpha w^Tw\r\n\\]</span></p>\r\n<p>那么线性回归的解就从：</p>\r\n<p><span class=\"math display\">\\[\r\nw = (X^TX)^{-1}X^Ty\r\n\\]</span></p>\r\n<p>变成了： <span class=\"math display\">\\[\r\nw = (X^TX + \\alpha I)^{-1}X^Ty\r\n\\]</span></p>\r\n<p>其中<span class=\"math inline\">\\(X\\)</span>可以写成如下,其中<span class=\"math inline\">\\(x_{ij}\\)</span>表示第i个样本<span class=\"math inline\">\\(x_i\\)</span>的第j维： <span class=\"math display\">\\[\r\nX=\\begin{bmatrix}\r\n    &amp;x_{11} &amp;x_{12} &amp;\\cdots &amp;x_{1m}\\\\\r\n    &amp;x_{21} &amp;x_{22} &amp;\\cdots &amp;x_{2m}\\\\\r\n    &amp;\\vdots &amp;\\vdots &amp;\\ddots &amp;\\vdots\\\\\r\n    &amp;x_{n1} &amp;x_{n1} &amp;\\cdots &amp;x_{nm}\\\\\r\n\\end{bmatrix}\r\n\\]</span></p>\r\n<p>则<span class=\"math inline\">\\(X^TX\\)</span>可以表示如下： <span class=\"math display\">\\[\r\nX^TX=\\begin{bmatrix}\r\n    &amp;\\sum_i^nx_{i1}x_{i1} &amp;\\sum_i^nx_{i1}x_{i2} &amp;\\cdots &amp;\\sum_i^nx_{i1}x_{im}\\\\\r\n    &amp;\\sum_i^nx_{i2}x_{i1} &amp;\\sum_i^nx_{i2}x_{i2} &amp;\\cdots &amp;\\sum_i^nx_{i2}x_{im}\\\\\r\n    &amp;\\vdots &amp;\\vdots &amp;\\ddots &amp;\\vdots\\\\\r\n    &amp;\\sum_i^nx_{im}x_{i1} &amp;\\sum_i^nx_{im}x_{i2} &amp;\\cdots &amp;\\sum_i^nx_{im}x_{im}\\\\\r\n\\end{bmatrix}\r\n\\]</span></p>\r\n<p><span class=\"math inline\">\\(X^TX\\)</span>同样可以正交对角化，<span class=\"math inline\">\\(X^TX = Q \\Lambda Q^T\\)</span>，这里的<span class=\"math inline\">\\(\\Lambda\\)</span>对角线上的值是<span class=\"math inline\">\\(X\\)</span>奇异值的平方，之后的推导和上面相同，可见，加入l2正则化项之后，模型参数倾向于对损失函数影响显著的方向偏移，从而忽略掉数据中的一些干扰，增强模型泛化能力。</p>\r\n<h2 id=\"记录几个简单的矩阵求导公式\">记录几个简单的矩阵求导公式</h2>\r\n<p><span class=\"math display\">\\[\r\n\\frac{\\partial \\beta^T X}{\\partial X} = \\beta\\\\\r\n\\quad\\\\\r\n\\frac{\\partial X^T X}{\\partial X} = X\\\\\r\n\\quad\\\\\r\n\\frac{\\partial X^T A X}{\\partial X} = (A+A^T)X\\\\\r\n\\]</span></p>\r\n<h1 id=\"section-4\">2019-02-25</h1>\r\n<h2 id=\"使用tensorflow后端的keras中的session和graph\">使用tensorflow后端的keras中的session和graph</h2>\r\n<p>这两天在进行将keras模型集成到项目系统的工作，这个系统需要加载不同的模型和训练不同的模型，而且顺序不固定，多个模型可能同时存在，一些在训练，一些在预测，因此直接加载模型，使用keras创建的session和graph的时候，遇到了很多错误，最后的解决方案是为模型手动创建session，并在加载或者使用模型之前使用Keras.backend.set_session方法设置当前使用的session。</p>\r\n<p>还有一个问题是模型使用完成之后，已经使用del model的形式删除，但是windows任务管理器上显存依旧占用，尝试过Keras.backend.clear_session的方式，显存依旧占用，但是其它模型也可以继续跑，不知道是不是tensorflow自动分配了显存的原因。</p>\r\n<h2 id=\"python-multiprocessing的问题\">python multiprocessing的问题</h2>\r\n<p>前两天遇到一个问题，在使用multiprocessing进行多进程任务时，发现所调用的函数所在的文件，多次执行import操作，导致已经被修改的import变量出现不一致的问题，现在还没有找到解决办法，只能把多进程改为了单进程执行。</p>\r\n<h1 id=\"section-5\">2019-02-26</h1>\r\n<h2 id=\"对l1正则化的理解\">对l1正则化的理解</h2>\r\n<p>进行l1正则化之后的损失函数如下：</p>\r\n<p><span class=\"math display\">\\[\r\n\\tilde{J} = \\alpha ||w||_1 + J(w;X,y)\r\n\\]</span></p>\r\n<p>对其进行求导： <span class=\"math display\">\\[\r\n\\nabla_w\\tilde{J} = \\alpha sign(w) + \\nabla_wJ(w;X,y)\r\n\\]</span> 从这里可以看出l1正则化项对导数的影响是一个固定值，和l2有很大区别。</p>\r\n<p>使用在l2正则化分析中的损失函数近似方法，将原本的损失函数二次近似为<span class=\"math inline\">\\(\\hat{J}(w)\\)</span>，其导数如下： <span class=\"math display\">\\[\r\n\\nabla_w\\hat{J} = H(w - w^\\ast)\r\n\\]</span></p>\r\n<p>加上正则化项之后，其损失函数的二次近似可以表示为：</p>\r\n<p><span class=\"math display\">\\[\r\n\\hat{J}(w;X,y) = J(w^\\ast;X,y) + \\sum_i[\\frac{1}{2}H_{i,i}(w_i-w_i^\\ast)^2 + \\alpha|w_i|]\r\n\\]</span></p>\r\n<p>这里将Hessian矩阵简化成了对角阵，这个类似对数据进行PCA之类的操作，将数据之间的相关性去掉了，因此Hessian矩阵变成对角阵，这样分析要简单一些。</p>\r\n<p>如果对<span class=\"math inline\">\\(w_i\\)</span>进行求导，如下： <span class=\"math display\">\\[\r\n\\nabla_{w_i}\\hat{J}(w;X,y) = H_{i,i}(w_i - w_i^\\ast) + \\alpha sign(w_i)\r\n\\]</span></p>\r\n<p>可以看出，加了l1正则化之后最优解变成了如下： <span class=\"math display\">\\[\r\nw_i = sign(w_i^\\ast)max\\{|w_i^\\ast| - \\frac{\\alpha}{H_{i,i}}, 0\\}\r\n\\]</span></p>\r\n<p>描述成图像大概如下：</p>\r\n<img src=\"/2019/02/13/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2019-02/l1%E6%AD%A3%E5%88%99%E5%8C%96%E6%95%88%E6%9E%9C%E7%A4%BA%E6%84%8F.png\" class=\"\" title=\"l1正则化效果示意\">\r\n<p>可以看出，l1使得权重变得更加稀疏，这在特征选择方面非常有用。</p>\r\n<h1 id=\"section-6\">2019-02-27</h1>\r\n<h2 id=\"数据增强之噪声\">数据增强之噪声</h2>\r\n<h3 id=\"直接在数据上添加噪声\">1.直接在数据上添加噪声</h3>\r\n<p>普遍来说，在数据上添加噪声比简单的收缩权重更加有效。</p>\r\n<h3 id=\"在权重上添加噪声\">2.在权重上添加噪声</h3>\r\n<p>类似贝叶斯方法中将权重参数也作为一个随机变量的做法，在权重上添加噪声类似于在损失函数上加上预测结果对权重的导数的l2正则化项:<span class=\"math inline\">\\(\\eta E_{p(x,y)}[||\\nabla_W \\hat{y}(x)||^2]\\)</span>，其中<span class=\"math inline\">\\(\\hat{y}\\)</span>表示模型的输出。这会使得模型偏好于一个对于参数来说更加平缓的区域。</p>\r\n<h3 id=\"在标签上添加噪声\">3.在标签上添加噪声</h3>\r\n<p>对于二分类问题，可以使用<span class=\"math inline\">\\(\\epsilon\\)</span>和<span class=\"math inline\">\\(1 - \\epsilon\\)</span>来代替0、1标签，对于<span class=\"math inline\">\\(k\\)</span>类多分类问题，则可以使用<span class=\"math inline\">\\(\\frac{\\epsilon}{k}\\)</span> 和 <span class=\"math inline\">\\(1 - \\frac{k - 1}{k}\\epsilon\\)</span>来代替，这对交叉熵损失函数可能没什么区别，因为交叉熵损失函数中为了避免<span class=\"math inline\">\\(log0\\)</span>，已经使用了这种标签平滑，但是对于最大似然学习并在最后一层使用softmax的算法，标签平滑可能会很有用，因为使用softmax之后，输出不可能为绝对的1和0，因此模型会尝试学习越来越大的权重，让输出更加极端，容易导致数值溢出和模型不稳定的问题，在标签上添加噪声可以解决这个问题，虽然通过权重衰减策略也可以解决这个问题，但是盲目的权重衰减可能影响到分类正确性，而在标签上添加噪声没有这个问题。</p>\r\n"},{"title":"目标检测","date":"2020-05-19T07:21:29.000Z","mathjax":true,"_content":"\n目标检测任务简单来说就是给定一张图片，模型需要根据图片找到感兴趣的目标，将其位置、大小标注出来。\n\n# 两阶段检测算法，从RCNN到Faster R-CNN\n目前的目标检测方法主要分为：两阶段的检测器、单阶段检测器两种，RCNN到Faster RCNN系列检测器是典型的两阶段检测器的代表。\n\n## R-CNN\n论文《Rich feature hierarchies for accurate oject detection and semantic segmentation》中提出了R-CNN模型用于目标检测，其基本流程如下：\n- 使用Selective Search算法提取出2k个region proposal。\n- 对于每个region proposal，先裁剪出对应区域的图片，然后使用AlexNet进行特征提取（特征取最后那个softmax层的输入，是一个4096维的向量）。\n- 训练K个线性SVM分类器，对每个region proposal进行K次分类，得到其属于K个类别的得分。\n- 对每个类别的proposal进行NMS后处理，得到最终的结果。\n\n这里的Selective Search算法本质上是一种先分割图片然后再尝试各种组合的穷举算法，这里没有详细研究。\n\n整体来说，R-CNN的流程非常粗暴，主要有两个阶段：1、获取region proposal，2、对region proposal进行分类，但这也是后面的两阶段检测方法的一般流程。\n\n## SPP-net\n在R-CNN中存在的一个问题是region proposal的大小不相等，但是AlexNet要求固定大小的输入图片，否则特征维度会发生变换（如果现在来看这个问题的话，加个global pooling就OK了，但是当时没想到），因此在RCNN里面只能通过缩放，让region proposal满足输入大小的要求，这样带来了畸变的问题，为了解决这个问题，论文《Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition》提出了空间金字塔池化操作(Spatial Pyramid Pooling，SPP)，而且顺便结局了RCNN中对每个region proposal都提取一次特征带来的重复计算问题。\n\n在SPP-net中，首先将整幅图作为输入（这里使用的模型不再是AlexNet，这里试验了ZF-5、Convnet*-5、Overfeat-5、Overfeat-7这几个模型，我这里不再详细研究），提取出特征图之后，再从特征图上抠出来region proposal对应的区域（这里不同于RCNN对每个region proposal进行一次特征提取，所以解决了重复计算问题），然后将抠出来的区域进行空间金字塔池化操作，使其变成固定大小的特征向量。再使用SVM对特征进行分类。\n\nSPP操作的示意图如下所示，主要思想是将特征图先划分成固定数量的网格，然后再每个格子内进行池化操作，最后将固定大小的池化结果展开、拼接成固定大小的特征向量。\n\n![SPP操作示意](SPP.png)\n\n## Fast R-CNN\n有了SPP-net的思路之后，RCNN也做出了相应的改变，在论文《Fast R-CNN》中，接纳了SPP-net中先全图特征提取再裁剪region proposal区域的方法，然后对SPP-net进行改进：\n\n1. 只使用一个固定大小的网格分割，称为ROI Pooling层（并提出了ROI Pooling层的反向传播方法）。（至于为什么要降SPP改成ROI Pooling，可能主要是ROI Pooling有反向传播功能，其实如果SPP也实现了反向传播，那么ROI Pooling不一定比SPP效果好，不知道有没有人进行过对比）\n2. 特征提取模型换成了VGG。\n3. 对特征的处理采用神经网络，不再使用SVM，对于分类，使用softmax函数得到类别概率。\n4. 特征除了用于分类之外，还用于更精确的位置回归（使用smooth L1损失），并且让这两个任务共享一部分全连接层，提出了多任务损失函数的训练方法。\n\nFast R-CNN的简略流程示意图如下所示：\n![Fast R-CNN示意图](FastRCNN.png)\n\n## Faster R-CNN\nFast R-CNN还是没有做到完全的端到端得到输出结果，因为还是要依赖Selective Search算法来生成region proposal。\n\n在论文《Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks》中，提出了Faster R-CNN，其中使用一个区域建议网络（Region Proposal Network，RPN）来替代Selective Search算法进行region proposal工作。\n\nRPN的思路应该来源于Fast R-CNN中的位置回归操作，Fast R-CNN中证明神经网络可以用于位置回归问题，因此自然就会有一个想法是使用神经网络来对输出候选位置。但是如果直接回归位置，有两个问题：\n\n1. 特征图上某个像素点，应该回归哪个目标的位置（在之前的方法中，位置回归的目标很明确，因为region proposal有了之后，查找数据标签，可以很容易知道对于这个位置应该去回归哪个目标，或者这里面没有目标，那么就不用回归位置）\n2. 如果是输出像素坐标，那么对于不同大小的图片，不同尺度和比例的目标，回归任务很难收敛。\n\n对于上面两个问题，这里提出了Anchor的机制，首先在图片上按照一定的规律分布好各种尺度和比例的方框（Anchor box，一个像素点可以有多个Anchor box），然后每个像素点（Anchor）输出anchor box的类别（前景还是背景）、缩放和偏移量。\n\n每个anchor box要么匹配一个目标（其分类类别为这个目标的类别且回归目标为这个目标的位置），要么在训练过程中将其分类为背景类，并且不计算其回归输出的loss。\n\nRPN和也是用和RCNN一样的特征提取网络（这个特征提取网络一般称为Backbone）。\n\n### Faster R-CNN中的预测结果计算\nFaster R-CNN的RPN和RCNN对于回归量的输出都是$\\delta_x, \\delta_y, \\delta_h, \\delta_w$，分别表示中心点的偏移值，以及box长宽的缩放值，而且不是像素单位，因此需要转换成像素单位需要进行一些转换如下，这里的$b_x, b_y, b_h, b_w$分别表示最终预测box的中心和长宽，然后$p_x, p_y, p_h, p_w$分别表示先验box（在RPN阶段先验box就是Anchor，在RCNN阶段先验box就是RPN的Proposal Box）的中心和长宽。\n$$\n\\begin{aligned}\n  b_x &= p_x + p_w \\delta_x\\\\\n  b_y &= p_y + p_h \\delta_y\\\\\n  b_w &= p_w \\exp(\\delta_w)\\\\\n  b_h &= p_h \\exp(\\delta_h)\\\\\n\\end{aligned}\n$$\n\n**这个计算方式可能导致的问题：1、$b_x,b_y$可能超出先验框的范围，2、由于roi pooling之后，RCNN只能看到Proposal Box之内的内容，因此RCNN预测得到的$\\delta_w,\\delta_h$如果大于0，那么则表示预测的box范围超出了Proposal Box区域，可能存在预测不准确的问题，当然，由于特征图上的像素点具有一定的感受野范围，因此按理说RCNN还是可以看到一些Proposal Box之外的信息的。**\n\n### 为什么要使用Smooth L1作为回归任务的损失函数\n在回归损失函数中，最常用的是L2损失函数，$L = (h(x;\\theta) - y) ^ 2$，其导数$\\frac{\\partial L}{\\partial \\theta} = \\frac{\\partial h(x;\\theta)}{\\partial \\theta}(h(x;\\theta) -y)$，这里的问题在于：如果模型初始化得不好，再训练开始时$h(x;\\theta) - y$特别大，则会导致梯度特别大，模型将难以训练。那么一个自然地想法是使用L1损失函数，但是L1损失函数不是处处可导，梯度会发生突变，也不利于训练，因此将L1损失函数进行一些平滑操作，给出了smooth L1损失函数如下所示：\n$$\n\\begin{aligned}\n    L(\\theta) = \\begin{cases}\n        0.5 x^2 &|x| < 1\\\\\n        |x| - 0.5 &|x| >= 1\n    \\end{cases}\n\\end{aligned}\n$$\n\n### RPN训练时的loss计算需要对负样本进行采样\nRPN的训练目标是将Anchor Box进行前景和背景分类，并且对分类为前景的Anchor Box进行位置偏移量和缩放量的回归。\n\n在训练时，需要首先分配Anchor Box的训练目标：给定一张图像样本上的所有gt box标签，首先需要计算所有gt box和每个Anchor Box的iou。\n\n对于每一个Anchor Box，找到和其iou最大的gt box，按照和这个gt box之间的iou，有三种可能：\n1. 如果大于某个iou阈值，那么则被分配给这个gt box作为前景来训练（即这个Anchor Box的分类目标是正样本，回归目标是对应的gt box）。\n2. 如果小于某个iou阈值，则作为背景类处理（即这个Anchor Box的分类目标是负样本，没有回归目标，不计算回归损失）。\n3. 如果iou不大又不小，则忽略这个Anchor Box，计算损失的过程中不考虑，这是因为一些Anchor Box包含了一部分目标，但是又包含得不够多，不能武断的将其作为前景或者背景来训练，因此干脆不管这一部分的训练，反正参数是共享的，其他的地方训练好之后，测试过程中这种地方输出是什么样影响不大。\n\n在分配Anchor Box训练目标的过程中，绝大部分Anchor Box是被分配成负样本（背景类）的，，因此RPN训练时存在前景和背景两个类别不平衡的情况。所以一般的Faster R-CNN实现中，RPN的训练部分，会对Anchor Box进行采样。(例如mmdetection中的实现中，RPN部分的默认配置是采样的256个Anchor Box进行训练，首先随机采样128个正样本，然后采样128个负样本，如果正样本不够，那么负样本多采样一些填满256个)\n\n# 多阶段检测器\n## Cascade R-CNN\n在论文《Cascade R-CNN: Delving into High Quality Object Detection》中指出了Faster R-CNN中按照某个iou阈值分配Proposal正负样本存在的问题：iou阈值过低会导致大量的低阈值预测框，对于regression head来说非常难修正，因此会出现大量的close false positive预测（即iou不够从而被判定为false positive的预测），iou阈值过大又导致匹配到的正样本非常少，容易引起过拟合或者类别不平衡问题。\n\nCascade R-CNN中为了解决这个问题，提出了使用多级head，将预测iou逐渐提高的方法。\n\nCascade R-CNN提出的多级head预测如下图(d)所示，这里的H1、H2、H3都是独立参数，且在训练过程中使用的iou不相同。\n\n![cascade rcnn多级head示意图](Cascade_R-CNN.png)\n\nCascade R-CNN的思路很简单：在理想状况中，H1接收RPN输出的ROI Proposal作为B0，对B0进行分类和修正，并从中筛除一些背景或者iou较低的box得到新的预测框集合B1，H2则接收B1作为输入，对B1进行分类和修正，得到B2，H3对B2进行同样的操作，得到最终的预测输出。\n\n三个步骤不同之处首先在于背景类的含义，例如存在三个iou阈值$u_1 < u_2 < u_3$，在H1中低于$u_1$的才被认为是负样本，而H2中低于$u_2$的被认为是负样本，H3则使用$u_2$作为负样本阈值，这样设置的意思就是每个Head负责筛掉不同精度的预测框，另外，就算这里$u_3$较高，也不会引起之前iou阈值高导致的匹配到的正样本非常少的问题，因为原始RPN的proposal虽然可能iou较低，但是经过H1、H2的逐步修正之后，B2之中iou普遍比较高了，即使$u_3$较高，也能匹配到较多的正样本用于H3的训练。\n\n在预测过程中，Cascade R-CNN的不同Head先后进行预测，最终的分类得分将综合三个Head的预测结果（例如C1、C2、C3取平均），而回归结果直接使用H3得到的B3。\n\n# 一阶段目标检测方法\n一阶段目标检测方法主要是相对于Faster R-CNN这类二阶段检测方法来说的，在Faster R-CNN中，主要被分为region proposal和proposal的分类回归。\n\n一阶段目标检测方法将region proposal这一步去掉了，不需要RPN来进行初步的定位，而是直接得到box输出结果。\n\n一阶段检测器按照时间顺序主要有：YOLO v1、SSD、DSSD、YOLO v2、Retinanet、YOLO v3、YOLO v4等，这里做个简单的梳理。\n\n## YOLO v1\n论文《You Only Look Once:Unified, Real-Time Object Detection》中提出了YOLO检测方法，其基本思想是将一幅图像分成$S \\times S$的网格，如果某个目标的中心落在这个网格中，那么这个网格就负责预测这个目标。\n\n网格中的一个格子可以输出$B$个bounding boxes，每个格子的输出如下：\n\n- $B$个bounding boxes的位置回归信息（中心偏移：x,y（使用格子大小来归一化）；box的宽高：w,h（使用图片的长宽来归一化））共$4\\times B$个值。\n- bounding boxes对应的$B$个confidence scores（这个confidence score表示当前bounding box包含目标的概率乘以bounding box位置回归的置信度），其的回归目标定义为$Pr(Object) \\times IOU^{truth}_{pred}$。\n- C个类别信息，表示为$Pr(Class_i|Object),i = 1,2,...,C$，这是个条件概率，其含义是如果这个格子有目标，那么这个目标属于不同的类别的概率。\n\n因此一个格子的输出个数是$5 \\times B + C$，整个YOLO v1模型的输出个数是$S\\times S \\times (5 \\times B + C)$。\n\n在YOLO v1模型中，输入图片大小固定为$448 \\times 448$，经过6次下采样之后变成$7\\times 7$大小的特征图，作者取$S=7$，$B=2$，Pascal VOC的类别个数$C=20$，因此YOLO v1的输出是个$7\\times 7 \\times 30$的特征图，整个模型结构图如下所示，其中激活函数使用Leaky ReLU，参数为0.1。\n\n![YOLO v1模型结构示意](YOLOv1.png)\n\n### 损失函数\nYOLO v1使用平方和误差来进行训练，但是论文中表示这样的损失函数不能完美的匹配优化Average Precision的目标（就是说定位误差和分类误差的权重相等不太合适），而且每张图片上不包含目标的格子居多，这些格子主导了梯度，导致预测置信度（confidence score）普遍偏低，同时模型训练过程容易震荡，因此作者在损失函数中引入了两个超参数：$\\lambda_{coord} = 5$、$\\lambda_{noobj} = 0.5$作为不同损失的权重。\n\n另外，论文中还提到平方和损失在回归位置的时候，还有个问题是large box和small box对于同样的宽高偏差敏感度不同，为了缓解这个问题，论文中提出直接让模型预测$w,h$的平方，需要获取真正的长宽时，需要将模型的预测输出开方进行还原。\n\n最终YOLO v1的损失函数如下：\n$$\n\\begin{aligned}\n    &\\lambda_{coord} \\sum\\limits_{i=0}^{S^2} \\sum\\limits_{j=0}^{B} \\mathbf{1}^{obj}_{ij}[(x_i - \\hat{x}_i)^2 + (y_i - \\hat{y}_i)^2]\\\\\n    &+ \\lambda_{coord} \\sum\\limits_{i=0}^{S^2} \\sum\\limits_{j=0}^{B} \\mathbf{1}^{obj}_{ij}[(w_i - \\sqrt{\\hat{w}_i})^2 + (h_i - \\sqrt{\\hat{h}_i})^2]\\\\\n    &+ \\sum\\limits_{i=0}^{S^2} \\sum\\limits_{j=0}^{B} \\mathbf{1}^{obj}_{ij}(C_i - \\hat{C}_i)^2\\\\\n    &+ \\lambda_{noobj} \\sum\\limits_{i=0}^{S^2} \\sum\\limits_{j=0}^{B} \\mathbf{1}^{noobj}_{ij}(C_i - \\hat{C}_i)^2\\\\\n    &+ \\sum\\limits_{i=0}^{S^2} \\mathbf{1}^{obj}_{i} \\sum\\limits_{c \\in Classes} (p_i(c) - \\hat{p}_i(c))^2\n\\end{aligned}\n$$\n\n其中$\\mathbf{1}^{obj}_{ij}$是个指示函数，表示第$i$个格子的第$j$个box是否对应一个目标。$\\mathbf{1}^{obj}_{i}$则表示第$i$个格子是否包含目标，$\\mathbf{1}^{noobj}_{ij} = 1 - \\mathbf{1}^{obj}_{ij}$表示第$i$个格子的第$j$个box是否不对应一个目标。\n\n### bounding box\nYOLO v1中虽然一个格子对应两个bounding box，但是这里的bounding box概念和anchor box不同。\n\nanchor box需要设置先验大小和长宽比，然后预测输出是相对于anchor box的偏移和缩放，计算loss时需要先用anchor box去和gt box匹配，确定好优化目标才能计算loss。\n\n而bounding box不需要先验大小和长宽比，bounding box输出的是相对于格子中心的像素偏移（使用格子大小来归一化）以及像素长宽比（使用图片大小来归一化），计算loss时要看这个格子输出的两个bounding box哪个和gt box的iou大，iou大的那个bounding box才进行回归优化。但是每个格子中的两个bounding box都会去训练confidence score，这个的回归目标也是和bounding box的输出有关（$Pr(Object) \\times IOU^{truth}_{pred}$，其中$IOU^{truth}_{pred}$是bounding box预测值和gt box的iou）。\n\n这样看来YOLO v1算是一种anchor free的检测方法。\n\n## YOLO v2\n在论文《YOLO9000:Better, Faster, Stronger》中对YOLO v1进行了一些改进最终提出了YOLO v2检测方法。\n### 相对于YOLO v1的改进点\n\n- batch norm，每个卷积层后面都添加Batch Normalization层，并且不再使用droput。\n- hi-res classifier，YOLO v1使用ImageNet上预训练的模型来进行初始化，这个预训练的输入图像是224大小的，但是为了提高检测结果的准确性，检测训练使用的输入大小是448，这样的切换模型难以适应，因此YOLO v2中增加了一个在ImageNet数据集上使用448大小finetune 10个epoch的过程。\n- Convolutional，在YOLO v1中，最终是使用全连接层来得到最终的预测，直接预测目标像素坐标和大小，这样做导致位置定位比较困难，难以训练，论文中参考了Faster RCNN的做法，认为使用卷积的方式直接得到每个像素点相对于Anchor box的偏移缩放信息的预测方法比较简便且容易训练，因此这里将全连接层改为卷积层的同时引入了Anchor box的做法（引入了Anchor box之后，预测的类别和是否包含目标这些问题都是针对每个anchor box的，之前是一个格子一个类别，且只判断格子是否包含目标。）\n- Anchor boxes，像Faster RCNN那样使用手工设计的Anchor box大小和长宽比，使用手工设计的Anchor box之后在Recall上面有提高，但是在mAP上面反而下降了一些（增加了一些假阳性）。\n- new network，YOLO v2使用一种更加接近VGG的backbone：Darknet-19，相比于YOLO v1的特征提取网络，这里除了替换$7\\times 7$的卷积、通道个数、卷积核层个数等变化以外，下采样次数从6次变成了5次，如果输入448大小的图片，那么输出大小将变成$14\\times 14$，但论文中认为最终输出的格子数为奇数比较好（便于那种占满整图的大目标的中心落在格子中心而不是格子边缘），因此将输入大小调整为$416$，得到$13\\times 13$大小的输出。\n- dimension clusters，论文中认为通过聚类学习Anchor box的大小设置比手工设计Anchor box的大小更便于模型回归目标位置，因此使用k-means聚类方法，从标注中的gt box形状中找到适合的Anchor box大小，在聚类时，两个box之间的距离使用$d = 1 - IOU$来表示（这里的IOU计算假定两个box的中心重合，这里不用考虑box的位置），作者给出的数据中，如果是手工设计的9个Anchor box大小，和gt box平均IOU在0.609，但是通过聚类学习到Anchor box，只需要5个大小就能够达到平均IOU0.61，明显好于手工设计的Anchor box先验大小。\n- location prediction，在基于Anchor的预测的方法中，模型给出预测的偏移量（$t_x, t_y$）之后，预测的中心位置计算方式为$x = (t_x \\times w_a) + x_a, y = (t_y \\times h_a) + y_a$，其中$w_a, h_a, x_a, y_a$分别表示Anchor Box的宽、高、中心x坐标、中心y坐标，但是这样的预测方式论文中认为没有对预测box的位置进行约束（对于一个左上角的Anchor box，其预测结果可能跑到图片右下角），论文中认为这给模型带来了不利于优化的不稳定性。因此论文中提出将预测方式改为$b_x=\\sigma(t_x) + C_x, b_y=\\sigma(t_y) + C_y, b_w = p_we^{t_w}, b_h = p_he^{t_h}, Pr(object)\\times IOU_{b}^{object} = \\sigma(t_o)$（这里的$C_y,C_x$代表格子的左上角坐标，因为$\\sigma(t_x),\\sigma(t_y)$都是在0~1范围，因此可以保证这样预测出来的中心不会超出这个格子，$p_w, p_h$则代表Anchor Box的宽高，可以见后面的YOLOv2预测方式示意图），这里的意思是，对于每个Anchor Box，都模型都输出四个值$\\sigma(t_x), \\sigma(t_y), t_w, t_h, \\sigma(t_o)$，对于模型的输出值，计算出最终预测框的中心：$b_x,b_y$以及预测框的宽高：$b_w, b_h$，以及预测框的置信度：$Pr(object)\\times IOU_{b}^{object}$。\n- passthrough，将$26\\times 26\\times 512$的特征图resize成$13\\times 13\\times 2048$然后和最后的$13\\times13\\times 1024$的特征图进行Concatenate操作，共同用于模型的最终预测，论文中解释说是为了提供更加细粒度的特征。\n- multi-scale，因为YOLO v2抛弃了之前的全连接操作，因此可以使用不同尺度的输入进行训练，在训练过程中，每10个batch随机按照$320:32:608$来选择一次尺度。\n- hi-res detector，这个论文中没有详细说明，不过从原文来看，这个意思就是在预测的时候使用$544\\times 544$大小的图片。\n\n以上各种改进的消融实验结果见下表：\n\n![YOLOv2各改进的消融实验结果](YOLOv2_exp.png)\n\nlocation prediction这一项改进中所描述的预测方法示意图：\n\n![YOLOv2预测方式示意图](YOLOv2_predict.png)\n\n这里有个细节是在$b_x=\\sigma(t_x) + C_x, b_y=\\sigma(t_y) + C_y$这两个运算里面所有的项都不是像素单位，如果想要转换成像素单位，那么需要除以S，然后乘上图片的或着高\n\n另外为了节省计算量，passthrough还有个改进版本：增加一次卷积操作，先将$26\\times 26\\times 512$的特征图变为$26\\times 26\\times 64$的特征图，然后再resize到$13\\times 13\\times 256$，最后拿去和$13\\times13\\times 1024$的特征图进行Concatenate操作。\n\n### YOLO v2的损失函数\n这部分其实类似YOLO v1，全都是L2损失，只不过其中YOLO v1使用的那个开方的操作这里不用了，而且计算的类别是相对于每个Anchor来说的，其输出大小为$S\\times S \\times B \\times (5 + C)$（这里的$B$是anchor个数），其他损失的计算和YOLO v1类似，论文中没有详细描述，如果感兴趣可以去研究YOLO v2的实现代码。\n\n## YOLO v3\n在论文《YOLOv3: An Incremental Improvement》中提出了YOLO v3检测方法。\n\n在YOLO v3中，相对于YOLO v2，首先更换了Backbone，从YOLO v2的Darknet-19改成了Darknet-53（根据论文中提供的Backbone分类任务训练结果中，Darknet-53的分类性能可以和ResNet-152比肩，但是速度是ResNet-152的两倍），这个新backbone中引入了resnet中的残差模块，顺便将Darknet-19中的池化层全替换成了步长为2的卷积层。\n\nYOLO v3的另外一个主要改进是借鉴了FPN的思路，不仅在$13\\times 13$大小的特征图上进行预测，同时也在$26\\times 26$，$52\\times 52$大小的特征图上进行预测，三种大小的特征图上分类的anchor box都是每个格子三个，但是大小不同，在$52\\times 52$大小的特征图的Anchor Box最小，在$13\\times 13$大小的特征图的Anchor Box最大，下图中最后的输出部分，每个像素点有255个预测值，其实就是$3 \\times (5 + 80)$，其中3是Anchor Box个数，80是COCO数据集的类别个数。\n\n在YOLO v3中，还有一个地方是其目标分配机制和YOLO v2稍有不同，对于一个格子，如果目标落在了这个格子中（这里因为三个不同大小的特征图都有输出，因此一个目标肯定会落在三个格子中，分别位于三个特征图上，这就关系到了$3\\times 3 = 9$个Anchor Box，YOLO v2因为是单尺度特征图预测，因此只需要考虑其设定的5个Anchor Box），YOLO v3在处理这个问题的时候，计算了这9个Anchor Box和当前目标的iou，iou最大的那个Anchor Box用于预测当前目标，其余Anchor Box如果iou大于某个阈值（），则计算损失时忽略其置信度损失，否则当成负样本处理。\n\nYOLO v3论文中并没有给出详细的结构描述图，下面的结构描述图来自于[CSDN博客《yolo系列之yolo v3【深度解析】》](https://blog.csdn.net/leviopku/article/details/82660381)\n\n![YOLOv3结构示意](YOLOv3结构示意.png)\n\nYOLO v3在COCO上进行的对比实验结果如下所示，这里可以看出YOLO v3在精度上并没有特别明显的竞争力，但是如果精度要求不是那么高的话，YOLO v3在速度上超越其他的单阶段检测器很多。\n\n![YOLOv3 对比实验结果](YOLOv3_exp.png)\n\n\n## YOLO v4\nYOLO v4在论文《YOLOv4: Optimal Speed and Accuracy of Object Detection》中被提出，其主要结构组件如下：\n- backbone：CSPDarknet53\n- neck：SPP、PANet\n- head：YOLO v3\n\n另外YOLO v4中使用了很多技巧：\n\n- Bag of Freebies (BoF) for backbone：\n  - CutMix and Mosaic data augmentation\n  - DropBlock regularization\n  - Class label smoothing\n- Bag of Specials (BoS) for backbone：\n  - Mish activa-tion\n  - Cross-stage partial connections (CSP)\n  - Multi-input weighted residual connections (MiWRC)\n- Bag of Freebies (BoF) for detector：\n  - CIoU-loss\n  - CmBN\n  - DropBlock\n  - regularization\n  - Mosaic data augmentation\n  - Self-Adversarial Training\n  - Eliminate grid sensitivity\n  - Using multiple anchors for a single groundtruth\n  - Cosine annealing scheduler\n  - Optimal hyper-parameters\n  - Random training shapes\n- Bag of Specials (BoS) for detector：\n  - Mish activation\n  - SPP-block\n  - SAM-block\n  - PAN path-aggregation block\n  - DIoU-NMS\n\n其中Bag of Freebies (BoF)表示只在训练过程中增加时间成本，不影响预测过程的技巧，Bag of Specials (BoS)表示在预测过程中会稍微增加一点时间成本的技巧。\n\nYOLO v4基本上算是个大型炼丹现场，作者在论文中都是用实验数据证明各种操作的效果，很少有理论分析，因此这篇论文更适合作为检测任务寻找提分手段的一个手册（一切以实验数据说话）。\n\n# 多尺度问题\n## 特征金字塔网络(Feature Pyramid Network，FPN)\n在论文《Feature Pyramid Networks for Object Detection》中提出的特征金字塔操作已经是目前目标检测模型上的标准配置，无论是单阶段检测模型还是两阶段检测模型，均可以添加FPN结构来让模型获得更好的尺度鲁棒性。\n\nFPN的结构示意图如下，其中左上角从下到上是检测模型中backbone的特征提取过程，每一层代表不同stride的特征图，原本的检测模型，例如Fatser R-CNN,是在最上面一个特征来进行roi proposal、roi classification和roi regression等操作，在加入FPN之后，获得了右上角的一系列特整图，不同stride的特征图预测不同大小的目标。\n\n![FPN结构示意图](FPN.png)\n\n上图中画得不是很完整，一般backbone中会进行5次下采样，得到的特征图stride分别是2、4、8、16、32，对应的特征图称为C1、C2、C3、C4、C5，在mmdetection检测框架的Faster R-CNN-FPN的实现中，C5将通过大小$3\\times 3$，stride为2的卷积下采样得到P6，另外C5经过$1\\times 1$卷积之后变为P5，P5上采样后和C4的$1\\times 1$卷积结果相加变成P4，P4再上采样和C3的$1\\times 1$卷积结果相加变成P3，以此类推，分别得到P2、P3、P4、P5、P6，其中P2、P3、P4、P5、P6均参与RPN的roi proposal，而对于不同大小的roi，则在P2、P3、P4、P5中选择对应尺度的特征图来进行roi pooling/roi align。\n\n原始FPN论文中，对于宽高为$w,h$的roi，其对应的特征金字塔等级计算方法如下，这里设置$k_0=4$表示第4级特征金字塔，即$P5$，因为ImageNet预训练的backbone，一般使用224大小的输入图像，而且最终结果是在C5得到的，因此这里让224以上的目标对应于$P5$特征图，小于224大小的目标则每小一倍，对应的特征等级减一\n$$\nlevel = \\lfloor k_0 + \\log_2(\\frac{\\sqrt{wh}}{224})\\rfloor\n$$\n\n在mmdetection框架的实现中，roi的分配则和原论文不同，mmdetection中，计算方式如下：\n- scale < finest_scale * 2: level 0\n- finest_scale * 2 <= scale < finest_scale * 4: level 1\n- finest_scale * 4 <= scale < finest_scale * 8: level 2\n- scale >= finest_scale * 8: level 3\n\n这里的scale是指$\\sqrt{wh}$即roi的尺度，finest_scale默认值为56，这里的level 0对应P2，level 1对应P3，以此类推。\n\nFPN在保证大目标的检测效果的同时，可以大幅提高小目标的检测效果，我认为其原因在于小目标物体的检测需要更加精细的位置信息，如果使用stride过大的特征图，会导致小目标的预测不够精确，同时小目标的信息容易被大目标掩盖，FPN中stride较小的特征图上融合了深层的特征图的信息，这可以让其的感受野非常大，而且感受野分布比较密集，小目标的检测可能也需要大感受野信息同时加上细粒度空间信息。\n","source":"_posts/学习笔记/目标检测.md","raw":"---\ntitle: 目标检测\ndate: 2020-05-19 15:21:29\ntags: [深度学习]\nmathjax: true\n---\n\n目标检测任务简单来说就是给定一张图片，模型需要根据图片找到感兴趣的目标，将其位置、大小标注出来。\n\n# 两阶段检测算法，从RCNN到Faster R-CNN\n目前的目标检测方法主要分为：两阶段的检测器、单阶段检测器两种，RCNN到Faster RCNN系列检测器是典型的两阶段检测器的代表。\n\n## R-CNN\n论文《Rich feature hierarchies for accurate oject detection and semantic segmentation》中提出了R-CNN模型用于目标检测，其基本流程如下：\n- 使用Selective Search算法提取出2k个region proposal。\n- 对于每个region proposal，先裁剪出对应区域的图片，然后使用AlexNet进行特征提取（特征取最后那个softmax层的输入，是一个4096维的向量）。\n- 训练K个线性SVM分类器，对每个region proposal进行K次分类，得到其属于K个类别的得分。\n- 对每个类别的proposal进行NMS后处理，得到最终的结果。\n\n这里的Selective Search算法本质上是一种先分割图片然后再尝试各种组合的穷举算法，这里没有详细研究。\n\n整体来说，R-CNN的流程非常粗暴，主要有两个阶段：1、获取region proposal，2、对region proposal进行分类，但这也是后面的两阶段检测方法的一般流程。\n\n## SPP-net\n在R-CNN中存在的一个问题是region proposal的大小不相等，但是AlexNet要求固定大小的输入图片，否则特征维度会发生变换（如果现在来看这个问题的话，加个global pooling就OK了，但是当时没想到），因此在RCNN里面只能通过缩放，让region proposal满足输入大小的要求，这样带来了畸变的问题，为了解决这个问题，论文《Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition》提出了空间金字塔池化操作(Spatial Pyramid Pooling，SPP)，而且顺便结局了RCNN中对每个region proposal都提取一次特征带来的重复计算问题。\n\n在SPP-net中，首先将整幅图作为输入（这里使用的模型不再是AlexNet，这里试验了ZF-5、Convnet*-5、Overfeat-5、Overfeat-7这几个模型，我这里不再详细研究），提取出特征图之后，再从特征图上抠出来region proposal对应的区域（这里不同于RCNN对每个region proposal进行一次特征提取，所以解决了重复计算问题），然后将抠出来的区域进行空间金字塔池化操作，使其变成固定大小的特征向量。再使用SVM对特征进行分类。\n\nSPP操作的示意图如下所示，主要思想是将特征图先划分成固定数量的网格，然后再每个格子内进行池化操作，最后将固定大小的池化结果展开、拼接成固定大小的特征向量。\n\n![SPP操作示意](SPP.png)\n\n## Fast R-CNN\n有了SPP-net的思路之后，RCNN也做出了相应的改变，在论文《Fast R-CNN》中，接纳了SPP-net中先全图特征提取再裁剪region proposal区域的方法，然后对SPP-net进行改进：\n\n1. 只使用一个固定大小的网格分割，称为ROI Pooling层（并提出了ROI Pooling层的反向传播方法）。（至于为什么要降SPP改成ROI Pooling，可能主要是ROI Pooling有反向传播功能，其实如果SPP也实现了反向传播，那么ROI Pooling不一定比SPP效果好，不知道有没有人进行过对比）\n2. 特征提取模型换成了VGG。\n3. 对特征的处理采用神经网络，不再使用SVM，对于分类，使用softmax函数得到类别概率。\n4. 特征除了用于分类之外，还用于更精确的位置回归（使用smooth L1损失），并且让这两个任务共享一部分全连接层，提出了多任务损失函数的训练方法。\n\nFast R-CNN的简略流程示意图如下所示：\n![Fast R-CNN示意图](FastRCNN.png)\n\n## Faster R-CNN\nFast R-CNN还是没有做到完全的端到端得到输出结果，因为还是要依赖Selective Search算法来生成region proposal。\n\n在论文《Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks》中，提出了Faster R-CNN，其中使用一个区域建议网络（Region Proposal Network，RPN）来替代Selective Search算法进行region proposal工作。\n\nRPN的思路应该来源于Fast R-CNN中的位置回归操作，Fast R-CNN中证明神经网络可以用于位置回归问题，因此自然就会有一个想法是使用神经网络来对输出候选位置。但是如果直接回归位置，有两个问题：\n\n1. 特征图上某个像素点，应该回归哪个目标的位置（在之前的方法中，位置回归的目标很明确，因为region proposal有了之后，查找数据标签，可以很容易知道对于这个位置应该去回归哪个目标，或者这里面没有目标，那么就不用回归位置）\n2. 如果是输出像素坐标，那么对于不同大小的图片，不同尺度和比例的目标，回归任务很难收敛。\n\n对于上面两个问题，这里提出了Anchor的机制，首先在图片上按照一定的规律分布好各种尺度和比例的方框（Anchor box，一个像素点可以有多个Anchor box），然后每个像素点（Anchor）输出anchor box的类别（前景还是背景）、缩放和偏移量。\n\n每个anchor box要么匹配一个目标（其分类类别为这个目标的类别且回归目标为这个目标的位置），要么在训练过程中将其分类为背景类，并且不计算其回归输出的loss。\n\nRPN和也是用和RCNN一样的特征提取网络（这个特征提取网络一般称为Backbone）。\n\n### Faster R-CNN中的预测结果计算\nFaster R-CNN的RPN和RCNN对于回归量的输出都是$\\delta_x, \\delta_y, \\delta_h, \\delta_w$，分别表示中心点的偏移值，以及box长宽的缩放值，而且不是像素单位，因此需要转换成像素单位需要进行一些转换如下，这里的$b_x, b_y, b_h, b_w$分别表示最终预测box的中心和长宽，然后$p_x, p_y, p_h, p_w$分别表示先验box（在RPN阶段先验box就是Anchor，在RCNN阶段先验box就是RPN的Proposal Box）的中心和长宽。\n$$\n\\begin{aligned}\n  b_x &= p_x + p_w \\delta_x\\\\\n  b_y &= p_y + p_h \\delta_y\\\\\n  b_w &= p_w \\exp(\\delta_w)\\\\\n  b_h &= p_h \\exp(\\delta_h)\\\\\n\\end{aligned}\n$$\n\n**这个计算方式可能导致的问题：1、$b_x,b_y$可能超出先验框的范围，2、由于roi pooling之后，RCNN只能看到Proposal Box之内的内容，因此RCNN预测得到的$\\delta_w,\\delta_h$如果大于0，那么则表示预测的box范围超出了Proposal Box区域，可能存在预测不准确的问题，当然，由于特征图上的像素点具有一定的感受野范围，因此按理说RCNN还是可以看到一些Proposal Box之外的信息的。**\n\n### 为什么要使用Smooth L1作为回归任务的损失函数\n在回归损失函数中，最常用的是L2损失函数，$L = (h(x;\\theta) - y) ^ 2$，其导数$\\frac{\\partial L}{\\partial \\theta} = \\frac{\\partial h(x;\\theta)}{\\partial \\theta}(h(x;\\theta) -y)$，这里的问题在于：如果模型初始化得不好，再训练开始时$h(x;\\theta) - y$特别大，则会导致梯度特别大，模型将难以训练。那么一个自然地想法是使用L1损失函数，但是L1损失函数不是处处可导，梯度会发生突变，也不利于训练，因此将L1损失函数进行一些平滑操作，给出了smooth L1损失函数如下所示：\n$$\n\\begin{aligned}\n    L(\\theta) = \\begin{cases}\n        0.5 x^2 &|x| < 1\\\\\n        |x| - 0.5 &|x| >= 1\n    \\end{cases}\n\\end{aligned}\n$$\n\n### RPN训练时的loss计算需要对负样本进行采样\nRPN的训练目标是将Anchor Box进行前景和背景分类，并且对分类为前景的Anchor Box进行位置偏移量和缩放量的回归。\n\n在训练时，需要首先分配Anchor Box的训练目标：给定一张图像样本上的所有gt box标签，首先需要计算所有gt box和每个Anchor Box的iou。\n\n对于每一个Anchor Box，找到和其iou最大的gt box，按照和这个gt box之间的iou，有三种可能：\n1. 如果大于某个iou阈值，那么则被分配给这个gt box作为前景来训练（即这个Anchor Box的分类目标是正样本，回归目标是对应的gt box）。\n2. 如果小于某个iou阈值，则作为背景类处理（即这个Anchor Box的分类目标是负样本，没有回归目标，不计算回归损失）。\n3. 如果iou不大又不小，则忽略这个Anchor Box，计算损失的过程中不考虑，这是因为一些Anchor Box包含了一部分目标，但是又包含得不够多，不能武断的将其作为前景或者背景来训练，因此干脆不管这一部分的训练，反正参数是共享的，其他的地方训练好之后，测试过程中这种地方输出是什么样影响不大。\n\n在分配Anchor Box训练目标的过程中，绝大部分Anchor Box是被分配成负样本（背景类）的，，因此RPN训练时存在前景和背景两个类别不平衡的情况。所以一般的Faster R-CNN实现中，RPN的训练部分，会对Anchor Box进行采样。(例如mmdetection中的实现中，RPN部分的默认配置是采样的256个Anchor Box进行训练，首先随机采样128个正样本，然后采样128个负样本，如果正样本不够，那么负样本多采样一些填满256个)\n\n# 多阶段检测器\n## Cascade R-CNN\n在论文《Cascade R-CNN: Delving into High Quality Object Detection》中指出了Faster R-CNN中按照某个iou阈值分配Proposal正负样本存在的问题：iou阈值过低会导致大量的低阈值预测框，对于regression head来说非常难修正，因此会出现大量的close false positive预测（即iou不够从而被判定为false positive的预测），iou阈值过大又导致匹配到的正样本非常少，容易引起过拟合或者类别不平衡问题。\n\nCascade R-CNN中为了解决这个问题，提出了使用多级head，将预测iou逐渐提高的方法。\n\nCascade R-CNN提出的多级head预测如下图(d)所示，这里的H1、H2、H3都是独立参数，且在训练过程中使用的iou不相同。\n\n![cascade rcnn多级head示意图](Cascade_R-CNN.png)\n\nCascade R-CNN的思路很简单：在理想状况中，H1接收RPN输出的ROI Proposal作为B0，对B0进行分类和修正，并从中筛除一些背景或者iou较低的box得到新的预测框集合B1，H2则接收B1作为输入，对B1进行分类和修正，得到B2，H3对B2进行同样的操作，得到最终的预测输出。\n\n三个步骤不同之处首先在于背景类的含义，例如存在三个iou阈值$u_1 < u_2 < u_3$，在H1中低于$u_1$的才被认为是负样本，而H2中低于$u_2$的被认为是负样本，H3则使用$u_2$作为负样本阈值，这样设置的意思就是每个Head负责筛掉不同精度的预测框，另外，就算这里$u_3$较高，也不会引起之前iou阈值高导致的匹配到的正样本非常少的问题，因为原始RPN的proposal虽然可能iou较低，但是经过H1、H2的逐步修正之后，B2之中iou普遍比较高了，即使$u_3$较高，也能匹配到较多的正样本用于H3的训练。\n\n在预测过程中，Cascade R-CNN的不同Head先后进行预测，最终的分类得分将综合三个Head的预测结果（例如C1、C2、C3取平均），而回归结果直接使用H3得到的B3。\n\n# 一阶段目标检测方法\n一阶段目标检测方法主要是相对于Faster R-CNN这类二阶段检测方法来说的，在Faster R-CNN中，主要被分为region proposal和proposal的分类回归。\n\n一阶段目标检测方法将region proposal这一步去掉了，不需要RPN来进行初步的定位，而是直接得到box输出结果。\n\n一阶段检测器按照时间顺序主要有：YOLO v1、SSD、DSSD、YOLO v2、Retinanet、YOLO v3、YOLO v4等，这里做个简单的梳理。\n\n## YOLO v1\n论文《You Only Look Once:Unified, Real-Time Object Detection》中提出了YOLO检测方法，其基本思想是将一幅图像分成$S \\times S$的网格，如果某个目标的中心落在这个网格中，那么这个网格就负责预测这个目标。\n\n网格中的一个格子可以输出$B$个bounding boxes，每个格子的输出如下：\n\n- $B$个bounding boxes的位置回归信息（中心偏移：x,y（使用格子大小来归一化）；box的宽高：w,h（使用图片的长宽来归一化））共$4\\times B$个值。\n- bounding boxes对应的$B$个confidence scores（这个confidence score表示当前bounding box包含目标的概率乘以bounding box位置回归的置信度），其的回归目标定义为$Pr(Object) \\times IOU^{truth}_{pred}$。\n- C个类别信息，表示为$Pr(Class_i|Object),i = 1,2,...,C$，这是个条件概率，其含义是如果这个格子有目标，那么这个目标属于不同的类别的概率。\n\n因此一个格子的输出个数是$5 \\times B + C$，整个YOLO v1模型的输出个数是$S\\times S \\times (5 \\times B + C)$。\n\n在YOLO v1模型中，输入图片大小固定为$448 \\times 448$，经过6次下采样之后变成$7\\times 7$大小的特征图，作者取$S=7$，$B=2$，Pascal VOC的类别个数$C=20$，因此YOLO v1的输出是个$7\\times 7 \\times 30$的特征图，整个模型结构图如下所示，其中激活函数使用Leaky ReLU，参数为0.1。\n\n![YOLO v1模型结构示意](YOLOv1.png)\n\n### 损失函数\nYOLO v1使用平方和误差来进行训练，但是论文中表示这样的损失函数不能完美的匹配优化Average Precision的目标（就是说定位误差和分类误差的权重相等不太合适），而且每张图片上不包含目标的格子居多，这些格子主导了梯度，导致预测置信度（confidence score）普遍偏低，同时模型训练过程容易震荡，因此作者在损失函数中引入了两个超参数：$\\lambda_{coord} = 5$、$\\lambda_{noobj} = 0.5$作为不同损失的权重。\n\n另外，论文中还提到平方和损失在回归位置的时候，还有个问题是large box和small box对于同样的宽高偏差敏感度不同，为了缓解这个问题，论文中提出直接让模型预测$w,h$的平方，需要获取真正的长宽时，需要将模型的预测输出开方进行还原。\n\n最终YOLO v1的损失函数如下：\n$$\n\\begin{aligned}\n    &\\lambda_{coord} \\sum\\limits_{i=0}^{S^2} \\sum\\limits_{j=0}^{B} \\mathbf{1}^{obj}_{ij}[(x_i - \\hat{x}_i)^2 + (y_i - \\hat{y}_i)^2]\\\\\n    &+ \\lambda_{coord} \\sum\\limits_{i=0}^{S^2} \\sum\\limits_{j=0}^{B} \\mathbf{1}^{obj}_{ij}[(w_i - \\sqrt{\\hat{w}_i})^2 + (h_i - \\sqrt{\\hat{h}_i})^2]\\\\\n    &+ \\sum\\limits_{i=0}^{S^2} \\sum\\limits_{j=0}^{B} \\mathbf{1}^{obj}_{ij}(C_i - \\hat{C}_i)^2\\\\\n    &+ \\lambda_{noobj} \\sum\\limits_{i=0}^{S^2} \\sum\\limits_{j=0}^{B} \\mathbf{1}^{noobj}_{ij}(C_i - \\hat{C}_i)^2\\\\\n    &+ \\sum\\limits_{i=0}^{S^2} \\mathbf{1}^{obj}_{i} \\sum\\limits_{c \\in Classes} (p_i(c) - \\hat{p}_i(c))^2\n\\end{aligned}\n$$\n\n其中$\\mathbf{1}^{obj}_{ij}$是个指示函数，表示第$i$个格子的第$j$个box是否对应一个目标。$\\mathbf{1}^{obj}_{i}$则表示第$i$个格子是否包含目标，$\\mathbf{1}^{noobj}_{ij} = 1 - \\mathbf{1}^{obj}_{ij}$表示第$i$个格子的第$j$个box是否不对应一个目标。\n\n### bounding box\nYOLO v1中虽然一个格子对应两个bounding box，但是这里的bounding box概念和anchor box不同。\n\nanchor box需要设置先验大小和长宽比，然后预测输出是相对于anchor box的偏移和缩放，计算loss时需要先用anchor box去和gt box匹配，确定好优化目标才能计算loss。\n\n而bounding box不需要先验大小和长宽比，bounding box输出的是相对于格子中心的像素偏移（使用格子大小来归一化）以及像素长宽比（使用图片大小来归一化），计算loss时要看这个格子输出的两个bounding box哪个和gt box的iou大，iou大的那个bounding box才进行回归优化。但是每个格子中的两个bounding box都会去训练confidence score，这个的回归目标也是和bounding box的输出有关（$Pr(Object) \\times IOU^{truth}_{pred}$，其中$IOU^{truth}_{pred}$是bounding box预测值和gt box的iou）。\n\n这样看来YOLO v1算是一种anchor free的检测方法。\n\n## YOLO v2\n在论文《YOLO9000:Better, Faster, Stronger》中对YOLO v1进行了一些改进最终提出了YOLO v2检测方法。\n### 相对于YOLO v1的改进点\n\n- batch norm，每个卷积层后面都添加Batch Normalization层，并且不再使用droput。\n- hi-res classifier，YOLO v1使用ImageNet上预训练的模型来进行初始化，这个预训练的输入图像是224大小的，但是为了提高检测结果的准确性，检测训练使用的输入大小是448，这样的切换模型难以适应，因此YOLO v2中增加了一个在ImageNet数据集上使用448大小finetune 10个epoch的过程。\n- Convolutional，在YOLO v1中，最终是使用全连接层来得到最终的预测，直接预测目标像素坐标和大小，这样做导致位置定位比较困难，难以训练，论文中参考了Faster RCNN的做法，认为使用卷积的方式直接得到每个像素点相对于Anchor box的偏移缩放信息的预测方法比较简便且容易训练，因此这里将全连接层改为卷积层的同时引入了Anchor box的做法（引入了Anchor box之后，预测的类别和是否包含目标这些问题都是针对每个anchor box的，之前是一个格子一个类别，且只判断格子是否包含目标。）\n- Anchor boxes，像Faster RCNN那样使用手工设计的Anchor box大小和长宽比，使用手工设计的Anchor box之后在Recall上面有提高，但是在mAP上面反而下降了一些（增加了一些假阳性）。\n- new network，YOLO v2使用一种更加接近VGG的backbone：Darknet-19，相比于YOLO v1的特征提取网络，这里除了替换$7\\times 7$的卷积、通道个数、卷积核层个数等变化以外，下采样次数从6次变成了5次，如果输入448大小的图片，那么输出大小将变成$14\\times 14$，但论文中认为最终输出的格子数为奇数比较好（便于那种占满整图的大目标的中心落在格子中心而不是格子边缘），因此将输入大小调整为$416$，得到$13\\times 13$大小的输出。\n- dimension clusters，论文中认为通过聚类学习Anchor box的大小设置比手工设计Anchor box的大小更便于模型回归目标位置，因此使用k-means聚类方法，从标注中的gt box形状中找到适合的Anchor box大小，在聚类时，两个box之间的距离使用$d = 1 - IOU$来表示（这里的IOU计算假定两个box的中心重合，这里不用考虑box的位置），作者给出的数据中，如果是手工设计的9个Anchor box大小，和gt box平均IOU在0.609，但是通过聚类学习到Anchor box，只需要5个大小就能够达到平均IOU0.61，明显好于手工设计的Anchor box先验大小。\n- location prediction，在基于Anchor的预测的方法中，模型给出预测的偏移量（$t_x, t_y$）之后，预测的中心位置计算方式为$x = (t_x \\times w_a) + x_a, y = (t_y \\times h_a) + y_a$，其中$w_a, h_a, x_a, y_a$分别表示Anchor Box的宽、高、中心x坐标、中心y坐标，但是这样的预测方式论文中认为没有对预测box的位置进行约束（对于一个左上角的Anchor box，其预测结果可能跑到图片右下角），论文中认为这给模型带来了不利于优化的不稳定性。因此论文中提出将预测方式改为$b_x=\\sigma(t_x) + C_x, b_y=\\sigma(t_y) + C_y, b_w = p_we^{t_w}, b_h = p_he^{t_h}, Pr(object)\\times IOU_{b}^{object} = \\sigma(t_o)$（这里的$C_y,C_x$代表格子的左上角坐标，因为$\\sigma(t_x),\\sigma(t_y)$都是在0~1范围，因此可以保证这样预测出来的中心不会超出这个格子，$p_w, p_h$则代表Anchor Box的宽高，可以见后面的YOLOv2预测方式示意图），这里的意思是，对于每个Anchor Box，都模型都输出四个值$\\sigma(t_x), \\sigma(t_y), t_w, t_h, \\sigma(t_o)$，对于模型的输出值，计算出最终预测框的中心：$b_x,b_y$以及预测框的宽高：$b_w, b_h$，以及预测框的置信度：$Pr(object)\\times IOU_{b}^{object}$。\n- passthrough，将$26\\times 26\\times 512$的特征图resize成$13\\times 13\\times 2048$然后和最后的$13\\times13\\times 1024$的特征图进行Concatenate操作，共同用于模型的最终预测，论文中解释说是为了提供更加细粒度的特征。\n- multi-scale，因为YOLO v2抛弃了之前的全连接操作，因此可以使用不同尺度的输入进行训练，在训练过程中，每10个batch随机按照$320:32:608$来选择一次尺度。\n- hi-res detector，这个论文中没有详细说明，不过从原文来看，这个意思就是在预测的时候使用$544\\times 544$大小的图片。\n\n以上各种改进的消融实验结果见下表：\n\n![YOLOv2各改进的消融实验结果](YOLOv2_exp.png)\n\nlocation prediction这一项改进中所描述的预测方法示意图：\n\n![YOLOv2预测方式示意图](YOLOv2_predict.png)\n\n这里有个细节是在$b_x=\\sigma(t_x) + C_x, b_y=\\sigma(t_y) + C_y$这两个运算里面所有的项都不是像素单位，如果想要转换成像素单位，那么需要除以S，然后乘上图片的或着高\n\n另外为了节省计算量，passthrough还有个改进版本：增加一次卷积操作，先将$26\\times 26\\times 512$的特征图变为$26\\times 26\\times 64$的特征图，然后再resize到$13\\times 13\\times 256$，最后拿去和$13\\times13\\times 1024$的特征图进行Concatenate操作。\n\n### YOLO v2的损失函数\n这部分其实类似YOLO v1，全都是L2损失，只不过其中YOLO v1使用的那个开方的操作这里不用了，而且计算的类别是相对于每个Anchor来说的，其输出大小为$S\\times S \\times B \\times (5 + C)$（这里的$B$是anchor个数），其他损失的计算和YOLO v1类似，论文中没有详细描述，如果感兴趣可以去研究YOLO v2的实现代码。\n\n## YOLO v3\n在论文《YOLOv3: An Incremental Improvement》中提出了YOLO v3检测方法。\n\n在YOLO v3中，相对于YOLO v2，首先更换了Backbone，从YOLO v2的Darknet-19改成了Darknet-53（根据论文中提供的Backbone分类任务训练结果中，Darknet-53的分类性能可以和ResNet-152比肩，但是速度是ResNet-152的两倍），这个新backbone中引入了resnet中的残差模块，顺便将Darknet-19中的池化层全替换成了步长为2的卷积层。\n\nYOLO v3的另外一个主要改进是借鉴了FPN的思路，不仅在$13\\times 13$大小的特征图上进行预测，同时也在$26\\times 26$，$52\\times 52$大小的特征图上进行预测，三种大小的特征图上分类的anchor box都是每个格子三个，但是大小不同，在$52\\times 52$大小的特征图的Anchor Box最小，在$13\\times 13$大小的特征图的Anchor Box最大，下图中最后的输出部分，每个像素点有255个预测值，其实就是$3 \\times (5 + 80)$，其中3是Anchor Box个数，80是COCO数据集的类别个数。\n\n在YOLO v3中，还有一个地方是其目标分配机制和YOLO v2稍有不同，对于一个格子，如果目标落在了这个格子中（这里因为三个不同大小的特征图都有输出，因此一个目标肯定会落在三个格子中，分别位于三个特征图上，这就关系到了$3\\times 3 = 9$个Anchor Box，YOLO v2因为是单尺度特征图预测，因此只需要考虑其设定的5个Anchor Box），YOLO v3在处理这个问题的时候，计算了这9个Anchor Box和当前目标的iou，iou最大的那个Anchor Box用于预测当前目标，其余Anchor Box如果iou大于某个阈值（），则计算损失时忽略其置信度损失，否则当成负样本处理。\n\nYOLO v3论文中并没有给出详细的结构描述图，下面的结构描述图来自于[CSDN博客《yolo系列之yolo v3【深度解析】》](https://blog.csdn.net/leviopku/article/details/82660381)\n\n![YOLOv3结构示意](YOLOv3结构示意.png)\n\nYOLO v3在COCO上进行的对比实验结果如下所示，这里可以看出YOLO v3在精度上并没有特别明显的竞争力，但是如果精度要求不是那么高的话，YOLO v3在速度上超越其他的单阶段检测器很多。\n\n![YOLOv3 对比实验结果](YOLOv3_exp.png)\n\n\n## YOLO v4\nYOLO v4在论文《YOLOv4: Optimal Speed and Accuracy of Object Detection》中被提出，其主要结构组件如下：\n- backbone：CSPDarknet53\n- neck：SPP、PANet\n- head：YOLO v3\n\n另外YOLO v4中使用了很多技巧：\n\n- Bag of Freebies (BoF) for backbone：\n  - CutMix and Mosaic data augmentation\n  - DropBlock regularization\n  - Class label smoothing\n- Bag of Specials (BoS) for backbone：\n  - Mish activa-tion\n  - Cross-stage partial connections (CSP)\n  - Multi-input weighted residual connections (MiWRC)\n- Bag of Freebies (BoF) for detector：\n  - CIoU-loss\n  - CmBN\n  - DropBlock\n  - regularization\n  - Mosaic data augmentation\n  - Self-Adversarial Training\n  - Eliminate grid sensitivity\n  - Using multiple anchors for a single groundtruth\n  - Cosine annealing scheduler\n  - Optimal hyper-parameters\n  - Random training shapes\n- Bag of Specials (BoS) for detector：\n  - Mish activation\n  - SPP-block\n  - SAM-block\n  - PAN path-aggregation block\n  - DIoU-NMS\n\n其中Bag of Freebies (BoF)表示只在训练过程中增加时间成本，不影响预测过程的技巧，Bag of Specials (BoS)表示在预测过程中会稍微增加一点时间成本的技巧。\n\nYOLO v4基本上算是个大型炼丹现场，作者在论文中都是用实验数据证明各种操作的效果，很少有理论分析，因此这篇论文更适合作为检测任务寻找提分手段的一个手册（一切以实验数据说话）。\n\n# 多尺度问题\n## 特征金字塔网络(Feature Pyramid Network，FPN)\n在论文《Feature Pyramid Networks for Object Detection》中提出的特征金字塔操作已经是目前目标检测模型上的标准配置，无论是单阶段检测模型还是两阶段检测模型，均可以添加FPN结构来让模型获得更好的尺度鲁棒性。\n\nFPN的结构示意图如下，其中左上角从下到上是检测模型中backbone的特征提取过程，每一层代表不同stride的特征图，原本的检测模型，例如Fatser R-CNN,是在最上面一个特征来进行roi proposal、roi classification和roi regression等操作，在加入FPN之后，获得了右上角的一系列特整图，不同stride的特征图预测不同大小的目标。\n\n![FPN结构示意图](FPN.png)\n\n上图中画得不是很完整，一般backbone中会进行5次下采样，得到的特征图stride分别是2、4、8、16、32，对应的特征图称为C1、C2、C3、C4、C5，在mmdetection检测框架的Faster R-CNN-FPN的实现中，C5将通过大小$3\\times 3$，stride为2的卷积下采样得到P6，另外C5经过$1\\times 1$卷积之后变为P5，P5上采样后和C4的$1\\times 1$卷积结果相加变成P4，P4再上采样和C3的$1\\times 1$卷积结果相加变成P3，以此类推，分别得到P2、P3、P4、P5、P6，其中P2、P3、P4、P5、P6均参与RPN的roi proposal，而对于不同大小的roi，则在P2、P3、P4、P5中选择对应尺度的特征图来进行roi pooling/roi align。\n\n原始FPN论文中，对于宽高为$w,h$的roi，其对应的特征金字塔等级计算方法如下，这里设置$k_0=4$表示第4级特征金字塔，即$P5$，因为ImageNet预训练的backbone，一般使用224大小的输入图像，而且最终结果是在C5得到的，因此这里让224以上的目标对应于$P5$特征图，小于224大小的目标则每小一倍，对应的特征等级减一\n$$\nlevel = \\lfloor k_0 + \\log_2(\\frac{\\sqrt{wh}}{224})\\rfloor\n$$\n\n在mmdetection框架的实现中，roi的分配则和原论文不同，mmdetection中，计算方式如下：\n- scale < finest_scale * 2: level 0\n- finest_scale * 2 <= scale < finest_scale * 4: level 1\n- finest_scale * 4 <= scale < finest_scale * 8: level 2\n- scale >= finest_scale * 8: level 3\n\n这里的scale是指$\\sqrt{wh}$即roi的尺度，finest_scale默认值为56，这里的level 0对应P2，level 1对应P3，以此类推。\n\nFPN在保证大目标的检测效果的同时，可以大幅提高小目标的检测效果，我认为其原因在于小目标物体的检测需要更加精细的位置信息，如果使用stride过大的特征图，会导致小目标的预测不够精确，同时小目标的信息容易被大目标掩盖，FPN中stride较小的特征图上融合了深层的特征图的信息，这可以让其的感受野非常大，而且感受野分布比较密集，小目标的检测可能也需要大感受野信息同时加上细粒度空间信息。\n","slug":"学习笔记/目标检测","published":1,"updated":"2020-09-03T02:58:53.108Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckjxpc3sw003844mqagq5dkgl","content":"<p>目标检测任务简单来说就是给定一张图片，模型需要根据图片找到感兴趣的目标，将其位置、大小标注出来。</p>\r\n<h1 id=\"两阶段检测算法从rcnn到faster-r-cnn\">两阶段检测算法，从RCNN到Faster R-CNN</h1>\r\n<p>目前的目标检测方法主要分为：两阶段的检测器、单阶段检测器两种，RCNN到Faster RCNN系列检测器是典型的两阶段检测器的代表。</p>\r\n<h2 id=\"r-cnn\">R-CNN</h2>\r\n<p>论文《Rich feature hierarchies for accurate oject detection and semantic segmentation》中提出了R-CNN模型用于目标检测，其基本流程如下： - 使用Selective Search算法提取出2k个region proposal。 - 对于每个region proposal，先裁剪出对应区域的图片，然后使用AlexNet进行特征提取（特征取最后那个softmax层的输入，是一个4096维的向量）。 - 训练K个线性SVM分类器，对每个region proposal进行K次分类，得到其属于K个类别的得分。 - 对每个类别的proposal进行NMS后处理，得到最终的结果。</p>\r\n<p>这里的Selective Search算法本质上是一种先分割图片然后再尝试各种组合的穷举算法，这里没有详细研究。</p>\r\n<p>整体来说，R-CNN的流程非常粗暴，主要有两个阶段：1、获取region proposal，2、对region proposal进行分类，但这也是后面的两阶段检测方法的一般流程。</p>\r\n<h2 id=\"spp-net\">SPP-net</h2>\r\n<p>在R-CNN中存在的一个问题是region proposal的大小不相等，但是AlexNet要求固定大小的输入图片，否则特征维度会发生变换（如果现在来看这个问题的话，加个global pooling就OK了，但是当时没想到），因此在RCNN里面只能通过缩放，让region proposal满足输入大小的要求，这样带来了畸变的问题，为了解决这个问题，论文《Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition》提出了空间金字塔池化操作(Spatial Pyramid Pooling，SPP)，而且顺便结局了RCNN中对每个region proposal都提取一次特征带来的重复计算问题。</p>\r\n<p>在SPP-net中，首先将整幅图作为输入（这里使用的模型不再是AlexNet，这里试验了ZF-5、Convnet*-5、Overfeat-5、Overfeat-7这几个模型，我这里不再详细研究），提取出特征图之后，再从特征图上抠出来region proposal对应的区域（这里不同于RCNN对每个region proposal进行一次特征提取，所以解决了重复计算问题），然后将抠出来的区域进行空间金字塔池化操作，使其变成固定大小的特征向量。再使用SVM对特征进行分类。</p>\r\n<p>SPP操作的示意图如下所示，主要思想是将特征图先划分成固定数量的网格，然后再每个格子内进行池化操作，最后将固定大小的池化结果展开、拼接成固定大小的特征向量。</p>\r\n<figure>\r\n<img src=\"SPP.png\" alt=\"SPP操作示意\" /><figcaption aria-hidden=\"true\">SPP操作示意</figcaption>\r\n</figure>\r\n<h2 id=\"fast-r-cnn\">Fast R-CNN</h2>\r\n<p>有了SPP-net的思路之后，RCNN也做出了相应的改变，在论文《Fast R-CNN》中，接纳了SPP-net中先全图特征提取再裁剪region proposal区域的方法，然后对SPP-net进行改进：</p>\r\n<ol type=\"1\">\r\n<li>只使用一个固定大小的网格分割，称为ROI Pooling层（并提出了ROI Pooling层的反向传播方法）。（至于为什么要降SPP改成ROI Pooling，可能主要是ROI Pooling有反向传播功能，其实如果SPP也实现了反向传播，那么ROI Pooling不一定比SPP效果好，不知道有没有人进行过对比）</li>\r\n<li>特征提取模型换成了VGG。</li>\r\n<li>对特征的处理采用神经网络，不再使用SVM，对于分类，使用softmax函数得到类别概率。</li>\r\n<li>特征除了用于分类之外，还用于更精确的位置回归（使用smooth L1损失），并且让这两个任务共享一部分全连接层，提出了多任务损失函数的训练方法。</li>\r\n</ol>\r\n<p>Fast R-CNN的简略流程示意图如下所示： <img src=\"FastRCNN.png\" alt=\"Fast R-CNN示意图\" /></p>\r\n<h2 id=\"faster-r-cnn\">Faster R-CNN</h2>\r\n<p>Fast R-CNN还是没有做到完全的端到端得到输出结果，因为还是要依赖Selective Search算法来生成region proposal。</p>\r\n<p>在论文《Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks》中，提出了Faster R-CNN，其中使用一个区域建议网络（Region Proposal Network，RPN）来替代Selective Search算法进行region proposal工作。</p>\r\n<p>RPN的思路应该来源于Fast R-CNN中的位置回归操作，Fast R-CNN中证明神经网络可以用于位置回归问题，因此自然就会有一个想法是使用神经网络来对输出候选位置。但是如果直接回归位置，有两个问题：</p>\r\n<ol type=\"1\">\r\n<li>特征图上某个像素点，应该回归哪个目标的位置（在之前的方法中，位置回归的目标很明确，因为region proposal有了之后，查找数据标签，可以很容易知道对于这个位置应该去回归哪个目标，或者这里面没有目标，那么就不用回归位置）</li>\r\n<li>如果是输出像素坐标，那么对于不同大小的图片，不同尺度和比例的目标，回归任务很难收敛。</li>\r\n</ol>\r\n<p>对于上面两个问题，这里提出了Anchor的机制，首先在图片上按照一定的规律分布好各种尺度和比例的方框（Anchor box，一个像素点可以有多个Anchor box），然后每个像素点（Anchor）输出anchor box的类别（前景还是背景）、缩放和偏移量。</p>\r\n<p>每个anchor box要么匹配一个目标（其分类类别为这个目标的类别且回归目标为这个目标的位置），要么在训练过程中将其分类为背景类，并且不计算其回归输出的loss。</p>\r\n<p>RPN和也是用和RCNN一样的特征提取网络（这个特征提取网络一般称为Backbone）。</p>\r\n<h3 id=\"faster-r-cnn中的预测结果计算\">Faster R-CNN中的预测结果计算</h3>\r\n<p>Faster R-CNN的RPN和RCNN对于回归量的输出都是<span class=\"math inline\">\\(\\delta_x, \\delta_y, \\delta_h, \\delta_w\\)</span>，分别表示中心点的偏移值，以及box长宽的缩放值，而且不是像素单位，因此需要转换成像素单位需要进行一些转换如下，这里的<span class=\"math inline\">\\(b_x, b_y, b_h, b_w\\)</span>分别表示最终预测box的中心和长宽，然后<span class=\"math inline\">\\(p_x, p_y, p_h, p_w\\)</span>分别表示先验box（在RPN阶段先验box就是Anchor，在RCNN阶段先验box就是RPN的Proposal Box）的中心和长宽。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n  b_x &amp;= p_x + p_w \\delta_x\\\\\r\n  b_y &amp;= p_y + p_h \\delta_y\\\\\r\n  b_w &amp;= p_w \\exp(\\delta_w)\\\\\r\n  b_h &amp;= p_h \\exp(\\delta_h)\\\\\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p><strong>这个计算方式可能导致的问题：1、<span class=\"math inline\">\\(b_x,b_y\\)</span>可能超出先验框的范围，2、由于roi pooling之后，RCNN只能看到Proposal Box之内的内容，因此RCNN预测得到的<span class=\"math inline\">\\(\\delta_w,\\delta_h\\)</span>如果大于0，那么则表示预测的box范围超出了Proposal Box区域，可能存在预测不准确的问题，当然，由于特征图上的像素点具有一定的感受野范围，因此按理说RCNN还是可以看到一些Proposal Box之外的信息的。</strong></p>\r\n<h3 id=\"为什么要使用smooth-l1作为回归任务的损失函数\">为什么要使用Smooth L1作为回归任务的损失函数</h3>\r\n<p>在回归损失函数中，最常用的是L2损失函数，<span class=\"math inline\">\\(L = (h(x;\\theta) - y) ^ 2\\)</span>，其导数<span class=\"math inline\">\\(\\frac{\\partial L}{\\partial \\theta} = \\frac{\\partial h(x;\\theta)}{\\partial \\theta}(h(x;\\theta) -y)\\)</span>，这里的问题在于：如果模型初始化得不好，再训练开始时<span class=\"math inline\">\\(h(x;\\theta) - y\\)</span>特别大，则会导致梯度特别大，模型将难以训练。那么一个自然地想法是使用L1损失函数，但是L1损失函数不是处处可导，梯度会发生突变，也不利于训练，因此将L1损失函数进行一些平滑操作，给出了smooth L1损失函数如下所示： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    L(\\theta) = \\begin{cases}\r\n        0.5 x^2 &amp;|x| &lt; 1\\\\\r\n        |x| - 0.5 &amp;|x| &gt;= 1\r\n    \\end{cases}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<h3 id=\"rpn训练时的loss计算需要对负样本进行采样\">RPN训练时的loss计算需要对负样本进行采样</h3>\r\n<p>RPN的训练目标是将Anchor Box进行前景和背景分类，并且对分类为前景的Anchor Box进行位置偏移量和缩放量的回归。</p>\r\n<p>在训练时，需要首先分配Anchor Box的训练目标：给定一张图像样本上的所有gt box标签，首先需要计算所有gt box和每个Anchor Box的iou。</p>\r\n<p>对于每一个Anchor Box，找到和其iou最大的gt box，按照和这个gt box之间的iou，有三种可能： 1. 如果大于某个iou阈值，那么则被分配给这个gt box作为前景来训练（即这个Anchor Box的分类目标是正样本，回归目标是对应的gt box）。 2. 如果小于某个iou阈值，则作为背景类处理（即这个Anchor Box的分类目标是负样本，没有回归目标，不计算回归损失）。 3. 如果iou不大又不小，则忽略这个Anchor Box，计算损失的过程中不考虑，这是因为一些Anchor Box包含了一部分目标，但是又包含得不够多，不能武断的将其作为前景或者背景来训练，因此干脆不管这一部分的训练，反正参数是共享的，其他的地方训练好之后，测试过程中这种地方输出是什么样影响不大。</p>\r\n<p>在分配Anchor Box训练目标的过程中，绝大部分Anchor Box是被分配成负样本（背景类）的，，因此RPN训练时存在前景和背景两个类别不平衡的情况。所以一般的Faster R-CNN实现中，RPN的训练部分，会对Anchor Box进行采样。(例如mmdetection中的实现中，RPN部分的默认配置是采样的256个Anchor Box进行训练，首先随机采样128个正样本，然后采样128个负样本，如果正样本不够，那么负样本多采样一些填满256个)</p>\r\n<h1 id=\"多阶段检测器\">多阶段检测器</h1>\r\n<h2 id=\"cascade-r-cnn\">Cascade R-CNN</h2>\r\n<p>在论文《Cascade R-CNN: Delving into High Quality Object Detection》中指出了Faster R-CNN中按照某个iou阈值分配Proposal正负样本存在的问题：iou阈值过低会导致大量的低阈值预测框，对于regression head来说非常难修正，因此会出现大量的close false positive预测（即iou不够从而被判定为false positive的预测），iou阈值过大又导致匹配到的正样本非常少，容易引起过拟合或者类别不平衡问题。</p>\r\n<p>Cascade R-CNN中为了解决这个问题，提出了使用多级head，将预测iou逐渐提高的方法。</p>\r\n<p>Cascade R-CNN提出的多级head预测如下图(d)所示，这里的H1、H2、H3都是独立参数，且在训练过程中使用的iou不相同。</p>\r\n<figure>\r\n<img src=\"Cascade_R-CNN.png\" alt=\"cascade rcnn多级head示意图\" /><figcaption aria-hidden=\"true\">cascade rcnn多级head示意图</figcaption>\r\n</figure>\r\n<p>Cascade R-CNN的思路很简单：在理想状况中，H1接收RPN输出的ROI Proposal作为B0，对B0进行分类和修正，并从中筛除一些背景或者iou较低的box得到新的预测框集合B1，H2则接收B1作为输入，对B1进行分类和修正，得到B2，H3对B2进行同样的操作，得到最终的预测输出。</p>\r\n<p>三个步骤不同之处首先在于背景类的含义，例如存在三个iou阈值<span class=\"math inline\">\\(u_1 &lt; u_2 &lt; u_3\\)</span>，在H1中低于<span class=\"math inline\">\\(u_1\\)</span>的才被认为是负样本，而H2中低于<span class=\"math inline\">\\(u_2\\)</span>的被认为是负样本，H3则使用<span class=\"math inline\">\\(u_2\\)</span>作为负样本阈值，这样设置的意思就是每个Head负责筛掉不同精度的预测框，另外，就算这里<span class=\"math inline\">\\(u_3\\)</span>较高，也不会引起之前iou阈值高导致的匹配到的正样本非常少的问题，因为原始RPN的proposal虽然可能iou较低，但是经过H1、H2的逐步修正之后，B2之中iou普遍比较高了，即使<span class=\"math inline\">\\(u_3\\)</span>较高，也能匹配到较多的正样本用于H3的训练。</p>\r\n<p>在预测过程中，Cascade R-CNN的不同Head先后进行预测，最终的分类得分将综合三个Head的预测结果（例如C1、C2、C3取平均），而回归结果直接使用H3得到的B3。</p>\r\n<h1 id=\"一阶段目标检测方法\">一阶段目标检测方法</h1>\r\n<p>一阶段目标检测方法主要是相对于Faster R-CNN这类二阶段检测方法来说的，在Faster R-CNN中，主要被分为region proposal和proposal的分类回归。</p>\r\n<p>一阶段目标检测方法将region proposal这一步去掉了，不需要RPN来进行初步的定位，而是直接得到box输出结果。</p>\r\n<p>一阶段检测器按照时间顺序主要有：YOLO v1、SSD、DSSD、YOLO v2、Retinanet、YOLO v3、YOLO v4等，这里做个简单的梳理。</p>\r\n<h2 id=\"yolo-v1\">YOLO v1</h2>\r\n<p>论文《You Only Look Once:Unified, Real-Time Object Detection》中提出了YOLO检测方法，其基本思想是将一幅图像分成<span class=\"math inline\">\\(S \\times S\\)</span>的网格，如果某个目标的中心落在这个网格中，那么这个网格就负责预测这个目标。</p>\r\n<p>网格中的一个格子可以输出<span class=\"math inline\">\\(B\\)</span>个bounding boxes，每个格子的输出如下：</p>\r\n<ul>\r\n<li><span class=\"math inline\">\\(B\\)</span>个bounding boxes的位置回归信息（中心偏移：x,y（使用格子大小来归一化）；box的宽高：w,h（使用图片的长宽来归一化））共<span class=\"math inline\">\\(4\\times B\\)</span>个值。</li>\r\n<li>bounding boxes对应的<span class=\"math inline\">\\(B\\)</span>个confidence scores（这个confidence score表示当前bounding box包含目标的概率乘以bounding box位置回归的置信度），其的回归目标定义为<span class=\"math inline\">\\(Pr(Object) \\times IOU^{truth}_{pred}\\)</span>。</li>\r\n<li>C个类别信息，表示为<span class=\"math inline\">\\(Pr(Class_i|Object),i = 1,2,...,C\\)</span>，这是个条件概率，其含义是如果这个格子有目标，那么这个目标属于不同的类别的概率。</li>\r\n</ul>\r\n<p>因此一个格子的输出个数是<span class=\"math inline\">\\(5 \\times B + C\\)</span>，整个YOLO v1模型的输出个数是<span class=\"math inline\">\\(S\\times S \\times (5 \\times B + C)\\)</span>。</p>\r\n<p>在YOLO v1模型中，输入图片大小固定为<span class=\"math inline\">\\(448 \\times 448\\)</span>，经过6次下采样之后变成<span class=\"math inline\">\\(7\\times 7\\)</span>大小的特征图，作者取<span class=\"math inline\">\\(S=7\\)</span>，<span class=\"math inline\">\\(B=2\\)</span>，Pascal VOC的类别个数<span class=\"math inline\">\\(C=20\\)</span>，因此YOLO v1的输出是个<span class=\"math inline\">\\(7\\times 7 \\times 30\\)</span>的特征图，整个模型结构图如下所示，其中激活函数使用Leaky ReLU，参数为0.1。</p>\r\n<figure>\r\n<img src=\"YOLOv1.png\" alt=\"YOLO v1模型结构示意\" /><figcaption aria-hidden=\"true\">YOLO v1模型结构示意</figcaption>\r\n</figure>\r\n<h3 id=\"损失函数\">损失函数</h3>\r\n<p>YOLO v1使用平方和误差来进行训练，但是论文中表示这样的损失函数不能完美的匹配优化Average Precision的目标（就是说定位误差和分类误差的权重相等不太合适），而且每张图片上不包含目标的格子居多，这些格子主导了梯度，导致预测置信度（confidence score）普遍偏低，同时模型训练过程容易震荡，因此作者在损失函数中引入了两个超参数：<span class=\"math inline\">\\(\\lambda_{coord} = 5\\)</span>、<span class=\"math inline\">\\(\\lambda_{noobj} = 0.5\\)</span>作为不同损失的权重。</p>\r\n<p>另外，论文中还提到平方和损失在回归位置的时候，还有个问题是large box和small box对于同样的宽高偏差敏感度不同，为了缓解这个问题，论文中提出直接让模型预测<span class=\"math inline\">\\(w,h\\)</span>的平方，需要获取真正的长宽时，需要将模型的预测输出开方进行还原。</p>\r\n<p>最终YOLO v1的损失函数如下： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    &amp;\\lambda_{coord} \\sum\\limits_{i=0}^{S^2} \\sum\\limits_{j=0}^{B} \\mathbf{1}^{obj}_{ij}[(x_i - \\hat{x}_i)^2 + (y_i - \\hat{y}_i)^2]\\\\\r\n    &amp;+ \\lambda_{coord} \\sum\\limits_{i=0}^{S^2} \\sum\\limits_{j=0}^{B} \\mathbf{1}^{obj}_{ij}[(w_i - \\sqrt{\\hat{w}_i})^2 + (h_i - \\sqrt{\\hat{h}_i})^2]\\\\\r\n    &amp;+ \\sum\\limits_{i=0}^{S^2} \\sum\\limits_{j=0}^{B} \\mathbf{1}^{obj}_{ij}(C_i - \\hat{C}_i)^2\\\\\r\n    &amp;+ \\lambda_{noobj} \\sum\\limits_{i=0}^{S^2} \\sum\\limits_{j=0}^{B} \\mathbf{1}^{noobj}_{ij}(C_i - \\hat{C}_i)^2\\\\\r\n    &amp;+ \\sum\\limits_{i=0}^{S^2} \\mathbf{1}^{obj}_{i} \\sum\\limits_{c \\in Classes} (p_i(c) - \\hat{p}_i(c))^2\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>其中<span class=\"math inline\">\\(\\mathbf{1}^{obj}_{ij}\\)</span>是个指示函数，表示第<span class=\"math inline\">\\(i\\)</span>个格子的第<span class=\"math inline\">\\(j\\)</span>个box是否对应一个目标。<span class=\"math inline\">\\(\\mathbf{1}^{obj}_{i}\\)</span>则表示第<span class=\"math inline\">\\(i\\)</span>个格子是否包含目标，<span class=\"math inline\">\\(\\mathbf{1}^{noobj}_{ij} = 1 - \\mathbf{1}^{obj}_{ij}\\)</span>表示第<span class=\"math inline\">\\(i\\)</span>个格子的第<span class=\"math inline\">\\(j\\)</span>个box是否不对应一个目标。</p>\r\n<h3 id=\"bounding-box\">bounding box</h3>\r\n<p>YOLO v1中虽然一个格子对应两个bounding box，但是这里的bounding box概念和anchor box不同。</p>\r\n<p>anchor box需要设置先验大小和长宽比，然后预测输出是相对于anchor box的偏移和缩放，计算loss时需要先用anchor box去和gt box匹配，确定好优化目标才能计算loss。</p>\r\n<p>而bounding box不需要先验大小和长宽比，bounding box输出的是相对于格子中心的像素偏移（使用格子大小来归一化）以及像素长宽比（使用图片大小来归一化），计算loss时要看这个格子输出的两个bounding box哪个和gt box的iou大，iou大的那个bounding box才进行回归优化。但是每个格子中的两个bounding box都会去训练confidence score，这个的回归目标也是和bounding box的输出有关（<span class=\"math inline\">\\(Pr(Object) \\times IOU^{truth}_{pred}\\)</span>，其中<span class=\"math inline\">\\(IOU^{truth}_{pred}\\)</span>是bounding box预测值和gt box的iou）。</p>\r\n<p>这样看来YOLO v1算是一种anchor free的检测方法。</p>\r\n<h2 id=\"yolo-v2\">YOLO v2</h2>\r\n<p>在论文《YOLO9000:Better, Faster, Stronger》中对YOLO v1进行了一些改进最终提出了YOLO v2检测方法。 ### 相对于YOLO v1的改进点</p>\r\n<ul>\r\n<li>batch norm，每个卷积层后面都添加Batch Normalization层，并且不再使用droput。</li>\r\n<li>hi-res classifier，YOLO v1使用ImageNet上预训练的模型来进行初始化，这个预训练的输入图像是224大小的，但是为了提高检测结果的准确性，检测训练使用的输入大小是448，这样的切换模型难以适应，因此YOLO v2中增加了一个在ImageNet数据集上使用448大小finetune 10个epoch的过程。</li>\r\n<li>Convolutional，在YOLO v1中，最终是使用全连接层来得到最终的预测，直接预测目标像素坐标和大小，这样做导致位置定位比较困难，难以训练，论文中参考了Faster RCNN的做法，认为使用卷积的方式直接得到每个像素点相对于Anchor box的偏移缩放信息的预测方法比较简便且容易训练，因此这里将全连接层改为卷积层的同时引入了Anchor box的做法（引入了Anchor box之后，预测的类别和是否包含目标这些问题都是针对每个anchor box的，之前是一个格子一个类别，且只判断格子是否包含目标。）</li>\r\n<li>Anchor boxes，像Faster RCNN那样使用手工设计的Anchor box大小和长宽比，使用手工设计的Anchor box之后在Recall上面有提高，但是在mAP上面反而下降了一些（增加了一些假阳性）。</li>\r\n<li>new network，YOLO v2使用一种更加接近VGG的backbone：Darknet-19，相比于YOLO v1的特征提取网络，这里除了替换<span class=\"math inline\">\\(7\\times 7\\)</span>的卷积、通道个数、卷积核层个数等变化以外，下采样次数从6次变成了5次，如果输入448大小的图片，那么输出大小将变成<span class=\"math inline\">\\(14\\times 14\\)</span>，但论文中认为最终输出的格子数为奇数比较好（便于那种占满整图的大目标的中心落在格子中心而不是格子边缘），因此将输入大小调整为<span class=\"math inline\">\\(416\\)</span>，得到<span class=\"math inline\">\\(13\\times 13\\)</span>大小的输出。</li>\r\n<li>dimension clusters，论文中认为通过聚类学习Anchor box的大小设置比手工设计Anchor box的大小更便于模型回归目标位置，因此使用k-means聚类方法，从标注中的gt box形状中找到适合的Anchor box大小，在聚类时，两个box之间的距离使用<span class=\"math inline\">\\(d = 1 - IOU\\)</span>来表示（这里的IOU计算假定两个box的中心重合，这里不用考虑box的位置），作者给出的数据中，如果是手工设计的9个Anchor box大小，和gt box平均IOU在0.609，但是通过聚类学习到Anchor box，只需要5个大小就能够达到平均IOU0.61，明显好于手工设计的Anchor box先验大小。</li>\r\n<li>location prediction，在基于Anchor的预测的方法中，模型给出预测的偏移量（<span class=\"math inline\">\\(t_x, t_y\\)</span>）之后，预测的中心位置计算方式为<span class=\"math inline\">\\(x = (t_x \\times w_a) + x_a, y = (t_y \\times h_a) + y_a\\)</span>，其中<span class=\"math inline\">\\(w_a, h_a, x_a, y_a\\)</span>分别表示Anchor Box的宽、高、中心x坐标、中心y坐标，但是这样的预测方式论文中认为没有对预测box的位置进行约束（对于一个左上角的Anchor box，其预测结果可能跑到图片右下角），论文中认为这给模型带来了不利于优化的不稳定性。因此论文中提出将预测方式改为<span class=\"math inline\">\\(b_x=\\sigma(t_x) + C_x, b_y=\\sigma(t_y) + C_y, b_w = p_we^{t_w}, b_h = p_he^{t_h}, Pr(object)\\times IOU_{b}^{object} = \\sigma(t_o)\\)</span>（这里的<span class=\"math inline\">\\(C_y,C_x\\)</span>代表格子的左上角坐标，因为<span class=\"math inline\">\\(\\sigma(t_x),\\sigma(t_y)\\)</span>都是在0~1范围，因此可以保证这样预测出来的中心不会超出这个格子，<span class=\"math inline\">\\(p_w, p_h\\)</span>则代表Anchor Box的宽高，可以见后面的YOLOv2预测方式示意图），这里的意思是，对于每个Anchor Box，都模型都输出四个值<span class=\"math inline\">\\(\\sigma(t_x), \\sigma(t_y), t_w, t_h, \\sigma(t_o)\\)</span>，对于模型的输出值，计算出最终预测框的中心：<span class=\"math inline\">\\(b_x,b_y\\)</span>以及预测框的宽高：<span class=\"math inline\">\\(b_w, b_h\\)</span>，以及预测框的置信度：<span class=\"math inline\">\\(Pr(object)\\times IOU_{b}^{object}\\)</span>。</li>\r\n<li>passthrough，将<span class=\"math inline\">\\(26\\times 26\\times 512\\)</span>的特征图resize成<span class=\"math inline\">\\(13\\times 13\\times 2048\\)</span>然后和最后的<span class=\"math inline\">\\(13\\times13\\times 1024\\)</span>的特征图进行Concatenate操作，共同用于模型的最终预测，论文中解释说是为了提供更加细粒度的特征。</li>\r\n<li>multi-scale，因为YOLO v2抛弃了之前的全连接操作，因此可以使用不同尺度的输入进行训练，在训练过程中，每10个batch随机按照<span class=\"math inline\">\\(320:32:608\\)</span>来选择一次尺度。</li>\r\n<li>hi-res detector，这个论文中没有详细说明，不过从原文来看，这个意思就是在预测的时候使用<span class=\"math inline\">\\(544\\times 544\\)</span>大小的图片。</li>\r\n</ul>\r\n<p>以上各种改进的消融实验结果见下表：</p>\r\n<figure>\r\n<img src=\"YOLOv2_exp.png\" alt=\"YOLOv2各改进的消融实验结果\" /><figcaption aria-hidden=\"true\">YOLOv2各改进的消融实验结果</figcaption>\r\n</figure>\r\n<p>location prediction这一项改进中所描述的预测方法示意图：</p>\r\n<figure>\r\n<img src=\"YOLOv2_predict.png\" alt=\"YOLOv2预测方式示意图\" /><figcaption aria-hidden=\"true\">YOLOv2预测方式示意图</figcaption>\r\n</figure>\r\n<p>这里有个细节是在<span class=\"math inline\">\\(b_x=\\sigma(t_x) + C_x, b_y=\\sigma(t_y) + C_y\\)</span>这两个运算里面所有的项都不是像素单位，如果想要转换成像素单位，那么需要除以S，然后乘上图片的或着高</p>\r\n<p>另外为了节省计算量，passthrough还有个改进版本：增加一次卷积操作，先将<span class=\"math inline\">\\(26\\times 26\\times 512\\)</span>的特征图变为<span class=\"math inline\">\\(26\\times 26\\times 64\\)</span>的特征图，然后再resize到<span class=\"math inline\">\\(13\\times 13\\times 256\\)</span>，最后拿去和<span class=\"math inline\">\\(13\\times13\\times 1024\\)</span>的特征图进行Concatenate操作。</p>\r\n<h3 id=\"yolo-v2的损失函数\">YOLO v2的损失函数</h3>\r\n<p>这部分其实类似YOLO v1，全都是L2损失，只不过其中YOLO v1使用的那个开方的操作这里不用了，而且计算的类别是相对于每个Anchor来说的，其输出大小为<span class=\"math inline\">\\(S\\times S \\times B \\times (5 + C)\\)</span>（这里的<span class=\"math inline\">\\(B\\)</span>是anchor个数），其他损失的计算和YOLO v1类似，论文中没有详细描述，如果感兴趣可以去研究YOLO v2的实现代码。</p>\r\n<h2 id=\"yolo-v3\">YOLO v3</h2>\r\n<p>在论文《YOLOv3: An Incremental Improvement》中提出了YOLO v3检测方法。</p>\r\n<p>在YOLO v3中，相对于YOLO v2，首先更换了Backbone，从YOLO v2的Darknet-19改成了Darknet-53（根据论文中提供的Backbone分类任务训练结果中，Darknet-53的分类性能可以和ResNet-152比肩，但是速度是ResNet-152的两倍），这个新backbone中引入了resnet中的残差模块，顺便将Darknet-19中的池化层全替换成了步长为2的卷积层。</p>\r\n<p>YOLO v3的另外一个主要改进是借鉴了FPN的思路，不仅在<span class=\"math inline\">\\(13\\times 13\\)</span>大小的特征图上进行预测，同时也在<span class=\"math inline\">\\(26\\times 26\\)</span>，<span class=\"math inline\">\\(52\\times 52\\)</span>大小的特征图上进行预测，三种大小的特征图上分类的anchor box都是每个格子三个，但是大小不同，在<span class=\"math inline\">\\(52\\times 52\\)</span>大小的特征图的Anchor Box最小，在<span class=\"math inline\">\\(13\\times 13\\)</span>大小的特征图的Anchor Box最大，下图中最后的输出部分，每个像素点有255个预测值，其实就是<span class=\"math inline\">\\(3 \\times (5 + 80)\\)</span>，其中3是Anchor Box个数，80是COCO数据集的类别个数。</p>\r\n<p>在YOLO v3中，还有一个地方是其目标分配机制和YOLO v2稍有不同，对于一个格子，如果目标落在了这个格子中（这里因为三个不同大小的特征图都有输出，因此一个目标肯定会落在三个格子中，分别位于三个特征图上，这就关系到了<span class=\"math inline\">\\(3\\times 3 = 9\\)</span>个Anchor Box，YOLO v2因为是单尺度特征图预测，因此只需要考虑其设定的5个Anchor Box），YOLO v3在处理这个问题的时候，计算了这9个Anchor Box和当前目标的iou，iou最大的那个Anchor Box用于预测当前目标，其余Anchor Box如果iou大于某个阈值（），则计算损失时忽略其置信度损失，否则当成负样本处理。</p>\r\n<p>YOLO v3论文中并没有给出详细的结构描述图，下面的结构描述图来自于<a href=\"https://blog.csdn.net/leviopku/article/details/82660381\">CSDN博客《yolo系列之yolo v3【深度解析】》</a></p>\r\n<figure>\r\n<img src=\"YOLOv3结构示意.png\" alt=\"YOLOv3结构示意\" /><figcaption aria-hidden=\"true\">YOLOv3结构示意</figcaption>\r\n</figure>\r\n<p>YOLO v3在COCO上进行的对比实验结果如下所示，这里可以看出YOLO v3在精度上并没有特别明显的竞争力，但是如果精度要求不是那么高的话，YOLO v3在速度上超越其他的单阶段检测器很多。</p>\r\n<figure>\r\n<img src=\"YOLOv3_exp.png\" alt=\"YOLOv3 对比实验结果\" /><figcaption aria-hidden=\"true\">YOLOv3 对比实验结果</figcaption>\r\n</figure>\r\n<h2 id=\"yolo-v4\">YOLO v4</h2>\r\n<p>YOLO v4在论文《YOLOv4: Optimal Speed and Accuracy of Object Detection》中被提出，其主要结构组件如下： - backbone：CSPDarknet53 - neck：SPP、PANet - head：YOLO v3</p>\r\n<p>另外YOLO v4中使用了很多技巧：</p>\r\n<ul>\r\n<li>Bag of Freebies (BoF) for backbone：\r\n<ul>\r\n<li>CutMix and Mosaic data augmentation</li>\r\n<li>DropBlock regularization</li>\r\n<li>Class label smoothing</li>\r\n</ul></li>\r\n<li>Bag of Specials (BoS) for backbone：\r\n<ul>\r\n<li>Mish activa-tion</li>\r\n<li>Cross-stage partial connections (CSP)</li>\r\n<li>Multi-input weighted residual connections (MiWRC)</li>\r\n</ul></li>\r\n<li>Bag of Freebies (BoF) for detector：\r\n<ul>\r\n<li>CIoU-loss</li>\r\n<li>CmBN</li>\r\n<li>DropBlock</li>\r\n<li>regularization</li>\r\n<li>Mosaic data augmentation</li>\r\n<li>Self-Adversarial Training</li>\r\n<li>Eliminate grid sensitivity</li>\r\n<li>Using multiple anchors for a single groundtruth</li>\r\n<li>Cosine annealing scheduler</li>\r\n<li>Optimal hyper-parameters</li>\r\n<li>Random training shapes</li>\r\n</ul></li>\r\n<li>Bag of Specials (BoS) for detector：\r\n<ul>\r\n<li>Mish activation</li>\r\n<li>SPP-block</li>\r\n<li>SAM-block</li>\r\n<li>PAN path-aggregation block</li>\r\n<li>DIoU-NMS</li>\r\n</ul></li>\r\n</ul>\r\n<p>其中Bag of Freebies (BoF)表示只在训练过程中增加时间成本，不影响预测过程的技巧，Bag of Specials (BoS)表示在预测过程中会稍微增加一点时间成本的技巧。</p>\r\n<p>YOLO v4基本上算是个大型炼丹现场，作者在论文中都是用实验数据证明各种操作的效果，很少有理论分析，因此这篇论文更适合作为检测任务寻找提分手段的一个手册（一切以实验数据说话）。</p>\r\n<h1 id=\"多尺度问题\">多尺度问题</h1>\r\n<h2 id=\"特征金字塔网络feature-pyramid-networkfpn\">特征金字塔网络(Feature Pyramid Network，FPN)</h2>\r\n<p>在论文《Feature Pyramid Networks for Object Detection》中提出的特征金字塔操作已经是目前目标检测模型上的标准配置，无论是单阶段检测模型还是两阶段检测模型，均可以添加FPN结构来让模型获得更好的尺度鲁棒性。</p>\r\n<p>FPN的结构示意图如下，其中左上角从下到上是检测模型中backbone的特征提取过程，每一层代表不同stride的特征图，原本的检测模型，例如Fatser R-CNN,是在最上面一个特征来进行roi proposal、roi classification和roi regression等操作，在加入FPN之后，获得了右上角的一系列特整图，不同stride的特征图预测不同大小的目标。</p>\r\n<figure>\r\n<img src=\"FPN.png\" alt=\"FPN结构示意图\" /><figcaption aria-hidden=\"true\">FPN结构示意图</figcaption>\r\n</figure>\r\n<p>上图中画得不是很完整，一般backbone中会进行5次下采样，得到的特征图stride分别是2、4、8、16、32，对应的特征图称为C1、C2、C3、C4、C5，在mmdetection检测框架的Faster R-CNN-FPN的实现中，C5将通过大小<span class=\"math inline\">\\(3\\times 3\\)</span>，stride为2的卷积下采样得到P6，另外C5经过<span class=\"math inline\">\\(1\\times 1\\)</span>卷积之后变为P5，P5上采样后和C4的<span class=\"math inline\">\\(1\\times 1\\)</span>卷积结果相加变成P4，P4再上采样和C3的<span class=\"math inline\">\\(1\\times 1\\)</span>卷积结果相加变成P3，以此类推，分别得到P2、P3、P4、P5、P6，其中P2、P3、P4、P5、P6均参与RPN的roi proposal，而对于不同大小的roi，则在P2、P3、P4、P5中选择对应尺度的特征图来进行roi pooling/roi align。</p>\r\n<p>原始FPN论文中，对于宽高为<span class=\"math inline\">\\(w,h\\)</span>的roi，其对应的特征金字塔等级计算方法如下，这里设置<span class=\"math inline\">\\(k_0=4\\)</span>表示第4级特征金字塔，即<span class=\"math inline\">\\(P5\\)</span>，因为ImageNet预训练的backbone，一般使用224大小的输入图像，而且最终结果是在C5得到的，因此这里让224以上的目标对应于<span class=\"math inline\">\\(P5\\)</span>特征图，小于224大小的目标则每小一倍，对应的特征等级减一 <span class=\"math display\">\\[\r\nlevel = \\lfloor k_0 + \\log_2(\\frac{\\sqrt{wh}}{224})\\rfloor\r\n\\]</span></p>\r\n<p>在mmdetection框架的实现中，roi的分配则和原论文不同，mmdetection中，计算方式如下： - scale &lt; finest_scale * 2: level 0 - finest_scale * 2 &lt;= scale &lt; finest_scale * 4: level 1 - finest_scale * 4 &lt;= scale &lt; finest_scale * 8: level 2 - scale &gt;= finest_scale * 8: level 3</p>\r\n<p>这里的scale是指<span class=\"math inline\">\\(\\sqrt{wh}\\)</span>即roi的尺度，finest_scale默认值为56，这里的level 0对应P2，level 1对应P3，以此类推。</p>\r\n<p>FPN在保证大目标的检测效果的同时，可以大幅提高小目标的检测效果，我认为其原因在于小目标物体的检测需要更加精细的位置信息，如果使用stride过大的特征图，会导致小目标的预测不够精确，同时小目标的信息容易被大目标掩盖，FPN中stride较小的特征图上融合了深层的特征图的信息，这可以让其的感受野非常大，而且感受野分布比较密集，小目标的检测可能也需要大感受野信息同时加上细粒度空间信息。</p>\r\n","site":{"data":{}},"excerpt":"","more":"<p>目标检测任务简单来说就是给定一张图片，模型需要根据图片找到感兴趣的目标，将其位置、大小标注出来。</p>\r\n<h1 id=\"两阶段检测算法从rcnn到faster-r-cnn\">两阶段检测算法，从RCNN到Faster R-CNN</h1>\r\n<p>目前的目标检测方法主要分为：两阶段的检测器、单阶段检测器两种，RCNN到Faster RCNN系列检测器是典型的两阶段检测器的代表。</p>\r\n<h2 id=\"r-cnn\">R-CNN</h2>\r\n<p>论文《Rich feature hierarchies for accurate oject detection and semantic segmentation》中提出了R-CNN模型用于目标检测，其基本流程如下： - 使用Selective Search算法提取出2k个region proposal。 - 对于每个region proposal，先裁剪出对应区域的图片，然后使用AlexNet进行特征提取（特征取最后那个softmax层的输入，是一个4096维的向量）。 - 训练K个线性SVM分类器，对每个region proposal进行K次分类，得到其属于K个类别的得分。 - 对每个类别的proposal进行NMS后处理，得到最终的结果。</p>\r\n<p>这里的Selective Search算法本质上是一种先分割图片然后再尝试各种组合的穷举算法，这里没有详细研究。</p>\r\n<p>整体来说，R-CNN的流程非常粗暴，主要有两个阶段：1、获取region proposal，2、对region proposal进行分类，但这也是后面的两阶段检测方法的一般流程。</p>\r\n<h2 id=\"spp-net\">SPP-net</h2>\r\n<p>在R-CNN中存在的一个问题是region proposal的大小不相等，但是AlexNet要求固定大小的输入图片，否则特征维度会发生变换（如果现在来看这个问题的话，加个global pooling就OK了，但是当时没想到），因此在RCNN里面只能通过缩放，让region proposal满足输入大小的要求，这样带来了畸变的问题，为了解决这个问题，论文《Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition》提出了空间金字塔池化操作(Spatial Pyramid Pooling，SPP)，而且顺便结局了RCNN中对每个region proposal都提取一次特征带来的重复计算问题。</p>\r\n<p>在SPP-net中，首先将整幅图作为输入（这里使用的模型不再是AlexNet，这里试验了ZF-5、Convnet*-5、Overfeat-5、Overfeat-7这几个模型，我这里不再详细研究），提取出特征图之后，再从特征图上抠出来region proposal对应的区域（这里不同于RCNN对每个region proposal进行一次特征提取，所以解决了重复计算问题），然后将抠出来的区域进行空间金字塔池化操作，使其变成固定大小的特征向量。再使用SVM对特征进行分类。</p>\r\n<p>SPP操作的示意图如下所示，主要思想是将特征图先划分成固定数量的网格，然后再每个格子内进行池化操作，最后将固定大小的池化结果展开、拼接成固定大小的特征向量。</p>\r\n<figure>\r\n<img src=\"SPP.png\" alt=\"SPP操作示意\" /><figcaption aria-hidden=\"true\">SPP操作示意</figcaption>\r\n</figure>\r\n<h2 id=\"fast-r-cnn\">Fast R-CNN</h2>\r\n<p>有了SPP-net的思路之后，RCNN也做出了相应的改变，在论文《Fast R-CNN》中，接纳了SPP-net中先全图特征提取再裁剪region proposal区域的方法，然后对SPP-net进行改进：</p>\r\n<ol type=\"1\">\r\n<li>只使用一个固定大小的网格分割，称为ROI Pooling层（并提出了ROI Pooling层的反向传播方法）。（至于为什么要降SPP改成ROI Pooling，可能主要是ROI Pooling有反向传播功能，其实如果SPP也实现了反向传播，那么ROI Pooling不一定比SPP效果好，不知道有没有人进行过对比）</li>\r\n<li>特征提取模型换成了VGG。</li>\r\n<li>对特征的处理采用神经网络，不再使用SVM，对于分类，使用softmax函数得到类别概率。</li>\r\n<li>特征除了用于分类之外，还用于更精确的位置回归（使用smooth L1损失），并且让这两个任务共享一部分全连接层，提出了多任务损失函数的训练方法。</li>\r\n</ol>\r\n<p>Fast R-CNN的简略流程示意图如下所示： <img src=\"FastRCNN.png\" alt=\"Fast R-CNN示意图\" /></p>\r\n<h2 id=\"faster-r-cnn\">Faster R-CNN</h2>\r\n<p>Fast R-CNN还是没有做到完全的端到端得到输出结果，因为还是要依赖Selective Search算法来生成region proposal。</p>\r\n<p>在论文《Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks》中，提出了Faster R-CNN，其中使用一个区域建议网络（Region Proposal Network，RPN）来替代Selective Search算法进行region proposal工作。</p>\r\n<p>RPN的思路应该来源于Fast R-CNN中的位置回归操作，Fast R-CNN中证明神经网络可以用于位置回归问题，因此自然就会有一个想法是使用神经网络来对输出候选位置。但是如果直接回归位置，有两个问题：</p>\r\n<ol type=\"1\">\r\n<li>特征图上某个像素点，应该回归哪个目标的位置（在之前的方法中，位置回归的目标很明确，因为region proposal有了之后，查找数据标签，可以很容易知道对于这个位置应该去回归哪个目标，或者这里面没有目标，那么就不用回归位置）</li>\r\n<li>如果是输出像素坐标，那么对于不同大小的图片，不同尺度和比例的目标，回归任务很难收敛。</li>\r\n</ol>\r\n<p>对于上面两个问题，这里提出了Anchor的机制，首先在图片上按照一定的规律分布好各种尺度和比例的方框（Anchor box，一个像素点可以有多个Anchor box），然后每个像素点（Anchor）输出anchor box的类别（前景还是背景）、缩放和偏移量。</p>\r\n<p>每个anchor box要么匹配一个目标（其分类类别为这个目标的类别且回归目标为这个目标的位置），要么在训练过程中将其分类为背景类，并且不计算其回归输出的loss。</p>\r\n<p>RPN和也是用和RCNN一样的特征提取网络（这个特征提取网络一般称为Backbone）。</p>\r\n<h3 id=\"faster-r-cnn中的预测结果计算\">Faster R-CNN中的预测结果计算</h3>\r\n<p>Faster R-CNN的RPN和RCNN对于回归量的输出都是<span class=\"math inline\">\\(\\delta_x, \\delta_y, \\delta_h, \\delta_w\\)</span>，分别表示中心点的偏移值，以及box长宽的缩放值，而且不是像素单位，因此需要转换成像素单位需要进行一些转换如下，这里的<span class=\"math inline\">\\(b_x, b_y, b_h, b_w\\)</span>分别表示最终预测box的中心和长宽，然后<span class=\"math inline\">\\(p_x, p_y, p_h, p_w\\)</span>分别表示先验box（在RPN阶段先验box就是Anchor，在RCNN阶段先验box就是RPN的Proposal Box）的中心和长宽。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n  b_x &amp;= p_x + p_w \\delta_x\\\\\r\n  b_y &amp;= p_y + p_h \\delta_y\\\\\r\n  b_w &amp;= p_w \\exp(\\delta_w)\\\\\r\n  b_h &amp;= p_h \\exp(\\delta_h)\\\\\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p><strong>这个计算方式可能导致的问题：1、<span class=\"math inline\">\\(b_x,b_y\\)</span>可能超出先验框的范围，2、由于roi pooling之后，RCNN只能看到Proposal Box之内的内容，因此RCNN预测得到的<span class=\"math inline\">\\(\\delta_w,\\delta_h\\)</span>如果大于0，那么则表示预测的box范围超出了Proposal Box区域，可能存在预测不准确的问题，当然，由于特征图上的像素点具有一定的感受野范围，因此按理说RCNN还是可以看到一些Proposal Box之外的信息的。</strong></p>\r\n<h3 id=\"为什么要使用smooth-l1作为回归任务的损失函数\">为什么要使用Smooth L1作为回归任务的损失函数</h3>\r\n<p>在回归损失函数中，最常用的是L2损失函数，<span class=\"math inline\">\\(L = (h(x;\\theta) - y) ^ 2\\)</span>，其导数<span class=\"math inline\">\\(\\frac{\\partial L}{\\partial \\theta} = \\frac{\\partial h(x;\\theta)}{\\partial \\theta}(h(x;\\theta) -y)\\)</span>，这里的问题在于：如果模型初始化得不好，再训练开始时<span class=\"math inline\">\\(h(x;\\theta) - y\\)</span>特别大，则会导致梯度特别大，模型将难以训练。那么一个自然地想法是使用L1损失函数，但是L1损失函数不是处处可导，梯度会发生突变，也不利于训练，因此将L1损失函数进行一些平滑操作，给出了smooth L1损失函数如下所示： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    L(\\theta) = \\begin{cases}\r\n        0.5 x^2 &amp;|x| &lt; 1\\\\\r\n        |x| - 0.5 &amp;|x| &gt;= 1\r\n    \\end{cases}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<h3 id=\"rpn训练时的loss计算需要对负样本进行采样\">RPN训练时的loss计算需要对负样本进行采样</h3>\r\n<p>RPN的训练目标是将Anchor Box进行前景和背景分类，并且对分类为前景的Anchor Box进行位置偏移量和缩放量的回归。</p>\r\n<p>在训练时，需要首先分配Anchor Box的训练目标：给定一张图像样本上的所有gt box标签，首先需要计算所有gt box和每个Anchor Box的iou。</p>\r\n<p>对于每一个Anchor Box，找到和其iou最大的gt box，按照和这个gt box之间的iou，有三种可能： 1. 如果大于某个iou阈值，那么则被分配给这个gt box作为前景来训练（即这个Anchor Box的分类目标是正样本，回归目标是对应的gt box）。 2. 如果小于某个iou阈值，则作为背景类处理（即这个Anchor Box的分类目标是负样本，没有回归目标，不计算回归损失）。 3. 如果iou不大又不小，则忽略这个Anchor Box，计算损失的过程中不考虑，这是因为一些Anchor Box包含了一部分目标，但是又包含得不够多，不能武断的将其作为前景或者背景来训练，因此干脆不管这一部分的训练，反正参数是共享的，其他的地方训练好之后，测试过程中这种地方输出是什么样影响不大。</p>\r\n<p>在分配Anchor Box训练目标的过程中，绝大部分Anchor Box是被分配成负样本（背景类）的，，因此RPN训练时存在前景和背景两个类别不平衡的情况。所以一般的Faster R-CNN实现中，RPN的训练部分，会对Anchor Box进行采样。(例如mmdetection中的实现中，RPN部分的默认配置是采样的256个Anchor Box进行训练，首先随机采样128个正样本，然后采样128个负样本，如果正样本不够，那么负样本多采样一些填满256个)</p>\r\n<h1 id=\"多阶段检测器\">多阶段检测器</h1>\r\n<h2 id=\"cascade-r-cnn\">Cascade R-CNN</h2>\r\n<p>在论文《Cascade R-CNN: Delving into High Quality Object Detection》中指出了Faster R-CNN中按照某个iou阈值分配Proposal正负样本存在的问题：iou阈值过低会导致大量的低阈值预测框，对于regression head来说非常难修正，因此会出现大量的close false positive预测（即iou不够从而被判定为false positive的预测），iou阈值过大又导致匹配到的正样本非常少，容易引起过拟合或者类别不平衡问题。</p>\r\n<p>Cascade R-CNN中为了解决这个问题，提出了使用多级head，将预测iou逐渐提高的方法。</p>\r\n<p>Cascade R-CNN提出的多级head预测如下图(d)所示，这里的H1、H2、H3都是独立参数，且在训练过程中使用的iou不相同。</p>\r\n<figure>\r\n<img src=\"Cascade_R-CNN.png\" alt=\"cascade rcnn多级head示意图\" /><figcaption aria-hidden=\"true\">cascade rcnn多级head示意图</figcaption>\r\n</figure>\r\n<p>Cascade R-CNN的思路很简单：在理想状况中，H1接收RPN输出的ROI Proposal作为B0，对B0进行分类和修正，并从中筛除一些背景或者iou较低的box得到新的预测框集合B1，H2则接收B1作为输入，对B1进行分类和修正，得到B2，H3对B2进行同样的操作，得到最终的预测输出。</p>\r\n<p>三个步骤不同之处首先在于背景类的含义，例如存在三个iou阈值<span class=\"math inline\">\\(u_1 &lt; u_2 &lt; u_3\\)</span>，在H1中低于<span class=\"math inline\">\\(u_1\\)</span>的才被认为是负样本，而H2中低于<span class=\"math inline\">\\(u_2\\)</span>的被认为是负样本，H3则使用<span class=\"math inline\">\\(u_2\\)</span>作为负样本阈值，这样设置的意思就是每个Head负责筛掉不同精度的预测框，另外，就算这里<span class=\"math inline\">\\(u_3\\)</span>较高，也不会引起之前iou阈值高导致的匹配到的正样本非常少的问题，因为原始RPN的proposal虽然可能iou较低，但是经过H1、H2的逐步修正之后，B2之中iou普遍比较高了，即使<span class=\"math inline\">\\(u_3\\)</span>较高，也能匹配到较多的正样本用于H3的训练。</p>\r\n<p>在预测过程中，Cascade R-CNN的不同Head先后进行预测，最终的分类得分将综合三个Head的预测结果（例如C1、C2、C3取平均），而回归结果直接使用H3得到的B3。</p>\r\n<h1 id=\"一阶段目标检测方法\">一阶段目标检测方法</h1>\r\n<p>一阶段目标检测方法主要是相对于Faster R-CNN这类二阶段检测方法来说的，在Faster R-CNN中，主要被分为region proposal和proposal的分类回归。</p>\r\n<p>一阶段目标检测方法将region proposal这一步去掉了，不需要RPN来进行初步的定位，而是直接得到box输出结果。</p>\r\n<p>一阶段检测器按照时间顺序主要有：YOLO v1、SSD、DSSD、YOLO v2、Retinanet、YOLO v3、YOLO v4等，这里做个简单的梳理。</p>\r\n<h2 id=\"yolo-v1\">YOLO v1</h2>\r\n<p>论文《You Only Look Once:Unified, Real-Time Object Detection》中提出了YOLO检测方法，其基本思想是将一幅图像分成<span class=\"math inline\">\\(S \\times S\\)</span>的网格，如果某个目标的中心落在这个网格中，那么这个网格就负责预测这个目标。</p>\r\n<p>网格中的一个格子可以输出<span class=\"math inline\">\\(B\\)</span>个bounding boxes，每个格子的输出如下：</p>\r\n<ul>\r\n<li><span class=\"math inline\">\\(B\\)</span>个bounding boxes的位置回归信息（中心偏移：x,y（使用格子大小来归一化）；box的宽高：w,h（使用图片的长宽来归一化））共<span class=\"math inline\">\\(4\\times B\\)</span>个值。</li>\r\n<li>bounding boxes对应的<span class=\"math inline\">\\(B\\)</span>个confidence scores（这个confidence score表示当前bounding box包含目标的概率乘以bounding box位置回归的置信度），其的回归目标定义为<span class=\"math inline\">\\(Pr(Object) \\times IOU^{truth}_{pred}\\)</span>。</li>\r\n<li>C个类别信息，表示为<span class=\"math inline\">\\(Pr(Class_i|Object),i = 1,2,...,C\\)</span>，这是个条件概率，其含义是如果这个格子有目标，那么这个目标属于不同的类别的概率。</li>\r\n</ul>\r\n<p>因此一个格子的输出个数是<span class=\"math inline\">\\(5 \\times B + C\\)</span>，整个YOLO v1模型的输出个数是<span class=\"math inline\">\\(S\\times S \\times (5 \\times B + C)\\)</span>。</p>\r\n<p>在YOLO v1模型中，输入图片大小固定为<span class=\"math inline\">\\(448 \\times 448\\)</span>，经过6次下采样之后变成<span class=\"math inline\">\\(7\\times 7\\)</span>大小的特征图，作者取<span class=\"math inline\">\\(S=7\\)</span>，<span class=\"math inline\">\\(B=2\\)</span>，Pascal VOC的类别个数<span class=\"math inline\">\\(C=20\\)</span>，因此YOLO v1的输出是个<span class=\"math inline\">\\(7\\times 7 \\times 30\\)</span>的特征图，整个模型结构图如下所示，其中激活函数使用Leaky ReLU，参数为0.1。</p>\r\n<figure>\r\n<img src=\"YOLOv1.png\" alt=\"YOLO v1模型结构示意\" /><figcaption aria-hidden=\"true\">YOLO v1模型结构示意</figcaption>\r\n</figure>\r\n<h3 id=\"损失函数\">损失函数</h3>\r\n<p>YOLO v1使用平方和误差来进行训练，但是论文中表示这样的损失函数不能完美的匹配优化Average Precision的目标（就是说定位误差和分类误差的权重相等不太合适），而且每张图片上不包含目标的格子居多，这些格子主导了梯度，导致预测置信度（confidence score）普遍偏低，同时模型训练过程容易震荡，因此作者在损失函数中引入了两个超参数：<span class=\"math inline\">\\(\\lambda_{coord} = 5\\)</span>、<span class=\"math inline\">\\(\\lambda_{noobj} = 0.5\\)</span>作为不同损失的权重。</p>\r\n<p>另外，论文中还提到平方和损失在回归位置的时候，还有个问题是large box和small box对于同样的宽高偏差敏感度不同，为了缓解这个问题，论文中提出直接让模型预测<span class=\"math inline\">\\(w,h\\)</span>的平方，需要获取真正的长宽时，需要将模型的预测输出开方进行还原。</p>\r\n<p>最终YOLO v1的损失函数如下： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    &amp;\\lambda_{coord} \\sum\\limits_{i=0}^{S^2} \\sum\\limits_{j=0}^{B} \\mathbf{1}^{obj}_{ij}[(x_i - \\hat{x}_i)^2 + (y_i - \\hat{y}_i)^2]\\\\\r\n    &amp;+ \\lambda_{coord} \\sum\\limits_{i=0}^{S^2} \\sum\\limits_{j=0}^{B} \\mathbf{1}^{obj}_{ij}[(w_i - \\sqrt{\\hat{w}_i})^2 + (h_i - \\sqrt{\\hat{h}_i})^2]\\\\\r\n    &amp;+ \\sum\\limits_{i=0}^{S^2} \\sum\\limits_{j=0}^{B} \\mathbf{1}^{obj}_{ij}(C_i - \\hat{C}_i)^2\\\\\r\n    &amp;+ \\lambda_{noobj} \\sum\\limits_{i=0}^{S^2} \\sum\\limits_{j=0}^{B} \\mathbf{1}^{noobj}_{ij}(C_i - \\hat{C}_i)^2\\\\\r\n    &amp;+ \\sum\\limits_{i=0}^{S^2} \\mathbf{1}^{obj}_{i} \\sum\\limits_{c \\in Classes} (p_i(c) - \\hat{p}_i(c))^2\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>其中<span class=\"math inline\">\\(\\mathbf{1}^{obj}_{ij}\\)</span>是个指示函数，表示第<span class=\"math inline\">\\(i\\)</span>个格子的第<span class=\"math inline\">\\(j\\)</span>个box是否对应一个目标。<span class=\"math inline\">\\(\\mathbf{1}^{obj}_{i}\\)</span>则表示第<span class=\"math inline\">\\(i\\)</span>个格子是否包含目标，<span class=\"math inline\">\\(\\mathbf{1}^{noobj}_{ij} = 1 - \\mathbf{1}^{obj}_{ij}\\)</span>表示第<span class=\"math inline\">\\(i\\)</span>个格子的第<span class=\"math inline\">\\(j\\)</span>个box是否不对应一个目标。</p>\r\n<h3 id=\"bounding-box\">bounding box</h3>\r\n<p>YOLO v1中虽然一个格子对应两个bounding box，但是这里的bounding box概念和anchor box不同。</p>\r\n<p>anchor box需要设置先验大小和长宽比，然后预测输出是相对于anchor box的偏移和缩放，计算loss时需要先用anchor box去和gt box匹配，确定好优化目标才能计算loss。</p>\r\n<p>而bounding box不需要先验大小和长宽比，bounding box输出的是相对于格子中心的像素偏移（使用格子大小来归一化）以及像素长宽比（使用图片大小来归一化），计算loss时要看这个格子输出的两个bounding box哪个和gt box的iou大，iou大的那个bounding box才进行回归优化。但是每个格子中的两个bounding box都会去训练confidence score，这个的回归目标也是和bounding box的输出有关（<span class=\"math inline\">\\(Pr(Object) \\times IOU^{truth}_{pred}\\)</span>，其中<span class=\"math inline\">\\(IOU^{truth}_{pred}\\)</span>是bounding box预测值和gt box的iou）。</p>\r\n<p>这样看来YOLO v1算是一种anchor free的检测方法。</p>\r\n<h2 id=\"yolo-v2\">YOLO v2</h2>\r\n<p>在论文《YOLO9000:Better, Faster, Stronger》中对YOLO v1进行了一些改进最终提出了YOLO v2检测方法。 ### 相对于YOLO v1的改进点</p>\r\n<ul>\r\n<li>batch norm，每个卷积层后面都添加Batch Normalization层，并且不再使用droput。</li>\r\n<li>hi-res classifier，YOLO v1使用ImageNet上预训练的模型来进行初始化，这个预训练的输入图像是224大小的，但是为了提高检测结果的准确性，检测训练使用的输入大小是448，这样的切换模型难以适应，因此YOLO v2中增加了一个在ImageNet数据集上使用448大小finetune 10个epoch的过程。</li>\r\n<li>Convolutional，在YOLO v1中，最终是使用全连接层来得到最终的预测，直接预测目标像素坐标和大小，这样做导致位置定位比较困难，难以训练，论文中参考了Faster RCNN的做法，认为使用卷积的方式直接得到每个像素点相对于Anchor box的偏移缩放信息的预测方法比较简便且容易训练，因此这里将全连接层改为卷积层的同时引入了Anchor box的做法（引入了Anchor box之后，预测的类别和是否包含目标这些问题都是针对每个anchor box的，之前是一个格子一个类别，且只判断格子是否包含目标。）</li>\r\n<li>Anchor boxes，像Faster RCNN那样使用手工设计的Anchor box大小和长宽比，使用手工设计的Anchor box之后在Recall上面有提高，但是在mAP上面反而下降了一些（增加了一些假阳性）。</li>\r\n<li>new network，YOLO v2使用一种更加接近VGG的backbone：Darknet-19，相比于YOLO v1的特征提取网络，这里除了替换<span class=\"math inline\">\\(7\\times 7\\)</span>的卷积、通道个数、卷积核层个数等变化以外，下采样次数从6次变成了5次，如果输入448大小的图片，那么输出大小将变成<span class=\"math inline\">\\(14\\times 14\\)</span>，但论文中认为最终输出的格子数为奇数比较好（便于那种占满整图的大目标的中心落在格子中心而不是格子边缘），因此将输入大小调整为<span class=\"math inline\">\\(416\\)</span>，得到<span class=\"math inline\">\\(13\\times 13\\)</span>大小的输出。</li>\r\n<li>dimension clusters，论文中认为通过聚类学习Anchor box的大小设置比手工设计Anchor box的大小更便于模型回归目标位置，因此使用k-means聚类方法，从标注中的gt box形状中找到适合的Anchor box大小，在聚类时，两个box之间的距离使用<span class=\"math inline\">\\(d = 1 - IOU\\)</span>来表示（这里的IOU计算假定两个box的中心重合，这里不用考虑box的位置），作者给出的数据中，如果是手工设计的9个Anchor box大小，和gt box平均IOU在0.609，但是通过聚类学习到Anchor box，只需要5个大小就能够达到平均IOU0.61，明显好于手工设计的Anchor box先验大小。</li>\r\n<li>location prediction，在基于Anchor的预测的方法中，模型给出预测的偏移量（<span class=\"math inline\">\\(t_x, t_y\\)</span>）之后，预测的中心位置计算方式为<span class=\"math inline\">\\(x = (t_x \\times w_a) + x_a, y = (t_y \\times h_a) + y_a\\)</span>，其中<span class=\"math inline\">\\(w_a, h_a, x_a, y_a\\)</span>分别表示Anchor Box的宽、高、中心x坐标、中心y坐标，但是这样的预测方式论文中认为没有对预测box的位置进行约束（对于一个左上角的Anchor box，其预测结果可能跑到图片右下角），论文中认为这给模型带来了不利于优化的不稳定性。因此论文中提出将预测方式改为<span class=\"math inline\">\\(b_x=\\sigma(t_x) + C_x, b_y=\\sigma(t_y) + C_y, b_w = p_we^{t_w}, b_h = p_he^{t_h}, Pr(object)\\times IOU_{b}^{object} = \\sigma(t_o)\\)</span>（这里的<span class=\"math inline\">\\(C_y,C_x\\)</span>代表格子的左上角坐标，因为<span class=\"math inline\">\\(\\sigma(t_x),\\sigma(t_y)\\)</span>都是在0~1范围，因此可以保证这样预测出来的中心不会超出这个格子，<span class=\"math inline\">\\(p_w, p_h\\)</span>则代表Anchor Box的宽高，可以见后面的YOLOv2预测方式示意图），这里的意思是，对于每个Anchor Box，都模型都输出四个值<span class=\"math inline\">\\(\\sigma(t_x), \\sigma(t_y), t_w, t_h, \\sigma(t_o)\\)</span>，对于模型的输出值，计算出最终预测框的中心：<span class=\"math inline\">\\(b_x,b_y\\)</span>以及预测框的宽高：<span class=\"math inline\">\\(b_w, b_h\\)</span>，以及预测框的置信度：<span class=\"math inline\">\\(Pr(object)\\times IOU_{b}^{object}\\)</span>。</li>\r\n<li>passthrough，将<span class=\"math inline\">\\(26\\times 26\\times 512\\)</span>的特征图resize成<span class=\"math inline\">\\(13\\times 13\\times 2048\\)</span>然后和最后的<span class=\"math inline\">\\(13\\times13\\times 1024\\)</span>的特征图进行Concatenate操作，共同用于模型的最终预测，论文中解释说是为了提供更加细粒度的特征。</li>\r\n<li>multi-scale，因为YOLO v2抛弃了之前的全连接操作，因此可以使用不同尺度的输入进行训练，在训练过程中，每10个batch随机按照<span class=\"math inline\">\\(320:32:608\\)</span>来选择一次尺度。</li>\r\n<li>hi-res detector，这个论文中没有详细说明，不过从原文来看，这个意思就是在预测的时候使用<span class=\"math inline\">\\(544\\times 544\\)</span>大小的图片。</li>\r\n</ul>\r\n<p>以上各种改进的消融实验结果见下表：</p>\r\n<figure>\r\n<img src=\"YOLOv2_exp.png\" alt=\"YOLOv2各改进的消融实验结果\" /><figcaption aria-hidden=\"true\">YOLOv2各改进的消融实验结果</figcaption>\r\n</figure>\r\n<p>location prediction这一项改进中所描述的预测方法示意图：</p>\r\n<figure>\r\n<img src=\"YOLOv2_predict.png\" alt=\"YOLOv2预测方式示意图\" /><figcaption aria-hidden=\"true\">YOLOv2预测方式示意图</figcaption>\r\n</figure>\r\n<p>这里有个细节是在<span class=\"math inline\">\\(b_x=\\sigma(t_x) + C_x, b_y=\\sigma(t_y) + C_y\\)</span>这两个运算里面所有的项都不是像素单位，如果想要转换成像素单位，那么需要除以S，然后乘上图片的或着高</p>\r\n<p>另外为了节省计算量，passthrough还有个改进版本：增加一次卷积操作，先将<span class=\"math inline\">\\(26\\times 26\\times 512\\)</span>的特征图变为<span class=\"math inline\">\\(26\\times 26\\times 64\\)</span>的特征图，然后再resize到<span class=\"math inline\">\\(13\\times 13\\times 256\\)</span>，最后拿去和<span class=\"math inline\">\\(13\\times13\\times 1024\\)</span>的特征图进行Concatenate操作。</p>\r\n<h3 id=\"yolo-v2的损失函数\">YOLO v2的损失函数</h3>\r\n<p>这部分其实类似YOLO v1，全都是L2损失，只不过其中YOLO v1使用的那个开方的操作这里不用了，而且计算的类别是相对于每个Anchor来说的，其输出大小为<span class=\"math inline\">\\(S\\times S \\times B \\times (5 + C)\\)</span>（这里的<span class=\"math inline\">\\(B\\)</span>是anchor个数），其他损失的计算和YOLO v1类似，论文中没有详细描述，如果感兴趣可以去研究YOLO v2的实现代码。</p>\r\n<h2 id=\"yolo-v3\">YOLO v3</h2>\r\n<p>在论文《YOLOv3: An Incremental Improvement》中提出了YOLO v3检测方法。</p>\r\n<p>在YOLO v3中，相对于YOLO v2，首先更换了Backbone，从YOLO v2的Darknet-19改成了Darknet-53（根据论文中提供的Backbone分类任务训练结果中，Darknet-53的分类性能可以和ResNet-152比肩，但是速度是ResNet-152的两倍），这个新backbone中引入了resnet中的残差模块，顺便将Darknet-19中的池化层全替换成了步长为2的卷积层。</p>\r\n<p>YOLO v3的另外一个主要改进是借鉴了FPN的思路，不仅在<span class=\"math inline\">\\(13\\times 13\\)</span>大小的特征图上进行预测，同时也在<span class=\"math inline\">\\(26\\times 26\\)</span>，<span class=\"math inline\">\\(52\\times 52\\)</span>大小的特征图上进行预测，三种大小的特征图上分类的anchor box都是每个格子三个，但是大小不同，在<span class=\"math inline\">\\(52\\times 52\\)</span>大小的特征图的Anchor Box最小，在<span class=\"math inline\">\\(13\\times 13\\)</span>大小的特征图的Anchor Box最大，下图中最后的输出部分，每个像素点有255个预测值，其实就是<span class=\"math inline\">\\(3 \\times (5 + 80)\\)</span>，其中3是Anchor Box个数，80是COCO数据集的类别个数。</p>\r\n<p>在YOLO v3中，还有一个地方是其目标分配机制和YOLO v2稍有不同，对于一个格子，如果目标落在了这个格子中（这里因为三个不同大小的特征图都有输出，因此一个目标肯定会落在三个格子中，分别位于三个特征图上，这就关系到了<span class=\"math inline\">\\(3\\times 3 = 9\\)</span>个Anchor Box，YOLO v2因为是单尺度特征图预测，因此只需要考虑其设定的5个Anchor Box），YOLO v3在处理这个问题的时候，计算了这9个Anchor Box和当前目标的iou，iou最大的那个Anchor Box用于预测当前目标，其余Anchor Box如果iou大于某个阈值（），则计算损失时忽略其置信度损失，否则当成负样本处理。</p>\r\n<p>YOLO v3论文中并没有给出详细的结构描述图，下面的结构描述图来自于<a href=\"https://blog.csdn.net/leviopku/article/details/82660381\">CSDN博客《yolo系列之yolo v3【深度解析】》</a></p>\r\n<figure>\r\n<img src=\"YOLOv3结构示意.png\" alt=\"YOLOv3结构示意\" /><figcaption aria-hidden=\"true\">YOLOv3结构示意</figcaption>\r\n</figure>\r\n<p>YOLO v3在COCO上进行的对比实验结果如下所示，这里可以看出YOLO v3在精度上并没有特别明显的竞争力，但是如果精度要求不是那么高的话，YOLO v3在速度上超越其他的单阶段检测器很多。</p>\r\n<figure>\r\n<img src=\"YOLOv3_exp.png\" alt=\"YOLOv3 对比实验结果\" /><figcaption aria-hidden=\"true\">YOLOv3 对比实验结果</figcaption>\r\n</figure>\r\n<h2 id=\"yolo-v4\">YOLO v4</h2>\r\n<p>YOLO v4在论文《YOLOv4: Optimal Speed and Accuracy of Object Detection》中被提出，其主要结构组件如下： - backbone：CSPDarknet53 - neck：SPP、PANet - head：YOLO v3</p>\r\n<p>另外YOLO v4中使用了很多技巧：</p>\r\n<ul>\r\n<li>Bag of Freebies (BoF) for backbone：\r\n<ul>\r\n<li>CutMix and Mosaic data augmentation</li>\r\n<li>DropBlock regularization</li>\r\n<li>Class label smoothing</li>\r\n</ul></li>\r\n<li>Bag of Specials (BoS) for backbone：\r\n<ul>\r\n<li>Mish activa-tion</li>\r\n<li>Cross-stage partial connections (CSP)</li>\r\n<li>Multi-input weighted residual connections (MiWRC)</li>\r\n</ul></li>\r\n<li>Bag of Freebies (BoF) for detector：\r\n<ul>\r\n<li>CIoU-loss</li>\r\n<li>CmBN</li>\r\n<li>DropBlock</li>\r\n<li>regularization</li>\r\n<li>Mosaic data augmentation</li>\r\n<li>Self-Adversarial Training</li>\r\n<li>Eliminate grid sensitivity</li>\r\n<li>Using multiple anchors for a single groundtruth</li>\r\n<li>Cosine annealing scheduler</li>\r\n<li>Optimal hyper-parameters</li>\r\n<li>Random training shapes</li>\r\n</ul></li>\r\n<li>Bag of Specials (BoS) for detector：\r\n<ul>\r\n<li>Mish activation</li>\r\n<li>SPP-block</li>\r\n<li>SAM-block</li>\r\n<li>PAN path-aggregation block</li>\r\n<li>DIoU-NMS</li>\r\n</ul></li>\r\n</ul>\r\n<p>其中Bag of Freebies (BoF)表示只在训练过程中增加时间成本，不影响预测过程的技巧，Bag of Specials (BoS)表示在预测过程中会稍微增加一点时间成本的技巧。</p>\r\n<p>YOLO v4基本上算是个大型炼丹现场，作者在论文中都是用实验数据证明各种操作的效果，很少有理论分析，因此这篇论文更适合作为检测任务寻找提分手段的一个手册（一切以实验数据说话）。</p>\r\n<h1 id=\"多尺度问题\">多尺度问题</h1>\r\n<h2 id=\"特征金字塔网络feature-pyramid-networkfpn\">特征金字塔网络(Feature Pyramid Network，FPN)</h2>\r\n<p>在论文《Feature Pyramid Networks for Object Detection》中提出的特征金字塔操作已经是目前目标检测模型上的标准配置，无论是单阶段检测模型还是两阶段检测模型，均可以添加FPN结构来让模型获得更好的尺度鲁棒性。</p>\r\n<p>FPN的结构示意图如下，其中左上角从下到上是检测模型中backbone的特征提取过程，每一层代表不同stride的特征图，原本的检测模型，例如Fatser R-CNN,是在最上面一个特征来进行roi proposal、roi classification和roi regression等操作，在加入FPN之后，获得了右上角的一系列特整图，不同stride的特征图预测不同大小的目标。</p>\r\n<figure>\r\n<img src=\"FPN.png\" alt=\"FPN结构示意图\" /><figcaption aria-hidden=\"true\">FPN结构示意图</figcaption>\r\n</figure>\r\n<p>上图中画得不是很完整，一般backbone中会进行5次下采样，得到的特征图stride分别是2、4、8、16、32，对应的特征图称为C1、C2、C3、C4、C5，在mmdetection检测框架的Faster R-CNN-FPN的实现中，C5将通过大小<span class=\"math inline\">\\(3\\times 3\\)</span>，stride为2的卷积下采样得到P6，另外C5经过<span class=\"math inline\">\\(1\\times 1\\)</span>卷积之后变为P5，P5上采样后和C4的<span class=\"math inline\">\\(1\\times 1\\)</span>卷积结果相加变成P4，P4再上采样和C3的<span class=\"math inline\">\\(1\\times 1\\)</span>卷积结果相加变成P3，以此类推，分别得到P2、P3、P4、P5、P6，其中P2、P3、P4、P5、P6均参与RPN的roi proposal，而对于不同大小的roi，则在P2、P3、P4、P5中选择对应尺度的特征图来进行roi pooling/roi align。</p>\r\n<p>原始FPN论文中，对于宽高为<span class=\"math inline\">\\(w,h\\)</span>的roi，其对应的特征金字塔等级计算方法如下，这里设置<span class=\"math inline\">\\(k_0=4\\)</span>表示第4级特征金字塔，即<span class=\"math inline\">\\(P5\\)</span>，因为ImageNet预训练的backbone，一般使用224大小的输入图像，而且最终结果是在C5得到的，因此这里让224以上的目标对应于<span class=\"math inline\">\\(P5\\)</span>特征图，小于224大小的目标则每小一倍，对应的特征等级减一 <span class=\"math display\">\\[\r\nlevel = \\lfloor k_0 + \\log_2(\\frac{\\sqrt{wh}}{224})\\rfloor\r\n\\]</span></p>\r\n<p>在mmdetection框架的实现中，roi的分配则和原论文不同，mmdetection中，计算方式如下： - scale &lt; finest_scale * 2: level 0 - finest_scale * 2 &lt;= scale &lt; finest_scale * 4: level 1 - finest_scale * 4 &lt;= scale &lt; finest_scale * 8: level 2 - scale &gt;= finest_scale * 8: level 3</p>\r\n<p>这里的scale是指<span class=\"math inline\">\\(\\sqrt{wh}\\)</span>即roi的尺度，finest_scale默认值为56，这里的level 0对应P2，level 1对应P3，以此类推。</p>\r\n<p>FPN在保证大目标的检测效果的同时，可以大幅提高小目标的检测效果，我认为其原因在于小目标物体的检测需要更加精细的位置信息，如果使用stride过大的特征图，会导致小目标的预测不够精确，同时小目标的信息容易被大目标掩盖，FPN中stride较小的特征图上融合了深层的特征图的信息，这可以让其的感受野非常大，而且感受野分布比较密集，小目标的检测可能也需要大感受野信息同时加上细粒度空间信息。</p>\r\n"},{"title":"算法总结","date":"2020-05-19T04:15:36.000Z","mathjax":true,"_content":"# 排序\n\n## 冒泡排序\n逐对交换，每次迭代，确定一个元素的最终位置。\n\n时间复杂度：$O(N^2)$\n\n空间复杂度：$O(1)$\n```C++\nint* bubbleSort(int* A, int n) {\n    if(A == NULL || n <= 1){\n        return A;\n    }\n    int temp;\n    for(int i = 1; i < n - 1; ++i){\n        for(int j = 0; j < n - i; ++j){\n            if(A[j] > A[j+1]){\n                temp = A[j+1];\n                A[j+1] = A[j];\n                A[j] = temp;\n            }\n        }\n    }\n    return A;\n}\n```\n\n## 选择排序\n\n每次选择一个剩下的最小或者最大元素放到对应位置。\n\n时间复杂度：$O(N^2)$\n\n空间复杂度：$O(1)$\n```C++\nint* selectionSort(int* A, int n) {\n    if(A == NULL || n <= 1){\n        return A;\n    }\n    int min_value, j, min_index;\n    for(int i = 0;i<n-1;++i){\n        min_value = 0x7fffffff;\n        min_index = j = i;\n        while(j < n){\n            if(A[j] < min_value){\n                min_value = A[j];\n                min_index = j;\n            }\n            ++j;\n        }\n        if(min_index != i){\n            A[i] ^= A[min_index];\n            A[min_index] ^= A[i];\n            A[i] ^= A[min_index];\n        }\n    }\n    return A;\n}\n```\n\n## 插入排序\n时间复杂度：$O(N^2)$\n\n空间复杂度：$O(1)$\n```C++\nint* insertionSort(int* A, int n) {\n    if(A == NULL || n <= 1){\n        return A;\n    }\n    int cur, j;\n    for(int i=1;i<n;++i){\n        cur = A[i];\n        for(j = i-1; j>= 0; --j){\n            if(cur < A[j]){\n                A[j+1] = A[j];\n            }else{\n                break;\n            }\n        }\n        A[j+1] = cur;\n    }\n    return A;\n}\n```\n\n## 归并排序\n时间复杂度：$O(N\\log(N))$\n\n空间复杂度：$O(N)$\n```C++\nint* merge(int* A, int lenA, int*B, int lenB){\n    int *result = new int[lenA + lenB];\n    int i=0,j=0;\n    while(i < lenA && j < lenB){\n        if(A[i] <= B[j]){\n            result[i+j] = A[i];\n            ++i;\n        }else{\n            result[i+j] = B[j];\n            ++j;\n        }\n    }\n    while(i < lenA){\n        result[i+j] = A[i];\n        ++i;\n    }\n    while(j < lenB){\n        result[i+j] = B[j];\n        ++j;\n    }\n    return result;\n}\nint* mergeSort(int* A, int n) {\n    if(A == NULL || n <= 1){\n        return A;\n    }\n    int center = n / 2;\n    return merge(mergeSort(A, center), center, mergeSort(A + center, n - center), n - center);\n}\n```\n\n## 快速排序\n时间复杂度：$O(N\\log(N))$\n\n空间复杂度：$O(\\log(N))$ ~ $O(N)$\n```C++\nint partition(int* A, int n){\n    if(n <= 1){\n        return 1;\n    }else{\n        int small = -1;\n        int i = 0, j = n-2;\n        while(i <= j){\n            if(A[i] <= A[n-1]){\n                small = i++;\n            }else if(A[j] <= A[n-1]){\n                if(j != i){\n                    A[j] ^= A[i];\n                    A[i] ^= A[j];\n                    A[j] ^= A[i];\n                }\n                small = i++;\n            }else{\n                --j;\n            }\n        }\n        if(small + 1 != n - 1){\n            A[small + 1] ^= A[n - 1];\n            A[n - 1] ^= A[small + 1];\n            A[small + 1] ^= A[n - 1];\n        }\n        return small + 1;\n    }\n}\nint* quickSort(int* A, int n) {\n    if(A == NULL || n <= 1){\n        return A;\n    }else{\n        int center = partition(A, n);\n        quickSort(A, center);\n        quickSort(A+center+1, n-center-1);\n        return A;\n    }\n}\n```\n\n## 堆排序\n时间复杂度：$O(N\\log(N))$\n\n空间复杂度：$O(1)$\n\n```C++\nint parent(int i){\n    return (i - 1) / 2;\n}\nint left(int i){\n    return i * 2 + 1;\n}\nint right(int i){\n    return i * 2 + 2;\n}\n\nvoid DownHeap(int* A, int cur, int n){\n    int left_idx, right_idx, max_idx, end = parent(n - 1);\n    while(cur <= end){\n        left_idx = left(cur);\n        right_idx = right(cur);\n        max_idx = A[cur] < A[left_idx]? left_idx : cur;\n        if(right_idx < n){\n            max_idx = A[max_idx] < A[right_idx]? right_idx : max_idx;\n        }\n        if(max_idx != cur){\n            A[cur] ^= A[max_idx];\n            A[max_idx] ^= A[cur];\n            A[cur] ^= A[max_idx];\n            cur = max_idx;\n        }else{\n            break;\n        }\n    }\n}\nvoid createHeap(int* A, int n){\n    int end = parent(n - 1);\n    for(int i = end; i >= 0; --i){\n        DownHeap(A, i, n);\n    }\n}\nint* heapSort(int* A, int n){\n    if(A == NULL || n <= 1){\n        return A;\n    }\n    createHeap(A, n);\n    for(int i = n-1;i>0;--i){\n        DownHeap(A, 0, i+1);\n        A[i] ^= A[0];\n        A[0] ^= A[i];\n        A[i] ^= A[0];\n    }\n    return A;\n}\n\nvoid DownHeap(vector<int>& heap, int i, int n){\n    // 大顶堆的下滤操作\n    int temp = heap[i];\n    int l,r,maxChild=i;\n    while(true){\n        l = i * 2 + 1;\n        r = l + 1;\n        maxChild = (r < n && heap[r] > heap[l]) ? r : l;\n        maxChild = (maxChild < n && heap[maxChild] > temp) ? maxChild : i;\n        if(maxChild != i){\n            heap[i] = heap[maxChild];\n            i = maxChild;\n        }else{\n            break;\n        }\n    }\n    heap[i] = temp;\n    return ;\n}\n\nvector<int> sortArray(vector<int>& nums) {\n    //堆排序\n    int n = nums.size();\n    //构建大顶堆\n    for(int i = n / 2; i >= 0; --i){\n        DownHeap(nums, i, n);\n    }\n\n    //逐个删除堆顶，进行排序\n    for(int i = n; i > 1; --i){\n        swap(nums[0], nums[i - 1]);\n        DownHeap(nums, 0, i - 1);\n    }\n    return nums;\n}\n```\n\n## 希尔排序\n时间复杂度：$O(N\\log(N))$\n\n空间复杂度：$O(1)$\n```C++\nvoid insertionSort(int* A, int n, int space) {\n    int cur, j;\n    for(int i=space;i<n;i++){\n        cur = A[i];\n        for(j = i-space; j>= 0; j-=space){\n            if(cur < A[j]){\n                A[j+space] = A[j];\n            }else{\n                break;\n            }\n        }\n        A[j+space] = cur;\n    }\n}\nint* shellSort(int* A, int n) {\n    if(A == NULL || n <= 1){\n        return A;\n    }\n    for(int space = n / 2; space>=1; space/= 2){\n        insertionSort(A, n, space);\n    }\n    return A;\n}\n\nvector<int> shellSort(vector<int>& nums) {\n    //希尔排序\n    int n = nums.size();\n    int inc = 2;\n    int temp;\n    int i,j;\n    // 计算Hibbard增量\n    while(inc * 2 < n){\n        inc *= 2;\n    }\n    inc -= 1;\n    while(inc >= 1){\n        for(i = inc; i < n; ++i){\n            // 执行增量插入排序\n            temp = nums[i];\n            for(j = i; j >= inc && nums[j-inc] > temp; j -= inc){\n                nums[j] = nums[j-inc];\n            }\n            nums[j] = temp;\n        }\n        inc = (inc - 1) / 2;\n    }\n    return nums;\n}\n\n```\n\n## 计数排序\n\n时间复杂度：$O(N)$\n\n空间复杂度：$O(M)$，这里的$M$和待排序元素的取值范围有关。\n\n```C++\nint* countingSort(int* A, int n) {\n    if(A==nullptr || n <=1){\n        return A;\n    }\n    int min = A[0], max = A[0];\n    for(int i = 1;i < n; ++i){\n        if(A[i] > max){\n            max = A[i];\n        }else if(A[i] < min){\n            min = A[i];\n        }\n    }\n    int num = max - min + 1;\n    int *count = new int[num]();\n    for(int i = 0;i < n; ++i){\n        count[A[i] - min] += 1;\n    }\n    int i = 0, j = 0;\n    while(i < n){\n        if(count[j] > 0){\n            count[j] -= 1;\n            A[i++] = j + min;\n        }else{\n            ++j;\n        }\n    }\n    delete []count;\n    return A;\n}\n```\n\n## 基数排序\n\n时间复杂度：$O(N)$\n\n空间复杂度：$O(M)$，这里的$M$和待排序元素的取值范围有关。\n\n```C++\nint getdig(int x, int base){\n    int ubase = 1;\n    for(int i = 0; i < base; i++){\n        ubase *= 10;\n    }\n    x = x % (ubase * 10);\n    return x / ubase;\n}\n\nint* radixSort(int* A, int n) {\n    int base = 0, dig, j;\n    vector<int> temp;\n    map<int, vector<int> > buckets;\n    while(base < 4){ // 如果待排序数字全都小于10000，那么这里只用到4\n        for(int i = 0; i < n; ++i){\n            dig = getdig(A[i], base);\n            buckets[dig].push_back(A[i]);\n        }\n        j = 0;\n        for(int i = 0; i < 10; ++i){\n            temp = buckets[i];\n            for(int k = 0; k < temp.size(); ++k){\n                A[j++] = temp[k];\n            }\n            buckets[i].clear();\n        }\n        ++base;\n    }\n    return A;\n}\n```\n\n## 基本有序的情况\n如果一个序列已经基本有序，即排好序之后，其中每个元素的移动位置偏移不超过K，那么这种情况下，可以使用改进的堆排序方法，每次使用前K个数字建立小根堆（建好后最小的数字肯定在第一个位置），每次向后移动一个位置，维持小根堆，这样一直往后移动最终完成排序，其时间复杂度为$O(N \\times \\log(K))$，其次，这种情况下插入排序是比较好的，其时间复杂度为$O(N \\times K)$，改进的堆排序算法如下（这里为了实现简单，使用了额外$O(K)$的空间，其实不需要，只不过代码可能会变得比较复杂）：\n\n```C++\nint parent(int i){\n    return (i - 1) / 2;\n}\nint left(int i){\n    return i * 2 + 1;\n}\nint right(int i){\n    return i * 2 + 2;\n}\nvoid modifyHeap(int *A, int cur, int n){\n    int left_idx, right_idx, min_idx, end = parent(n - 1);\n    while(cur <= end){\n        left_idx = left(cur);\n        right_idx = right(cur);\n        min_idx = A[cur] > A[left_idx]? left_idx : cur;\n        if(right_idx < n){\n            min_idx = A[min_idx] > A[right_idx]? right_idx : min_idx;\n        }\n        if(min_idx != cur){\n            A[cur] ^= A[min_idx];\n            A[min_idx] ^= A[cur];\n            A[cur] ^= A[min_idx];\n            cur = min_idx;\n        }else{\n            break;\n        }\n    }\n}\nvoid createHeap(int *A, int n){\n    int end = parent(n - 1);\n    for(int i = end; i >= 0; --i){\n        modifyHeap(A, i, n);\n    }\n}\nvector<int> sortElement(vector<int> A, int n, int k) {\n    if(A.empty() || n <= 1 || k < 1){\n        return A;\n    }else{\n        int start = 0, group_num = 0;\n        int* B = new int[k + 1];\n        for(int i = 0; i < n && i < k + 1; ++i){\n            B[i] = A[i];\n            group_num += 1;\n        }\n        createHeap(B, group_num);\n        while(group_num > 1){\n            A[start++] = B[0];\n            if(start + group_num > n){\n                group_num--;\n                B[0] = B[group_num];\n                modifyHeap(B, 0, group_num);\n            }else{\n                B[0] = A[start + group_num - 1];\n                modifyHeap(B, 0, group_num);\n            }\n        }\n        if(group_num == 1){\n            A[start] = B[0];\n        }\n        return A;\n    }\n}\n```\n\n# 字符串\n## KMP\n字符串匹配算法。\n\n```C++\n\nbool kmp(string A, int lena, string B, int lenb){\n    // 在A中查找B\n    int *next = new int[lenb]();\n    int i=1,j=0;\n    while(i < lenb){// 构造next数组\n        if(B[i] == B[j]){\n            next[i++] = ++j;\n        }else{\n            j = next[j - 1];\n            if(j == 0){\n                next[i++] = 0;\n            }\n        }\n    }\n    i = j = 0;\n    while(i <= lena - lenb + j){//  进行匹配\n        while(A[i] == B[j]){\n            ++i;\n            ++j;\n            if(j == lenb){\n                return true;\n            }\n        }\n        if(j == 0){\n            ++i;\n        }else{\n            j = next[j - 1];\n        }\n    }\n    return false;\n}\n```\n\n# 栈和队列\n\n## 滑动窗口最大值\n\n```C++\nvector<int> slide(vector<int> arr, int n, int w) {\n    // write code here\n    vector<int> result;\n    deque<int> maxque;\n    for(int i=0;i<n;++i){\n        while(!maxque.empty() && arr[maxque.back()] < arr[i]){\n            maxque.pop_back();\n        }\n        maxque.push_back(i);\n        if(i >= w - 1){\n            while(maxque.front() <= i - w){\n                maxque.pop_front();\n            }\n            result.push_back(arr[maxque.front()]);\n        }\n    }\n    return result;\n```\n\n## 构造MaxTree\n对于一个没有重复元素的整数数组，请用其中元素构造一棵MaxTree，MaxTree定义为一棵二叉树，其中的节点与数组元素一一对应，同时对于MaxTree的每棵子树，它的根的元素值为子树的最大值。现有一建树方法，对于数组中的每个元素，其在树中的父亲为数组中它左边比它大的第一个数和右边比它大的第一个数中更小的一个。若两边都不存在比它大的数，那么它就是树根。请设计O(n)的算法实现这个方法。\n\n给定一个无重复元素的数组A和它的大小n，请返回一个数组，其中每个元素为原数组中对应位置元素在树中的父亲节点的编号，若为根则值为-1。\n\n测试样例：\n\n[3,1,4,2],4\n\n返回：[2,0,-1,2]\n```C++\nvector<int> buildMaxTree(vector<int> A, int n) {\n    // write code here\n    vector<int> result;\n    int l,r;\n    vector<int> left_max, right_max;\n    stack<int> left_stack, right_stack;\n    for(int i = 0; i < n; ++i){\n        if(left_stack.empty()){\n            left_max.push_back(-1);\n        }else{\n            while(!left_stack.empty() && A[left_stack.top()] <= A[i]){\n                left_stack.pop();\n            }\n            if(left_stack.empty()){\n                left_max.push_back(-1);\n            }else{\n                left_max.push_back(left_stack.top());\n            }\n        }\n        left_stack.push(i);\n        \n        if(right_stack.empty()){\n            right_max.push_back(-1);\n        }else{\n            while(!right_stack.empty() && A[right_stack.top()] <= A[n - i - 1]){\n                right_stack.pop();\n            }\n            if(right_stack.empty()){\n                right_max.push_back(-1);\n            }else{\n                right_max.push_back(right_stack.top());\n            }\n        }\n        right_stack.push(n - i - 1);\n    }\n    for(int i = 0; i < n; ++i){\n        l = left_max[i];\n        r = right_max[n - i - 1];\n        if(l == -1 && r == -1){\n            result.push_back(-1);\n        }else if(l == -1){\n            result.push_back(r);\n        }else if(r == -1){\n            result.push_back(l);\n        }else{\n            result.push_back(A[l] > A[r]? r : l);\n        }\n    }\n    return result;\n}\n```\n\n# 链表\n\n## 链表的回文判断\n\n请编写一个函数，检查链表是否为回文。\n\n给定一个链表ListNode* pHead，请返回一个bool，代表链表是否为回文。\n\n测试样例：\n\n{1,2,3,2,1}\n\n返回：true\n\n{1,2,3,2,3}\n\n返回：false\n\n```C++\n/*\nstruct ListNode {\n    int val;\n    struct ListNode *next;\n    ListNode(int x) : val(x), next(NULL) {}\n};*/\nvoid reverse(ListNode *pHead, ListNode *pTail){\n    if(pHead == nullptr || pTail == pHead){\n        return;\n    }\n    ListNode *cur=pHead, *next=pHead->next, *temp;\n    while(next!=pTail){\n        temp = next->next;\n        next->next = cur;\n        cur = next;\n        next = temp;\n    }\n    pTail->next = cur;\n    pHead->next = nullptr;\n}\n\nbool isPalindrome(ListNode* pHead) {\n    // write code here\n    if(pHead == nullptr){\n        return false;\n    }\n    ListNode *center=pHead, *tail=pHead, *pcur=pHead, *ncur;\n    bool result = true;\n    while(center->next != nullptr && tail->next!=nullptr){\n        center = center->next;\n        tail = tail->next;\n        if(tail->next!=nullptr){\n            tail = tail->next;\n        }else{\n            break;\n        }\n    }\n    if(center == tail){\n        if(pHead->val == tail->val){\n            return true;\n        }else{\n            return false;\n        }\n    }\n    reverse(center, tail);\n    ncur = tail;\n    while(pcur!=center){\n        if(pcur->val != ncur->val){\n            result = false;\n            break;\n        }else{\n            pcur = pcur->next;\n            ncur = ncur->next;\n        }\n    }\n    reverse(tail, center);\n    return result;\n}\n```\n\n## 复杂链表的复制\n\n输入一个复杂链表（每个节点中有节点值，以及两个指针，一个指向下一个节点，另一个特殊指针指向任意一个节点），复制这个复杂链表并返回。\n\n```C++\n/*\nstruct RandomListNode {\n    int label;\n    struct RandomListNode *next, *random;\n    RandomListNode(int x) :\n            label(x), next(NULL), random(NULL) {\n    }\n};\n*/\nRandomListNode* Clone(RandomListNode* pHead){\n    RandomListNode *cur=pHead, *copy, *resultHead, *resultCur;\n    if(pHead==nullptr){\n        return nullptr;\n    }\n    while(cur!=nullptr){\n        copy = new RandomListNode(cur->label);\n        copy->next = cur->next;\n        cur->next = copy;\n        cur = copy->next;\n    }\n    cur = pHead;\n    while(cur!=nullptr){\n        if(cur->random != nullptr){\n            cur->next->random = cur->random->next;\n        }\n        cur = cur->next->next;\n    }\n    resultCur = resultHead = pHead->next;\n    cur = pHead;\n    while(cur!=nullptr && resultCur!=nullptr){\n        cur->next = cur->next->next;\n        if(resultCur->next!=nullptr){\n            resultCur->next = resultCur->next->next;\n        }\n        cur = cur->next;\n        resultCur = resultCur->next;\n    }\n    return resultHead;\n}\n```\n\n## 判断链表是否有环\n\n如何判断一个单链表是否有环？有环的话返回进入环的第一个节点的值，无环的话返回-1。如果链表的长度为N，请做到时间复杂度O(N)，额外空间复杂度O(1)。\n\n给定一个单链表的头结点head，请返回所求值。\n\n```C++\nint chkLoop(ListNode* head){\n    // write code here\n    ListNode *fast=head, *slow=head;\n    bool hasLoop = false;\n    while(fast != nullptr){\n        slow = slow->next;\n        fast = fast->next;\n        if(fast){\n            fast = fast->next;\n        }else{\n            break;\n        }\n        if(fast == slow){\n            hasLoop = true;\n            break;\n        }\n    }\n    if(!hasLoop){\n        return -1;\n    }\n    fast = head;\n    while(fast!=slow){\n        fast = fast->next;\n        slow = slow->next;\n    }\n    return fast->val;\n}\n```\n\n## 无环链表是否相交\n\n直接判断最后一个节点是否相等即可，如果要求返回第一个共同节点，那么则需要先统计各链表长度$m,n$，假如$m \\le n$，那么第二个链表从第$n-m$个节点开始遍历，第一个链表从头结点开始遍历，两个遍历同步，同时比较当前节点是否相同，第一个相同节点即返回。\n\n```C++\n/*\nstruct ListNode {\n    int val;\n    struct ListNode *next;\n    ListNode(int x) : val(x), next(NULL) {}\n};*/\nbool chkIntersect(ListNode* headA, ListNode* headB) {\n    // write code here\n    ListNode *nodeA=headA, *nodeB=headB;\n    if(nodeA == nullptr || nodeB == nullptr){\n        return false;\n    }\n    while(nodeA->next){\n        nodeA = nodeA->next;\n    }\n    while(nodeB->next){\n        nodeB = nodeB->next;\n    }\n    if(nodeA == nodeB){\n        return true;\n    }else{\n        return false;\n    }\n}\n```\n\n## 有环链表是否相交\n如何判断两个有环单链表是否相交？相交的话返回第一个相交的节点，不想交的话返回空。如果两个链表长度分别为N和M，请做到时间复杂度O(N+M)，额外空间复杂度O(1)。\n\n给定两个链表的头结点head1和head2。请返回一个bool值代表它们是否相交。\n\n如果还需要找到第一个共同节点，先看是否相交，如果相交，有两种情况，第一种是入环之前相交，这样的话和首先将环去掉，然后按照无环链表的情况来进行查找，第二种情况则直接返回任意一个链表的入环节点都可以算作第一个相交节点。\n\n```C++\n/*\nstruct ListNode {\n    int val;\n    struct ListNode *next;\n    ListNode(int x) : val(x), next(NULL) {}\n};*/\nListNode* firstLoopNode(ListNode* head) {\n    // write code here\n    ListNode *fast=head, *slow=head;\n    bool hasLoop = false;\n    while(fast != nullptr){\n        slow = slow->next;\n        fast = fast->next;\n        if(fast){\n            fast = fast->next;\n        }else{\n            break;\n        }\n        if(fast == slow){\n            hasLoop = true;\n            break;\n        }\n    }\n    if(!hasLoop){\n        return nullptr;\n    }\n    fast = head;\n    while(fast!=slow){\n        fast = fast->next;\n        slow = slow->next;\n    }\n    return fast;\n}\n\nbool chkInter(ListNode* head1, ListNode* head2) {\n    // write code here\n    ListNode *loopNode1 = firstLoopNode(head1);\n    ListNode *loopNode2 = firstLoopNode(head2);\n    ListNode *temp;\n    if(head1 == nullptr || head2 == nullptr){\n        return false;\n    }\n    if(loopNode1 == loopNode2){\n        return true;\n    }\n    temp = loopNode2;\n    while(temp->next != loopNode2){\n        if(temp == loopNode1){\n            return true;\n        }\n        temp = temp->next;\n    }\n    return false;\n}\n```\n\n\n# 二分查找\n\n## 查找局部最小值\n定义局部最小的概念。arr长度为1时，arr[0]是局部最小。arr的长度为N(N>1)时，如果arr[0]< arr[1]，那么arr[0]是局部最小；如果arr[N-1] < arr[N-2]，那么arr[N-1]是局部最小；如果0 < i < N-1，既有arr[i] < arr[i-1]又有arr[i] < arr[i+1]，那么arr[i]是局部最小。 给定无序数组arr，已知arr中任意两个相邻的数都不相等，写一个函数，只需返回arr中任意一个局部最小出现的位置即可。\n```C++\nint getLessIndex(vector<int> arr) {\n    int start = 0, end = arr.size(), mid;\n    if(end == 0){\n        return -1;\n    }else if(end == 1){\n        return 0;\n    }\n    \n    if(arr[start] < arr[start]){\n        return start;\n    }\n    \n    if(arr[end - 2] > arr[end - 1]){\n        return end;\n    }\n    \n    while(start < end - 1){\n        mid = start + (end - start)/2;\n        if(arr[mid] > arr[mid - 1]){\n            end = mid;\n        }else{\n            start = mid;\n        }\n    }\n    return start;\n}\n```\n\n## 查找循环有序数组的最小值\n\n对于一个有序循环数组arr，返回arr中的最小值。有序循环数组是指，有序数组左边任意长度的部分放到右边去，右边的部分拿到左边来。比如数组[1,2,3,3,4]，是有序循环数组，[4,1,2,3,3]也是。\n\n给定数组arr及它的大小n，请返回最小值。\n\n```C++\nint getMin(vector<int> arr, int n) {\n    int start = 0, end = n, mid;\n    if(n == 0){\n        return -1;\n    }else if(n == 1){\n        return arr[0];\n    }else if(n == 2){\n        return arr[0] > arr[1] ? arr[1] : arr[0];\n    }\n    while(start < end - 1){\n        mid = start + (end - start) / 2;\n        if(arr[start] > arr[end - 1]){\n            if(arr[mid] > arr[end-1]){\n                start = mid + 1;\n            }else if(arr[mid] < arr[end-1]){\n                end = mid + 1;\n            }else{\n                start = mid;\n            }\n        }else if(arr[start] < arr[end]){\n            return arr[start];\n        }else{\n            if(arr[mid] < arr[end - 1]){\n                end = mid + 1;\n            }else{\n                start += 1;\n            }\n        }\n    }\n    return arr[start];\n}\n```\n\n## 完全二叉树的节点个数\n给定一棵完全二叉树的根节点root，返回这棵树的节点个数。如果完全二叉树的节点数为N，请实现时间复杂度低于O(N)的解法。\n\n给定树的根结点root，请返回树的大小。\n\n```C++\nint count(TreeNode* root) {\n    int nodeNum = 0;\n    int lldepth = -1, rldepth = 0;\n    TreeNode *cur_root = root, *cur;\n    while(cur_root){\n        if(lldepth==-1){\n            cur = cur_root;\n            while(cur){\n                lldepth += 1;\n                cur = cur->left;\n            }\n        }\n        cur = cur_root->right;\n        while(cur){\n            rldepth += 1;\n            cur = cur->left;\n        }\n        if(rldepth == lldepth){\n            nodeNum += 1 << lldepth;\n            cur_root = cur_root->right;\n            lldepth = rldepth - 1;\n            rldepth = 0;\n        }else{\n            nodeNum += 1 << rldepth;\n            cur_root = cur_root->left;\n            lldepth -= 1;\n        }\n    }\n    return nodeNum;\n}\n```\n\n# 二叉树\n\n## 前序、中序、后序三种遍历方式\n请用递归方式实现二叉树的先序、中序和后序的遍历打印。\n\n给定一个二叉树的根结点root，请依次返回二叉树的先序，中序和后续遍历(二维数组的形式)。\n\n```C++\nvoid pre(TreeNode* root, vector<int> &result){\n    stack<TreeNode*> s;\n    TreeNode *cur;\n    if(root == nullptr){\n        return ;\n    }\n    s.push(root);\n    while(!s.empty()){\n        cur = s.top();\n        s.pop();\n        if(cur->right){\n            s.push(cur->right);\n        }\n        if(cur->left){\n            s.push(cur->left);\n        }\n        result.push_back(cur->val);\n    }\n    return ;\n}\n\nvoid center(TreeNode* root, vector<int> &result){\n    stack<TreeNode*> s;\n    TreeNode *cur = root;\n    if(root == nullptr){\n        return ;\n    }\n    while(!s.empty() || cur != nullptr){\n        if(cur){\n            while(cur){\n                s.push(cur);\n                cur = cur->left;\n            }\n        }\n        cur = s.top();\n        s.pop();\n        result.push_back(cur->val);\n        cur = cur->right;\n    }\n    return ;\n}\n\nvoid post(TreeNode* root, vector<int> &result){\n    stack<TreeNode*> s;\n    TreeNode *cur, *h=root;\n    if(root == nullptr){\n        return ;\n    }\n    s.push(root);\n    while(!s.empty()){\n        cur = s.top();\n        if(cur->left && h != cur->left && h != cur->right){\n            while(cur->left){\n                s.push(cur->left);\n                cur = cur->left;\n            }\n        }else if(cur->right && h != cur->right){\n            s.push(cur->right);\n        }else{\n            s.pop();\n            result.push_back(cur->val);\n            h = cur;\n        }\n    }\n    return ;\n}\n\nvector<vector<int> > convert(TreeNode* root) {\n    // write code here\n    vector<vector<int> > result(3);\n    pre(root, result[0]);\n    center(root, result[1]);\n    post(root, result[2]);\n    return result;\n}\n```\n\n## 二叉搜索树的后序遍历序列\n输入一个整数数组，判断该数组是不是某二叉搜索树的后序遍历结果。如果是则返回 true，否则返回 false。假设输入的数组的任意两个数字都互不相同。\n\n```C++\nbool verifyPostorder(vector<int>& postorder) {\n    stack<int> s;//这个栈在下面的过程中，其中的内容保证是单调递增的\n    int root = 0x7fffffff;//整棵树都当做一个正无穷节点的左子树，保证当前访问的节点是root的左子树节点。\n    for(int i = postorder.size() - 1; i >= 0; --i){\n        if(postorder[i] > root) return false;//左子树的节点值比父节点大，直接判定false\n        while(! s.empty() && postorder[i] < s.top()){\n            /*倒序访问过程中，访问顺序是当前根节点的左子树的根节点->左子树根节点的右子树->左子树根节点的左子树,出现当前节点比较小，说明左子树根节点的右子树访问完了（或者左子树根节点没有右子树），现在访问的是到了左子树根节点的左子树，这个时候需要把左子树根节点的右子树清除，并调整root为当前左子树的根节点*/\n            root = s.top();//栈中最后一个比当前元素大的那个节点是当前节点的父节点\n            s.pop();\n        }\n        s.push(postorder[i]);//当前父节点的左子树节点入栈\n    }\n    return true;//访问完都没有出现不符合条件的，则判定为true\n}\n```\n\n# 动态规划\n## 找零问题\n有数组penny，penny中所有的值都为正数且不重复。每个值代表一种面值的货币，每种面值的货币可以使用任意张，再给定一个整数aim(小于等于1000)代表要找的钱数，求换钱有多少种方法。\n\n给定数组penny及它的大小(小于等于50)，同时给定一个整数aim，请返回有多少种方法可以凑成aim。\n\n```C++\nint countWays(vector<int> penny, int n, int aim) {\n    int record[1001] = {0};\n    record[0] = 1;\n    for(int i=0; i<n;++i){\n        for(int j=penny[i];j<=aim;++j){\n            record[j] += record[j - penny[i]];\n        }\n    }\n    \n    return record[aim];\n}\n```\n\n## 最长上升子序列\n这是一个经典的LIS(即最长上升子序列)问题，请设计一个尽量优的解法求出序列的最长上升子序列的长度。\n\n给定一个序列A及它的长度n(长度小于等于500)，请返回LIS的长度。\n\n测试样例：\n\n[1,4,2,5,3],5\n\n返回：3\n```C++\nint getLIS(vector<int> A, int n) {\n    // write code here\n    if(n == 0){\n        return 0;\n    }\n    int max_len=1;\n    int *record = new int[n];\n    int max_pos;\n    record[0] = 1;\n    for(int i = 1; i < n; ++i){\n        max_pos = -1;\n        for(int j = i-1; j>=0;--j){\n            if(A[j] < A[i] && (max_pos < 0 || record[j] > record[max_pos])){\n                max_pos = j;\n            }\n        }\n        if(max_pos >= 0){\n            record[i] = record[max_pos] + 1;\n        }else{\n            record[i] = 1;\n        }\n        if(record[i] > max_len){\n            max_len = record[i];\n        }\n    }\n    return max_len;\n}\n```\n\n","source":"_posts/学习笔记/算法总结.md","raw":"---\ntitle: 算法总结\ndate: 2020-05-19 12:15:36\ntags: [算法]\nmathjax: true\n---\n# 排序\n\n## 冒泡排序\n逐对交换，每次迭代，确定一个元素的最终位置。\n\n时间复杂度：$O(N^2)$\n\n空间复杂度：$O(1)$\n```C++\nint* bubbleSort(int* A, int n) {\n    if(A == NULL || n <= 1){\n        return A;\n    }\n    int temp;\n    for(int i = 1; i < n - 1; ++i){\n        for(int j = 0; j < n - i; ++j){\n            if(A[j] > A[j+1]){\n                temp = A[j+1];\n                A[j+1] = A[j];\n                A[j] = temp;\n            }\n        }\n    }\n    return A;\n}\n```\n\n## 选择排序\n\n每次选择一个剩下的最小或者最大元素放到对应位置。\n\n时间复杂度：$O(N^2)$\n\n空间复杂度：$O(1)$\n```C++\nint* selectionSort(int* A, int n) {\n    if(A == NULL || n <= 1){\n        return A;\n    }\n    int min_value, j, min_index;\n    for(int i = 0;i<n-1;++i){\n        min_value = 0x7fffffff;\n        min_index = j = i;\n        while(j < n){\n            if(A[j] < min_value){\n                min_value = A[j];\n                min_index = j;\n            }\n            ++j;\n        }\n        if(min_index != i){\n            A[i] ^= A[min_index];\n            A[min_index] ^= A[i];\n            A[i] ^= A[min_index];\n        }\n    }\n    return A;\n}\n```\n\n## 插入排序\n时间复杂度：$O(N^2)$\n\n空间复杂度：$O(1)$\n```C++\nint* insertionSort(int* A, int n) {\n    if(A == NULL || n <= 1){\n        return A;\n    }\n    int cur, j;\n    for(int i=1;i<n;++i){\n        cur = A[i];\n        for(j = i-1; j>= 0; --j){\n            if(cur < A[j]){\n                A[j+1] = A[j];\n            }else{\n                break;\n            }\n        }\n        A[j+1] = cur;\n    }\n    return A;\n}\n```\n\n## 归并排序\n时间复杂度：$O(N\\log(N))$\n\n空间复杂度：$O(N)$\n```C++\nint* merge(int* A, int lenA, int*B, int lenB){\n    int *result = new int[lenA + lenB];\n    int i=0,j=0;\n    while(i < lenA && j < lenB){\n        if(A[i] <= B[j]){\n            result[i+j] = A[i];\n            ++i;\n        }else{\n            result[i+j] = B[j];\n            ++j;\n        }\n    }\n    while(i < lenA){\n        result[i+j] = A[i];\n        ++i;\n    }\n    while(j < lenB){\n        result[i+j] = B[j];\n        ++j;\n    }\n    return result;\n}\nint* mergeSort(int* A, int n) {\n    if(A == NULL || n <= 1){\n        return A;\n    }\n    int center = n / 2;\n    return merge(mergeSort(A, center), center, mergeSort(A + center, n - center), n - center);\n}\n```\n\n## 快速排序\n时间复杂度：$O(N\\log(N))$\n\n空间复杂度：$O(\\log(N))$ ~ $O(N)$\n```C++\nint partition(int* A, int n){\n    if(n <= 1){\n        return 1;\n    }else{\n        int small = -1;\n        int i = 0, j = n-2;\n        while(i <= j){\n            if(A[i] <= A[n-1]){\n                small = i++;\n            }else if(A[j] <= A[n-1]){\n                if(j != i){\n                    A[j] ^= A[i];\n                    A[i] ^= A[j];\n                    A[j] ^= A[i];\n                }\n                small = i++;\n            }else{\n                --j;\n            }\n        }\n        if(small + 1 != n - 1){\n            A[small + 1] ^= A[n - 1];\n            A[n - 1] ^= A[small + 1];\n            A[small + 1] ^= A[n - 1];\n        }\n        return small + 1;\n    }\n}\nint* quickSort(int* A, int n) {\n    if(A == NULL || n <= 1){\n        return A;\n    }else{\n        int center = partition(A, n);\n        quickSort(A, center);\n        quickSort(A+center+1, n-center-1);\n        return A;\n    }\n}\n```\n\n## 堆排序\n时间复杂度：$O(N\\log(N))$\n\n空间复杂度：$O(1)$\n\n```C++\nint parent(int i){\n    return (i - 1) / 2;\n}\nint left(int i){\n    return i * 2 + 1;\n}\nint right(int i){\n    return i * 2 + 2;\n}\n\nvoid DownHeap(int* A, int cur, int n){\n    int left_idx, right_idx, max_idx, end = parent(n - 1);\n    while(cur <= end){\n        left_idx = left(cur);\n        right_idx = right(cur);\n        max_idx = A[cur] < A[left_idx]? left_idx : cur;\n        if(right_idx < n){\n            max_idx = A[max_idx] < A[right_idx]? right_idx : max_idx;\n        }\n        if(max_idx != cur){\n            A[cur] ^= A[max_idx];\n            A[max_idx] ^= A[cur];\n            A[cur] ^= A[max_idx];\n            cur = max_idx;\n        }else{\n            break;\n        }\n    }\n}\nvoid createHeap(int* A, int n){\n    int end = parent(n - 1);\n    for(int i = end; i >= 0; --i){\n        DownHeap(A, i, n);\n    }\n}\nint* heapSort(int* A, int n){\n    if(A == NULL || n <= 1){\n        return A;\n    }\n    createHeap(A, n);\n    for(int i = n-1;i>0;--i){\n        DownHeap(A, 0, i+1);\n        A[i] ^= A[0];\n        A[0] ^= A[i];\n        A[i] ^= A[0];\n    }\n    return A;\n}\n\nvoid DownHeap(vector<int>& heap, int i, int n){\n    // 大顶堆的下滤操作\n    int temp = heap[i];\n    int l,r,maxChild=i;\n    while(true){\n        l = i * 2 + 1;\n        r = l + 1;\n        maxChild = (r < n && heap[r] > heap[l]) ? r : l;\n        maxChild = (maxChild < n && heap[maxChild] > temp) ? maxChild : i;\n        if(maxChild != i){\n            heap[i] = heap[maxChild];\n            i = maxChild;\n        }else{\n            break;\n        }\n    }\n    heap[i] = temp;\n    return ;\n}\n\nvector<int> sortArray(vector<int>& nums) {\n    //堆排序\n    int n = nums.size();\n    //构建大顶堆\n    for(int i = n / 2; i >= 0; --i){\n        DownHeap(nums, i, n);\n    }\n\n    //逐个删除堆顶，进行排序\n    for(int i = n; i > 1; --i){\n        swap(nums[0], nums[i - 1]);\n        DownHeap(nums, 0, i - 1);\n    }\n    return nums;\n}\n```\n\n## 希尔排序\n时间复杂度：$O(N\\log(N))$\n\n空间复杂度：$O(1)$\n```C++\nvoid insertionSort(int* A, int n, int space) {\n    int cur, j;\n    for(int i=space;i<n;i++){\n        cur = A[i];\n        for(j = i-space; j>= 0; j-=space){\n            if(cur < A[j]){\n                A[j+space] = A[j];\n            }else{\n                break;\n            }\n        }\n        A[j+space] = cur;\n    }\n}\nint* shellSort(int* A, int n) {\n    if(A == NULL || n <= 1){\n        return A;\n    }\n    for(int space = n / 2; space>=1; space/= 2){\n        insertionSort(A, n, space);\n    }\n    return A;\n}\n\nvector<int> shellSort(vector<int>& nums) {\n    //希尔排序\n    int n = nums.size();\n    int inc = 2;\n    int temp;\n    int i,j;\n    // 计算Hibbard增量\n    while(inc * 2 < n){\n        inc *= 2;\n    }\n    inc -= 1;\n    while(inc >= 1){\n        for(i = inc; i < n; ++i){\n            // 执行增量插入排序\n            temp = nums[i];\n            for(j = i; j >= inc && nums[j-inc] > temp; j -= inc){\n                nums[j] = nums[j-inc];\n            }\n            nums[j] = temp;\n        }\n        inc = (inc - 1) / 2;\n    }\n    return nums;\n}\n\n```\n\n## 计数排序\n\n时间复杂度：$O(N)$\n\n空间复杂度：$O(M)$，这里的$M$和待排序元素的取值范围有关。\n\n```C++\nint* countingSort(int* A, int n) {\n    if(A==nullptr || n <=1){\n        return A;\n    }\n    int min = A[0], max = A[0];\n    for(int i = 1;i < n; ++i){\n        if(A[i] > max){\n            max = A[i];\n        }else if(A[i] < min){\n            min = A[i];\n        }\n    }\n    int num = max - min + 1;\n    int *count = new int[num]();\n    for(int i = 0;i < n; ++i){\n        count[A[i] - min] += 1;\n    }\n    int i = 0, j = 0;\n    while(i < n){\n        if(count[j] > 0){\n            count[j] -= 1;\n            A[i++] = j + min;\n        }else{\n            ++j;\n        }\n    }\n    delete []count;\n    return A;\n}\n```\n\n## 基数排序\n\n时间复杂度：$O(N)$\n\n空间复杂度：$O(M)$，这里的$M$和待排序元素的取值范围有关。\n\n```C++\nint getdig(int x, int base){\n    int ubase = 1;\n    for(int i = 0; i < base; i++){\n        ubase *= 10;\n    }\n    x = x % (ubase * 10);\n    return x / ubase;\n}\n\nint* radixSort(int* A, int n) {\n    int base = 0, dig, j;\n    vector<int> temp;\n    map<int, vector<int> > buckets;\n    while(base < 4){ // 如果待排序数字全都小于10000，那么这里只用到4\n        for(int i = 0; i < n; ++i){\n            dig = getdig(A[i], base);\n            buckets[dig].push_back(A[i]);\n        }\n        j = 0;\n        for(int i = 0; i < 10; ++i){\n            temp = buckets[i];\n            for(int k = 0; k < temp.size(); ++k){\n                A[j++] = temp[k];\n            }\n            buckets[i].clear();\n        }\n        ++base;\n    }\n    return A;\n}\n```\n\n## 基本有序的情况\n如果一个序列已经基本有序，即排好序之后，其中每个元素的移动位置偏移不超过K，那么这种情况下，可以使用改进的堆排序方法，每次使用前K个数字建立小根堆（建好后最小的数字肯定在第一个位置），每次向后移动一个位置，维持小根堆，这样一直往后移动最终完成排序，其时间复杂度为$O(N \\times \\log(K))$，其次，这种情况下插入排序是比较好的，其时间复杂度为$O(N \\times K)$，改进的堆排序算法如下（这里为了实现简单，使用了额外$O(K)$的空间，其实不需要，只不过代码可能会变得比较复杂）：\n\n```C++\nint parent(int i){\n    return (i - 1) / 2;\n}\nint left(int i){\n    return i * 2 + 1;\n}\nint right(int i){\n    return i * 2 + 2;\n}\nvoid modifyHeap(int *A, int cur, int n){\n    int left_idx, right_idx, min_idx, end = parent(n - 1);\n    while(cur <= end){\n        left_idx = left(cur);\n        right_idx = right(cur);\n        min_idx = A[cur] > A[left_idx]? left_idx : cur;\n        if(right_idx < n){\n            min_idx = A[min_idx] > A[right_idx]? right_idx : min_idx;\n        }\n        if(min_idx != cur){\n            A[cur] ^= A[min_idx];\n            A[min_idx] ^= A[cur];\n            A[cur] ^= A[min_idx];\n            cur = min_idx;\n        }else{\n            break;\n        }\n    }\n}\nvoid createHeap(int *A, int n){\n    int end = parent(n - 1);\n    for(int i = end; i >= 0; --i){\n        modifyHeap(A, i, n);\n    }\n}\nvector<int> sortElement(vector<int> A, int n, int k) {\n    if(A.empty() || n <= 1 || k < 1){\n        return A;\n    }else{\n        int start = 0, group_num = 0;\n        int* B = new int[k + 1];\n        for(int i = 0; i < n && i < k + 1; ++i){\n            B[i] = A[i];\n            group_num += 1;\n        }\n        createHeap(B, group_num);\n        while(group_num > 1){\n            A[start++] = B[0];\n            if(start + group_num > n){\n                group_num--;\n                B[0] = B[group_num];\n                modifyHeap(B, 0, group_num);\n            }else{\n                B[0] = A[start + group_num - 1];\n                modifyHeap(B, 0, group_num);\n            }\n        }\n        if(group_num == 1){\n            A[start] = B[0];\n        }\n        return A;\n    }\n}\n```\n\n# 字符串\n## KMP\n字符串匹配算法。\n\n```C++\n\nbool kmp(string A, int lena, string B, int lenb){\n    // 在A中查找B\n    int *next = new int[lenb]();\n    int i=1,j=0;\n    while(i < lenb){// 构造next数组\n        if(B[i] == B[j]){\n            next[i++] = ++j;\n        }else{\n            j = next[j - 1];\n            if(j == 0){\n                next[i++] = 0;\n            }\n        }\n    }\n    i = j = 0;\n    while(i <= lena - lenb + j){//  进行匹配\n        while(A[i] == B[j]){\n            ++i;\n            ++j;\n            if(j == lenb){\n                return true;\n            }\n        }\n        if(j == 0){\n            ++i;\n        }else{\n            j = next[j - 1];\n        }\n    }\n    return false;\n}\n```\n\n# 栈和队列\n\n## 滑动窗口最大值\n\n```C++\nvector<int> slide(vector<int> arr, int n, int w) {\n    // write code here\n    vector<int> result;\n    deque<int> maxque;\n    for(int i=0;i<n;++i){\n        while(!maxque.empty() && arr[maxque.back()] < arr[i]){\n            maxque.pop_back();\n        }\n        maxque.push_back(i);\n        if(i >= w - 1){\n            while(maxque.front() <= i - w){\n                maxque.pop_front();\n            }\n            result.push_back(arr[maxque.front()]);\n        }\n    }\n    return result;\n```\n\n## 构造MaxTree\n对于一个没有重复元素的整数数组，请用其中元素构造一棵MaxTree，MaxTree定义为一棵二叉树，其中的节点与数组元素一一对应，同时对于MaxTree的每棵子树，它的根的元素值为子树的最大值。现有一建树方法，对于数组中的每个元素，其在树中的父亲为数组中它左边比它大的第一个数和右边比它大的第一个数中更小的一个。若两边都不存在比它大的数，那么它就是树根。请设计O(n)的算法实现这个方法。\n\n给定一个无重复元素的数组A和它的大小n，请返回一个数组，其中每个元素为原数组中对应位置元素在树中的父亲节点的编号，若为根则值为-1。\n\n测试样例：\n\n[3,1,4,2],4\n\n返回：[2,0,-1,2]\n```C++\nvector<int> buildMaxTree(vector<int> A, int n) {\n    // write code here\n    vector<int> result;\n    int l,r;\n    vector<int> left_max, right_max;\n    stack<int> left_stack, right_stack;\n    for(int i = 0; i < n; ++i){\n        if(left_stack.empty()){\n            left_max.push_back(-1);\n        }else{\n            while(!left_stack.empty() && A[left_stack.top()] <= A[i]){\n                left_stack.pop();\n            }\n            if(left_stack.empty()){\n                left_max.push_back(-1);\n            }else{\n                left_max.push_back(left_stack.top());\n            }\n        }\n        left_stack.push(i);\n        \n        if(right_stack.empty()){\n            right_max.push_back(-1);\n        }else{\n            while(!right_stack.empty() && A[right_stack.top()] <= A[n - i - 1]){\n                right_stack.pop();\n            }\n            if(right_stack.empty()){\n                right_max.push_back(-1);\n            }else{\n                right_max.push_back(right_stack.top());\n            }\n        }\n        right_stack.push(n - i - 1);\n    }\n    for(int i = 0; i < n; ++i){\n        l = left_max[i];\n        r = right_max[n - i - 1];\n        if(l == -1 && r == -1){\n            result.push_back(-1);\n        }else if(l == -1){\n            result.push_back(r);\n        }else if(r == -1){\n            result.push_back(l);\n        }else{\n            result.push_back(A[l] > A[r]? r : l);\n        }\n    }\n    return result;\n}\n```\n\n# 链表\n\n## 链表的回文判断\n\n请编写一个函数，检查链表是否为回文。\n\n给定一个链表ListNode* pHead，请返回一个bool，代表链表是否为回文。\n\n测试样例：\n\n{1,2,3,2,1}\n\n返回：true\n\n{1,2,3,2,3}\n\n返回：false\n\n```C++\n/*\nstruct ListNode {\n    int val;\n    struct ListNode *next;\n    ListNode(int x) : val(x), next(NULL) {}\n};*/\nvoid reverse(ListNode *pHead, ListNode *pTail){\n    if(pHead == nullptr || pTail == pHead){\n        return;\n    }\n    ListNode *cur=pHead, *next=pHead->next, *temp;\n    while(next!=pTail){\n        temp = next->next;\n        next->next = cur;\n        cur = next;\n        next = temp;\n    }\n    pTail->next = cur;\n    pHead->next = nullptr;\n}\n\nbool isPalindrome(ListNode* pHead) {\n    // write code here\n    if(pHead == nullptr){\n        return false;\n    }\n    ListNode *center=pHead, *tail=pHead, *pcur=pHead, *ncur;\n    bool result = true;\n    while(center->next != nullptr && tail->next!=nullptr){\n        center = center->next;\n        tail = tail->next;\n        if(tail->next!=nullptr){\n            tail = tail->next;\n        }else{\n            break;\n        }\n    }\n    if(center == tail){\n        if(pHead->val == tail->val){\n            return true;\n        }else{\n            return false;\n        }\n    }\n    reverse(center, tail);\n    ncur = tail;\n    while(pcur!=center){\n        if(pcur->val != ncur->val){\n            result = false;\n            break;\n        }else{\n            pcur = pcur->next;\n            ncur = ncur->next;\n        }\n    }\n    reverse(tail, center);\n    return result;\n}\n```\n\n## 复杂链表的复制\n\n输入一个复杂链表（每个节点中有节点值，以及两个指针，一个指向下一个节点，另一个特殊指针指向任意一个节点），复制这个复杂链表并返回。\n\n```C++\n/*\nstruct RandomListNode {\n    int label;\n    struct RandomListNode *next, *random;\n    RandomListNode(int x) :\n            label(x), next(NULL), random(NULL) {\n    }\n};\n*/\nRandomListNode* Clone(RandomListNode* pHead){\n    RandomListNode *cur=pHead, *copy, *resultHead, *resultCur;\n    if(pHead==nullptr){\n        return nullptr;\n    }\n    while(cur!=nullptr){\n        copy = new RandomListNode(cur->label);\n        copy->next = cur->next;\n        cur->next = copy;\n        cur = copy->next;\n    }\n    cur = pHead;\n    while(cur!=nullptr){\n        if(cur->random != nullptr){\n            cur->next->random = cur->random->next;\n        }\n        cur = cur->next->next;\n    }\n    resultCur = resultHead = pHead->next;\n    cur = pHead;\n    while(cur!=nullptr && resultCur!=nullptr){\n        cur->next = cur->next->next;\n        if(resultCur->next!=nullptr){\n            resultCur->next = resultCur->next->next;\n        }\n        cur = cur->next;\n        resultCur = resultCur->next;\n    }\n    return resultHead;\n}\n```\n\n## 判断链表是否有环\n\n如何判断一个单链表是否有环？有环的话返回进入环的第一个节点的值，无环的话返回-1。如果链表的长度为N，请做到时间复杂度O(N)，额外空间复杂度O(1)。\n\n给定一个单链表的头结点head，请返回所求值。\n\n```C++\nint chkLoop(ListNode* head){\n    // write code here\n    ListNode *fast=head, *slow=head;\n    bool hasLoop = false;\n    while(fast != nullptr){\n        slow = slow->next;\n        fast = fast->next;\n        if(fast){\n            fast = fast->next;\n        }else{\n            break;\n        }\n        if(fast == slow){\n            hasLoop = true;\n            break;\n        }\n    }\n    if(!hasLoop){\n        return -1;\n    }\n    fast = head;\n    while(fast!=slow){\n        fast = fast->next;\n        slow = slow->next;\n    }\n    return fast->val;\n}\n```\n\n## 无环链表是否相交\n\n直接判断最后一个节点是否相等即可，如果要求返回第一个共同节点，那么则需要先统计各链表长度$m,n$，假如$m \\le n$，那么第二个链表从第$n-m$个节点开始遍历，第一个链表从头结点开始遍历，两个遍历同步，同时比较当前节点是否相同，第一个相同节点即返回。\n\n```C++\n/*\nstruct ListNode {\n    int val;\n    struct ListNode *next;\n    ListNode(int x) : val(x), next(NULL) {}\n};*/\nbool chkIntersect(ListNode* headA, ListNode* headB) {\n    // write code here\n    ListNode *nodeA=headA, *nodeB=headB;\n    if(nodeA == nullptr || nodeB == nullptr){\n        return false;\n    }\n    while(nodeA->next){\n        nodeA = nodeA->next;\n    }\n    while(nodeB->next){\n        nodeB = nodeB->next;\n    }\n    if(nodeA == nodeB){\n        return true;\n    }else{\n        return false;\n    }\n}\n```\n\n## 有环链表是否相交\n如何判断两个有环单链表是否相交？相交的话返回第一个相交的节点，不想交的话返回空。如果两个链表长度分别为N和M，请做到时间复杂度O(N+M)，额外空间复杂度O(1)。\n\n给定两个链表的头结点head1和head2。请返回一个bool值代表它们是否相交。\n\n如果还需要找到第一个共同节点，先看是否相交，如果相交，有两种情况，第一种是入环之前相交，这样的话和首先将环去掉，然后按照无环链表的情况来进行查找，第二种情况则直接返回任意一个链表的入环节点都可以算作第一个相交节点。\n\n```C++\n/*\nstruct ListNode {\n    int val;\n    struct ListNode *next;\n    ListNode(int x) : val(x), next(NULL) {}\n};*/\nListNode* firstLoopNode(ListNode* head) {\n    // write code here\n    ListNode *fast=head, *slow=head;\n    bool hasLoop = false;\n    while(fast != nullptr){\n        slow = slow->next;\n        fast = fast->next;\n        if(fast){\n            fast = fast->next;\n        }else{\n            break;\n        }\n        if(fast == slow){\n            hasLoop = true;\n            break;\n        }\n    }\n    if(!hasLoop){\n        return nullptr;\n    }\n    fast = head;\n    while(fast!=slow){\n        fast = fast->next;\n        slow = slow->next;\n    }\n    return fast;\n}\n\nbool chkInter(ListNode* head1, ListNode* head2) {\n    // write code here\n    ListNode *loopNode1 = firstLoopNode(head1);\n    ListNode *loopNode2 = firstLoopNode(head2);\n    ListNode *temp;\n    if(head1 == nullptr || head2 == nullptr){\n        return false;\n    }\n    if(loopNode1 == loopNode2){\n        return true;\n    }\n    temp = loopNode2;\n    while(temp->next != loopNode2){\n        if(temp == loopNode1){\n            return true;\n        }\n        temp = temp->next;\n    }\n    return false;\n}\n```\n\n\n# 二分查找\n\n## 查找局部最小值\n定义局部最小的概念。arr长度为1时，arr[0]是局部最小。arr的长度为N(N>1)时，如果arr[0]< arr[1]，那么arr[0]是局部最小；如果arr[N-1] < arr[N-2]，那么arr[N-1]是局部最小；如果0 < i < N-1，既有arr[i] < arr[i-1]又有arr[i] < arr[i+1]，那么arr[i]是局部最小。 给定无序数组arr，已知arr中任意两个相邻的数都不相等，写一个函数，只需返回arr中任意一个局部最小出现的位置即可。\n```C++\nint getLessIndex(vector<int> arr) {\n    int start = 0, end = arr.size(), mid;\n    if(end == 0){\n        return -1;\n    }else if(end == 1){\n        return 0;\n    }\n    \n    if(arr[start] < arr[start]){\n        return start;\n    }\n    \n    if(arr[end - 2] > arr[end - 1]){\n        return end;\n    }\n    \n    while(start < end - 1){\n        mid = start + (end - start)/2;\n        if(arr[mid] > arr[mid - 1]){\n            end = mid;\n        }else{\n            start = mid;\n        }\n    }\n    return start;\n}\n```\n\n## 查找循环有序数组的最小值\n\n对于一个有序循环数组arr，返回arr中的最小值。有序循环数组是指，有序数组左边任意长度的部分放到右边去，右边的部分拿到左边来。比如数组[1,2,3,3,4]，是有序循环数组，[4,1,2,3,3]也是。\n\n给定数组arr及它的大小n，请返回最小值。\n\n```C++\nint getMin(vector<int> arr, int n) {\n    int start = 0, end = n, mid;\n    if(n == 0){\n        return -1;\n    }else if(n == 1){\n        return arr[0];\n    }else if(n == 2){\n        return arr[0] > arr[1] ? arr[1] : arr[0];\n    }\n    while(start < end - 1){\n        mid = start + (end - start) / 2;\n        if(arr[start] > arr[end - 1]){\n            if(arr[mid] > arr[end-1]){\n                start = mid + 1;\n            }else if(arr[mid] < arr[end-1]){\n                end = mid + 1;\n            }else{\n                start = mid;\n            }\n        }else if(arr[start] < arr[end]){\n            return arr[start];\n        }else{\n            if(arr[mid] < arr[end - 1]){\n                end = mid + 1;\n            }else{\n                start += 1;\n            }\n        }\n    }\n    return arr[start];\n}\n```\n\n## 完全二叉树的节点个数\n给定一棵完全二叉树的根节点root，返回这棵树的节点个数。如果完全二叉树的节点数为N，请实现时间复杂度低于O(N)的解法。\n\n给定树的根结点root，请返回树的大小。\n\n```C++\nint count(TreeNode* root) {\n    int nodeNum = 0;\n    int lldepth = -1, rldepth = 0;\n    TreeNode *cur_root = root, *cur;\n    while(cur_root){\n        if(lldepth==-1){\n            cur = cur_root;\n            while(cur){\n                lldepth += 1;\n                cur = cur->left;\n            }\n        }\n        cur = cur_root->right;\n        while(cur){\n            rldepth += 1;\n            cur = cur->left;\n        }\n        if(rldepth == lldepth){\n            nodeNum += 1 << lldepth;\n            cur_root = cur_root->right;\n            lldepth = rldepth - 1;\n            rldepth = 0;\n        }else{\n            nodeNum += 1 << rldepth;\n            cur_root = cur_root->left;\n            lldepth -= 1;\n        }\n    }\n    return nodeNum;\n}\n```\n\n# 二叉树\n\n## 前序、中序、后序三种遍历方式\n请用递归方式实现二叉树的先序、中序和后序的遍历打印。\n\n给定一个二叉树的根结点root，请依次返回二叉树的先序，中序和后续遍历(二维数组的形式)。\n\n```C++\nvoid pre(TreeNode* root, vector<int> &result){\n    stack<TreeNode*> s;\n    TreeNode *cur;\n    if(root == nullptr){\n        return ;\n    }\n    s.push(root);\n    while(!s.empty()){\n        cur = s.top();\n        s.pop();\n        if(cur->right){\n            s.push(cur->right);\n        }\n        if(cur->left){\n            s.push(cur->left);\n        }\n        result.push_back(cur->val);\n    }\n    return ;\n}\n\nvoid center(TreeNode* root, vector<int> &result){\n    stack<TreeNode*> s;\n    TreeNode *cur = root;\n    if(root == nullptr){\n        return ;\n    }\n    while(!s.empty() || cur != nullptr){\n        if(cur){\n            while(cur){\n                s.push(cur);\n                cur = cur->left;\n            }\n        }\n        cur = s.top();\n        s.pop();\n        result.push_back(cur->val);\n        cur = cur->right;\n    }\n    return ;\n}\n\nvoid post(TreeNode* root, vector<int> &result){\n    stack<TreeNode*> s;\n    TreeNode *cur, *h=root;\n    if(root == nullptr){\n        return ;\n    }\n    s.push(root);\n    while(!s.empty()){\n        cur = s.top();\n        if(cur->left && h != cur->left && h != cur->right){\n            while(cur->left){\n                s.push(cur->left);\n                cur = cur->left;\n            }\n        }else if(cur->right && h != cur->right){\n            s.push(cur->right);\n        }else{\n            s.pop();\n            result.push_back(cur->val);\n            h = cur;\n        }\n    }\n    return ;\n}\n\nvector<vector<int> > convert(TreeNode* root) {\n    // write code here\n    vector<vector<int> > result(3);\n    pre(root, result[0]);\n    center(root, result[1]);\n    post(root, result[2]);\n    return result;\n}\n```\n\n## 二叉搜索树的后序遍历序列\n输入一个整数数组，判断该数组是不是某二叉搜索树的后序遍历结果。如果是则返回 true，否则返回 false。假设输入的数组的任意两个数字都互不相同。\n\n```C++\nbool verifyPostorder(vector<int>& postorder) {\n    stack<int> s;//这个栈在下面的过程中，其中的内容保证是单调递增的\n    int root = 0x7fffffff;//整棵树都当做一个正无穷节点的左子树，保证当前访问的节点是root的左子树节点。\n    for(int i = postorder.size() - 1; i >= 0; --i){\n        if(postorder[i] > root) return false;//左子树的节点值比父节点大，直接判定false\n        while(! s.empty() && postorder[i] < s.top()){\n            /*倒序访问过程中，访问顺序是当前根节点的左子树的根节点->左子树根节点的右子树->左子树根节点的左子树,出现当前节点比较小，说明左子树根节点的右子树访问完了（或者左子树根节点没有右子树），现在访问的是到了左子树根节点的左子树，这个时候需要把左子树根节点的右子树清除，并调整root为当前左子树的根节点*/\n            root = s.top();//栈中最后一个比当前元素大的那个节点是当前节点的父节点\n            s.pop();\n        }\n        s.push(postorder[i]);//当前父节点的左子树节点入栈\n    }\n    return true;//访问完都没有出现不符合条件的，则判定为true\n}\n```\n\n# 动态规划\n## 找零问题\n有数组penny，penny中所有的值都为正数且不重复。每个值代表一种面值的货币，每种面值的货币可以使用任意张，再给定一个整数aim(小于等于1000)代表要找的钱数，求换钱有多少种方法。\n\n给定数组penny及它的大小(小于等于50)，同时给定一个整数aim，请返回有多少种方法可以凑成aim。\n\n```C++\nint countWays(vector<int> penny, int n, int aim) {\n    int record[1001] = {0};\n    record[0] = 1;\n    for(int i=0; i<n;++i){\n        for(int j=penny[i];j<=aim;++j){\n            record[j] += record[j - penny[i]];\n        }\n    }\n    \n    return record[aim];\n}\n```\n\n## 最长上升子序列\n这是一个经典的LIS(即最长上升子序列)问题，请设计一个尽量优的解法求出序列的最长上升子序列的长度。\n\n给定一个序列A及它的长度n(长度小于等于500)，请返回LIS的长度。\n\n测试样例：\n\n[1,4,2,5,3],5\n\n返回：3\n```C++\nint getLIS(vector<int> A, int n) {\n    // write code here\n    if(n == 0){\n        return 0;\n    }\n    int max_len=1;\n    int *record = new int[n];\n    int max_pos;\n    record[0] = 1;\n    for(int i = 1; i < n; ++i){\n        max_pos = -1;\n        for(int j = i-1; j>=0;--j){\n            if(A[j] < A[i] && (max_pos < 0 || record[j] > record[max_pos])){\n                max_pos = j;\n            }\n        }\n        if(max_pos >= 0){\n            record[i] = record[max_pos] + 1;\n        }else{\n            record[i] = 1;\n        }\n        if(record[i] > max_len){\n            max_len = record[i];\n        }\n    }\n    return max_len;\n}\n```\n\n","slug":"学习笔记/算法总结","published":1,"updated":"2020-08-31T06:39:20.784Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckjxpc3sy003a44mqatxv0ec1","content":"<h1 id=\"排序\">排序</h1>\r\n<h2 id=\"冒泡排序\">冒泡排序</h2>\r\n<p>逐对交换，每次迭代，确定一个元素的最终位置。</p>\r\n<p>时间复杂度：<span class=\"math inline\">\\(O(N^2)\\)</span></p>\r\n<p>空间复杂度：<span class=\"math inline\">\\(O(1)\\)</span> <figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span>* <span class=\"title\">bubbleSort</span><span class=\"params\">(<span class=\"keyword\">int</span>* A, <span class=\"keyword\">int</span> n)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(A == <span class=\"literal\">NULL</span> || n &lt;= <span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> A;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> temp;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">1</span>; i &lt; n - <span class=\"number\">1</span>; ++i)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> j = <span class=\"number\">0</span>; j &lt; n - i; ++j)&#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(A[j] &gt; A[j+<span class=\"number\">1</span>])&#123;</span><br><span class=\"line\">                temp = A[j+<span class=\"number\">1</span>];</span><br><span class=\"line\">                A[j+<span class=\"number\">1</span>] = A[j];</span><br><span class=\"line\">                A[j] = temp;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> A;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure> ## 选择排序</p>\r\n<p>每次选择一个剩下的最小或者最大元素放到对应位置。</p>\r\n<p>时间复杂度：<span class=\"math inline\">\\(O(N^2)\\)</span></p>\r\n<p>空间复杂度：<span class=\"math inline\">\\(O(1)\\)</span> <figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span>* <span class=\"title\">selectionSort</span><span class=\"params\">(<span class=\"keyword\">int</span>* A, <span class=\"keyword\">int</span> n)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(A == <span class=\"literal\">NULL</span> || n &lt;= <span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> A;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> min_value, j, min_index;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>;i&lt;n<span class=\"number\">-1</span>;++i)&#123;</span><br><span class=\"line\">        min_value = <span class=\"number\">0x7fffffff</span>;</span><br><span class=\"line\">        min_index = j = i;</span><br><span class=\"line\">        <span class=\"keyword\">while</span>(j &lt; n)&#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(A[j] &lt; min_value)&#123;</span><br><span class=\"line\">                min_value = A[j];</span><br><span class=\"line\">                min_index = j;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            ++j;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(min_index != i)&#123;</span><br><span class=\"line\">            A[i] ^= A[min_index];</span><br><span class=\"line\">            A[min_index] ^= A[i];</span><br><span class=\"line\">            A[i] ^= A[min_index];</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> A;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure> ## 插入排序 时间复杂度：<span class=\"math inline\">\\(O(N^2)\\)</span></p>\r\n<p>空间复杂度：<span class=\"math inline\">\\(O(1)\\)</span> <figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span>* <span class=\"title\">insertionSort</span><span class=\"params\">(<span class=\"keyword\">int</span>* A, <span class=\"keyword\">int</span> n)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(A == <span class=\"literal\">NULL</span> || n &lt;= <span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> A;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> cur, j;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i=<span class=\"number\">1</span>;i&lt;n;++i)&#123;</span><br><span class=\"line\">        cur = A[i];</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(j = i<span class=\"number\">-1</span>; j&gt;= <span class=\"number\">0</span>; --j)&#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(cur &lt; A[j])&#123;</span><br><span class=\"line\">                A[j+<span class=\"number\">1</span>] = A[j];</span><br><span class=\"line\">            &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">                <span class=\"keyword\">break</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        A[j+<span class=\"number\">1</span>] = cur;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> A;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure> ## 归并排序 时间复杂度：<span class=\"math inline\">\\(O(N\\log(N))\\)</span></p>\r\n<p>空间复杂度：<span class=\"math inline\">\\(O(N)\\)</span> <figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span>* <span class=\"title\">merge</span><span class=\"params\">(<span class=\"keyword\">int</span>* A, <span class=\"keyword\">int</span> lenA, <span class=\"keyword\">int</span>*B, <span class=\"keyword\">int</span> lenB)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> *result = <span class=\"keyword\">new</span> <span class=\"keyword\">int</span>[lenA + lenB];</span><br><span class=\"line\">    <span class=\"keyword\">int</span> i=<span class=\"number\">0</span>,j=<span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(i &lt; lenA &amp;&amp; j &lt; lenB)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(A[i] &lt;= B[j])&#123;</span><br><span class=\"line\">            result[i+j] = A[i];</span><br><span class=\"line\">            ++i;</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">            result[i+j] = B[j];</span><br><span class=\"line\">            ++j;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(i &lt; lenA)&#123;</span><br><span class=\"line\">        result[i+j] = A[i];</span><br><span class=\"line\">        ++i;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(j &lt; lenB)&#123;</span><br><span class=\"line\">        result[i+j] = B[j];</span><br><span class=\"line\">        ++j;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> result;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span>* <span class=\"title\">mergeSort</span><span class=\"params\">(<span class=\"keyword\">int</span>* A, <span class=\"keyword\">int</span> n)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(A == <span class=\"literal\">NULL</span> || n &lt;= <span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> A;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> center = n / <span class=\"number\">2</span>;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> merge(mergeSort(A, center), center, mergeSort(A + center, n - center), n - center);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure> ## 快速排序 时间复杂度：<span class=\"math inline\">\\(O(N\\log(N))\\)</span></p>\r\n<p>空间复杂度：<span class=\"math inline\">\\(O(\\log(N))\\)</span> ~ <span class=\"math inline\">\\(O(N)\\)</span> <figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">partition</span><span class=\"params\">(<span class=\"keyword\">int</span>* A, <span class=\"keyword\">int</span> n)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(n &lt;= <span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> small = <span class=\"number\">-1</span>;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> i = <span class=\"number\">0</span>, j = n<span class=\"number\">-2</span>;</span><br><span class=\"line\">        <span class=\"keyword\">while</span>(i &lt;= j)&#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(A[i] &lt;= A[n<span class=\"number\">-1</span>])&#123;</span><br><span class=\"line\">                small = i++;</span><br><span class=\"line\">            &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(A[j] &lt;= A[n<span class=\"number\">-1</span>])&#123;</span><br><span class=\"line\">                <span class=\"keyword\">if</span>(j != i)&#123;</span><br><span class=\"line\">                    A[j] ^= A[i];</span><br><span class=\"line\">                    A[i] ^= A[j];</span><br><span class=\"line\">                    A[j] ^= A[i];</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">                small = i++;</span><br><span class=\"line\">            &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">                --j;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(small + <span class=\"number\">1</span> != n - <span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">            A[small + <span class=\"number\">1</span>] ^= A[n - <span class=\"number\">1</span>];</span><br><span class=\"line\">            A[n - <span class=\"number\">1</span>] ^= A[small + <span class=\"number\">1</span>];</span><br><span class=\"line\">            A[small + <span class=\"number\">1</span>] ^= A[n - <span class=\"number\">1</span>];</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> small + <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span>* <span class=\"title\">quickSort</span><span class=\"params\">(<span class=\"keyword\">int</span>* A, <span class=\"keyword\">int</span> n)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(A == <span class=\"literal\">NULL</span> || n &lt;= <span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> A;</span><br><span class=\"line\">    &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> center = partition(A, n);</span><br><span class=\"line\">        quickSort(A, center);</span><br><span class=\"line\">        quickSort(A+center+<span class=\"number\">1</span>, n-center<span class=\"number\">-1</span>);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> A;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure> ## 堆排序 时间复杂度：<span class=\"math inline\">\\(O(N\\log(N))\\)</span></p>\r\n<p>空间复杂度：<span class=\"math inline\">\\(O(1)\\)</span></p>\r\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">parent</span><span class=\"params\">(<span class=\"keyword\">int</span> i)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> (i - <span class=\"number\">1</span>) / <span class=\"number\">2</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">left</span><span class=\"params\">(<span class=\"keyword\">int</span> i)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> i * <span class=\"number\">2</span> + <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">right</span><span class=\"params\">(<span class=\"keyword\">int</span> i)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> i * <span class=\"number\">2</span> + <span class=\"number\">2</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">DownHeap</span><span class=\"params\">(<span class=\"keyword\">int</span>* A, <span class=\"keyword\">int</span> cur, <span class=\"keyword\">int</span> n)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> left_idx, right_idx, max_idx, end = parent(n - <span class=\"number\">1</span>);</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(cur &lt;= end)&#123;</span><br><span class=\"line\">        left_idx = left(cur);</span><br><span class=\"line\">        right_idx = right(cur);</span><br><span class=\"line\">        max_idx = A[cur] &lt; A[left_idx]? left_idx : cur;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(right_idx &lt; n)&#123;</span><br><span class=\"line\">            max_idx = A[max_idx] &lt; A[right_idx]? right_idx : max_idx;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(max_idx != cur)&#123;</span><br><span class=\"line\">            A[cur] ^= A[max_idx];</span><br><span class=\"line\">            A[max_idx] ^= A[cur];</span><br><span class=\"line\">            A[cur] ^= A[max_idx];</span><br><span class=\"line\">            cur = max_idx;</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">createHeap</span><span class=\"params\">(<span class=\"keyword\">int</span>* A, <span class=\"keyword\">int</span> n)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> end = parent(n - <span class=\"number\">1</span>);</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = end; i &gt;= <span class=\"number\">0</span>; --i)&#123;</span><br><span class=\"line\">        DownHeap(A, i, n);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span>* <span class=\"title\">heapSort</span><span class=\"params\">(<span class=\"keyword\">int</span>* A, <span class=\"keyword\">int</span> n)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(A == <span class=\"literal\">NULL</span> || n &lt;= <span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> A;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    createHeap(A, n);</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = n<span class=\"number\">-1</span>;i&gt;<span class=\"number\">0</span>;--i)&#123;</span><br><span class=\"line\">        DownHeap(A, <span class=\"number\">0</span>, i+<span class=\"number\">1</span>);</span><br><span class=\"line\">        A[i] ^= A[<span class=\"number\">0</span>];</span><br><span class=\"line\">        A[<span class=\"number\">0</span>] ^= A[i];</span><br><span class=\"line\">        A[i] ^= A[<span class=\"number\">0</span>];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> A;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">DownHeap</span><span class=\"params\">(<span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt;&amp; heap, <span class=\"keyword\">int</span> i, <span class=\"keyword\">int</span> n)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// 大顶堆的下滤操作</span></span><br><span class=\"line\">    <span class=\"keyword\">int</span> temp = heap[i];</span><br><span class=\"line\">    <span class=\"keyword\">int</span> l,r,maxChild=i;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(<span class=\"literal\">true</span>)&#123;</span><br><span class=\"line\">        l = i * <span class=\"number\">2</span> + <span class=\"number\">1</span>;</span><br><span class=\"line\">        r = l + <span class=\"number\">1</span>;</span><br><span class=\"line\">        maxChild = (r &lt; n &amp;&amp; heap[r] &gt; heap[l]) ? r : l;</span><br><span class=\"line\">        maxChild = (maxChild &lt; n &amp;&amp; heap[maxChild] &gt; temp) ? maxChild : i;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(maxChild != i)&#123;</span><br><span class=\"line\">            heap[i] = heap[maxChild];</span><br><span class=\"line\">            i = maxChild;</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    heap[i] = temp;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> ;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; <span class=\"title\">sortArray</span><span class=\"params\">(<span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">//堆排序</span></span><br><span class=\"line\">    <span class=\"keyword\">int</span> n = nums.size();</span><br><span class=\"line\">    <span class=\"comment\">//构建大顶堆</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = n / <span class=\"number\">2</span>; i &gt;= <span class=\"number\">0</span>; --i)&#123;</span><br><span class=\"line\">        DownHeap(nums, i, n);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//逐个删除堆顶，进行排序</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = n; i &gt; <span class=\"number\">1</span>; --i)&#123;</span><br><span class=\"line\">        swap(nums[<span class=\"number\">0</span>], nums[i - <span class=\"number\">1</span>]);</span><br><span class=\"line\">        DownHeap(nums, <span class=\"number\">0</span>, i - <span class=\"number\">1</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> nums;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\r\n<h2 id=\"希尔排序\">希尔排序</h2>\r\n<p>时间复杂度：<span class=\"math inline\">\\(O(N\\log(N))\\)</span></p>\r\n<p>空间复杂度：<span class=\"math inline\">\\(O(1)\\)</span> <figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">insertionSort</span><span class=\"params\">(<span class=\"keyword\">int</span>* A, <span class=\"keyword\">int</span> n, <span class=\"keyword\">int</span> space)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> cur, j;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i=space;i&lt;n;i++)&#123;</span><br><span class=\"line\">        cur = A[i];</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(j = i-space; j&gt;= <span class=\"number\">0</span>; j-=space)&#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(cur &lt; A[j])&#123;</span><br><span class=\"line\">                A[j+space] = A[j];</span><br><span class=\"line\">            &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">                <span class=\"keyword\">break</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        A[j+space] = cur;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span>* <span class=\"title\">shellSort</span><span class=\"params\">(<span class=\"keyword\">int</span>* A, <span class=\"keyword\">int</span> n)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(A == <span class=\"literal\">NULL</span> || n &lt;= <span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> A;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> space = n / <span class=\"number\">2</span>; space&gt;=<span class=\"number\">1</span>; space/= <span class=\"number\">2</span>)&#123;</span><br><span class=\"line\">        insertionSort(A, n, space);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> A;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; <span class=\"title\">shellSort</span><span class=\"params\">(<span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">//希尔排序</span></span><br><span class=\"line\">    <span class=\"keyword\">int</span> n = nums.size();</span><br><span class=\"line\">    <span class=\"keyword\">int</span> inc = <span class=\"number\">2</span>;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> temp;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> i,j;</span><br><span class=\"line\">    <span class=\"comment\">// 计算Hibbard增量</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span>(inc * <span class=\"number\">2</span> &lt; n)&#123;</span><br><span class=\"line\">        inc *= <span class=\"number\">2</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    inc -= <span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(inc &gt;= <span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(i = inc; i &lt; n; ++i)&#123;</span><br><span class=\"line\">            <span class=\"comment\">// 执行增量插入排序</span></span><br><span class=\"line\">            temp = nums[i];</span><br><span class=\"line\">            <span class=\"keyword\">for</span>(j = i; j &gt;= inc &amp;&amp; nums[j-inc] &gt; temp; j -= inc)&#123;</span><br><span class=\"line\">                nums[j] = nums[j-inc];</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            nums[j] = temp;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        inc = (inc - <span class=\"number\">1</span>) / <span class=\"number\">2</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> nums;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure> ## 计数排序</p>\r\n<p>时间复杂度：<span class=\"math inline\">\\(O(N)\\)</span></p>\r\n<p>空间复杂度：<span class=\"math inline\">\\(O(M)\\)</span>，这里的<span class=\"math inline\">\\(M\\)</span>和待排序元素的取值范围有关。</p>\r\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span>* <span class=\"title\">countingSort</span><span class=\"params\">(<span class=\"keyword\">int</span>* A, <span class=\"keyword\">int</span> n)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(A==<span class=\"literal\">nullptr</span> || n &lt;=<span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> A;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> min = A[<span class=\"number\">0</span>], max = A[<span class=\"number\">0</span>];</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">1</span>;i &lt; n; ++i)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(A[i] &gt; max)&#123;</span><br><span class=\"line\">            max = A[i];</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(A[i] &lt; min)&#123;</span><br><span class=\"line\">            min = A[i];</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> num = max - min + <span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> *count = <span class=\"keyword\">new</span> <span class=\"keyword\">int</span>[num]();</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>;i &lt; n; ++i)&#123;</span><br><span class=\"line\">        count[A[i] - min] += <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> i = <span class=\"number\">0</span>, j = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(i &lt; n)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(count[j] &gt; <span class=\"number\">0</span>)&#123;</span><br><span class=\"line\">            count[j] -= <span class=\"number\">1</span>;</span><br><span class=\"line\">            A[i++] = j + min;</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">            ++j;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">delete</span> []count;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> A;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\r\n<h2 id=\"基数排序\">基数排序</h2>\r\n<p>时间复杂度：<span class=\"math inline\">\\(O(N)\\)</span></p>\r\n<p>空间复杂度：<span class=\"math inline\">\\(O(M)\\)</span>，这里的<span class=\"math inline\">\\(M\\)</span>和待排序元素的取值范围有关。</p>\r\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">getdig</span><span class=\"params\">(<span class=\"keyword\">int</span> x, <span class=\"keyword\">int</span> base)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> ubase = <span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; base; i++)&#123;</span><br><span class=\"line\">        ubase *= <span class=\"number\">10</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    x = x % (ubase * <span class=\"number\">10</span>);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> x / ubase;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span>* <span class=\"title\">radixSort</span><span class=\"params\">(<span class=\"keyword\">int</span>* A, <span class=\"keyword\">int</span> n)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> base = <span class=\"number\">0</span>, dig, j;</span><br><span class=\"line\">    <span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; temp;</span><br><span class=\"line\">    <span class=\"built_in\">map</span>&lt;<span class=\"keyword\">int</span>, <span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; &gt; buckets;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(base &lt; <span class=\"number\">4</span>)&#123; <span class=\"comment\">// 如果待排序数字全都小于10000，那么这里只用到4</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; n; ++i)&#123;</span><br><span class=\"line\">            dig = getdig(A[i], base);</span><br><span class=\"line\">            buckets[dig].push_back(A[i]);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        j = <span class=\"number\">0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"number\">10</span>; ++i)&#123;</span><br><span class=\"line\">            temp = buckets[i];</span><br><span class=\"line\">            <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> k = <span class=\"number\">0</span>; k &lt; temp.size(); ++k)&#123;</span><br><span class=\"line\">                A[j++] = temp[k];</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            buckets[i].clear();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        ++base;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> A;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\r\n<h2 id=\"基本有序的情况\">基本有序的情况</h2>\r\n<p>如果一个序列已经基本有序，即排好序之后，其中每个元素的移动位置偏移不超过K，那么这种情况下，可以使用改进的堆排序方法，每次使用前K个数字建立小根堆（建好后最小的数字肯定在第一个位置），每次向后移动一个位置，维持小根堆，这样一直往后移动最终完成排序，其时间复杂度为<span class=\"math inline\">\\(O(N \\times \\log(K))\\)</span>，其次，这种情况下插入排序是比较好的，其时间复杂度为<span class=\"math inline\">\\(O(N \\times K)\\)</span>，改进的堆排序算法如下（这里为了实现简单，使用了额外<span class=\"math inline\">\\(O(K)\\)</span>的空间，其实不需要，只不过代码可能会变得比较复杂）：</p>\r\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">parent</span><span class=\"params\">(<span class=\"keyword\">int</span> i)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> (i - <span class=\"number\">1</span>) / <span class=\"number\">2</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">left</span><span class=\"params\">(<span class=\"keyword\">int</span> i)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> i * <span class=\"number\">2</span> + <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">right</span><span class=\"params\">(<span class=\"keyword\">int</span> i)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> i * <span class=\"number\">2</span> + <span class=\"number\">2</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">modifyHeap</span><span class=\"params\">(<span class=\"keyword\">int</span> *A, <span class=\"keyword\">int</span> cur, <span class=\"keyword\">int</span> n)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> left_idx, right_idx, min_idx, end = parent(n - <span class=\"number\">1</span>);</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(cur &lt;= end)&#123;</span><br><span class=\"line\">        left_idx = left(cur);</span><br><span class=\"line\">        right_idx = right(cur);</span><br><span class=\"line\">        min_idx = A[cur] &gt; A[left_idx]? left_idx : cur;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(right_idx &lt; n)&#123;</span><br><span class=\"line\">            min_idx = A[min_idx] &gt; A[right_idx]? right_idx : min_idx;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(min_idx != cur)&#123;</span><br><span class=\"line\">            A[cur] ^= A[min_idx];</span><br><span class=\"line\">            A[min_idx] ^= A[cur];</span><br><span class=\"line\">            A[cur] ^= A[min_idx];</span><br><span class=\"line\">            cur = min_idx;</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">createHeap</span><span class=\"params\">(<span class=\"keyword\">int</span> *A, <span class=\"keyword\">int</span> n)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> end = parent(n - <span class=\"number\">1</span>);</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = end; i &gt;= <span class=\"number\">0</span>; --i)&#123;</span><br><span class=\"line\">        modifyHeap(A, i, n);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"function\"><span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; <span class=\"title\">sortElement</span><span class=\"params\">(<span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; A, <span class=\"keyword\">int</span> n, <span class=\"keyword\">int</span> k)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(A.empty() || n &lt;= <span class=\"number\">1</span> || k &lt; <span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> A;</span><br><span class=\"line\">    &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> start = <span class=\"number\">0</span>, group_num = <span class=\"number\">0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">int</span>* B = <span class=\"keyword\">new</span> <span class=\"keyword\">int</span>[k + <span class=\"number\">1</span>];</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; n &amp;&amp; i &lt; k + <span class=\"number\">1</span>; ++i)&#123;</span><br><span class=\"line\">            B[i] = A[i];</span><br><span class=\"line\">            group_num += <span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        createHeap(B, group_num);</span><br><span class=\"line\">        <span class=\"keyword\">while</span>(group_num &gt; <span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">            A[start++] = B[<span class=\"number\">0</span>];</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(start + group_num &gt; n)&#123;</span><br><span class=\"line\">                group_num--;</span><br><span class=\"line\">                B[<span class=\"number\">0</span>] = B[group_num];</span><br><span class=\"line\">                modifyHeap(B, <span class=\"number\">0</span>, group_num);</span><br><span class=\"line\">            &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">                B[<span class=\"number\">0</span>] = A[start + group_num - <span class=\"number\">1</span>];</span><br><span class=\"line\">                modifyHeap(B, <span class=\"number\">0</span>, group_num);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(group_num == <span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">            A[start] = B[<span class=\"number\">0</span>];</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> A;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\r\n<h1 id=\"字符串\">字符串</h1>\r\n<h2 id=\"kmp\">KMP</h2>\r\n<p>字符串匹配算法。</p>\r\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">bool</span> <span class=\"title\">kmp</span><span class=\"params\">(<span class=\"built_in\">string</span> A, <span class=\"keyword\">int</span> lena, <span class=\"built_in\">string</span> B, <span class=\"keyword\">int</span> lenb)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// 在A中查找B</span></span><br><span class=\"line\">    <span class=\"keyword\">int</span> *next = <span class=\"keyword\">new</span> <span class=\"keyword\">int</span>[lenb]();</span><br><span class=\"line\">    <span class=\"keyword\">int</span> i=<span class=\"number\">1</span>,j=<span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(i &lt; lenb)&#123;<span class=\"comment\">// 构造next数组</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span>(B[i] == B[j])&#123;</span><br><span class=\"line\">            next[i++] = ++j;</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">            j = next[j - <span class=\"number\">1</span>];</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(j == <span class=\"number\">0</span>)&#123;</span><br><span class=\"line\">                next[i++] = <span class=\"number\">0</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    i = j = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(i &lt;= lena - lenb + j)&#123;<span class=\"comment\">//  进行匹配</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span>(A[i] == B[j])&#123;</span><br><span class=\"line\">            ++i;</span><br><span class=\"line\">            ++j;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(j == lenb)&#123;</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"literal\">true</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(j == <span class=\"number\">0</span>)&#123;</span><br><span class=\"line\">            ++i;</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">            j = next[j - <span class=\"number\">1</span>];</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"literal\">false</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\r\n<h1 id=\"栈和队列\">栈和队列</h1>\r\n<h2 id=\"滑动窗口最大值\">滑动窗口最大值</h2>\r\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; <span class=\"title\">slide</span><span class=\"params\">(<span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; arr, <span class=\"keyword\">int</span> n, <span class=\"keyword\">int</span> w)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// write code here</span></span><br><span class=\"line\">    <span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; result;</span><br><span class=\"line\">    <span class=\"built_in\">deque</span>&lt;<span class=\"keyword\">int</span>&gt; maxque;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i=<span class=\"number\">0</span>;i&lt;n;++i)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">while</span>(!maxque.empty() &amp;&amp; arr[maxque.back()] &lt; arr[i])&#123;</span><br><span class=\"line\">            maxque.pop_back();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        maxque.push_back(i);</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(i &gt;= w - <span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">            <span class=\"keyword\">while</span>(maxque.front() &lt;= i - w)&#123;</span><br><span class=\"line\">                maxque.pop_front();</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            result.push_back(arr[maxque.front()]);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> result;</span><br></pre></td></tr></table></figure>\r\n<h2 id=\"构造maxtree\">构造MaxTree</h2>\r\n<p>对于一个没有重复元素的整数数组，请用其中元素构造一棵MaxTree，MaxTree定义为一棵二叉树，其中的节点与数组元素一一对应，同时对于MaxTree的每棵子树，它的根的元素值为子树的最大值。现有一建树方法，对于数组中的每个元素，其在树中的父亲为数组中它左边比它大的第一个数和右边比它大的第一个数中更小的一个。若两边都不存在比它大的数，那么它就是树根。请设计O(n)的算法实现这个方法。</p>\r\n<p>给定一个无重复元素的数组A和它的大小n，请返回一个数组，其中每个元素为原数组中对应位置元素在树中的父亲节点的编号，若为根则值为-1。</p>\r\n<p>测试样例：</p>\r\n<p>[3,1,4,2],4</p>\r\n<p>返回：[2,0,-1,2] <figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; <span class=\"title\">buildMaxTree</span><span class=\"params\">(<span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; A, <span class=\"keyword\">int</span> n)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// write code here</span></span><br><span class=\"line\">    <span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; result;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> l,r;</span><br><span class=\"line\">    <span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; left_max, right_max;</span><br><span class=\"line\">    <span class=\"built_in\">stack</span>&lt;<span class=\"keyword\">int</span>&gt; left_stack, right_stack;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; n; ++i)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(left_stack.empty())&#123;</span><br><span class=\"line\">            left_max.push_back(<span class=\"number\">-1</span>);</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">while</span>(!left_stack.empty() &amp;&amp; A[left_stack.top()] &lt;= A[i])&#123;</span><br><span class=\"line\">                left_stack.pop();</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(left_stack.empty())&#123;</span><br><span class=\"line\">                left_max.push_back(<span class=\"number\">-1</span>);</span><br><span class=\"line\">            &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">                left_max.push_back(left_stack.top());</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        left_stack.push(i);</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">if</span>(right_stack.empty())&#123;</span><br><span class=\"line\">            right_max.push_back(<span class=\"number\">-1</span>);</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">while</span>(!right_stack.empty() &amp;&amp; A[right_stack.top()] &lt;= A[n - i - <span class=\"number\">1</span>])&#123;</span><br><span class=\"line\">                right_stack.pop();</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(right_stack.empty())&#123;</span><br><span class=\"line\">                right_max.push_back(<span class=\"number\">-1</span>);</span><br><span class=\"line\">            &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">                right_max.push_back(right_stack.top());</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        right_stack.push(n - i - <span class=\"number\">1</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; n; ++i)&#123;</span><br><span class=\"line\">        l = left_max[i];</span><br><span class=\"line\">        r = right_max[n - i - <span class=\"number\">1</span>];</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(l == <span class=\"number\">-1</span> &amp;&amp; r == <span class=\"number\">-1</span>)&#123;</span><br><span class=\"line\">            result.push_back(<span class=\"number\">-1</span>);</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(l == <span class=\"number\">-1</span>)&#123;</span><br><span class=\"line\">            result.push_back(r);</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(r == <span class=\"number\">-1</span>)&#123;</span><br><span class=\"line\">            result.push_back(l);</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">            result.push_back(A[l] &gt; A[r]? r : l);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> result;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure> # 链表</p>\r\n<h2 id=\"链表的回文判断\">链表的回文判断</h2>\r\n<p>请编写一个函数，检查链表是否为回文。</p>\r\n<p>给定一个链表ListNode* pHead，请返回一个bool，代表链表是否为回文。</p>\r\n<p>测试样例：</p>\r\n<p>{1,2,3,2,1}</p>\r\n<p>返回：true</p>\r\n<p>{1,2,3,2,3}</p>\r\n<p>返回：false</p>\r\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/*</span></span><br><span class=\"line\"><span class=\"comment\">struct ListNode &#123;</span></span><br><span class=\"line\"><span class=\"comment\">    int val;</span></span><br><span class=\"line\"><span class=\"comment\">    struct ListNode *next;</span></span><br><span class=\"line\"><span class=\"comment\">    ListNode(int x) : val(x), next(NULL) &#123;&#125;</span></span><br><span class=\"line\"><span class=\"comment\">&#125;;*/</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">reverse</span><span class=\"params\">(ListNode *pHead, ListNode *pTail)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(pHead == <span class=\"literal\">nullptr</span> || pTail == pHead)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ListNode *cur=pHead, *next=pHead-&gt;next, *temp;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(next!=pTail)&#123;</span><br><span class=\"line\">        temp = next-&gt;next;</span><br><span class=\"line\">        next-&gt;next = cur;</span><br><span class=\"line\">        cur = next;</span><br><span class=\"line\">        next = temp;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    pTail-&gt;next = cur;</span><br><span class=\"line\">    pHead-&gt;next = <span class=\"literal\">nullptr</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">bool</span> <span class=\"title\">isPalindrome</span><span class=\"params\">(ListNode* pHead)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// write code here</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span>(pHead == <span class=\"literal\">nullptr</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">false</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ListNode *center=pHead, *tail=pHead, *pcur=pHead, *ncur;</span><br><span class=\"line\">    <span class=\"keyword\">bool</span> result = <span class=\"literal\">true</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(center-&gt;next != <span class=\"literal\">nullptr</span> &amp;&amp; tail-&gt;next!=<span class=\"literal\">nullptr</span>)&#123;</span><br><span class=\"line\">        center = center-&gt;next;</span><br><span class=\"line\">        tail = tail-&gt;next;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(tail-&gt;next!=<span class=\"literal\">nullptr</span>)&#123;</span><br><span class=\"line\">            tail = tail-&gt;next;</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(center == tail)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(pHead-&gt;val == tail-&gt;val)&#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"literal\">true</span>;</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"literal\">false</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    reverse(center, tail);</span><br><span class=\"line\">    ncur = tail;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(pcur!=center)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(pcur-&gt;val != ncur-&gt;val)&#123;</span><br><span class=\"line\">            result = <span class=\"literal\">false</span>;</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">            pcur = pcur-&gt;next;</span><br><span class=\"line\">            ncur = ncur-&gt;next;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    reverse(tail, center);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> result;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\r\n<h2 id=\"复杂链表的复制\">复杂链表的复制</h2>\r\n<p>输入一个复杂链表（每个节点中有节点值，以及两个指针，一个指向下一个节点，另一个特殊指针指向任意一个节点），复制这个复杂链表并返回。</p>\r\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/*</span></span><br><span class=\"line\"><span class=\"comment\">struct RandomListNode &#123;</span></span><br><span class=\"line\"><span class=\"comment\">    int label;</span></span><br><span class=\"line\"><span class=\"comment\">    struct RandomListNode *next, *random;</span></span><br><span class=\"line\"><span class=\"comment\">    RandomListNode(int x) :</span></span><br><span class=\"line\"><span class=\"comment\">            label(x), next(NULL), random(NULL) &#123;</span></span><br><span class=\"line\"><span class=\"comment\">    &#125;</span></span><br><span class=\"line\"><span class=\"comment\">&#125;;</span></span><br><span class=\"line\"><span class=\"comment\">*/</span></span><br><span class=\"line\"><span class=\"function\">RandomListNode* <span class=\"title\">Clone</span><span class=\"params\">(RandomListNode* pHead)</span></span>&#123;</span><br><span class=\"line\">    RandomListNode *cur=pHead, *copy, *resultHead, *resultCur;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(pHead==<span class=\"literal\">nullptr</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">nullptr</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(cur!=<span class=\"literal\">nullptr</span>)&#123;</span><br><span class=\"line\">        copy = <span class=\"keyword\">new</span> RandomListNode(cur-&gt;label);</span><br><span class=\"line\">        copy-&gt;next = cur-&gt;next;</span><br><span class=\"line\">        cur-&gt;next = copy;</span><br><span class=\"line\">        cur = copy-&gt;next;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    cur = pHead;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(cur!=<span class=\"literal\">nullptr</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(cur-&gt;random != <span class=\"literal\">nullptr</span>)&#123;</span><br><span class=\"line\">            cur-&gt;next-&gt;random = cur-&gt;random-&gt;next;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        cur = cur-&gt;next-&gt;next;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    resultCur = resultHead = pHead-&gt;next;</span><br><span class=\"line\">    cur = pHead;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(cur!=<span class=\"literal\">nullptr</span> &amp;&amp; resultCur!=<span class=\"literal\">nullptr</span>)&#123;</span><br><span class=\"line\">        cur-&gt;next = cur-&gt;next-&gt;next;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(resultCur-&gt;next!=<span class=\"literal\">nullptr</span>)&#123;</span><br><span class=\"line\">            resultCur-&gt;next = resultCur-&gt;next-&gt;next;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        cur = cur-&gt;next;</span><br><span class=\"line\">        resultCur = resultCur-&gt;next;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> resultHead;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\r\n<h2 id=\"判断链表是否有环\">判断链表是否有环</h2>\r\n<p>如何判断一个单链表是否有环？有环的话返回进入环的第一个节点的值，无环的话返回-1。如果链表的长度为N，请做到时间复杂度O(N)，额外空间复杂度O(1)。</p>\r\n<p>给定一个单链表的头结点head，请返回所求值。</p>\r\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">chkLoop</span><span class=\"params\">(ListNode* head)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// write code here</span></span><br><span class=\"line\">    ListNode *fast=head, *slow=head;</span><br><span class=\"line\">    <span class=\"keyword\">bool</span> hasLoop = <span class=\"literal\">false</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(fast != <span class=\"literal\">nullptr</span>)&#123;</span><br><span class=\"line\">        slow = slow-&gt;next;</span><br><span class=\"line\">        fast = fast-&gt;next;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(fast)&#123;</span><br><span class=\"line\">            fast = fast-&gt;next;</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(fast == slow)&#123;</span><br><span class=\"line\">            hasLoop = <span class=\"literal\">true</span>;</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(!hasLoop)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">-1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    fast = head;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(fast!=slow)&#123;</span><br><span class=\"line\">        fast = fast-&gt;next;</span><br><span class=\"line\">        slow = slow-&gt;next;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> fast-&gt;val;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\r\n<h2 id=\"无环链表是否相交\">无环链表是否相交</h2>\r\n<p>直接判断最后一个节点是否相等即可，如果要求返回第一个共同节点，那么则需要先统计各链表长度<span class=\"math inline\">\\(m,n\\)</span>，假如<span class=\"math inline\">\\(m \\le n\\)</span>，那么第二个链表从第<span class=\"math inline\">\\(n-m\\)</span>个节点开始遍历，第一个链表从头结点开始遍历，两个遍历同步，同时比较当前节点是否相同，第一个相同节点即返回。</p>\r\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/*</span></span><br><span class=\"line\"><span class=\"comment\">struct ListNode &#123;</span></span><br><span class=\"line\"><span class=\"comment\">    int val;</span></span><br><span class=\"line\"><span class=\"comment\">    struct ListNode *next;</span></span><br><span class=\"line\"><span class=\"comment\">    ListNode(int x) : val(x), next(NULL) &#123;&#125;</span></span><br><span class=\"line\"><span class=\"comment\">&#125;;*/</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">bool</span> <span class=\"title\">chkIntersect</span><span class=\"params\">(ListNode* headA, ListNode* headB)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// write code here</span></span><br><span class=\"line\">    ListNode *nodeA=headA, *nodeB=headB;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(nodeA == <span class=\"literal\">nullptr</span> || nodeB == <span class=\"literal\">nullptr</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">false</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(nodeA-&gt;next)&#123;</span><br><span class=\"line\">        nodeA = nodeA-&gt;next;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(nodeB-&gt;next)&#123;</span><br><span class=\"line\">        nodeB = nodeB-&gt;next;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(nodeA == nodeB)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">true</span>;</span><br><span class=\"line\">    &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">false</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\r\n<h2 id=\"有环链表是否相交\">有环链表是否相交</h2>\r\n<p>如何判断两个有环单链表是否相交？相交的话返回第一个相交的节点，不想交的话返回空。如果两个链表长度分别为N和M，请做到时间复杂度O(N+M)，额外空间复杂度O(1)。</p>\r\n<p>给定两个链表的头结点head1和head2。请返回一个bool值代表它们是否相交。</p>\r\n<p>如果还需要找到第一个共同节点，先看是否相交，如果相交，有两种情况，第一种是入环之前相交，这样的话和首先将环去掉，然后按照无环链表的情况来进行查找，第二种情况则直接返回任意一个链表的入环节点都可以算作第一个相交节点。</p>\r\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/*</span></span><br><span class=\"line\"><span class=\"comment\">struct ListNode &#123;</span></span><br><span class=\"line\"><span class=\"comment\">    int val;</span></span><br><span class=\"line\"><span class=\"comment\">    struct ListNode *next;</span></span><br><span class=\"line\"><span class=\"comment\">    ListNode(int x) : val(x), next(NULL) &#123;&#125;</span></span><br><span class=\"line\"><span class=\"comment\">&#125;;*/</span></span><br><span class=\"line\"><span class=\"function\">ListNode* <span class=\"title\">firstLoopNode</span><span class=\"params\">(ListNode* head)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// write code here</span></span><br><span class=\"line\">    ListNode *fast=head, *slow=head;</span><br><span class=\"line\">    <span class=\"keyword\">bool</span> hasLoop = <span class=\"literal\">false</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(fast != <span class=\"literal\">nullptr</span>)&#123;</span><br><span class=\"line\">        slow = slow-&gt;next;</span><br><span class=\"line\">        fast = fast-&gt;next;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(fast)&#123;</span><br><span class=\"line\">            fast = fast-&gt;next;</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(fast == slow)&#123;</span><br><span class=\"line\">            hasLoop = <span class=\"literal\">true</span>;</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(!hasLoop)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">nullptr</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    fast = head;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(fast!=slow)&#123;</span><br><span class=\"line\">        fast = fast-&gt;next;</span><br><span class=\"line\">        slow = slow-&gt;next;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> fast;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">bool</span> <span class=\"title\">chkInter</span><span class=\"params\">(ListNode* head1, ListNode* head2)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// write code here</span></span><br><span class=\"line\">    ListNode *loopNode1 = firstLoopNode(head1);</span><br><span class=\"line\">    ListNode *loopNode2 = firstLoopNode(head2);</span><br><span class=\"line\">    ListNode *temp;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(head1 == <span class=\"literal\">nullptr</span> || head2 == <span class=\"literal\">nullptr</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">false</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(loopNode1 == loopNode2)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">true</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    temp = loopNode2;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(temp-&gt;next != loopNode2)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(temp == loopNode1)&#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"literal\">true</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        temp = temp-&gt;next;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"literal\">false</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\r\n<h1 id=\"二分查找\">二分查找</h1>\r\n<h2 id=\"查找局部最小值\">查找局部最小值</h2>\r\n<p>定义局部最小的概念。arr长度为1时，arr[0]是局部最小。arr的长度为N(N&gt;1)时，如果arr[0]&lt; arr[1]，那么arr[0]是局部最小；如果arr[N-1] &lt; arr[N-2]，那么arr[N-1]是局部最小；如果0 &lt; i &lt; N-1，既有arr[i] &lt; arr[i-1]又有arr[i] &lt; arr[i+1]，那么arr[i]是局部最小。 给定无序数组arr，已知arr中任意两个相邻的数都不相等，写一个函数，只需返回arr中任意一个局部最小出现的位置即可。 <figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">getLessIndex</span><span class=\"params\">(<span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; arr)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> start = <span class=\"number\">0</span>, end = arr.size(), mid;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(end == <span class=\"number\">0</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">-1</span>;</span><br><span class=\"line\">    &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(end == <span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">if</span>(arr[start] &lt; arr[start])&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> start;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">if</span>(arr[end - <span class=\"number\">2</span>] &gt; arr[end - <span class=\"number\">1</span>])&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> end;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">while</span>(start &lt; end - <span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">        mid = start + (end - start)/<span class=\"number\">2</span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(arr[mid] &gt; arr[mid - <span class=\"number\">1</span>])&#123;</span><br><span class=\"line\">            end = mid;</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">            start = mid;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> start;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure> ## 查找循环有序数组的最小值</p>\r\n<p>对于一个有序循环数组arr，返回arr中的最小值。有序循环数组是指，有序数组左边任意长度的部分放到右边去，右边的部分拿到左边来。比如数组[1,2,3,3,4]，是有序循环数组，[4,1,2,3,3]也是。</p>\r\n<p>给定数组arr及它的大小n，请返回最小值。</p>\r\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">getMin</span><span class=\"params\">(<span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; arr, <span class=\"keyword\">int</span> n)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> start = <span class=\"number\">0</span>, end = n, mid;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(n == <span class=\"number\">0</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">-1</span>;</span><br><span class=\"line\">    &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(n == <span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> arr[<span class=\"number\">0</span>];</span><br><span class=\"line\">    &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(n == <span class=\"number\">2</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> arr[<span class=\"number\">0</span>] &gt; arr[<span class=\"number\">1</span>] ? arr[<span class=\"number\">1</span>] : arr[<span class=\"number\">0</span>];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(start &lt; end - <span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">        mid = start + (end - start) / <span class=\"number\">2</span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(arr[start] &gt; arr[end - <span class=\"number\">1</span>])&#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(arr[mid] &gt; arr[end<span class=\"number\">-1</span>])&#123;</span><br><span class=\"line\">                start = mid + <span class=\"number\">1</span>;</span><br><span class=\"line\">            &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(arr[mid] &lt; arr[end<span class=\"number\">-1</span>])&#123;</span><br><span class=\"line\">                end = mid + <span class=\"number\">1</span>;</span><br><span class=\"line\">            &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">                start = mid;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(arr[start] &lt; arr[end])&#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> arr[start];</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(arr[mid] &lt; arr[end - <span class=\"number\">1</span>])&#123;</span><br><span class=\"line\">                end = mid + <span class=\"number\">1</span>;</span><br><span class=\"line\">            &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">                start += <span class=\"number\">1</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> arr[start];</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\r\n<h2 id=\"完全二叉树的节点个数\">完全二叉树的节点个数</h2>\r\n<p>给定一棵完全二叉树的根节点root，返回这棵树的节点个数。如果完全二叉树的节点数为N，请实现时间复杂度低于O(N)的解法。</p>\r\n<p>给定树的根结点root，请返回树的大小。</p>\r\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">count</span><span class=\"params\">(TreeNode* root)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> nodeNum = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> lldepth = <span class=\"number\">-1</span>, rldepth = <span class=\"number\">0</span>;</span><br><span class=\"line\">    TreeNode *cur_root = root, *cur;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(cur_root)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(lldepth==<span class=\"number\">-1</span>)&#123;</span><br><span class=\"line\">            cur = cur_root;</span><br><span class=\"line\">            <span class=\"keyword\">while</span>(cur)&#123;</span><br><span class=\"line\">                lldepth += <span class=\"number\">1</span>;</span><br><span class=\"line\">                cur = cur-&gt;left;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        cur = cur_root-&gt;right;</span><br><span class=\"line\">        <span class=\"keyword\">while</span>(cur)&#123;</span><br><span class=\"line\">            rldepth += <span class=\"number\">1</span>;</span><br><span class=\"line\">            cur = cur-&gt;left;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(rldepth == lldepth)&#123;</span><br><span class=\"line\">            nodeNum += <span class=\"number\">1</span> &lt;&lt; lldepth;</span><br><span class=\"line\">            cur_root = cur_root-&gt;right;</span><br><span class=\"line\">            lldepth = rldepth - <span class=\"number\">1</span>;</span><br><span class=\"line\">            rldepth = <span class=\"number\">0</span>;</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">            nodeNum += <span class=\"number\">1</span> &lt;&lt; rldepth;</span><br><span class=\"line\">            cur_root = cur_root-&gt;left;</span><br><span class=\"line\">            lldepth -= <span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> nodeNum;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\r\n<h1 id=\"二叉树\">二叉树</h1>\r\n<h2 id=\"前序中序后序三种遍历方式\">前序、中序、后序三种遍历方式</h2>\r\n<p>请用递归方式实现二叉树的先序、中序和后序的遍历打印。</p>\r\n<p>给定一个二叉树的根结点root，请依次返回二叉树的先序，中序和后续遍历(二维数组的形式)。</p>\r\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">pre</span><span class=\"params\">(TreeNode* root, <span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; &amp;result)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"built_in\">stack</span>&lt;TreeNode*&gt; s;</span><br><span class=\"line\">    TreeNode *cur;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(root == <span class=\"literal\">nullptr</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> ;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    s.push(root);</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(!s.empty())&#123;</span><br><span class=\"line\">        cur = s.top();</span><br><span class=\"line\">        s.pop();</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(cur-&gt;right)&#123;</span><br><span class=\"line\">            s.push(cur-&gt;right);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(cur-&gt;left)&#123;</span><br><span class=\"line\">            s.push(cur-&gt;left);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        result.push_back(cur-&gt;val);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> ;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">center</span><span class=\"params\">(TreeNode* root, <span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; &amp;result)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"built_in\">stack</span>&lt;TreeNode*&gt; s;</span><br><span class=\"line\">    TreeNode *cur = root;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(root == <span class=\"literal\">nullptr</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> ;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(!s.empty() || cur != <span class=\"literal\">nullptr</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(cur)&#123;</span><br><span class=\"line\">            <span class=\"keyword\">while</span>(cur)&#123;</span><br><span class=\"line\">                s.push(cur);</span><br><span class=\"line\">                cur = cur-&gt;left;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        cur = s.top();</span><br><span class=\"line\">        s.pop();</span><br><span class=\"line\">        result.push_back(cur-&gt;val);</span><br><span class=\"line\">        cur = cur-&gt;right;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> ;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">post</span><span class=\"params\">(TreeNode* root, <span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; &amp;result)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"built_in\">stack</span>&lt;TreeNode*&gt; s;</span><br><span class=\"line\">    TreeNode *cur, *h=root;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(root == <span class=\"literal\">nullptr</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> ;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    s.push(root);</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(!s.empty())&#123;</span><br><span class=\"line\">        cur = s.top();</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(cur-&gt;left &amp;&amp; h != cur-&gt;left &amp;&amp; h != cur-&gt;right)&#123;</span><br><span class=\"line\">            <span class=\"keyword\">while</span>(cur-&gt;left)&#123;</span><br><span class=\"line\">                s.push(cur-&gt;left);</span><br><span class=\"line\">                cur = cur-&gt;left;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(cur-&gt;right &amp;&amp; h != cur-&gt;right)&#123;</span><br><span class=\"line\">            s.push(cur-&gt;right);</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">            s.pop();</span><br><span class=\"line\">            result.push_back(cur-&gt;val);</span><br><span class=\"line\">            h = cur;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> ;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">vector</span>&lt;<span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; &gt; convert(TreeNode* root) &#123;</span><br><span class=\"line\">    <span class=\"comment\">// write code here</span></span><br><span class=\"line\">    <span class=\"built_in\">vector</span>&lt;<span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; &gt; result(<span class=\"number\">3</span>);</span><br><span class=\"line\">    pre(root, result[<span class=\"number\">0</span>]);</span><br><span class=\"line\">    center(root, result[<span class=\"number\">1</span>]);</span><br><span class=\"line\">    post(root, result[<span class=\"number\">2</span>]);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> result;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\r\n<h2 id=\"二叉搜索树的后序遍历序列\">二叉搜索树的后序遍历序列</h2>\r\n<p>输入一个整数数组，判断该数组是不是某二叉搜索树的后序遍历结果。如果是则返回 true，否则返回 false。假设输入的数组的任意两个数字都互不相同。</p>\r\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">bool</span> <span class=\"title\">verifyPostorder</span><span class=\"params\">(<span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt;&amp; postorder)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"built_in\">stack</span>&lt;<span class=\"keyword\">int</span>&gt; s;<span class=\"comment\">//这个栈在下面的过程中，其中的内容保证是单调递增的</span></span><br><span class=\"line\">    <span class=\"keyword\">int</span> root = <span class=\"number\">0x7fffffff</span>;<span class=\"comment\">//整棵树都当做一个正无穷节点的左子树，保证当前访问的节点是root的左子树节点。</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = postorder.size() - <span class=\"number\">1</span>; i &gt;= <span class=\"number\">0</span>; --i)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(postorder[i] &gt; root) <span class=\"keyword\">return</span> <span class=\"literal\">false</span>;<span class=\"comment\">//左子树的节点值比父节点大，直接判定false</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span>(! s.empty() &amp;&amp; postorder[i] &lt; s.top())&#123;</span><br><span class=\"line\">            <span class=\"comment\">/*倒序访问过程中，访问顺序是当前根节点的左子树的根节点-&gt;左子树根节点的右子树-&gt;左子树根节点的左子树,出现当前节点比较小，说明左子树根节点的右子树访问完了（或者左子树根节点没有右子树），现在访问的是到了左子树根节点的左子树，这个时候需要把左子树根节点的右子树清除，并调整root为当前左子树的根节点*/</span></span><br><span class=\"line\">            root = s.top();<span class=\"comment\">//栈中最后一个比当前元素大的那个节点是当前节点的父节点</span></span><br><span class=\"line\">            s.pop();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        s.push(postorder[i]);<span class=\"comment\">//当前父节点的左子树节点入栈</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"literal\">true</span>;<span class=\"comment\">//访问完都没有出现不符合条件的，则判定为true</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\r\n<h1 id=\"动态规划\">动态规划</h1>\r\n<h2 id=\"找零问题\">找零问题</h2>\r\n<p>有数组penny，penny中所有的值都为正数且不重复。每个值代表一种面值的货币，每种面值的货币可以使用任意张，再给定一个整数aim(小于等于1000)代表要找的钱数，求换钱有多少种方法。</p>\r\n<p>给定数组penny及它的大小(小于等于50)，同时给定一个整数aim，请返回有多少种方法可以凑成aim。</p>\r\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">countWays</span><span class=\"params\">(<span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; penny, <span class=\"keyword\">int</span> n, <span class=\"keyword\">int</span> aim)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> record[<span class=\"number\">1001</span>] = &#123;<span class=\"number\">0</span>&#125;;</span><br><span class=\"line\">    record[<span class=\"number\">0</span>] = <span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i=<span class=\"number\">0</span>; i&lt;n;++i)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> j=penny[i];j&lt;=aim;++j)&#123;</span><br><span class=\"line\">            record[j] += record[j - penny[i]];</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> record[aim];</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\r\n<h2 id=\"最长上升子序列\">最长上升子序列</h2>\r\n<p>这是一个经典的LIS(即最长上升子序列)问题，请设计一个尽量优的解法求出序列的最长上升子序列的长度。</p>\r\n<p>给定一个序列A及它的长度n(长度小于等于500)，请返回LIS的长度。</p>\r\n<p>测试样例：</p>\r\n<p>[1,4,2,5,3],5</p>\r\n<p>返回：3 <figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">getLIS</span><span class=\"params\">(<span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; A, <span class=\"keyword\">int</span> n)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// write code here</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span>(n == <span class=\"number\">0</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> max_len=<span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> *record = <span class=\"keyword\">new</span> <span class=\"keyword\">int</span>[n];</span><br><span class=\"line\">    <span class=\"keyword\">int</span> max_pos;</span><br><span class=\"line\">    record[<span class=\"number\">0</span>] = <span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">1</span>; i &lt; n; ++i)&#123;</span><br><span class=\"line\">        max_pos = <span class=\"number\">-1</span>;</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> j = i<span class=\"number\">-1</span>; j&gt;=<span class=\"number\">0</span>;--j)&#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(A[j] &lt; A[i] &amp;&amp; (max_pos &lt; <span class=\"number\">0</span> || record[j] &gt; record[max_pos]))&#123;</span><br><span class=\"line\">                max_pos = j;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(max_pos &gt;= <span class=\"number\">0</span>)&#123;</span><br><span class=\"line\">            record[i] = record[max_pos] + <span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">            record[i] = <span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(record[i] &gt; max_len)&#123;</span><br><span class=\"line\">            max_len = record[i];</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> max_len;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\r\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"排序\">排序</h1>\r\n<h2 id=\"冒泡排序\">冒泡排序</h2>\r\n<p>逐对交换，每次迭代，确定一个元素的最终位置。</p>\r\n<p>时间复杂度：<span class=\"math inline\">\\(O(N^2)\\)</span></p>\r\n<p>空间复杂度：<span class=\"math inline\">\\(O(1)\\)</span> <figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span>* <span class=\"title\">bubbleSort</span><span class=\"params\">(<span class=\"keyword\">int</span>* A, <span class=\"keyword\">int</span> n)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(A == <span class=\"literal\">NULL</span> || n &lt;= <span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> A;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> temp;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">1</span>; i &lt; n - <span class=\"number\">1</span>; ++i)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> j = <span class=\"number\">0</span>; j &lt; n - i; ++j)&#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(A[j] &gt; A[j+<span class=\"number\">1</span>])&#123;</span><br><span class=\"line\">                temp = A[j+<span class=\"number\">1</span>];</span><br><span class=\"line\">                A[j+<span class=\"number\">1</span>] = A[j];</span><br><span class=\"line\">                A[j] = temp;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> A;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure> ## 选择排序</p>\r\n<p>每次选择一个剩下的最小或者最大元素放到对应位置。</p>\r\n<p>时间复杂度：<span class=\"math inline\">\\(O(N^2)\\)</span></p>\r\n<p>空间复杂度：<span class=\"math inline\">\\(O(1)\\)</span> <figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span>* <span class=\"title\">selectionSort</span><span class=\"params\">(<span class=\"keyword\">int</span>* A, <span class=\"keyword\">int</span> n)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(A == <span class=\"literal\">NULL</span> || n &lt;= <span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> A;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> min_value, j, min_index;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>;i&lt;n<span class=\"number\">-1</span>;++i)&#123;</span><br><span class=\"line\">        min_value = <span class=\"number\">0x7fffffff</span>;</span><br><span class=\"line\">        min_index = j = i;</span><br><span class=\"line\">        <span class=\"keyword\">while</span>(j &lt; n)&#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(A[j] &lt; min_value)&#123;</span><br><span class=\"line\">                min_value = A[j];</span><br><span class=\"line\">                min_index = j;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            ++j;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(min_index != i)&#123;</span><br><span class=\"line\">            A[i] ^= A[min_index];</span><br><span class=\"line\">            A[min_index] ^= A[i];</span><br><span class=\"line\">            A[i] ^= A[min_index];</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> A;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure> ## 插入排序 时间复杂度：<span class=\"math inline\">\\(O(N^2)\\)</span></p>\r\n<p>空间复杂度：<span class=\"math inline\">\\(O(1)\\)</span> <figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span>* <span class=\"title\">insertionSort</span><span class=\"params\">(<span class=\"keyword\">int</span>* A, <span class=\"keyword\">int</span> n)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(A == <span class=\"literal\">NULL</span> || n &lt;= <span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> A;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> cur, j;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i=<span class=\"number\">1</span>;i&lt;n;++i)&#123;</span><br><span class=\"line\">        cur = A[i];</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(j = i<span class=\"number\">-1</span>; j&gt;= <span class=\"number\">0</span>; --j)&#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(cur &lt; A[j])&#123;</span><br><span class=\"line\">                A[j+<span class=\"number\">1</span>] = A[j];</span><br><span class=\"line\">            &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">                <span class=\"keyword\">break</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        A[j+<span class=\"number\">1</span>] = cur;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> A;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure> ## 归并排序 时间复杂度：<span class=\"math inline\">\\(O(N\\log(N))\\)</span></p>\r\n<p>空间复杂度：<span class=\"math inline\">\\(O(N)\\)</span> <figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span>* <span class=\"title\">merge</span><span class=\"params\">(<span class=\"keyword\">int</span>* A, <span class=\"keyword\">int</span> lenA, <span class=\"keyword\">int</span>*B, <span class=\"keyword\">int</span> lenB)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> *result = <span class=\"keyword\">new</span> <span class=\"keyword\">int</span>[lenA + lenB];</span><br><span class=\"line\">    <span class=\"keyword\">int</span> i=<span class=\"number\">0</span>,j=<span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(i &lt; lenA &amp;&amp; j &lt; lenB)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(A[i] &lt;= B[j])&#123;</span><br><span class=\"line\">            result[i+j] = A[i];</span><br><span class=\"line\">            ++i;</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">            result[i+j] = B[j];</span><br><span class=\"line\">            ++j;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(i &lt; lenA)&#123;</span><br><span class=\"line\">        result[i+j] = A[i];</span><br><span class=\"line\">        ++i;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(j &lt; lenB)&#123;</span><br><span class=\"line\">        result[i+j] = B[j];</span><br><span class=\"line\">        ++j;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> result;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span>* <span class=\"title\">mergeSort</span><span class=\"params\">(<span class=\"keyword\">int</span>* A, <span class=\"keyword\">int</span> n)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(A == <span class=\"literal\">NULL</span> || n &lt;= <span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> A;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> center = n / <span class=\"number\">2</span>;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> merge(mergeSort(A, center), center, mergeSort(A + center, n - center), n - center);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure> ## 快速排序 时间复杂度：<span class=\"math inline\">\\(O(N\\log(N))\\)</span></p>\r\n<p>空间复杂度：<span class=\"math inline\">\\(O(\\log(N))\\)</span> ~ <span class=\"math inline\">\\(O(N)\\)</span> <figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">partition</span><span class=\"params\">(<span class=\"keyword\">int</span>* A, <span class=\"keyword\">int</span> n)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(n &lt;= <span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> small = <span class=\"number\">-1</span>;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> i = <span class=\"number\">0</span>, j = n<span class=\"number\">-2</span>;</span><br><span class=\"line\">        <span class=\"keyword\">while</span>(i &lt;= j)&#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(A[i] &lt;= A[n<span class=\"number\">-1</span>])&#123;</span><br><span class=\"line\">                small = i++;</span><br><span class=\"line\">            &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(A[j] &lt;= A[n<span class=\"number\">-1</span>])&#123;</span><br><span class=\"line\">                <span class=\"keyword\">if</span>(j != i)&#123;</span><br><span class=\"line\">                    A[j] ^= A[i];</span><br><span class=\"line\">                    A[i] ^= A[j];</span><br><span class=\"line\">                    A[j] ^= A[i];</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">                small = i++;</span><br><span class=\"line\">            &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">                --j;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(small + <span class=\"number\">1</span> != n - <span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">            A[small + <span class=\"number\">1</span>] ^= A[n - <span class=\"number\">1</span>];</span><br><span class=\"line\">            A[n - <span class=\"number\">1</span>] ^= A[small + <span class=\"number\">1</span>];</span><br><span class=\"line\">            A[small + <span class=\"number\">1</span>] ^= A[n - <span class=\"number\">1</span>];</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> small + <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span>* <span class=\"title\">quickSort</span><span class=\"params\">(<span class=\"keyword\">int</span>* A, <span class=\"keyword\">int</span> n)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(A == <span class=\"literal\">NULL</span> || n &lt;= <span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> A;</span><br><span class=\"line\">    &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> center = partition(A, n);</span><br><span class=\"line\">        quickSort(A, center);</span><br><span class=\"line\">        quickSort(A+center+<span class=\"number\">1</span>, n-center<span class=\"number\">-1</span>);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> A;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure> ## 堆排序 时间复杂度：<span class=\"math inline\">\\(O(N\\log(N))\\)</span></p>\r\n<p>空间复杂度：<span class=\"math inline\">\\(O(1)\\)</span></p>\r\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">parent</span><span class=\"params\">(<span class=\"keyword\">int</span> i)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> (i - <span class=\"number\">1</span>) / <span class=\"number\">2</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">left</span><span class=\"params\">(<span class=\"keyword\">int</span> i)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> i * <span class=\"number\">2</span> + <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">right</span><span class=\"params\">(<span class=\"keyword\">int</span> i)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> i * <span class=\"number\">2</span> + <span class=\"number\">2</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">DownHeap</span><span class=\"params\">(<span class=\"keyword\">int</span>* A, <span class=\"keyword\">int</span> cur, <span class=\"keyword\">int</span> n)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> left_idx, right_idx, max_idx, end = parent(n - <span class=\"number\">1</span>);</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(cur &lt;= end)&#123;</span><br><span class=\"line\">        left_idx = left(cur);</span><br><span class=\"line\">        right_idx = right(cur);</span><br><span class=\"line\">        max_idx = A[cur] &lt; A[left_idx]? left_idx : cur;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(right_idx &lt; n)&#123;</span><br><span class=\"line\">            max_idx = A[max_idx] &lt; A[right_idx]? right_idx : max_idx;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(max_idx != cur)&#123;</span><br><span class=\"line\">            A[cur] ^= A[max_idx];</span><br><span class=\"line\">            A[max_idx] ^= A[cur];</span><br><span class=\"line\">            A[cur] ^= A[max_idx];</span><br><span class=\"line\">            cur = max_idx;</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">createHeap</span><span class=\"params\">(<span class=\"keyword\">int</span>* A, <span class=\"keyword\">int</span> n)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> end = parent(n - <span class=\"number\">1</span>);</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = end; i &gt;= <span class=\"number\">0</span>; --i)&#123;</span><br><span class=\"line\">        DownHeap(A, i, n);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span>* <span class=\"title\">heapSort</span><span class=\"params\">(<span class=\"keyword\">int</span>* A, <span class=\"keyword\">int</span> n)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(A == <span class=\"literal\">NULL</span> || n &lt;= <span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> A;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    createHeap(A, n);</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = n<span class=\"number\">-1</span>;i&gt;<span class=\"number\">0</span>;--i)&#123;</span><br><span class=\"line\">        DownHeap(A, <span class=\"number\">0</span>, i+<span class=\"number\">1</span>);</span><br><span class=\"line\">        A[i] ^= A[<span class=\"number\">0</span>];</span><br><span class=\"line\">        A[<span class=\"number\">0</span>] ^= A[i];</span><br><span class=\"line\">        A[i] ^= A[<span class=\"number\">0</span>];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> A;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">DownHeap</span><span class=\"params\">(<span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt;&amp; heap, <span class=\"keyword\">int</span> i, <span class=\"keyword\">int</span> n)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// 大顶堆的下滤操作</span></span><br><span class=\"line\">    <span class=\"keyword\">int</span> temp = heap[i];</span><br><span class=\"line\">    <span class=\"keyword\">int</span> l,r,maxChild=i;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(<span class=\"literal\">true</span>)&#123;</span><br><span class=\"line\">        l = i * <span class=\"number\">2</span> + <span class=\"number\">1</span>;</span><br><span class=\"line\">        r = l + <span class=\"number\">1</span>;</span><br><span class=\"line\">        maxChild = (r &lt; n &amp;&amp; heap[r] &gt; heap[l]) ? r : l;</span><br><span class=\"line\">        maxChild = (maxChild &lt; n &amp;&amp; heap[maxChild] &gt; temp) ? maxChild : i;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(maxChild != i)&#123;</span><br><span class=\"line\">            heap[i] = heap[maxChild];</span><br><span class=\"line\">            i = maxChild;</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    heap[i] = temp;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> ;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; <span class=\"title\">sortArray</span><span class=\"params\">(<span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">//堆排序</span></span><br><span class=\"line\">    <span class=\"keyword\">int</span> n = nums.size();</span><br><span class=\"line\">    <span class=\"comment\">//构建大顶堆</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = n / <span class=\"number\">2</span>; i &gt;= <span class=\"number\">0</span>; --i)&#123;</span><br><span class=\"line\">        DownHeap(nums, i, n);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//逐个删除堆顶，进行排序</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = n; i &gt; <span class=\"number\">1</span>; --i)&#123;</span><br><span class=\"line\">        swap(nums[<span class=\"number\">0</span>], nums[i - <span class=\"number\">1</span>]);</span><br><span class=\"line\">        DownHeap(nums, <span class=\"number\">0</span>, i - <span class=\"number\">1</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> nums;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\r\n<h2 id=\"希尔排序\">希尔排序</h2>\r\n<p>时间复杂度：<span class=\"math inline\">\\(O(N\\log(N))\\)</span></p>\r\n<p>空间复杂度：<span class=\"math inline\">\\(O(1)\\)</span> <figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">insertionSort</span><span class=\"params\">(<span class=\"keyword\">int</span>* A, <span class=\"keyword\">int</span> n, <span class=\"keyword\">int</span> space)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> cur, j;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i=space;i&lt;n;i++)&#123;</span><br><span class=\"line\">        cur = A[i];</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(j = i-space; j&gt;= <span class=\"number\">0</span>; j-=space)&#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(cur &lt; A[j])&#123;</span><br><span class=\"line\">                A[j+space] = A[j];</span><br><span class=\"line\">            &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">                <span class=\"keyword\">break</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        A[j+space] = cur;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span>* <span class=\"title\">shellSort</span><span class=\"params\">(<span class=\"keyword\">int</span>* A, <span class=\"keyword\">int</span> n)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(A == <span class=\"literal\">NULL</span> || n &lt;= <span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> A;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> space = n / <span class=\"number\">2</span>; space&gt;=<span class=\"number\">1</span>; space/= <span class=\"number\">2</span>)&#123;</span><br><span class=\"line\">        insertionSort(A, n, space);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> A;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; <span class=\"title\">shellSort</span><span class=\"params\">(<span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">//希尔排序</span></span><br><span class=\"line\">    <span class=\"keyword\">int</span> n = nums.size();</span><br><span class=\"line\">    <span class=\"keyword\">int</span> inc = <span class=\"number\">2</span>;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> temp;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> i,j;</span><br><span class=\"line\">    <span class=\"comment\">// 计算Hibbard增量</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span>(inc * <span class=\"number\">2</span> &lt; n)&#123;</span><br><span class=\"line\">        inc *= <span class=\"number\">2</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    inc -= <span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(inc &gt;= <span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(i = inc; i &lt; n; ++i)&#123;</span><br><span class=\"line\">            <span class=\"comment\">// 执行增量插入排序</span></span><br><span class=\"line\">            temp = nums[i];</span><br><span class=\"line\">            <span class=\"keyword\">for</span>(j = i; j &gt;= inc &amp;&amp; nums[j-inc] &gt; temp; j -= inc)&#123;</span><br><span class=\"line\">                nums[j] = nums[j-inc];</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            nums[j] = temp;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        inc = (inc - <span class=\"number\">1</span>) / <span class=\"number\">2</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> nums;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure> ## 计数排序</p>\r\n<p>时间复杂度：<span class=\"math inline\">\\(O(N)\\)</span></p>\r\n<p>空间复杂度：<span class=\"math inline\">\\(O(M)\\)</span>，这里的<span class=\"math inline\">\\(M\\)</span>和待排序元素的取值范围有关。</p>\r\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span>* <span class=\"title\">countingSort</span><span class=\"params\">(<span class=\"keyword\">int</span>* A, <span class=\"keyword\">int</span> n)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(A==<span class=\"literal\">nullptr</span> || n &lt;=<span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> A;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> min = A[<span class=\"number\">0</span>], max = A[<span class=\"number\">0</span>];</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">1</span>;i &lt; n; ++i)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(A[i] &gt; max)&#123;</span><br><span class=\"line\">            max = A[i];</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(A[i] &lt; min)&#123;</span><br><span class=\"line\">            min = A[i];</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> num = max - min + <span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> *count = <span class=\"keyword\">new</span> <span class=\"keyword\">int</span>[num]();</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>;i &lt; n; ++i)&#123;</span><br><span class=\"line\">        count[A[i] - min] += <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> i = <span class=\"number\">0</span>, j = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(i &lt; n)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(count[j] &gt; <span class=\"number\">0</span>)&#123;</span><br><span class=\"line\">            count[j] -= <span class=\"number\">1</span>;</span><br><span class=\"line\">            A[i++] = j + min;</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">            ++j;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">delete</span> []count;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> A;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\r\n<h2 id=\"基数排序\">基数排序</h2>\r\n<p>时间复杂度：<span class=\"math inline\">\\(O(N)\\)</span></p>\r\n<p>空间复杂度：<span class=\"math inline\">\\(O(M)\\)</span>，这里的<span class=\"math inline\">\\(M\\)</span>和待排序元素的取值范围有关。</p>\r\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">getdig</span><span class=\"params\">(<span class=\"keyword\">int</span> x, <span class=\"keyword\">int</span> base)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> ubase = <span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; base; i++)&#123;</span><br><span class=\"line\">        ubase *= <span class=\"number\">10</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    x = x % (ubase * <span class=\"number\">10</span>);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> x / ubase;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span>* <span class=\"title\">radixSort</span><span class=\"params\">(<span class=\"keyword\">int</span>* A, <span class=\"keyword\">int</span> n)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> base = <span class=\"number\">0</span>, dig, j;</span><br><span class=\"line\">    <span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; temp;</span><br><span class=\"line\">    <span class=\"built_in\">map</span>&lt;<span class=\"keyword\">int</span>, <span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; &gt; buckets;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(base &lt; <span class=\"number\">4</span>)&#123; <span class=\"comment\">// 如果待排序数字全都小于10000，那么这里只用到4</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; n; ++i)&#123;</span><br><span class=\"line\">            dig = getdig(A[i], base);</span><br><span class=\"line\">            buckets[dig].push_back(A[i]);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        j = <span class=\"number\">0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"number\">10</span>; ++i)&#123;</span><br><span class=\"line\">            temp = buckets[i];</span><br><span class=\"line\">            <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> k = <span class=\"number\">0</span>; k &lt; temp.size(); ++k)&#123;</span><br><span class=\"line\">                A[j++] = temp[k];</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            buckets[i].clear();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        ++base;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> A;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\r\n<h2 id=\"基本有序的情况\">基本有序的情况</h2>\r\n<p>如果一个序列已经基本有序，即排好序之后，其中每个元素的移动位置偏移不超过K，那么这种情况下，可以使用改进的堆排序方法，每次使用前K个数字建立小根堆（建好后最小的数字肯定在第一个位置），每次向后移动一个位置，维持小根堆，这样一直往后移动最终完成排序，其时间复杂度为<span class=\"math inline\">\\(O(N \\times \\log(K))\\)</span>，其次，这种情况下插入排序是比较好的，其时间复杂度为<span class=\"math inline\">\\(O(N \\times K)\\)</span>，改进的堆排序算法如下（这里为了实现简单，使用了额外<span class=\"math inline\">\\(O(K)\\)</span>的空间，其实不需要，只不过代码可能会变得比较复杂）：</p>\r\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">parent</span><span class=\"params\">(<span class=\"keyword\">int</span> i)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> (i - <span class=\"number\">1</span>) / <span class=\"number\">2</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">left</span><span class=\"params\">(<span class=\"keyword\">int</span> i)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> i * <span class=\"number\">2</span> + <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">right</span><span class=\"params\">(<span class=\"keyword\">int</span> i)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> i * <span class=\"number\">2</span> + <span class=\"number\">2</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">modifyHeap</span><span class=\"params\">(<span class=\"keyword\">int</span> *A, <span class=\"keyword\">int</span> cur, <span class=\"keyword\">int</span> n)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> left_idx, right_idx, min_idx, end = parent(n - <span class=\"number\">1</span>);</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(cur &lt;= end)&#123;</span><br><span class=\"line\">        left_idx = left(cur);</span><br><span class=\"line\">        right_idx = right(cur);</span><br><span class=\"line\">        min_idx = A[cur] &gt; A[left_idx]? left_idx : cur;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(right_idx &lt; n)&#123;</span><br><span class=\"line\">            min_idx = A[min_idx] &gt; A[right_idx]? right_idx : min_idx;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(min_idx != cur)&#123;</span><br><span class=\"line\">            A[cur] ^= A[min_idx];</span><br><span class=\"line\">            A[min_idx] ^= A[cur];</span><br><span class=\"line\">            A[cur] ^= A[min_idx];</span><br><span class=\"line\">            cur = min_idx;</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">createHeap</span><span class=\"params\">(<span class=\"keyword\">int</span> *A, <span class=\"keyword\">int</span> n)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> end = parent(n - <span class=\"number\">1</span>);</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = end; i &gt;= <span class=\"number\">0</span>; --i)&#123;</span><br><span class=\"line\">        modifyHeap(A, i, n);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"function\"><span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; <span class=\"title\">sortElement</span><span class=\"params\">(<span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; A, <span class=\"keyword\">int</span> n, <span class=\"keyword\">int</span> k)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(A.empty() || n &lt;= <span class=\"number\">1</span> || k &lt; <span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> A;</span><br><span class=\"line\">    &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> start = <span class=\"number\">0</span>, group_num = <span class=\"number\">0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">int</span>* B = <span class=\"keyword\">new</span> <span class=\"keyword\">int</span>[k + <span class=\"number\">1</span>];</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; n &amp;&amp; i &lt; k + <span class=\"number\">1</span>; ++i)&#123;</span><br><span class=\"line\">            B[i] = A[i];</span><br><span class=\"line\">            group_num += <span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        createHeap(B, group_num);</span><br><span class=\"line\">        <span class=\"keyword\">while</span>(group_num &gt; <span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">            A[start++] = B[<span class=\"number\">0</span>];</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(start + group_num &gt; n)&#123;</span><br><span class=\"line\">                group_num--;</span><br><span class=\"line\">                B[<span class=\"number\">0</span>] = B[group_num];</span><br><span class=\"line\">                modifyHeap(B, <span class=\"number\">0</span>, group_num);</span><br><span class=\"line\">            &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">                B[<span class=\"number\">0</span>] = A[start + group_num - <span class=\"number\">1</span>];</span><br><span class=\"line\">                modifyHeap(B, <span class=\"number\">0</span>, group_num);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(group_num == <span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">            A[start] = B[<span class=\"number\">0</span>];</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> A;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\r\n<h1 id=\"字符串\">字符串</h1>\r\n<h2 id=\"kmp\">KMP</h2>\r\n<p>字符串匹配算法。</p>\r\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">bool</span> <span class=\"title\">kmp</span><span class=\"params\">(<span class=\"built_in\">string</span> A, <span class=\"keyword\">int</span> lena, <span class=\"built_in\">string</span> B, <span class=\"keyword\">int</span> lenb)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// 在A中查找B</span></span><br><span class=\"line\">    <span class=\"keyword\">int</span> *next = <span class=\"keyword\">new</span> <span class=\"keyword\">int</span>[lenb]();</span><br><span class=\"line\">    <span class=\"keyword\">int</span> i=<span class=\"number\">1</span>,j=<span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(i &lt; lenb)&#123;<span class=\"comment\">// 构造next数组</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span>(B[i] == B[j])&#123;</span><br><span class=\"line\">            next[i++] = ++j;</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">            j = next[j - <span class=\"number\">1</span>];</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(j == <span class=\"number\">0</span>)&#123;</span><br><span class=\"line\">                next[i++] = <span class=\"number\">0</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    i = j = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(i &lt;= lena - lenb + j)&#123;<span class=\"comment\">//  进行匹配</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span>(A[i] == B[j])&#123;</span><br><span class=\"line\">            ++i;</span><br><span class=\"line\">            ++j;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(j == lenb)&#123;</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"literal\">true</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(j == <span class=\"number\">0</span>)&#123;</span><br><span class=\"line\">            ++i;</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">            j = next[j - <span class=\"number\">1</span>];</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"literal\">false</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\r\n<h1 id=\"栈和队列\">栈和队列</h1>\r\n<h2 id=\"滑动窗口最大值\">滑动窗口最大值</h2>\r\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; <span class=\"title\">slide</span><span class=\"params\">(<span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; arr, <span class=\"keyword\">int</span> n, <span class=\"keyword\">int</span> w)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// write code here</span></span><br><span class=\"line\">    <span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; result;</span><br><span class=\"line\">    <span class=\"built_in\">deque</span>&lt;<span class=\"keyword\">int</span>&gt; maxque;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i=<span class=\"number\">0</span>;i&lt;n;++i)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">while</span>(!maxque.empty() &amp;&amp; arr[maxque.back()] &lt; arr[i])&#123;</span><br><span class=\"line\">            maxque.pop_back();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        maxque.push_back(i);</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(i &gt;= w - <span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">            <span class=\"keyword\">while</span>(maxque.front() &lt;= i - w)&#123;</span><br><span class=\"line\">                maxque.pop_front();</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            result.push_back(arr[maxque.front()]);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> result;</span><br></pre></td></tr></table></figure>\r\n<h2 id=\"构造maxtree\">构造MaxTree</h2>\r\n<p>对于一个没有重复元素的整数数组，请用其中元素构造一棵MaxTree，MaxTree定义为一棵二叉树，其中的节点与数组元素一一对应，同时对于MaxTree的每棵子树，它的根的元素值为子树的最大值。现有一建树方法，对于数组中的每个元素，其在树中的父亲为数组中它左边比它大的第一个数和右边比它大的第一个数中更小的一个。若两边都不存在比它大的数，那么它就是树根。请设计O(n)的算法实现这个方法。</p>\r\n<p>给定一个无重复元素的数组A和它的大小n，请返回一个数组，其中每个元素为原数组中对应位置元素在树中的父亲节点的编号，若为根则值为-1。</p>\r\n<p>测试样例：</p>\r\n<p>[3,1,4,2],4</p>\r\n<p>返回：[2,0,-1,2] <figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; <span class=\"title\">buildMaxTree</span><span class=\"params\">(<span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; A, <span class=\"keyword\">int</span> n)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// write code here</span></span><br><span class=\"line\">    <span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; result;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> l,r;</span><br><span class=\"line\">    <span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; left_max, right_max;</span><br><span class=\"line\">    <span class=\"built_in\">stack</span>&lt;<span class=\"keyword\">int</span>&gt; left_stack, right_stack;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; n; ++i)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(left_stack.empty())&#123;</span><br><span class=\"line\">            left_max.push_back(<span class=\"number\">-1</span>);</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">while</span>(!left_stack.empty() &amp;&amp; A[left_stack.top()] &lt;= A[i])&#123;</span><br><span class=\"line\">                left_stack.pop();</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(left_stack.empty())&#123;</span><br><span class=\"line\">                left_max.push_back(<span class=\"number\">-1</span>);</span><br><span class=\"line\">            &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">                left_max.push_back(left_stack.top());</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        left_stack.push(i);</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">if</span>(right_stack.empty())&#123;</span><br><span class=\"line\">            right_max.push_back(<span class=\"number\">-1</span>);</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">while</span>(!right_stack.empty() &amp;&amp; A[right_stack.top()] &lt;= A[n - i - <span class=\"number\">1</span>])&#123;</span><br><span class=\"line\">                right_stack.pop();</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(right_stack.empty())&#123;</span><br><span class=\"line\">                right_max.push_back(<span class=\"number\">-1</span>);</span><br><span class=\"line\">            &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">                right_max.push_back(right_stack.top());</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        right_stack.push(n - i - <span class=\"number\">1</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; n; ++i)&#123;</span><br><span class=\"line\">        l = left_max[i];</span><br><span class=\"line\">        r = right_max[n - i - <span class=\"number\">1</span>];</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(l == <span class=\"number\">-1</span> &amp;&amp; r == <span class=\"number\">-1</span>)&#123;</span><br><span class=\"line\">            result.push_back(<span class=\"number\">-1</span>);</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(l == <span class=\"number\">-1</span>)&#123;</span><br><span class=\"line\">            result.push_back(r);</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(r == <span class=\"number\">-1</span>)&#123;</span><br><span class=\"line\">            result.push_back(l);</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">            result.push_back(A[l] &gt; A[r]? r : l);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> result;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure> # 链表</p>\r\n<h2 id=\"链表的回文判断\">链表的回文判断</h2>\r\n<p>请编写一个函数，检查链表是否为回文。</p>\r\n<p>给定一个链表ListNode* pHead，请返回一个bool，代表链表是否为回文。</p>\r\n<p>测试样例：</p>\r\n<p>{1,2,3,2,1}</p>\r\n<p>返回：true</p>\r\n<p>{1,2,3,2,3}</p>\r\n<p>返回：false</p>\r\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/*</span></span><br><span class=\"line\"><span class=\"comment\">struct ListNode &#123;</span></span><br><span class=\"line\"><span class=\"comment\">    int val;</span></span><br><span class=\"line\"><span class=\"comment\">    struct ListNode *next;</span></span><br><span class=\"line\"><span class=\"comment\">    ListNode(int x) : val(x), next(NULL) &#123;&#125;</span></span><br><span class=\"line\"><span class=\"comment\">&#125;;*/</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">reverse</span><span class=\"params\">(ListNode *pHead, ListNode *pTail)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(pHead == <span class=\"literal\">nullptr</span> || pTail == pHead)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ListNode *cur=pHead, *next=pHead-&gt;next, *temp;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(next!=pTail)&#123;</span><br><span class=\"line\">        temp = next-&gt;next;</span><br><span class=\"line\">        next-&gt;next = cur;</span><br><span class=\"line\">        cur = next;</span><br><span class=\"line\">        next = temp;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    pTail-&gt;next = cur;</span><br><span class=\"line\">    pHead-&gt;next = <span class=\"literal\">nullptr</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">bool</span> <span class=\"title\">isPalindrome</span><span class=\"params\">(ListNode* pHead)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// write code here</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span>(pHead == <span class=\"literal\">nullptr</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">false</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ListNode *center=pHead, *tail=pHead, *pcur=pHead, *ncur;</span><br><span class=\"line\">    <span class=\"keyword\">bool</span> result = <span class=\"literal\">true</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(center-&gt;next != <span class=\"literal\">nullptr</span> &amp;&amp; tail-&gt;next!=<span class=\"literal\">nullptr</span>)&#123;</span><br><span class=\"line\">        center = center-&gt;next;</span><br><span class=\"line\">        tail = tail-&gt;next;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(tail-&gt;next!=<span class=\"literal\">nullptr</span>)&#123;</span><br><span class=\"line\">            tail = tail-&gt;next;</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(center == tail)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(pHead-&gt;val == tail-&gt;val)&#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"literal\">true</span>;</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"literal\">false</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    reverse(center, tail);</span><br><span class=\"line\">    ncur = tail;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(pcur!=center)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(pcur-&gt;val != ncur-&gt;val)&#123;</span><br><span class=\"line\">            result = <span class=\"literal\">false</span>;</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">            pcur = pcur-&gt;next;</span><br><span class=\"line\">            ncur = ncur-&gt;next;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    reverse(tail, center);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> result;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\r\n<h2 id=\"复杂链表的复制\">复杂链表的复制</h2>\r\n<p>输入一个复杂链表（每个节点中有节点值，以及两个指针，一个指向下一个节点，另一个特殊指针指向任意一个节点），复制这个复杂链表并返回。</p>\r\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/*</span></span><br><span class=\"line\"><span class=\"comment\">struct RandomListNode &#123;</span></span><br><span class=\"line\"><span class=\"comment\">    int label;</span></span><br><span class=\"line\"><span class=\"comment\">    struct RandomListNode *next, *random;</span></span><br><span class=\"line\"><span class=\"comment\">    RandomListNode(int x) :</span></span><br><span class=\"line\"><span class=\"comment\">            label(x), next(NULL), random(NULL) &#123;</span></span><br><span class=\"line\"><span class=\"comment\">    &#125;</span></span><br><span class=\"line\"><span class=\"comment\">&#125;;</span></span><br><span class=\"line\"><span class=\"comment\">*/</span></span><br><span class=\"line\"><span class=\"function\">RandomListNode* <span class=\"title\">Clone</span><span class=\"params\">(RandomListNode* pHead)</span></span>&#123;</span><br><span class=\"line\">    RandomListNode *cur=pHead, *copy, *resultHead, *resultCur;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(pHead==<span class=\"literal\">nullptr</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">nullptr</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(cur!=<span class=\"literal\">nullptr</span>)&#123;</span><br><span class=\"line\">        copy = <span class=\"keyword\">new</span> RandomListNode(cur-&gt;label);</span><br><span class=\"line\">        copy-&gt;next = cur-&gt;next;</span><br><span class=\"line\">        cur-&gt;next = copy;</span><br><span class=\"line\">        cur = copy-&gt;next;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    cur = pHead;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(cur!=<span class=\"literal\">nullptr</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(cur-&gt;random != <span class=\"literal\">nullptr</span>)&#123;</span><br><span class=\"line\">            cur-&gt;next-&gt;random = cur-&gt;random-&gt;next;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        cur = cur-&gt;next-&gt;next;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    resultCur = resultHead = pHead-&gt;next;</span><br><span class=\"line\">    cur = pHead;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(cur!=<span class=\"literal\">nullptr</span> &amp;&amp; resultCur!=<span class=\"literal\">nullptr</span>)&#123;</span><br><span class=\"line\">        cur-&gt;next = cur-&gt;next-&gt;next;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(resultCur-&gt;next!=<span class=\"literal\">nullptr</span>)&#123;</span><br><span class=\"line\">            resultCur-&gt;next = resultCur-&gt;next-&gt;next;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        cur = cur-&gt;next;</span><br><span class=\"line\">        resultCur = resultCur-&gt;next;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> resultHead;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\r\n<h2 id=\"判断链表是否有环\">判断链表是否有环</h2>\r\n<p>如何判断一个单链表是否有环？有环的话返回进入环的第一个节点的值，无环的话返回-1。如果链表的长度为N，请做到时间复杂度O(N)，额外空间复杂度O(1)。</p>\r\n<p>给定一个单链表的头结点head，请返回所求值。</p>\r\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">chkLoop</span><span class=\"params\">(ListNode* head)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// write code here</span></span><br><span class=\"line\">    ListNode *fast=head, *slow=head;</span><br><span class=\"line\">    <span class=\"keyword\">bool</span> hasLoop = <span class=\"literal\">false</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(fast != <span class=\"literal\">nullptr</span>)&#123;</span><br><span class=\"line\">        slow = slow-&gt;next;</span><br><span class=\"line\">        fast = fast-&gt;next;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(fast)&#123;</span><br><span class=\"line\">            fast = fast-&gt;next;</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(fast == slow)&#123;</span><br><span class=\"line\">            hasLoop = <span class=\"literal\">true</span>;</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(!hasLoop)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">-1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    fast = head;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(fast!=slow)&#123;</span><br><span class=\"line\">        fast = fast-&gt;next;</span><br><span class=\"line\">        slow = slow-&gt;next;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> fast-&gt;val;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\r\n<h2 id=\"无环链表是否相交\">无环链表是否相交</h2>\r\n<p>直接判断最后一个节点是否相等即可，如果要求返回第一个共同节点，那么则需要先统计各链表长度<span class=\"math inline\">\\(m,n\\)</span>，假如<span class=\"math inline\">\\(m \\le n\\)</span>，那么第二个链表从第<span class=\"math inline\">\\(n-m\\)</span>个节点开始遍历，第一个链表从头结点开始遍历，两个遍历同步，同时比较当前节点是否相同，第一个相同节点即返回。</p>\r\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/*</span></span><br><span class=\"line\"><span class=\"comment\">struct ListNode &#123;</span></span><br><span class=\"line\"><span class=\"comment\">    int val;</span></span><br><span class=\"line\"><span class=\"comment\">    struct ListNode *next;</span></span><br><span class=\"line\"><span class=\"comment\">    ListNode(int x) : val(x), next(NULL) &#123;&#125;</span></span><br><span class=\"line\"><span class=\"comment\">&#125;;*/</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">bool</span> <span class=\"title\">chkIntersect</span><span class=\"params\">(ListNode* headA, ListNode* headB)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// write code here</span></span><br><span class=\"line\">    ListNode *nodeA=headA, *nodeB=headB;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(nodeA == <span class=\"literal\">nullptr</span> || nodeB == <span class=\"literal\">nullptr</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">false</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(nodeA-&gt;next)&#123;</span><br><span class=\"line\">        nodeA = nodeA-&gt;next;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(nodeB-&gt;next)&#123;</span><br><span class=\"line\">        nodeB = nodeB-&gt;next;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(nodeA == nodeB)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">true</span>;</span><br><span class=\"line\">    &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">false</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\r\n<h2 id=\"有环链表是否相交\">有环链表是否相交</h2>\r\n<p>如何判断两个有环单链表是否相交？相交的话返回第一个相交的节点，不想交的话返回空。如果两个链表长度分别为N和M，请做到时间复杂度O(N+M)，额外空间复杂度O(1)。</p>\r\n<p>给定两个链表的头结点head1和head2。请返回一个bool值代表它们是否相交。</p>\r\n<p>如果还需要找到第一个共同节点，先看是否相交，如果相交，有两种情况，第一种是入环之前相交，这样的话和首先将环去掉，然后按照无环链表的情况来进行查找，第二种情况则直接返回任意一个链表的入环节点都可以算作第一个相交节点。</p>\r\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/*</span></span><br><span class=\"line\"><span class=\"comment\">struct ListNode &#123;</span></span><br><span class=\"line\"><span class=\"comment\">    int val;</span></span><br><span class=\"line\"><span class=\"comment\">    struct ListNode *next;</span></span><br><span class=\"line\"><span class=\"comment\">    ListNode(int x) : val(x), next(NULL) &#123;&#125;</span></span><br><span class=\"line\"><span class=\"comment\">&#125;;*/</span></span><br><span class=\"line\"><span class=\"function\">ListNode* <span class=\"title\">firstLoopNode</span><span class=\"params\">(ListNode* head)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// write code here</span></span><br><span class=\"line\">    ListNode *fast=head, *slow=head;</span><br><span class=\"line\">    <span class=\"keyword\">bool</span> hasLoop = <span class=\"literal\">false</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(fast != <span class=\"literal\">nullptr</span>)&#123;</span><br><span class=\"line\">        slow = slow-&gt;next;</span><br><span class=\"line\">        fast = fast-&gt;next;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(fast)&#123;</span><br><span class=\"line\">            fast = fast-&gt;next;</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(fast == slow)&#123;</span><br><span class=\"line\">            hasLoop = <span class=\"literal\">true</span>;</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(!hasLoop)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">nullptr</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    fast = head;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(fast!=slow)&#123;</span><br><span class=\"line\">        fast = fast-&gt;next;</span><br><span class=\"line\">        slow = slow-&gt;next;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> fast;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">bool</span> <span class=\"title\">chkInter</span><span class=\"params\">(ListNode* head1, ListNode* head2)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// write code here</span></span><br><span class=\"line\">    ListNode *loopNode1 = firstLoopNode(head1);</span><br><span class=\"line\">    ListNode *loopNode2 = firstLoopNode(head2);</span><br><span class=\"line\">    ListNode *temp;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(head1 == <span class=\"literal\">nullptr</span> || head2 == <span class=\"literal\">nullptr</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">false</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(loopNode1 == loopNode2)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">true</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    temp = loopNode2;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(temp-&gt;next != loopNode2)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(temp == loopNode1)&#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"literal\">true</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        temp = temp-&gt;next;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"literal\">false</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\r\n<h1 id=\"二分查找\">二分查找</h1>\r\n<h2 id=\"查找局部最小值\">查找局部最小值</h2>\r\n<p>定义局部最小的概念。arr长度为1时，arr[0]是局部最小。arr的长度为N(N&gt;1)时，如果arr[0]&lt; arr[1]，那么arr[0]是局部最小；如果arr[N-1] &lt; arr[N-2]，那么arr[N-1]是局部最小；如果0 &lt; i &lt; N-1，既有arr[i] &lt; arr[i-1]又有arr[i] &lt; arr[i+1]，那么arr[i]是局部最小。 给定无序数组arr，已知arr中任意两个相邻的数都不相等，写一个函数，只需返回arr中任意一个局部最小出现的位置即可。 <figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">getLessIndex</span><span class=\"params\">(<span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; arr)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> start = <span class=\"number\">0</span>, end = arr.size(), mid;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(end == <span class=\"number\">0</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">-1</span>;</span><br><span class=\"line\">    &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(end == <span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">if</span>(arr[start] &lt; arr[start])&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> start;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">if</span>(arr[end - <span class=\"number\">2</span>] &gt; arr[end - <span class=\"number\">1</span>])&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> end;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">while</span>(start &lt; end - <span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">        mid = start + (end - start)/<span class=\"number\">2</span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(arr[mid] &gt; arr[mid - <span class=\"number\">1</span>])&#123;</span><br><span class=\"line\">            end = mid;</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">            start = mid;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> start;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure> ## 查找循环有序数组的最小值</p>\r\n<p>对于一个有序循环数组arr，返回arr中的最小值。有序循环数组是指，有序数组左边任意长度的部分放到右边去，右边的部分拿到左边来。比如数组[1,2,3,3,4]，是有序循环数组，[4,1,2,3,3]也是。</p>\r\n<p>给定数组arr及它的大小n，请返回最小值。</p>\r\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">getMin</span><span class=\"params\">(<span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; arr, <span class=\"keyword\">int</span> n)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> start = <span class=\"number\">0</span>, end = n, mid;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(n == <span class=\"number\">0</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">-1</span>;</span><br><span class=\"line\">    &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(n == <span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> arr[<span class=\"number\">0</span>];</span><br><span class=\"line\">    &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(n == <span class=\"number\">2</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> arr[<span class=\"number\">0</span>] &gt; arr[<span class=\"number\">1</span>] ? arr[<span class=\"number\">1</span>] : arr[<span class=\"number\">0</span>];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(start &lt; end - <span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">        mid = start + (end - start) / <span class=\"number\">2</span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(arr[start] &gt; arr[end - <span class=\"number\">1</span>])&#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(arr[mid] &gt; arr[end<span class=\"number\">-1</span>])&#123;</span><br><span class=\"line\">                start = mid + <span class=\"number\">1</span>;</span><br><span class=\"line\">            &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(arr[mid] &lt; arr[end<span class=\"number\">-1</span>])&#123;</span><br><span class=\"line\">                end = mid + <span class=\"number\">1</span>;</span><br><span class=\"line\">            &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">                start = mid;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(arr[start] &lt; arr[end])&#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> arr[start];</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(arr[mid] &lt; arr[end - <span class=\"number\">1</span>])&#123;</span><br><span class=\"line\">                end = mid + <span class=\"number\">1</span>;</span><br><span class=\"line\">            &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">                start += <span class=\"number\">1</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> arr[start];</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\r\n<h2 id=\"完全二叉树的节点个数\">完全二叉树的节点个数</h2>\r\n<p>给定一棵完全二叉树的根节点root，返回这棵树的节点个数。如果完全二叉树的节点数为N，请实现时间复杂度低于O(N)的解法。</p>\r\n<p>给定树的根结点root，请返回树的大小。</p>\r\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">count</span><span class=\"params\">(TreeNode* root)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> nodeNum = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> lldepth = <span class=\"number\">-1</span>, rldepth = <span class=\"number\">0</span>;</span><br><span class=\"line\">    TreeNode *cur_root = root, *cur;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(cur_root)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(lldepth==<span class=\"number\">-1</span>)&#123;</span><br><span class=\"line\">            cur = cur_root;</span><br><span class=\"line\">            <span class=\"keyword\">while</span>(cur)&#123;</span><br><span class=\"line\">                lldepth += <span class=\"number\">1</span>;</span><br><span class=\"line\">                cur = cur-&gt;left;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        cur = cur_root-&gt;right;</span><br><span class=\"line\">        <span class=\"keyword\">while</span>(cur)&#123;</span><br><span class=\"line\">            rldepth += <span class=\"number\">1</span>;</span><br><span class=\"line\">            cur = cur-&gt;left;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(rldepth == lldepth)&#123;</span><br><span class=\"line\">            nodeNum += <span class=\"number\">1</span> &lt;&lt; lldepth;</span><br><span class=\"line\">            cur_root = cur_root-&gt;right;</span><br><span class=\"line\">            lldepth = rldepth - <span class=\"number\">1</span>;</span><br><span class=\"line\">            rldepth = <span class=\"number\">0</span>;</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">            nodeNum += <span class=\"number\">1</span> &lt;&lt; rldepth;</span><br><span class=\"line\">            cur_root = cur_root-&gt;left;</span><br><span class=\"line\">            lldepth -= <span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> nodeNum;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\r\n<h1 id=\"二叉树\">二叉树</h1>\r\n<h2 id=\"前序中序后序三种遍历方式\">前序、中序、后序三种遍历方式</h2>\r\n<p>请用递归方式实现二叉树的先序、中序和后序的遍历打印。</p>\r\n<p>给定一个二叉树的根结点root，请依次返回二叉树的先序，中序和后续遍历(二维数组的形式)。</p>\r\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">pre</span><span class=\"params\">(TreeNode* root, <span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; &amp;result)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"built_in\">stack</span>&lt;TreeNode*&gt; s;</span><br><span class=\"line\">    TreeNode *cur;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(root == <span class=\"literal\">nullptr</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> ;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    s.push(root);</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(!s.empty())&#123;</span><br><span class=\"line\">        cur = s.top();</span><br><span class=\"line\">        s.pop();</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(cur-&gt;right)&#123;</span><br><span class=\"line\">            s.push(cur-&gt;right);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(cur-&gt;left)&#123;</span><br><span class=\"line\">            s.push(cur-&gt;left);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        result.push_back(cur-&gt;val);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> ;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">center</span><span class=\"params\">(TreeNode* root, <span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; &amp;result)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"built_in\">stack</span>&lt;TreeNode*&gt; s;</span><br><span class=\"line\">    TreeNode *cur = root;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(root == <span class=\"literal\">nullptr</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> ;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(!s.empty() || cur != <span class=\"literal\">nullptr</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(cur)&#123;</span><br><span class=\"line\">            <span class=\"keyword\">while</span>(cur)&#123;</span><br><span class=\"line\">                s.push(cur);</span><br><span class=\"line\">                cur = cur-&gt;left;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        cur = s.top();</span><br><span class=\"line\">        s.pop();</span><br><span class=\"line\">        result.push_back(cur-&gt;val);</span><br><span class=\"line\">        cur = cur-&gt;right;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> ;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">post</span><span class=\"params\">(TreeNode* root, <span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; &amp;result)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"built_in\">stack</span>&lt;TreeNode*&gt; s;</span><br><span class=\"line\">    TreeNode *cur, *h=root;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(root == <span class=\"literal\">nullptr</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> ;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    s.push(root);</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(!s.empty())&#123;</span><br><span class=\"line\">        cur = s.top();</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(cur-&gt;left &amp;&amp; h != cur-&gt;left &amp;&amp; h != cur-&gt;right)&#123;</span><br><span class=\"line\">            <span class=\"keyword\">while</span>(cur-&gt;left)&#123;</span><br><span class=\"line\">                s.push(cur-&gt;left);</span><br><span class=\"line\">                cur = cur-&gt;left;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(cur-&gt;right &amp;&amp; h != cur-&gt;right)&#123;</span><br><span class=\"line\">            s.push(cur-&gt;right);</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">            s.pop();</span><br><span class=\"line\">            result.push_back(cur-&gt;val);</span><br><span class=\"line\">            h = cur;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> ;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">vector</span>&lt;<span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; &gt; convert(TreeNode* root) &#123;</span><br><span class=\"line\">    <span class=\"comment\">// write code here</span></span><br><span class=\"line\">    <span class=\"built_in\">vector</span>&lt;<span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; &gt; result(<span class=\"number\">3</span>);</span><br><span class=\"line\">    pre(root, result[<span class=\"number\">0</span>]);</span><br><span class=\"line\">    center(root, result[<span class=\"number\">1</span>]);</span><br><span class=\"line\">    post(root, result[<span class=\"number\">2</span>]);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> result;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\r\n<h2 id=\"二叉搜索树的后序遍历序列\">二叉搜索树的后序遍历序列</h2>\r\n<p>输入一个整数数组，判断该数组是不是某二叉搜索树的后序遍历结果。如果是则返回 true，否则返回 false。假设输入的数组的任意两个数字都互不相同。</p>\r\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">bool</span> <span class=\"title\">verifyPostorder</span><span class=\"params\">(<span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt;&amp; postorder)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"built_in\">stack</span>&lt;<span class=\"keyword\">int</span>&gt; s;<span class=\"comment\">//这个栈在下面的过程中，其中的内容保证是单调递增的</span></span><br><span class=\"line\">    <span class=\"keyword\">int</span> root = <span class=\"number\">0x7fffffff</span>;<span class=\"comment\">//整棵树都当做一个正无穷节点的左子树，保证当前访问的节点是root的左子树节点。</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = postorder.size() - <span class=\"number\">1</span>; i &gt;= <span class=\"number\">0</span>; --i)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(postorder[i] &gt; root) <span class=\"keyword\">return</span> <span class=\"literal\">false</span>;<span class=\"comment\">//左子树的节点值比父节点大，直接判定false</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span>(! s.empty() &amp;&amp; postorder[i] &lt; s.top())&#123;</span><br><span class=\"line\">            <span class=\"comment\">/*倒序访问过程中，访问顺序是当前根节点的左子树的根节点-&gt;左子树根节点的右子树-&gt;左子树根节点的左子树,出现当前节点比较小，说明左子树根节点的右子树访问完了（或者左子树根节点没有右子树），现在访问的是到了左子树根节点的左子树，这个时候需要把左子树根节点的右子树清除，并调整root为当前左子树的根节点*/</span></span><br><span class=\"line\">            root = s.top();<span class=\"comment\">//栈中最后一个比当前元素大的那个节点是当前节点的父节点</span></span><br><span class=\"line\">            s.pop();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        s.push(postorder[i]);<span class=\"comment\">//当前父节点的左子树节点入栈</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"literal\">true</span>;<span class=\"comment\">//访问完都没有出现不符合条件的，则判定为true</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\r\n<h1 id=\"动态规划\">动态规划</h1>\r\n<h2 id=\"找零问题\">找零问题</h2>\r\n<p>有数组penny，penny中所有的值都为正数且不重复。每个值代表一种面值的货币，每种面值的货币可以使用任意张，再给定一个整数aim(小于等于1000)代表要找的钱数，求换钱有多少种方法。</p>\r\n<p>给定数组penny及它的大小(小于等于50)，同时给定一个整数aim，请返回有多少种方法可以凑成aim。</p>\r\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">countWays</span><span class=\"params\">(<span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; penny, <span class=\"keyword\">int</span> n, <span class=\"keyword\">int</span> aim)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> record[<span class=\"number\">1001</span>] = &#123;<span class=\"number\">0</span>&#125;;</span><br><span class=\"line\">    record[<span class=\"number\">0</span>] = <span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i=<span class=\"number\">0</span>; i&lt;n;++i)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> j=penny[i];j&lt;=aim;++j)&#123;</span><br><span class=\"line\">            record[j] += record[j - penny[i]];</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> record[aim];</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\r\n<h2 id=\"最长上升子序列\">最长上升子序列</h2>\r\n<p>这是一个经典的LIS(即最长上升子序列)问题，请设计一个尽量优的解法求出序列的最长上升子序列的长度。</p>\r\n<p>给定一个序列A及它的长度n(长度小于等于500)，请返回LIS的长度。</p>\r\n<p>测试样例：</p>\r\n<p>[1,4,2,5,3],5</p>\r\n<p>返回：3 <figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">getLIS</span><span class=\"params\">(<span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; A, <span class=\"keyword\">int</span> n)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// write code here</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span>(n == <span class=\"number\">0</span>)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> max_len=<span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> *record = <span class=\"keyword\">new</span> <span class=\"keyword\">int</span>[n];</span><br><span class=\"line\">    <span class=\"keyword\">int</span> max_pos;</span><br><span class=\"line\">    record[<span class=\"number\">0</span>] = <span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">1</span>; i &lt; n; ++i)&#123;</span><br><span class=\"line\">        max_pos = <span class=\"number\">-1</span>;</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> j = i<span class=\"number\">-1</span>; j&gt;=<span class=\"number\">0</span>;--j)&#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(A[j] &lt; A[i] &amp;&amp; (max_pos &lt; <span class=\"number\">0</span> || record[j] &gt; record[max_pos]))&#123;</span><br><span class=\"line\">                max_pos = j;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(max_pos &gt;= <span class=\"number\">0</span>)&#123;</span><br><span class=\"line\">            record[i] = record[max_pos] + <span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125;<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">            record[i] = <span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(record[i] &gt; max_len)&#123;</span><br><span class=\"line\">            max_len = record[i];</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> max_len;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\r\n"},{"title":"《机器学习》读书笔记","date":"2020-04-17T09:27:30.000Z","mathjax":true,"_content":"\n# 线性模型\n\n## 基于均方误差最小化来进行求解的方法称为最小二乘法\n\n## 用最小二乘法来优化线性回归\n线性回归的目标是学习函数$f(X) = Xw$，使得$f(X) \\approx Y$，其中$X=\\begin{bmatrix}x_1 & 1\\\\ x_2 & 1\\\\ \\vdots & \\vdots \\\\ x_m & 1 \\end{bmatrix}$，$Y=\\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_m\\end{bmatrix}$，$w \\in R^{(n+1) \\times 1}$，$n$是数据特征维数。\n\n如果用均方误差作为损失函数$L = (Xw-Y)^T (Xw-Y)$，那么问题可以描述为\n\n$$\n\\begin{aligned}\n&\\arg \\min_{w} (Xw-Y)^T (Xw-Y)\\\\\n\\end{aligned}\n$$\n直接求导：\n$$\n\\frac{\\partial L}{\\partial w} = \\frac{\\partial (w^T X^T X w - Y^TXw -XwY - Y^TY)}{\\partial w} = 2X^T(Xw-Y)\\\\\n$$\n令：\n$$\n\\begin{aligned}\n&\\frac{\\partial L}{\\partial w} = 0\\\\\n&\\Rightarrow w = (X^TX)^{-1}X^TY\n\\end{aligned}\n$$\n即得到使用均方误差的线性回归问题的解。\n\n## 对数几率回归（逻辑回归， Logistic Regression）\n对数几率回归即用线性回归去拟合对数几率$ln\\frac{y}{1-y} = w^T x + b$\n\n对数几率回归也等价于$y=\\frac{1}{1+e^{-z}}, z = w^T x + b$\n\n若将$y$视为后验概率，则$ln \\frac{P(y=1|x)}{P(y=0|x)} = z, z=w^Tx + b$\n\n显然有$P(y=1|x) = \\frac{e^z}{1 + e^z}, P(y=0|x)=\\frac{1}{1 + e^z}$\n\n使用极大似然法求解对数几率回归：$\\mathop{\\arg\\max}\\limits_{w, b}\\prod\\limits_{i=1}^m P(y=y_i| x_i) \\Rightarrow \\mathop{\\arg\\max}\\limits_{w, b}\\sum\\limits_{i=1}^m ln(P(y=y_i|x_i))$\n\n令$\\beta = \\begin{bmatrix}w ^ T& b\\end{bmatrix} ^ T \\in R^{n+1}, Y=\\begin{bmatrix}y_1&y_2&\\dots&y_m\\end{bmatrix} ^ T, X=\\begin{bmatrix} x_1&x_2&\\dots&x_m\\\\1&1&\\dots&1\\end{bmatrix},x_i \\in R^n, X \\in R^{(n+1) \\times m}$，其中$m$是数据量。\n\n使用极大似然法求解对数几率回归可以重写为：\n$$\n\\begin{aligned}\n&\\mathop{\\arg\\max}\\limits_{\\beta} l(Z)\\\\\n&Z = X^T \\beta\\\\\n&l(Z) = Y^Tln\\frac{e^Z}{\\mathbf{1} + e^Z} + (\\mathbf{1}-Y)^Tln\\frac{\\mathbf{1}}{\\mathbf{1}+e^Z}\\\\\n&=Y^TZ - ln(\\mathbf{1}+e^Z)\n\\end{aligned}\n$$\n\n使用牛顿法，第$t$次更新为$\\beta^{t+1} \\leftarrow \\beta ^ t - (\\triangledown_2l)^{-1}\\frac{\\partial l}{\\partial \\beta}$\n\n$$\n\\begin{aligned}\ndl &= Y^TdZ - \\mathbf{1}^T\\frac{e^Z}{\\mathbf{1}+e^Z} \\odot dZ\\\\\n&=Y^TdZ -\\mathbf{1}^T \\hat{P}_1 \\odot dZ, \\hat{P} = \\begin{bmatrix} P(y=1|x_1) & P(y=1|x_2)& \\dots & P(y=1|x_m)\\end{bmatrix}^T\\\\\n&=Y^TX^Td\\beta - \\mathbf{1}^T \\hat{P}_1 \\odot (X^Td\\beta)\\\\\n&=Y^TX^Td\\beta - (\\mathbf{1} \\odot \\hat{P}_1)^TX^Td\\beta\\\\\n&=(Y^T-\\hat{P}_1^T)X^Td\\beta\n\\end{aligned}\n$$\n\n所以$\\frac{\\partial l}{\\partial \\beta} = X(Y-\\hat{P}_1)$\n\n$$\n\\begin{aligned}\n    d(\\frac{\\partial l}{\\partial \\beta}) &= d(X(Y-\\hat{P}_1))\\\\\n    &=Xd\\hat{P}_1\\\\\n    &=Xd\\frac{e^Z}{\\mathbf{1}+e^Z}\\\\\n    &=X(\\frac{1}{1+e^Z}\\odot\\frac{e^Z}{1+e^Z} \\odot dZ)\\\\\n    &=X(\\hat{P}_0 \\odot \\hat{P}_1 \\odot (X^Td\\beta))\\\\\n    &=X diag(\\hat{P}_0) diag(\\hat{P}_1) X^Td\\beta,\\ diag(\\hat{P}_0) = \\begin{bmatrix}\n        P(y=0|x_1)&\\cdots&0\\\\\n        \\vdots&\\ddots&\\vdots\\\\\n        0&\\cdots&P(y=0|x_m)\n    \\end{bmatrix}\n\\end{aligned}\n$$\n\n所以$\\frac{\\partial^2 l}{\\partial \\beta \\partial \\beta^T} = \\frac{\\partial^2 l}{\\partial \\beta^T \\partial \\beta} = Xdiag(\\hat{P}_1) diag(\\hat{P}_0)X^T$\n\n即如果用牛顿法来求解极大似然对数几率回归，第$t$次更新为\n\n$$\n\\beta^{t+1} \\leftarrow \\beta ^ t - (Xdiag(\\hat{P}_1) diag(\\hat{P}_0)X^T)^{-1} X(Y-\\hat{P}_1)\n$$\n\n## 线性判别分析\n线性判别分析也叫“Fisher判别分析”，其思想非常朴素：试图找到一个低维空间（直线），可以表示为$y=wx$(这个表达式中的$y$表示$x$投影到这个空间（直线）后和原点的距离)，使得当样本投影到该直线时不同类别样本的距离尽可能远，而相同类别的数据尽可能接近，预测时按照样本的投影位置对其进行分类。\n\n以两类数据$x_1, x_2$为例，设$\\mu_1,\\mu_2,\\Sigma_1,\\Sigma_2$分别表示两类数据的均值和方差，则投影之后的均值和方差为$w\\mu_1,w\\mu_2,w^T\\Sigma_1w,w^T\\Sigma_2w$，因为直线是一维空间，所以这些均为实数，投影之后的类内距离可以用方差来衡量，这里使用$w^T\\Sigma_1w + w^T\\Sigma_2w$来度量投影之后的类内距离，而类间距离可以写成$||w\\mu_2 - w\\mu_1||_2^2$，同时考虑两种距离，给出希望最大化的目标函数如下。\n$$\n\\begin{aligned}\nJ &= \\frac{||w^T\\mu_2 - w^T\\mu_1||_2^2}{w^T\\Sigma_1w + w^T\\Sigma_2w}\\\\\n&= \\frac{w^T(\\mu_2 - \\mu_1)(\\mu_2 - \\mu_1)^Tw}{w^T(\\Sigma_1 + \\Sigma_2)w}\n\\end{aligned}\n$$\n定义类内散度矩阵$S_w = \\Sigma_1 + \\Sigma_2$，类间散度矩阵$S_b = (\\mu_2 - \\mu_1)(\\mu_2 - \\mu_1)^T$，上面的优化目标可以简写为如下。\n$$\n\\begin{aligned}\n    J = \\frac{w^TS_bw}{w^TS_ww}\n\\end{aligned}\n$$\n这个优化目标又称为$S_b$和$S_w$的广义瑞利商，注意到分子分母中都有$w$的二次项，因此和$w$大小无关，只和w方向有关，所以优化问题可以写成下式。\n$$\n\\begin{aligned}\n\\min_w& \\quad-w^TS_bw\\\\\ns.t.& \\quad w^TS_ww = 1\n\\end{aligned}\n$$\n用拉格朗日乘子法进行优化，求解$S_bw = \\lambda S_ww$，因$S_bw$方向和$\\mu_2 - \\mu_1$相同，因此令$S_bw = \\lambda(\\mu_2 - \\mu_1)$，代入求解，可以得到$w = S_w^{-1}(\\mu_2 - \\mu_1)$。\n\n将数据进行降维，使得类内散度最小，类间散度最大，本质上是有监督的降维。\n\n## 多分类\n可以将多分类问题拆解为二分类问题，拆解策略有三种：一对一（OvO）、一对其余（OvR）、多对多（MvM）\n\n在MvM中，最常用的是纠错输出码（Error Correcting Output Codes，ECOC）,有$C_1C_2C_3...C_n$共$n$个类别，每个样本属于其中的一种，训练m个二分类器$f_1, f_2, ..., f_m$，每个分类器将一些类作为正类，另一些类作为负类，这样对于某个类别的样本，理想情况是$m$个分类器对其进行预测的输出组成的0,1串，构成一种长度为$m$的固定的类别组合串，$n$个类就有$n$种组合，但在预测时，对一个样本预测得到的输出串，可能不在$n$个类的$n$种组合中，这时，计算预测输出串和每个类别组合串的距离（海明距离或者欧式距离），将样本判定为距离最小的那个类别组合串对应的类别。\n\n## 类别不平衡\n解决办法主要有三种：\n- 再缩放（再平衡），根据样本数量移动判定阈值或者缩放预测概率。\n- 欠采样，将样本量过多的类别进行采样，减少该类别的样本数量，再拿去训练，但是这个方法容易丢失数据中的信息，最好是分成多个模型，每个模型使用该类别的一部分数据。\n- 过采样，将样本量过少的类别样本进行重复，然后训练，但是这个方法容易严重过拟合，一个办法是用两个该类别样本进行插值，生成新的该类别样本。\n\n# 决策树\n\n## 信息熵\n样本集合$D$中第$k$类样本所占比例为$p_k$，则信息熵定义为$Ent(D) = -\\sum\\limits_{k=1}^C p_k\\log_2p_k$，其中$C$为类别个数。\n\n## 信息熵增益\n假设离散属性$a$有$v$个取值：$a_1, a_2, ..., a_v$，可以将当前数据集合分成$V$个子集：$D_1, D_2, ..., D^V$，那么信息熵增益定义为$Gain(D,a)=Ent(D)-\\sum\\limits_{v=1}^V\\frac{|D^v|}{|D|}Ent(D^v)$\n\n决策树构造过程，即每次选择一个信息熵增益最大的属性$a$，将数据划分为$V$个子集，然后在每个子集中进行同样的操作（不能选择已经使用过的属性），若无属性可用或者当前子集类别相同，则当前子集标记为类别数量最多的类别，停止继续划分。\n\n## 增益率\n信息熵增益的定义导致其对数量较多的$D^v$更加敏感，因此又提出了增益率的概念：$Gain_ratio(D,a)=\\frac{Gain(D,a)}{IV(a)}$，其中，$IV(a)=-\\sum\\limits_{v=1}^V \\frac{|D^v|}{|D|} \\log_2\\frac{|D^v|}{|D|}$，称为属性$a$的固有值。\n\n## 基尼指数\n基尼值定义为$Gini(D) = \\sum\\limits_{k=1}^C\\sum\\limits_{k' \\ne k}p_k p_{k'} = 1-\\sum\\limits_{k=1}^Cp_k^2$，其反映了在$D$中随机抽取两个样本，属于同一类别的概率。\n和信息熵增益类似，定义基尼指数为$Gini_index(D, a) = \\sum\\limits_{v=1}^V\\frac{|D^v|}{|D|}Gini(D^v)$，构造决策树时的最优划分属性选择也可以使用能够使基尼指数最小的属性。\n\n# 决策树的剪枝\n\n## 预剪枝\n在生成决策树的过程中，利用验证集验证当前划分是否能提升泛化效果，如果不能，则停止继续划分，直接将当前样本集作为决策树叶节点，并指定数量最多的类别做为叶节点类别。预剪枝训练时间短，但是由于其是贪心策略，容易引起欠拟合。\n\n## 后剪枝\n在决策树生成完成之后，自底向上的对非叶节点进行考察，观察将该节点直接指定为叶节点是否能提升验证集泛化效果，如果能则将此节点的子树一起合并为一个叶节点。后剪枝的训练时间比预剪枝长很多，但是其欠拟合风险小，而且泛化性能往往优于预剪枝。\n\n# 包含连续值的决策树\n如果连续属性$a$在$D$中出现$n$个取值，则将其从小到大排序为$\\begin{bmatrix}a_1, a_2, ... a_n\\end{bmatrix}$，这样产生$n-1$个离散值$T_a = \\{\\frac{a_i + a_{i+1}}{2}|1 \\le i \\le n-1\\}$\n则$Gain(D,a)= \\max\\limits_{t \\in T_a}Gain(D, a, t)$，其中$Gain(D, a, t)$表示将$a$属性使用$t$划分为两部分，这样，连续值的决策树构造和离散值类似了，不过连续值属性在构造决策树的过程中可以使用多次。\n\n# 属性缺失的处理\n令$\\tilde{D}$是所有没有缺失属性a的样本集合，对于样本$x$，有样本权重$w_x$，定义如下参数。\n$$\n\\rho = \\frac{\\sum_{x\\in \\tilde{D}} w_x}{\\sum_{x\\in D}w_x}\\\\\n\\tilde{p}_k = \\frac{\\sum_{x\\in \\tilde{D}_k w_x}}{\\sum_{x\\in \\tilde{D}}w_x}, (1\\le k \\le C)\\\\\n\\tilde{r}_v = \\frac{\\sum_{x\\in \\tilde{D}^v}w_x}{\\sum_{x \\in \\tilde{D}} w_x}, (1 \\le v \\le V)\n$$\n显然，$\\rho$表示属性无缺失样本所占比例，$\\tilde{p}_k$表示属性无缺失样本中第$k$类所占比例，$\\tilde{r}_v$表示属性无缺失样本中在属性$a$上取值$a^v$的样本比例。\n\n由此推广信息增益为：\n$$\n\\begin{aligned}\nGain(D, a) &= \\rho \\times Gain(\\tilde{D}, a)\\\\\n&=\\rho \\times (Ent(\\tilde{D}) - \\sum\\limits_{v=1}^V \\tilde{r}_v Ent(\\tilde{D}^v))\n\\end{aligned}\n$$\n其中：\n$$\nEnt(\\tilde{D}) = -\\sum\\limits_{k=1}^C \\tilde{p}_k log_2 \\tilde{p}_k\n$$\n这样解决了最优划分的属性选择问题，在构造子树时，如果样本$x$在属性$a$上的取值已知，那么$x$划分到相应子节点，且权重保持为$w_x$，如果属性$a$未知，则将$s$划分入所有的子节点，且权重调整为$\\tilde{r}_v w_x$。\n\n## 多变量决策树\n叶节点不再针对某个属性，而是针对属性的线性组合进行划分。\n\n# 神经网络\n\n## 感知机\n两层神经元，输入层（没有权重，直接前馈数据）和输出层，输出层是M-P神经元（阈值逻辑单元），感知机只能拟合线性可分的数据，否则其学习过程将变得震荡，难以收敛。\n\n## BP算法\n对于$l$层神经网络,输入$x \\in R^n$，标签$y \\in R^c$，第$i$层权重表示为$w_i \\in R^{O_i \\times I_i}, I_1 = n，O_l = c$，第$i$层偏移表示为$b_i \\in R^{O_i}$，第$i$层激活函数表示为$\\sigma_i$，这一般是个逐元素函数，第$i$层输入即第$i-1$层的输出，表示为$l_{i-1}$，其中$l_0 = x, z_i = w_i l_{i-1} + b_i, l_i = \\sigma_i(z_i)$\n\nloss函数记为$E(l_l, y)$，BP算法每次更新$w_i = w_i - \\eta \\frac{\\partial E}{\\partial w_i}$， $b_i = b_i - \\eta \\frac{\\partial E}{\\partial b_i}$，即让参数像梯度最小的方向前进。\n\n首先定义$E$对$l_l$的偏导为$\\frac{\\partial E}{\\partial l_l} = E'$，这个值由loss函数决定。\n因此\n$$\n\\begin{aligned}\ndE &= E'^Tdl_l\\\\\n&=E'^T(\\sigma_l'(z_l) \\odot (dz_l))\\\\\n&=E'^Tdiag(\\sigma_l'(z_l))da_l\\\\\n\\Rightarrow \\frac{\\partial E}{\\partial z_l} &= diag(\\sigma_l'(z_l))E'\n\\end{aligned}\n$$\n\n这里把$\\frac{\\partial E}{\\partial z_l}$记作$\\delta_l$\n\n因为：\n$$\n\\begin{aligned}\n    da_i &= w_idl_{i-1}\\\\\n    &= w_i(\\sigma_i'(z_{i-1}) \\odot (dz_{i-1}))\\\\\n    &=w_idiag(\\sigma_{i-1}'(z_{i-1}))dz_{i-1}\\\\\n    \\Rightarrow \\frac{\\partial z_i}{\\partial z_{i-1}} &= diag(\\sigma_{i-1}'(z_{i-1}))w_i^T\\\\\n\\end{aligned}\n$$\n\n所以定义:\n$$\n\\begin{aligned}\n    \\delta_i &= \\frac{\\partial E}{\\partial z_i},\\ i=1,2,...,l-1\\\\\n    \\Rightarrow \\delta_{i-1} &= \\frac{\\partial z_i}{\\partial z_{i-1}}\\frac{\\partial E}{\\partial z_i},\\ i=2,...,l\\\\\n    &= diag(\\sigma_{i-1}'(z_{i-1}))w_i^T\\delta_i\\\\\n\\end{aligned}\n$$\n\n现在再来考虑$E$对$w_{l-k}$的导数：\n$$\n\\begin{aligned}\n    dE &= \\frac{\\partial E}{\\partial z_{l-k}}^Tdz_{l-k}\\\\\n    &= \\delta_{l-k}^T(dw_{l-k}l_{l-k-1} + db_{l-k})\\\\\n    &= tr(\\delta_{l-k}^Tdw_{l-k}l_{l-k-1} + \\delta_{l-k}^Tdb_{l-k})\\\\\n    &= tr(l_{l-k-1}\\delta_{l-k}^Tdw_{l-k} + \\delta_{l-k}^Tdb_{l-k})\\\\\n    \\Rightarrow \\frac{\\partial E}{\\partial w_{l-k}} &= \\delta_{l-k}l_{l-k-1}^T\\\\\n    \\Rightarrow \\frac{\\partial E}{\\partial b_{l-k}} &= \\delta_{l-k}\n\\end{aligned}\n$$\n这里的变换属于标量对矩阵求导$d f = tr((\\frac{\\partial f}{\\partial X}) ^ T dX)$，且用到了迹的一个性质：$tr(A B) = tr(B A)$，其中$A$和$B^T$大小相同\n\n全连接层的BP算法看起来很复杂，其实非常简单，只要使用以下几个等式即可求出任一层的权重和偏置的导数：\n$$\n\\begin{aligned}\n    \\delta_l = \\frac{\\partial E}{\\partial z_l} &= diag(\\sigma_l'(z_l))E'\\\\\n    \\delta_{i} = \\frac{\\partial E}{\\partial z_i} &= diag(\\sigma_i'(z_i))w_{i+1}^T\\delta_{i+1},\\ i=1,2,...,l-1\\\\\n    \\frac{\\partial E}{\\partial w_i} &= \\delta_il_{i-1}^T,\\ i=1,2,...,l\\\\\n    \\frac{\\partial E}{\\partial b_i} &= \\delta_i,\\ i=1,2,...,l\n\\end{aligned}\n$$\n\n## RBF（Radial Basis Function，径向基函数）网络\n\nRBF网络是指使用径向基函数作为隐层激活函数的单隐层前馈神经网络$\\phi(x) = \\sum\\limits_{i=1}^q w_i\\rho(x, c_i)$，常用的高斯径向基函数$\\rho(x, c_i) = e^{-\\beta_i||x-c_i||^2}$，其中$c_i,w_i$分别表示第$i$个神经元的中心和权重。\n\n$c_i$可以使用随机采样或者聚类来获得，而其他参数$w_i, \\beta_i$由BP算法等方法来获得。\n\n# SVM\n支持向量机的相关内容可以见我的另一篇文章{% post_link SVM学习笔记 SVM学习笔记 %}，这里不再重新做笔记。\n\n# 贝叶斯分类器\n## 条件风险\n条件风险$R(c_i|x) = \\sum\\limits_{j=1}^N \\lambda_{ij}P(c_j|x)$其中$\\lambda_{ij}$表示将$j$类样本分类为$i$时的损失。\n\n机器学习的任务是寻找一个判定准则$h:x \\rightarrow y$以最小化总体风险$\\min\\limits_{h} R(h)=E_x[R(h(x)|x)]$，即在每个样本上选择$h^\\star(x) = \\mathop{\\arg\\max}\\limits_c R(c|x)$，这样的分类器$h(x)$被称为贝叶斯最优分类器。与之对应的总体风险$R(h^\\star)$被称为贝叶斯风险，$1-R(h^\\star)$是分类器能达到的最好性能，即通过机器学习能产生的模型精度理论上限。\n\n如果$\\lambda_{ij} = \\begin{cases}1&i \\ne j\\\\0&i = j\\end{cases}$，那么条件风险将变成$R(c|x) = 1-P(c|x)$，于是最小化分类错误率的贝叶斯最优分类器变成$h^\\star(x) = \\mathop{\\arg\\max}\\limits_c P(c|x)$即最大化后验概率。\n\n## 生成式与判别式模型\n在使用贝叶斯分类器时，需要获取后验概率$P(c|x)$，但是非常难以直接获取，因此有两种方式：\n\n第一种是直接对$P(c|x)$进行建模，称为判别式方法。\n\n第二种是生成式模型，考虑$P(c|x) = \\frac{P(x, c)}{P(x)}=\\frac{P(c)P(x|c)}{p(x)}$，其中$P(c)$称为类先验概率，$P(x|c)$是样本$x$相对于类别$c$的类条件概率（似然），$P(c)$可以在样本量足够大时用各类样本出现的频率来估计（大数定律），但是$P(x|c)$非常难估计，因为这涉及到关于$x$的所有属性的联合概率，很难直接用样本出现的频率来进行估计。\n\n## 极大似然估计\n再生成式模型中，估计$P(x|c)$的一种策略是首先确定其满足某种确定的概率分布形式，假设$P(x|c)$被参数向量$\\theta_c$唯一确定，因此$P(x|c)$可以被记为$P(x|\\theta_c)$，概率模型的训练过程就是参数估计的过程，参数估计有两种方案：\n\n- 参数虽然未知，但是却是客观存在的固定值（频率主义学派）\n- 参数也是一种未观察到的随机变量，其本身也满足一定的分布，因此需要先假设参数服从一个先验分布，然后基于观测数据来计算参数的后验分布（贝叶斯学派）\n\n极大似然估计属于频率主义学派，将参数当成未知的固定值来处理。首先令$D_c$表示训练集$D$的第$c$类样本集合，并假设这些样本独立同分布，因此其似然可以表示为$P(D_c|\\theta_c) = \\prod\\limits_{x \\in D_c}P(x|\\theta_c)$，极大化似然，就能找到参数$\\theta_c$。\n\n似然的表达式中有连乘，容易造成下溢，因此通常使用对数似然$\\log P(D_c|\\theta_c) = \\sum\\limits_{x \\in D_c} \\log P(x|\\theta_c)$。\n\n像极大似然法这种参数化的方法的准确性严重依赖于对$P(x|\\theta_c)$分布的假设，在实际应用中需要利用应用任务的经验知识，才能得到比较好的分类器。\n\n## 朴素贝叶斯分类器\n在生成式模型中，$P(c|x) = \\frac{P(x, c)}{P(x)}=\\frac{P(c)P(x|c)}{P(x)}$，这里估计$P(x|c)$的困难在于类条件概率$P(x|c)$是$x$所有属性的联合分布，难以从有限的训练样本中估计得到。\n\n朴素贝叶斯分类器采用属性条件独立性假设：对所有已知类别，样本$x$的所有属性相互独立。\n\n因此$P(c|x) = \\frac{P(c)P(x|c)}{P(x)} = \\frac{P(c)}{P(x)} \\prod\\limits_{i=1}^dP(x_i|c)$，其中$d$为样本的属性数量，$x_i$表示样本$x$的第$i$个属性值。\n\n对于所有类别来说，$P(x)$相同，因此基于$\\lambda_{ij} = \\begin{cases}1&i \\ne j\\\\0&i = j\\end{cases}$，$h^\\star(x) = \\mathop{\\arg\\max}\\limits_c P(c|x)$即最大化后验概率的朴素贝叶斯分类器就可以表达为$h_{nb}(x) = \\mathop{\\arg\\max}\\limits_c P(c)\\prod\\limits_{i=1}^dP(x_i|c)$\n\n对于类先验概率，可以从训练集中使用$P(c) = \\frac{|D_c|}{|D|}$估计。\n\n对于离散属性，估计$P(x_i|c)$的方式常用$P(x_i|c) = \\frac{|D_{c,x_i}|}{|D_c|}$，其中$D_{c,x_i}$表示$D_c$中第$i$个属性取值为$x_i$的样本集合。\n\n对于连续属性，则可以使用概率密度函数，假定$P(x_i|c)$服从某种分布，然后对其进行估计。\n\n在离散属性的处理上，有个问题是：如果训练集中某种属性在类别$c$上没有出现，或者类别$c$在训练集上没有出现，则$P(c)\\prod\\limits_{i=1}^dP(x_i|c)$直接就为0了，因此需要进行修正平滑处理。\n\n常用的是拉普拉斯修正，即将$P(c) = \\frac{|D_c|}{|D|}$更改为$P(c) = \\frac{|D_c|+1}{|D|+N}$，其中N表示类别个数, 将$P(x_i|c) = \\frac{|D_{c,x_i}|}{|D_c|}$更改为$P(x_i|c) = \\frac{|D_{c,x_i}| + 1}{|D_c| + N_i}$，其中$N_i$表示第$i$个属性的可取值个数。这样可以避免因训练集样本不充分而导致的概率估计为零的问题。\n\n## 半朴素贝叶斯分类器\n由于属性条件独立性假设很难成立，因此尝试对条件独立性假设进行一定程度的放松。\n\n例如独依赖估计（One-Dependent Estimator，ODE），即每个属性在类别之外最多依赖一个其他属性：$P(c|x)\\propto P(c) \\prod\\limits_{i=1}^dP(x_i|c,pa_i)$，其中$pa_i$为属性$x_i$所依赖的属性，称为$x_i$的父属性。\n\n问题的关键在于如何确定父属性，最直接的办法是假设所有属性都依赖于同一个父属性，称为超父（super-parent）由此形成了SPODE(Super-Parent ODE)方法。\n\n# 集成学习\n构建并结合多个学习器来完成学习任务。一般是先产生一组个体学习器，然后使用某种策略将他们结合。\n\n根据个体学习器的生成方式，集成学习方法大致可以分为两类：\n- 个体学习器之间存在强依赖关系，必须串行生成，例如：Boosting。\n- 个体学习器之间不存在强依赖关系，可以同时并行生成，例如：Bagging和随机森林（Random Forest）\n\n## Boosting\n先训练一个基学习器，然后使用基学习器对训练样本分布进行调整，使得基学习器预测错误的样本在后续得到更多关注，然后进行下一个基学习器的训练，直到学习器数量达到指定数量。\n\nBoosting中最著名的是AdaBoosting，可以理解为加性模型，即使用基学习器的线性组合$H(x) = \\sum\\limits_{t=1}^T\\alpha_t h_t (x)$来最小化指数损失函数$l_{exp}(H|D) = E_{x \\sim D}[e^{-f(x)H(x)}]$，其中$h_i$表示第$i$个基分类器，$f(x)$表示真实函数$f(x) = y$。\n\nAdaBoosting只适用于二分类任务。\n\n## Bagging\n在原始数据中每次有放回的采样$m$个样本，组成一个子训练集，最终得到共$T$个子训练集，用每个子训练集训练一个基学习器，再将所有基学习器进行结合。\n\n假设基学习器的计算复杂度为$O(m)$，Bagging的复杂度大致为$T(O(m) + O(s))$，其中$O(s)$为投票和采样的复杂度，一般非常小，而$T$是个比较小的常数，因此Bagging的训练复杂度和训练一个学习器的复杂程度同阶，非常高效。\n\n## Bagging和Boosting的区别\n\n\n## 随机森林（Random Forest，RF）\n以决策树为基学习器的Bagging集成基础上，引入随机属性选择。在构造决策树过程中，选择最优划分属性时，可以在当前可用属性的随机子集中进行选择。\n\n## 集成学习的结合策略\n- 平均法\n- 投票法\n- 学习法\n\nstacking是学习法的一个例子：先用原始训练集训练出初级学习器，然后将初级学习器的输出作为输入特征，训练一个次级学习器，用于结合初级学习器输出得到最终输出。最好在初级学习器的验证集上对次级学习器进行训练。\n\n## 多样性\n如果用$h_1,h_2,...,h_T$通过加权平均法来集成得到$H$来估计真实函数$f:R^d \\rightarrow R$，则对样本$x$，定义学习器$h_i$的分歧为：$A(h_i|x) = (h(x_i) - H(x))^2$。集成的分歧表示为$\\overline{A}(H|x) = \\sum\\limits_{i=1}^T w_i A(h_i|x)=\\sum\\limits_{i=1}^Tw_i(h_i(x) - H(x))^2$。分歧表示个体学习器在样本$x$上的不一致性。\n\n而集成的泛化误差可以表示为$E=\\overline{E} - \\overline{A}$（$\\overline{E} = \\sum\\limits_{i=1}^Tw_iE_i$，$\\overline{A}=\\sum_{i=1}^Tw_iA_i$，推导过程略过），这说明个体学习器误差越低，多样性越大，集成的效果越好。\n\n增加个体学习器多样性的方法：\n- 数据样本扰动：通常基于采样法，产生不同的数据子集。但是有些学习器对数据样本量的扰动不敏感（如线性学习器，支持向量机，朴素贝叶斯，k近邻，这些学习器称为稳定基学习器）\n- 输入属性扰动：抽取属性子集来训练学习器。不适合属性较少的数据。\n- 输出表示扰动：将样本的类别标记稍作改动，例如随机改变一些标记，或者进行编码调制，例如ECOC。\n- 算法参数扰动：随机设置不同的参数，例如隐层神经元个数、初始连接权值等。\n\n# 聚类\n\n## 原型聚类\n又称基于原型的聚类方法，此类算法假设聚类结构可以通过一组原型来刻画，通常先对原型进行初始化，然后对原型进行迭代更新求解。不同的原型表示、不同的求解方式就会产生不同的算法\n\n### k均值算法\n给定样本集$D=\\{x_1,x_2,...,x_m\\}$，聚类结果$C = \\{C_1, C_2, ...,C_k\\}$，定义平方误差$E = \\sum\\limits_{i=1}^k \\sum\\limits_{x\\in C_i} ||x-\\mu_i||^2_2$，其中$\\mu_i = \\frac{1}{C_i}\\sum\\limits_{x \\in C_i}x$表示簇$C_i$的均值向量。\n\nk均值算法针对平方误差$E$进行优化，使用贪心策略，通过迭代优化方式来进行：\n\n1、初始化$k$个初始均值向量$\\mu_1, \\mu_2, ..., \\mu_k$。\n\n2、计算每个样本到$k$个均值向量的值，将每个样本划入最近的均值向量对应的簇中，得到一个划分。\n\n3、使用划分好的簇计算新的均值向量。\n\n4、如果没有均值向量被大幅更新或者达到了最大迭代次数，那么停止，否则从第2步继续循环。\n\n### 学习向量量化（Learning Vector Quantization，LVQ）\n和k均值算法类似，但是学习向量量化在学习过程中还利用样本的监督信息来辅助聚类：\n\n1、初始化原型向量$\\{p_1, p_2, ..., p_q\\}$\n\n2、随机选取样本$x_j$，计算$x_j$和每个原型向量的距离，并找出距离最小对应的原型向量。\n\n3、如果$x_j$的标签和原型向量的标签相同，则使用$p=p + \\eta (x_j - p)$来对选出的原型向量进行更新，否则使用$p = p - \\eta (x_j - p)$来对选出的原型向量进行更新\n\n4、达到最大迭代轮数或者更新幅度很小则停止更新，否则从第2步继续循环。\n\n其中$\\eta \\in (0, 1)$表示学习速率，在迭代停止之后，对于任意样本$x$，可以将其划分到与其距离最近的原型向量所代表的的簇中。\n\n### 高斯混合聚类（Mixture-of-Gaussian）\n假设数据服从高斯混合分布$p_M(x) = \\sum\\limits_{i=1}^k \\alpha_i p(x|\\mu_i, \\Sigma_i), \\sum\\limits_{i=1}^k \\alpha_i=1$。\n\n令随机变量$z_j,j\\in \\{1,2,...,k\\}$表示样本$x_j$预测类别，其先验概率$p(z_j = i) = \\alpha_i$，则$p_M(z_j = i|x_j) = \\frac{\\alpha_ip(x_i|\\mu_i,\\Sigma_i)}{\\sum\\limits_{l=1}^k \\alpha_lp(x_j|\\mu_l, \\Sigma_l)}$\n\n当高斯混合分布已知时，即可将样本划分成$k$个簇，其标记为$\\mathop{\\arg\\max}\\limits_i p_M(z_j = i|x_j)$\n\n估计这样的高斯混合分布可以使用极大似然法，令对数似然$LL(p_M) = \\sum\\limits_{j=1}^m ln(\\sum\\limits_{i=1}^k \\alpha_ip(x_i|\\mu_i,\\Sigma_i))$，使用EM算法即可求解。\n\n## 密度聚类\n### DBSCAN\n密度聚类算法的代表是DBSCAN算法，使用一组邻域参数$(\\epsilon, MinPts)$来刻画样本分布的紧密程度。\n\n- $\\epsilon$-邻域：$N_\\epsilon(x_j) = \\{x_i | x_i \\in D, dist(x_j,x_i)\\le \\epsilon\\}$，其中$D$表示全部数据集合。\n- 核心对象：满足$|N_\\epsilon(x_j)| \\ge MinPts$的$x_j$称为核心对象，其中$|N_\\epsilon(x_j)|$表示$x_j$的$\\epsilon$-邻域中的样本数量。\n- 密度直达：$x_j$是一个核心对象，且$x_i$在$x_j$的$\\epsilon$-邻域中，则$x_i$由$x_j$密度直达。\n- 密度可达：如果存在样本序列$p_1, p_2, ..., p_n, p_1 = x_j, p_n = x_i$使得$p_{m+1}$由$p_m$密度直达，则称$x_i$由$x_j$密度可达。\n- 密度相连：如果存在$x_k$使得$x_j$和$x_i$均可由$x_k$密度可达，则称$x_i$和$x_j$密度相连。\n\n基于以上概念，DBSCAN将簇定义为由密度可达关系到处的最大密度相连样本集合。\n\nDBSCAN的聚类过程如下：\n\n1、找到所有的核心对象。\n\n2、循环随机取一个还没有访问过的核心对象，将其所有密度相连的样本生成一个簇并标记为已访问，如果没有未访问的核心对象，则停止循环\n\n3、仍未访问的样本被视为噪声样本。\n\n## 层次聚类\n层次聚类方法试图在不同层次上对数据进行划分，形成树形的聚类结构。\n\n### AGNES\n\nAGNES是一种自底向上的层次聚类算法，首先将所有样本单独看做一个簇，然后每次迭代找到距离最近的两个簇进行合并，直到簇数量等于指定数量。\n\n这里的关键是如何定义两个簇的距离，主要有三种方式，使用三种距离计算的AGNES分别被称为：\n- 最大距离：单链接算法\n- 最小距离：全链接算法\n- 平均距离：均链接算法\n\n# 降维与度量学习\n\n## k近邻（k-Nearest Neighbor）学习\nk近邻方法没有训练过程，在给定测试样本时，直接使用训练样本中与其最靠近的$k$个样本，基于这$k$个样本的信息来对测试样本进行预测。\n\nk近邻方法是懒惰学习（lazy learning）的一个代表，而那些在训练阶段就对样本进行学习处理的方法称为急切学习（eager learning）\n\n## 低维嵌入\n大部分时候，观测到的数据是高维数据，但与学习任务密切相关的很可能是高维空间中的一个低维嵌入。\n\n### 多维缩放（Multiple Dimensional Scaling，MDS）\n多维缩放的思路是找到一个低维空间，使得在这个低维空间中的欧氏距离和原始空间中的距离相等。\n\n假如原始空间的维度为$d$，所有数据的距离矩阵$D\\in R^{m \\times m}$，其中$m$为样本数量，$d_{ij}$表示样本$x_i$和$x_j$的距离，降维后的数据$z \\in R^{d'}$，所有样本表示为$Z\\in R^{d' \\times m}$。\n\n令$B = Z^T Z \\in R^{m\\times m}$是降维后的内积矩阵，有$b_{ij} = z_i^T z_j$\n\n则$d_{ij}^2 = ||z_i||^2 + ||z_j||^2 - 2z_i^T z_j = b_{ii} + b_{jj} - 2b_{ij}$\n\n令降维后的样本$Z$被中心化，即$\\sum\\limits_{i=1}^m z_i = \\mathbf{0}$，可得到$\\sum\\limits_{i=1}^m b_{ij} = \\sum\\limits_{j=1}^m b_{ij} = 0$\n\n因此：\n\n$$\n\\sum\\limits_{i=1}^md_{ij}^2 = tr(B) + mb_{jj}\\\\\n\\sum\\limits_{j=1}^md_{ij}^2 = tr(B) + mb_{ii}\\\\\n\\sum\\limits_{i=1}^m\\sum\\limits_{j=1}^md_{ij}^2 = 2m\\ tr(B)\\\\\ntr(B) = \\sum\\limits_{i=1}^m b_{ii}\n$$\n\n则有：\n\n$$\n\\begin{aligned}\n    b_{ij} &= \\frac{b_{ii} + b_{jj} - d_{ij}^2}{2}\\\\\n    &=\\frac{1}{2m}(\\sum\\limits_{i=1}^md_{ij}^2) + \\frac{1}{2m}(\\sum\\limits_{j=1}^md_{ij}^2) - \\frac{1}{2m^2} \\sum\\limits_{i=1}^m\\sum\\limits_{j=1}^md_{ij}^2 - \\frac{d_{ij}^2}{2}\n\\end{aligned}\n$$\n\n这样就能根据原来的距离矩阵求出内积矩阵$B$，再对$B$进行特征分解得到$B=V \\Lambda V^T$，其中$\\Lambda = diag(\\lambda_1, ..., \\lambda_d)$为特征值构成的对角阵，$\\lambda_1 \\ge \\lambda_2\\ge ...\\ge \\lambda_d$，为了降维，我们可以取其中$d'$个非零值，构成对角矩阵$\\Lambda_\\star$与其对应的特征向量矩阵$V_\\star$，则$Z = \\Lambda_\\star ^{\\frac{1}{2}} V_\\star^T \\in R^{d' \\times m}$\n\n## 主成分分析（PCA）\n如果希望使用一个超平面来对数据进行表示，那么可以从两个方面去考虑：\n- 最近重构性：样本点到这个超平面的距离都足够近\n- 最大可分性：样本点在这个超平面上的投影尽可能分开\n\n但是两个方面的考虑最终都会得到PCA的等价推导，即PCA既保证了最近重构性也保证了最大可分性。\n\n假定数据$x_i \\in R^d$已经进行过中心化，即$\\sum_i x_i = \\mathbf{0}$，现在使用一组标准正交基对$x_i$进行投影，得到$z_i = Wx_i, W = \\begin{bmatrix}w_1^T\\\\ w_2^T\\\\ \\vdots\\\\ w_{d'}^T \\end{bmatrix}\\in R^{d' \\times d}, w_i^T w_j = \\begin{cases}1 & i=j\\\\ 0& i\\ne j\\end{cases}$，其中$z_{ij} = w_j^Tx_i$，如果使用$z_i$来还原$x_i$则得到$\\hat{x}_i = \\sum\\limits_{j=1}^{d'}z_{ij}w_j = W^T z_i$。\n\n如果从最近重构性来考虑，我们希望最小化$\\sum\\limits_{i=1}^m ||\\hat{x}_i - x_i||^2_2$，即：\n$$\n\\begin{aligned}\n    &\\min \\sum\\limits_{i=1}^m ||\\hat{x}_i - x_i||^2_2\\\\\n    &=\\min \\sum\\limits_{i=1}^m || \\sum\\limits_{j=1}^{d'}z_{ij}w_j - x_i||^2_2\\\\\n    &=\\min \\sum\\limits_{i=1}^m || W^Tz_i - x_i||^2_2\\\\\n    &=\\min \\sum\\limits_{i=1}^m (z_i^TWW^Tz_i - z_i^TWx_i - x^T_iW^Tz_i + x^T_i x_i)\\\\\n    &=\\min \\sum\\limits_{i=1}^m (z_i^Tz_i - z_i^TWx_i - x^T_iW^Tz_i)\\\\\n    &=\\min \\sum\\limits_{i=1}^m tr(z_i^Tz_i - z_i^TWx_i - x^T_iW^Tz_i)\\\\\n    &=\\min \\sum\\limits_{i=1}^m tr(z_i^Tz_i - 2z_i^TWx_i)\\\\\n    &=\\min \\sum\\limits_{i=1}^m -tr(z_i^T z_i)\\\\\n    &=\\min -tr(Z^T Z),\\ Z = \\begin{bmatrix}z_1 &z_2 & \\cdots &z_m\\end{bmatrix} = WX,\\ X = \\begin{bmatrix}x_1 &x_2 & \\cdots &x_m\\end{bmatrix}\\\\\n    &=\\min -tr(X^TW^TWX)\\\\\n    &=\\min -tr(WXX^TW^T)\\\\\n\\end{aligned}\n$$\n\n因此我们需要解决的问题就是：\n$$\n\\begin{aligned}\n    \\min\\ & -tr(WXX^TW^T)\\\\\n    s.t.\\ & WW^T = I_{d'}\n\\end{aligned}\n$$\n\n另一方面，如果从最大可分性来考虑，我们希望最大化$z_i$之间的方差$\\sum\\limits_{i=1}^m (z_i - \\frac{1}{m}\\sum\\limits_{j=1}^mz_j)^2$\n\n$$\n\\begin{aligned}\n    &\\max \\sum\\limits_{i=1}^m ||z_i - \\frac{1}{m}\\sum\\limits_{j=1}^mz_j||_2^2\\\\\n    &=\\max \\sum\\limits_{i=1}^m ||z_i - \\frac{1}{m}\\sum\\limits_{j=1}^mWx_j||_2^2\\\\\n    &=\\max \\sum\\limits_{i=1}^m ||z_i - \\frac{1}{m}W\\sum\\limits_{j=1}^mx_j||_2^2\\\\\n    &=\\max \\sum\\limits_{i=1}^m ||z_i||_2^2\\\\\n    &=\\max \\sum\\limits_{i=1}^m (z^T_iz_i)\\\\\n    &=\\max\\ tr(Z^TZ)\\\\\n    &=\\max\\ tr(X^TW^TWX)\\\\\n    &=\\min\\ -tr(X^TW^TWX)\\\\\n    &=\\min\\ -tr(WXX^TW^T)\\\\\n\\end{aligned}\n$$\n\n因此我们需要解决的问题就是：\n$$\n\\begin{aligned}\n    \\min\\ & -tr(WXX^TW^T)\\\\\n    s.t.\\ & WW^T = I_{d'}\\\\\n\\end{aligned}\n$$\n\n由此可见，在$\\sum x_i = 0$的情况下，从两个方面得到的结果完全相同。\n\n求解PCA可以使用拉格朗日法，首先得到拉格朗日函数$L(W) = -tr(X^TW^TWX) + \\lambda (WW^T - I_{d'}), \\lambda \\ge 0$\n\n$$\n\\begin{aligned}\n    dL(W) &= -tr(WXX^TdW^T + dWXX^TW^T) + \\lambda tr(dW W^T + WdW^T)\\\\\n    &= -tr(2XX^TW^TdW - 2\\lambda W^TdW)\\\\\n    \\\\\n    \\frac{\\partial dL(W)}{\\partial W} &= 2\\lambda W - 2WXX^T\n\\end{aligned}\n$$\n\n令$\\frac{\\partial dL(W)}{\\partial W} = 0$可得$\\lambda W^T = XX^TW^T$，即求出协方差矩阵$XX^T$的特征向量即可构成$W^T$，在这个过程中，可以舍弃一部分特征向量，只取特征值最大的$d'$个特征向量，即可将数据维度从$d$维缩减到$d'$维。\n\n如果将$X$进行奇异值分解，则有$X=D\\Sigma V^T$，$XX^T = D \\Sigma \\Sigma^T D^T$，其中$D$是$X$的左奇异矩阵，也就是$XX^T$的特征矩阵，如果令$W=D^T$可以得到$Z=D^TX=D^TD\\Sigma V^T = \\Sigma V^T$，因此求出$X^TX$的特征矩阵也可以求出$Z$。\n\n## 核化线性降维\n\n### 核主成分分析（KPCA）\n\nPCA是一种线性降维方式，在降维时假设从高维空间到低维空间的映射是线性的，其线性映射由$W$确定，且有$\\lambda W^T = XX^TW^T = (\\sum\\limits_{i=1}^m x_i x^T_i)W^T$，假设在高维空间$\\phi(x)$中进行PCA，则有$\\lambda W^T = (\\sum\\limits_{i=1}^m \\phi(x_i) \\phi(x_i)^T)W^T$，有如下推导：\n\n$$\n\\begin{aligned}\n    \\lambda W^T &= (\\sum\\limits_{i=1}^m \\phi(x_i) \\phi(x_i)^T)W^T\\\\\n    &= (\\sum\\limits_{i=1}^m \\phi(x_i) \\phi(x_i)^T)W^T\\\\\n    &= \\sum\\limits_{i=1}^m \\phi(x_i)\\phi(x_i)^TW^T\\\\\n    &= \\sum\\limits_{i=1}^m\\phi(x_i)\\alpha_i,\\ \\alpha_i = \\phi(x_i)^TW^T\\\\\n\\end{aligned}\n$$\n\n由于$\\phi$函数无法明确求出，因此引入核函数。\n$$\n\\begin{aligned}\n    \\lambda \\phi(x_j)^TW^T &= \\phi(x_j)^T\\sum\\limits_{i=1}^m\\phi(x_i)\\alpha_i\\\\\n    \\lambda A&=KA\\\\\n\\end{aligned}\n$$\n\n其中$K$是核矩阵$K_{ij} = \\phi(x_i)^T\\phi(x_j)$，$A = \\begin{bmatrix}\\alpha_1, \\alpha_2, ..., \\alpha_m\\end{bmatrix}$\n\n则$z_{ij} = w_j^T \\phi(x_i) = \\sum\\limits_{k=1}^m\\alpha_{k,j}\\phi(x_k)^T\\phi(x_i)=\\sum\\limits_{k=1}^m\\alpha_{k,j}K_{ki}$表示降维之后$x_i$对应向量的第$j$个分量。\n\n其中W可以由$\\phi(X)^T\\phi(X)$的特征矩阵求出，但在计算$z$时，由于每个分量需要求和$\\sum\\limits_{k=1}^m\\alpha_{k,j}$，计算量非常大。\n\n## 流形学习\n流形是指在局部与欧式空间统配的空间，如果数据是嵌入在高维空间中的低维流形，则可以利用其局部与欧式空间同胚的性质，使用局部的欧式距离来计算数据样本之间的距离。\n\n### 等度量映射（Isometric Mapping，Isomap）\n将低维嵌入流形上两点的距离定义为“测地线”距离，即两个样本沿着流形的最短距离，测地线距离的计算可以将邻近点之间进行连接，然后转换计算近邻连接图上两点之间的最短路径问题（Dijkstra算法或者Floyd算法）。得到距离矩阵之后，可以使用多维缩放算法（MDS）来进行降维。\n\n邻近图的构建一般有两种做法，一个是指定最近的$k$个点作为邻近点，这样得到的邻近图称为$k$邻近图，另一个是指定距离阈值$\\epsilon$，小于$\\epsilon$的点被认为是邻近点，这样得到的邻近图称为$\\epsilon$邻近图，两种方式各有优劣。\n\n### 局部线性嵌入（Locally Linear Embedding，LLE）\nIsomap试图保持邻近样本之间的距离，而LLE试图保持邻域内样本的线性关系，假定样本点$x_i$可以通过$x_j,x_k,x_l$线性组合而得到，即$x_i = w_{ij}x_j+w_{ik}x_k+w_{il}x_l$\n\n首先对于每个样本$x_i$，找到其邻近下标集合$Q_i$，然后计算$Q_i$对$x_i$的线性重构系数：\n$$\n\\begin{aligned}\n    \\min\\limits_{w_1, w_2, ..., w_m}\\ &\\sum\\limits_{i=1}^m ||x_i - \\sum\\limits_{j\\in Q_i}w_{ij} x_j||_2^2\\\\\n    s.t.\\ &\\sum\\limits_{j\\in Q_i} w_{ij} = 1\n\\end{aligned}\n$$\n\n求得$w_{ij}$之后，$x_i$对应的低维空间坐标$z_i = \\min\\limits_{z_1, z_2, ..., z_m} \\sum\\limits_{i=1}^m ||z_i - \\sum\\limits_{j\\in Q_i} w_{ij} z_j||_2^2$。\n\n令$Z = \\begin{bmatrix}z_1 & z_2 & \\cdots & z_m\\end{bmatrix} \\in R^{d' \\times m}, W_{ij} = w_{ij}$\n\n则确定$W$之后，$Z$可以通过：\n$$\n\\begin{aligned}\n    \\min\\limits_Z\\ &tr(Z(\\mathbf{I} - W)^T(\\mathbf{I} - W)Z^T)\\\\\n    s.t.\\ &ZZ^T = \\mathbf{I}\n\\end{aligned}\n$$\n来求得，即对$(\\mathbf{I} - W)^T(\\mathbf{I} - W)$进行特征分解，取最小的$d'$个特征值对应的特征向量构成$Z^T$\n\n## 度量学习\n对高维数据的降维主要是希望找到一个合适的低维空间使得此空间中学习能比原始空间性能更好，度量学习的思路是尝试学习出一个距离度量。\n\n对于两个$d$维样本$x_i$，$x_j$，其欧氏距离的平方$||x_i - x_j||^2_2 = dist_{ij,1} + dist_{ij,2} + \\cdots + dist_{ij,d}$，其中$dist_{ij,k}$表示在第$k$维上的距离。\n\n如果假定不同属性的重要性不同，则可以引入权重$w$，$||x_i - x_j||^2_2 = w_1 dist_{ij,1} + w_2 dist_{ij,2} + \\cdots + w_d dist_{ij,d} = (x_i - x_j)^T W (x_i - x_j)$，其中$W = diag(w), w_i \\ge 0$\n\n如果令$W$不再是一个对角矩阵，而是让其等于一个半正定对称矩阵$M$，则可以定义马氏距离$dist_{mah}^2(x_i, x_j) = (x_i - x_j)^TM(x_i - x_j) = ||x_i - x_j||^2_M$，则可以对这个$M$进行学习，得到满足要求的距离表达。\n\n在近邻成分分析（Neighbourhood Component Analysis, NCA）中可以用对$M$进行训练，提高其分类正确率。\n\n又例如根据一些领域知识已知某些样本相似（必连约束集合），另外一些样本不相似（勿连约束及合），则可以对$M$进行训练，使得必连约束集合中的样本距离尽可能小，而勿连约束集合中的样本的尽可能大。\n\n不管以任何方式训练得到的$M$，都可以对M进行特征值分解，然后去掉一部分特征向量，得到降维矩阵，用于数据的降维。","source":"_posts/学习笔记/《机器学习》读书笔记.md","raw":"---\ntitle: 《机器学习》读书笔记\ndate: 2020-04-17 17:27:30\ntags: [机器学习, 读书笔记, 杂项]\nmathjax: true\n---\n\n# 线性模型\n\n## 基于均方误差最小化来进行求解的方法称为最小二乘法\n\n## 用最小二乘法来优化线性回归\n线性回归的目标是学习函数$f(X) = Xw$，使得$f(X) \\approx Y$，其中$X=\\begin{bmatrix}x_1 & 1\\\\ x_2 & 1\\\\ \\vdots & \\vdots \\\\ x_m & 1 \\end{bmatrix}$，$Y=\\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_m\\end{bmatrix}$，$w \\in R^{(n+1) \\times 1}$，$n$是数据特征维数。\n\n如果用均方误差作为损失函数$L = (Xw-Y)^T (Xw-Y)$，那么问题可以描述为\n\n$$\n\\begin{aligned}\n&\\arg \\min_{w} (Xw-Y)^T (Xw-Y)\\\\\n\\end{aligned}\n$$\n直接求导：\n$$\n\\frac{\\partial L}{\\partial w} = \\frac{\\partial (w^T X^T X w - Y^TXw -XwY - Y^TY)}{\\partial w} = 2X^T(Xw-Y)\\\\\n$$\n令：\n$$\n\\begin{aligned}\n&\\frac{\\partial L}{\\partial w} = 0\\\\\n&\\Rightarrow w = (X^TX)^{-1}X^TY\n\\end{aligned}\n$$\n即得到使用均方误差的线性回归问题的解。\n\n## 对数几率回归（逻辑回归， Logistic Regression）\n对数几率回归即用线性回归去拟合对数几率$ln\\frac{y}{1-y} = w^T x + b$\n\n对数几率回归也等价于$y=\\frac{1}{1+e^{-z}}, z = w^T x + b$\n\n若将$y$视为后验概率，则$ln \\frac{P(y=1|x)}{P(y=0|x)} = z, z=w^Tx + b$\n\n显然有$P(y=1|x) = \\frac{e^z}{1 + e^z}, P(y=0|x)=\\frac{1}{1 + e^z}$\n\n使用极大似然法求解对数几率回归：$\\mathop{\\arg\\max}\\limits_{w, b}\\prod\\limits_{i=1}^m P(y=y_i| x_i) \\Rightarrow \\mathop{\\arg\\max}\\limits_{w, b}\\sum\\limits_{i=1}^m ln(P(y=y_i|x_i))$\n\n令$\\beta = \\begin{bmatrix}w ^ T& b\\end{bmatrix} ^ T \\in R^{n+1}, Y=\\begin{bmatrix}y_1&y_2&\\dots&y_m\\end{bmatrix} ^ T, X=\\begin{bmatrix} x_1&x_2&\\dots&x_m\\\\1&1&\\dots&1\\end{bmatrix},x_i \\in R^n, X \\in R^{(n+1) \\times m}$，其中$m$是数据量。\n\n使用极大似然法求解对数几率回归可以重写为：\n$$\n\\begin{aligned}\n&\\mathop{\\arg\\max}\\limits_{\\beta} l(Z)\\\\\n&Z = X^T \\beta\\\\\n&l(Z) = Y^Tln\\frac{e^Z}{\\mathbf{1} + e^Z} + (\\mathbf{1}-Y)^Tln\\frac{\\mathbf{1}}{\\mathbf{1}+e^Z}\\\\\n&=Y^TZ - ln(\\mathbf{1}+e^Z)\n\\end{aligned}\n$$\n\n使用牛顿法，第$t$次更新为$\\beta^{t+1} \\leftarrow \\beta ^ t - (\\triangledown_2l)^{-1}\\frac{\\partial l}{\\partial \\beta}$\n\n$$\n\\begin{aligned}\ndl &= Y^TdZ - \\mathbf{1}^T\\frac{e^Z}{\\mathbf{1}+e^Z} \\odot dZ\\\\\n&=Y^TdZ -\\mathbf{1}^T \\hat{P}_1 \\odot dZ, \\hat{P} = \\begin{bmatrix} P(y=1|x_1) & P(y=1|x_2)& \\dots & P(y=1|x_m)\\end{bmatrix}^T\\\\\n&=Y^TX^Td\\beta - \\mathbf{1}^T \\hat{P}_1 \\odot (X^Td\\beta)\\\\\n&=Y^TX^Td\\beta - (\\mathbf{1} \\odot \\hat{P}_1)^TX^Td\\beta\\\\\n&=(Y^T-\\hat{P}_1^T)X^Td\\beta\n\\end{aligned}\n$$\n\n所以$\\frac{\\partial l}{\\partial \\beta} = X(Y-\\hat{P}_1)$\n\n$$\n\\begin{aligned}\n    d(\\frac{\\partial l}{\\partial \\beta}) &= d(X(Y-\\hat{P}_1))\\\\\n    &=Xd\\hat{P}_1\\\\\n    &=Xd\\frac{e^Z}{\\mathbf{1}+e^Z}\\\\\n    &=X(\\frac{1}{1+e^Z}\\odot\\frac{e^Z}{1+e^Z} \\odot dZ)\\\\\n    &=X(\\hat{P}_0 \\odot \\hat{P}_1 \\odot (X^Td\\beta))\\\\\n    &=X diag(\\hat{P}_0) diag(\\hat{P}_1) X^Td\\beta,\\ diag(\\hat{P}_0) = \\begin{bmatrix}\n        P(y=0|x_1)&\\cdots&0\\\\\n        \\vdots&\\ddots&\\vdots\\\\\n        0&\\cdots&P(y=0|x_m)\n    \\end{bmatrix}\n\\end{aligned}\n$$\n\n所以$\\frac{\\partial^2 l}{\\partial \\beta \\partial \\beta^T} = \\frac{\\partial^2 l}{\\partial \\beta^T \\partial \\beta} = Xdiag(\\hat{P}_1) diag(\\hat{P}_0)X^T$\n\n即如果用牛顿法来求解极大似然对数几率回归，第$t$次更新为\n\n$$\n\\beta^{t+1} \\leftarrow \\beta ^ t - (Xdiag(\\hat{P}_1) diag(\\hat{P}_0)X^T)^{-1} X(Y-\\hat{P}_1)\n$$\n\n## 线性判别分析\n线性判别分析也叫“Fisher判别分析”，其思想非常朴素：试图找到一个低维空间（直线），可以表示为$y=wx$(这个表达式中的$y$表示$x$投影到这个空间（直线）后和原点的距离)，使得当样本投影到该直线时不同类别样本的距离尽可能远，而相同类别的数据尽可能接近，预测时按照样本的投影位置对其进行分类。\n\n以两类数据$x_1, x_2$为例，设$\\mu_1,\\mu_2,\\Sigma_1,\\Sigma_2$分别表示两类数据的均值和方差，则投影之后的均值和方差为$w\\mu_1,w\\mu_2,w^T\\Sigma_1w,w^T\\Sigma_2w$，因为直线是一维空间，所以这些均为实数，投影之后的类内距离可以用方差来衡量，这里使用$w^T\\Sigma_1w + w^T\\Sigma_2w$来度量投影之后的类内距离，而类间距离可以写成$||w\\mu_2 - w\\mu_1||_2^2$，同时考虑两种距离，给出希望最大化的目标函数如下。\n$$\n\\begin{aligned}\nJ &= \\frac{||w^T\\mu_2 - w^T\\mu_1||_2^2}{w^T\\Sigma_1w + w^T\\Sigma_2w}\\\\\n&= \\frac{w^T(\\mu_2 - \\mu_1)(\\mu_2 - \\mu_1)^Tw}{w^T(\\Sigma_1 + \\Sigma_2)w}\n\\end{aligned}\n$$\n定义类内散度矩阵$S_w = \\Sigma_1 + \\Sigma_2$，类间散度矩阵$S_b = (\\mu_2 - \\mu_1)(\\mu_2 - \\mu_1)^T$，上面的优化目标可以简写为如下。\n$$\n\\begin{aligned}\n    J = \\frac{w^TS_bw}{w^TS_ww}\n\\end{aligned}\n$$\n这个优化目标又称为$S_b$和$S_w$的广义瑞利商，注意到分子分母中都有$w$的二次项，因此和$w$大小无关，只和w方向有关，所以优化问题可以写成下式。\n$$\n\\begin{aligned}\n\\min_w& \\quad-w^TS_bw\\\\\ns.t.& \\quad w^TS_ww = 1\n\\end{aligned}\n$$\n用拉格朗日乘子法进行优化，求解$S_bw = \\lambda S_ww$，因$S_bw$方向和$\\mu_2 - \\mu_1$相同，因此令$S_bw = \\lambda(\\mu_2 - \\mu_1)$，代入求解，可以得到$w = S_w^{-1}(\\mu_2 - \\mu_1)$。\n\n将数据进行降维，使得类内散度最小，类间散度最大，本质上是有监督的降维。\n\n## 多分类\n可以将多分类问题拆解为二分类问题，拆解策略有三种：一对一（OvO）、一对其余（OvR）、多对多（MvM）\n\n在MvM中，最常用的是纠错输出码（Error Correcting Output Codes，ECOC）,有$C_1C_2C_3...C_n$共$n$个类别，每个样本属于其中的一种，训练m个二分类器$f_1, f_2, ..., f_m$，每个分类器将一些类作为正类，另一些类作为负类，这样对于某个类别的样本，理想情况是$m$个分类器对其进行预测的输出组成的0,1串，构成一种长度为$m$的固定的类别组合串，$n$个类就有$n$种组合，但在预测时，对一个样本预测得到的输出串，可能不在$n$个类的$n$种组合中，这时，计算预测输出串和每个类别组合串的距离（海明距离或者欧式距离），将样本判定为距离最小的那个类别组合串对应的类别。\n\n## 类别不平衡\n解决办法主要有三种：\n- 再缩放（再平衡），根据样本数量移动判定阈值或者缩放预测概率。\n- 欠采样，将样本量过多的类别进行采样，减少该类别的样本数量，再拿去训练，但是这个方法容易丢失数据中的信息，最好是分成多个模型，每个模型使用该类别的一部分数据。\n- 过采样，将样本量过少的类别样本进行重复，然后训练，但是这个方法容易严重过拟合，一个办法是用两个该类别样本进行插值，生成新的该类别样本。\n\n# 决策树\n\n## 信息熵\n样本集合$D$中第$k$类样本所占比例为$p_k$，则信息熵定义为$Ent(D) = -\\sum\\limits_{k=1}^C p_k\\log_2p_k$，其中$C$为类别个数。\n\n## 信息熵增益\n假设离散属性$a$有$v$个取值：$a_1, a_2, ..., a_v$，可以将当前数据集合分成$V$个子集：$D_1, D_2, ..., D^V$，那么信息熵增益定义为$Gain(D,a)=Ent(D)-\\sum\\limits_{v=1}^V\\frac{|D^v|}{|D|}Ent(D^v)$\n\n决策树构造过程，即每次选择一个信息熵增益最大的属性$a$，将数据划分为$V$个子集，然后在每个子集中进行同样的操作（不能选择已经使用过的属性），若无属性可用或者当前子集类别相同，则当前子集标记为类别数量最多的类别，停止继续划分。\n\n## 增益率\n信息熵增益的定义导致其对数量较多的$D^v$更加敏感，因此又提出了增益率的概念：$Gain_ratio(D,a)=\\frac{Gain(D,a)}{IV(a)}$，其中，$IV(a)=-\\sum\\limits_{v=1}^V \\frac{|D^v|}{|D|} \\log_2\\frac{|D^v|}{|D|}$，称为属性$a$的固有值。\n\n## 基尼指数\n基尼值定义为$Gini(D) = \\sum\\limits_{k=1}^C\\sum\\limits_{k' \\ne k}p_k p_{k'} = 1-\\sum\\limits_{k=1}^Cp_k^2$，其反映了在$D$中随机抽取两个样本，属于同一类别的概率。\n和信息熵增益类似，定义基尼指数为$Gini_index(D, a) = \\sum\\limits_{v=1}^V\\frac{|D^v|}{|D|}Gini(D^v)$，构造决策树时的最优划分属性选择也可以使用能够使基尼指数最小的属性。\n\n# 决策树的剪枝\n\n## 预剪枝\n在生成决策树的过程中，利用验证集验证当前划分是否能提升泛化效果，如果不能，则停止继续划分，直接将当前样本集作为决策树叶节点，并指定数量最多的类别做为叶节点类别。预剪枝训练时间短，但是由于其是贪心策略，容易引起欠拟合。\n\n## 后剪枝\n在决策树生成完成之后，自底向上的对非叶节点进行考察，观察将该节点直接指定为叶节点是否能提升验证集泛化效果，如果能则将此节点的子树一起合并为一个叶节点。后剪枝的训练时间比预剪枝长很多，但是其欠拟合风险小，而且泛化性能往往优于预剪枝。\n\n# 包含连续值的决策树\n如果连续属性$a$在$D$中出现$n$个取值，则将其从小到大排序为$\\begin{bmatrix}a_1, a_2, ... a_n\\end{bmatrix}$，这样产生$n-1$个离散值$T_a = \\{\\frac{a_i + a_{i+1}}{2}|1 \\le i \\le n-1\\}$\n则$Gain(D,a)= \\max\\limits_{t \\in T_a}Gain(D, a, t)$，其中$Gain(D, a, t)$表示将$a$属性使用$t$划分为两部分，这样，连续值的决策树构造和离散值类似了，不过连续值属性在构造决策树的过程中可以使用多次。\n\n# 属性缺失的处理\n令$\\tilde{D}$是所有没有缺失属性a的样本集合，对于样本$x$，有样本权重$w_x$，定义如下参数。\n$$\n\\rho = \\frac{\\sum_{x\\in \\tilde{D}} w_x}{\\sum_{x\\in D}w_x}\\\\\n\\tilde{p}_k = \\frac{\\sum_{x\\in \\tilde{D}_k w_x}}{\\sum_{x\\in \\tilde{D}}w_x}, (1\\le k \\le C)\\\\\n\\tilde{r}_v = \\frac{\\sum_{x\\in \\tilde{D}^v}w_x}{\\sum_{x \\in \\tilde{D}} w_x}, (1 \\le v \\le V)\n$$\n显然，$\\rho$表示属性无缺失样本所占比例，$\\tilde{p}_k$表示属性无缺失样本中第$k$类所占比例，$\\tilde{r}_v$表示属性无缺失样本中在属性$a$上取值$a^v$的样本比例。\n\n由此推广信息增益为：\n$$\n\\begin{aligned}\nGain(D, a) &= \\rho \\times Gain(\\tilde{D}, a)\\\\\n&=\\rho \\times (Ent(\\tilde{D}) - \\sum\\limits_{v=1}^V \\tilde{r}_v Ent(\\tilde{D}^v))\n\\end{aligned}\n$$\n其中：\n$$\nEnt(\\tilde{D}) = -\\sum\\limits_{k=1}^C \\tilde{p}_k log_2 \\tilde{p}_k\n$$\n这样解决了最优划分的属性选择问题，在构造子树时，如果样本$x$在属性$a$上的取值已知，那么$x$划分到相应子节点，且权重保持为$w_x$，如果属性$a$未知，则将$s$划分入所有的子节点，且权重调整为$\\tilde{r}_v w_x$。\n\n## 多变量决策树\n叶节点不再针对某个属性，而是针对属性的线性组合进行划分。\n\n# 神经网络\n\n## 感知机\n两层神经元，输入层（没有权重，直接前馈数据）和输出层，输出层是M-P神经元（阈值逻辑单元），感知机只能拟合线性可分的数据，否则其学习过程将变得震荡，难以收敛。\n\n## BP算法\n对于$l$层神经网络,输入$x \\in R^n$，标签$y \\in R^c$，第$i$层权重表示为$w_i \\in R^{O_i \\times I_i}, I_1 = n，O_l = c$，第$i$层偏移表示为$b_i \\in R^{O_i}$，第$i$层激活函数表示为$\\sigma_i$，这一般是个逐元素函数，第$i$层输入即第$i-1$层的输出，表示为$l_{i-1}$，其中$l_0 = x, z_i = w_i l_{i-1} + b_i, l_i = \\sigma_i(z_i)$\n\nloss函数记为$E(l_l, y)$，BP算法每次更新$w_i = w_i - \\eta \\frac{\\partial E}{\\partial w_i}$， $b_i = b_i - \\eta \\frac{\\partial E}{\\partial b_i}$，即让参数像梯度最小的方向前进。\n\n首先定义$E$对$l_l$的偏导为$\\frac{\\partial E}{\\partial l_l} = E'$，这个值由loss函数决定。\n因此\n$$\n\\begin{aligned}\ndE &= E'^Tdl_l\\\\\n&=E'^T(\\sigma_l'(z_l) \\odot (dz_l))\\\\\n&=E'^Tdiag(\\sigma_l'(z_l))da_l\\\\\n\\Rightarrow \\frac{\\partial E}{\\partial z_l} &= diag(\\sigma_l'(z_l))E'\n\\end{aligned}\n$$\n\n这里把$\\frac{\\partial E}{\\partial z_l}$记作$\\delta_l$\n\n因为：\n$$\n\\begin{aligned}\n    da_i &= w_idl_{i-1}\\\\\n    &= w_i(\\sigma_i'(z_{i-1}) \\odot (dz_{i-1}))\\\\\n    &=w_idiag(\\sigma_{i-1}'(z_{i-1}))dz_{i-1}\\\\\n    \\Rightarrow \\frac{\\partial z_i}{\\partial z_{i-1}} &= diag(\\sigma_{i-1}'(z_{i-1}))w_i^T\\\\\n\\end{aligned}\n$$\n\n所以定义:\n$$\n\\begin{aligned}\n    \\delta_i &= \\frac{\\partial E}{\\partial z_i},\\ i=1,2,...,l-1\\\\\n    \\Rightarrow \\delta_{i-1} &= \\frac{\\partial z_i}{\\partial z_{i-1}}\\frac{\\partial E}{\\partial z_i},\\ i=2,...,l\\\\\n    &= diag(\\sigma_{i-1}'(z_{i-1}))w_i^T\\delta_i\\\\\n\\end{aligned}\n$$\n\n现在再来考虑$E$对$w_{l-k}$的导数：\n$$\n\\begin{aligned}\n    dE &= \\frac{\\partial E}{\\partial z_{l-k}}^Tdz_{l-k}\\\\\n    &= \\delta_{l-k}^T(dw_{l-k}l_{l-k-1} + db_{l-k})\\\\\n    &= tr(\\delta_{l-k}^Tdw_{l-k}l_{l-k-1} + \\delta_{l-k}^Tdb_{l-k})\\\\\n    &= tr(l_{l-k-1}\\delta_{l-k}^Tdw_{l-k} + \\delta_{l-k}^Tdb_{l-k})\\\\\n    \\Rightarrow \\frac{\\partial E}{\\partial w_{l-k}} &= \\delta_{l-k}l_{l-k-1}^T\\\\\n    \\Rightarrow \\frac{\\partial E}{\\partial b_{l-k}} &= \\delta_{l-k}\n\\end{aligned}\n$$\n这里的变换属于标量对矩阵求导$d f = tr((\\frac{\\partial f}{\\partial X}) ^ T dX)$，且用到了迹的一个性质：$tr(A B) = tr(B A)$，其中$A$和$B^T$大小相同\n\n全连接层的BP算法看起来很复杂，其实非常简单，只要使用以下几个等式即可求出任一层的权重和偏置的导数：\n$$\n\\begin{aligned}\n    \\delta_l = \\frac{\\partial E}{\\partial z_l} &= diag(\\sigma_l'(z_l))E'\\\\\n    \\delta_{i} = \\frac{\\partial E}{\\partial z_i} &= diag(\\sigma_i'(z_i))w_{i+1}^T\\delta_{i+1},\\ i=1,2,...,l-1\\\\\n    \\frac{\\partial E}{\\partial w_i} &= \\delta_il_{i-1}^T,\\ i=1,2,...,l\\\\\n    \\frac{\\partial E}{\\partial b_i} &= \\delta_i,\\ i=1,2,...,l\n\\end{aligned}\n$$\n\n## RBF（Radial Basis Function，径向基函数）网络\n\nRBF网络是指使用径向基函数作为隐层激活函数的单隐层前馈神经网络$\\phi(x) = \\sum\\limits_{i=1}^q w_i\\rho(x, c_i)$，常用的高斯径向基函数$\\rho(x, c_i) = e^{-\\beta_i||x-c_i||^2}$，其中$c_i,w_i$分别表示第$i$个神经元的中心和权重。\n\n$c_i$可以使用随机采样或者聚类来获得，而其他参数$w_i, \\beta_i$由BP算法等方法来获得。\n\n# SVM\n支持向量机的相关内容可以见我的另一篇文章{% post_link SVM学习笔记 SVM学习笔记 %}，这里不再重新做笔记。\n\n# 贝叶斯分类器\n## 条件风险\n条件风险$R(c_i|x) = \\sum\\limits_{j=1}^N \\lambda_{ij}P(c_j|x)$其中$\\lambda_{ij}$表示将$j$类样本分类为$i$时的损失。\n\n机器学习的任务是寻找一个判定准则$h:x \\rightarrow y$以最小化总体风险$\\min\\limits_{h} R(h)=E_x[R(h(x)|x)]$，即在每个样本上选择$h^\\star(x) = \\mathop{\\arg\\max}\\limits_c R(c|x)$，这样的分类器$h(x)$被称为贝叶斯最优分类器。与之对应的总体风险$R(h^\\star)$被称为贝叶斯风险，$1-R(h^\\star)$是分类器能达到的最好性能，即通过机器学习能产生的模型精度理论上限。\n\n如果$\\lambda_{ij} = \\begin{cases}1&i \\ne j\\\\0&i = j\\end{cases}$，那么条件风险将变成$R(c|x) = 1-P(c|x)$，于是最小化分类错误率的贝叶斯最优分类器变成$h^\\star(x) = \\mathop{\\arg\\max}\\limits_c P(c|x)$即最大化后验概率。\n\n## 生成式与判别式模型\n在使用贝叶斯分类器时，需要获取后验概率$P(c|x)$，但是非常难以直接获取，因此有两种方式：\n\n第一种是直接对$P(c|x)$进行建模，称为判别式方法。\n\n第二种是生成式模型，考虑$P(c|x) = \\frac{P(x, c)}{P(x)}=\\frac{P(c)P(x|c)}{p(x)}$，其中$P(c)$称为类先验概率，$P(x|c)$是样本$x$相对于类别$c$的类条件概率（似然），$P(c)$可以在样本量足够大时用各类样本出现的频率来估计（大数定律），但是$P(x|c)$非常难估计，因为这涉及到关于$x$的所有属性的联合概率，很难直接用样本出现的频率来进行估计。\n\n## 极大似然估计\n再生成式模型中，估计$P(x|c)$的一种策略是首先确定其满足某种确定的概率分布形式，假设$P(x|c)$被参数向量$\\theta_c$唯一确定，因此$P(x|c)$可以被记为$P(x|\\theta_c)$，概率模型的训练过程就是参数估计的过程，参数估计有两种方案：\n\n- 参数虽然未知，但是却是客观存在的固定值（频率主义学派）\n- 参数也是一种未观察到的随机变量，其本身也满足一定的分布，因此需要先假设参数服从一个先验分布，然后基于观测数据来计算参数的后验分布（贝叶斯学派）\n\n极大似然估计属于频率主义学派，将参数当成未知的固定值来处理。首先令$D_c$表示训练集$D$的第$c$类样本集合，并假设这些样本独立同分布，因此其似然可以表示为$P(D_c|\\theta_c) = \\prod\\limits_{x \\in D_c}P(x|\\theta_c)$，极大化似然，就能找到参数$\\theta_c$。\n\n似然的表达式中有连乘，容易造成下溢，因此通常使用对数似然$\\log P(D_c|\\theta_c) = \\sum\\limits_{x \\in D_c} \\log P(x|\\theta_c)$。\n\n像极大似然法这种参数化的方法的准确性严重依赖于对$P(x|\\theta_c)$分布的假设，在实际应用中需要利用应用任务的经验知识，才能得到比较好的分类器。\n\n## 朴素贝叶斯分类器\n在生成式模型中，$P(c|x) = \\frac{P(x, c)}{P(x)}=\\frac{P(c)P(x|c)}{P(x)}$，这里估计$P(x|c)$的困难在于类条件概率$P(x|c)$是$x$所有属性的联合分布，难以从有限的训练样本中估计得到。\n\n朴素贝叶斯分类器采用属性条件独立性假设：对所有已知类别，样本$x$的所有属性相互独立。\n\n因此$P(c|x) = \\frac{P(c)P(x|c)}{P(x)} = \\frac{P(c)}{P(x)} \\prod\\limits_{i=1}^dP(x_i|c)$，其中$d$为样本的属性数量，$x_i$表示样本$x$的第$i$个属性值。\n\n对于所有类别来说，$P(x)$相同，因此基于$\\lambda_{ij} = \\begin{cases}1&i \\ne j\\\\0&i = j\\end{cases}$，$h^\\star(x) = \\mathop{\\arg\\max}\\limits_c P(c|x)$即最大化后验概率的朴素贝叶斯分类器就可以表达为$h_{nb}(x) = \\mathop{\\arg\\max}\\limits_c P(c)\\prod\\limits_{i=1}^dP(x_i|c)$\n\n对于类先验概率，可以从训练集中使用$P(c) = \\frac{|D_c|}{|D|}$估计。\n\n对于离散属性，估计$P(x_i|c)$的方式常用$P(x_i|c) = \\frac{|D_{c,x_i}|}{|D_c|}$，其中$D_{c,x_i}$表示$D_c$中第$i$个属性取值为$x_i$的样本集合。\n\n对于连续属性，则可以使用概率密度函数，假定$P(x_i|c)$服从某种分布，然后对其进行估计。\n\n在离散属性的处理上，有个问题是：如果训练集中某种属性在类别$c$上没有出现，或者类别$c$在训练集上没有出现，则$P(c)\\prod\\limits_{i=1}^dP(x_i|c)$直接就为0了，因此需要进行修正平滑处理。\n\n常用的是拉普拉斯修正，即将$P(c) = \\frac{|D_c|}{|D|}$更改为$P(c) = \\frac{|D_c|+1}{|D|+N}$，其中N表示类别个数, 将$P(x_i|c) = \\frac{|D_{c,x_i}|}{|D_c|}$更改为$P(x_i|c) = \\frac{|D_{c,x_i}| + 1}{|D_c| + N_i}$，其中$N_i$表示第$i$个属性的可取值个数。这样可以避免因训练集样本不充分而导致的概率估计为零的问题。\n\n## 半朴素贝叶斯分类器\n由于属性条件独立性假设很难成立，因此尝试对条件独立性假设进行一定程度的放松。\n\n例如独依赖估计（One-Dependent Estimator，ODE），即每个属性在类别之外最多依赖一个其他属性：$P(c|x)\\propto P(c) \\prod\\limits_{i=1}^dP(x_i|c,pa_i)$，其中$pa_i$为属性$x_i$所依赖的属性，称为$x_i$的父属性。\n\n问题的关键在于如何确定父属性，最直接的办法是假设所有属性都依赖于同一个父属性，称为超父（super-parent）由此形成了SPODE(Super-Parent ODE)方法。\n\n# 集成学习\n构建并结合多个学习器来完成学习任务。一般是先产生一组个体学习器，然后使用某种策略将他们结合。\n\n根据个体学习器的生成方式，集成学习方法大致可以分为两类：\n- 个体学习器之间存在强依赖关系，必须串行生成，例如：Boosting。\n- 个体学习器之间不存在强依赖关系，可以同时并行生成，例如：Bagging和随机森林（Random Forest）\n\n## Boosting\n先训练一个基学习器，然后使用基学习器对训练样本分布进行调整，使得基学习器预测错误的样本在后续得到更多关注，然后进行下一个基学习器的训练，直到学习器数量达到指定数量。\n\nBoosting中最著名的是AdaBoosting，可以理解为加性模型，即使用基学习器的线性组合$H(x) = \\sum\\limits_{t=1}^T\\alpha_t h_t (x)$来最小化指数损失函数$l_{exp}(H|D) = E_{x \\sim D}[e^{-f(x)H(x)}]$，其中$h_i$表示第$i$个基分类器，$f(x)$表示真实函数$f(x) = y$。\n\nAdaBoosting只适用于二分类任务。\n\n## Bagging\n在原始数据中每次有放回的采样$m$个样本，组成一个子训练集，最终得到共$T$个子训练集，用每个子训练集训练一个基学习器，再将所有基学习器进行结合。\n\n假设基学习器的计算复杂度为$O(m)$，Bagging的复杂度大致为$T(O(m) + O(s))$，其中$O(s)$为投票和采样的复杂度，一般非常小，而$T$是个比较小的常数，因此Bagging的训练复杂度和训练一个学习器的复杂程度同阶，非常高效。\n\n## Bagging和Boosting的区别\n\n\n## 随机森林（Random Forest，RF）\n以决策树为基学习器的Bagging集成基础上，引入随机属性选择。在构造决策树过程中，选择最优划分属性时，可以在当前可用属性的随机子集中进行选择。\n\n## 集成学习的结合策略\n- 平均法\n- 投票法\n- 学习法\n\nstacking是学习法的一个例子：先用原始训练集训练出初级学习器，然后将初级学习器的输出作为输入特征，训练一个次级学习器，用于结合初级学习器输出得到最终输出。最好在初级学习器的验证集上对次级学习器进行训练。\n\n## 多样性\n如果用$h_1,h_2,...,h_T$通过加权平均法来集成得到$H$来估计真实函数$f:R^d \\rightarrow R$，则对样本$x$，定义学习器$h_i$的分歧为：$A(h_i|x) = (h(x_i) - H(x))^2$。集成的分歧表示为$\\overline{A}(H|x) = \\sum\\limits_{i=1}^T w_i A(h_i|x)=\\sum\\limits_{i=1}^Tw_i(h_i(x) - H(x))^2$。分歧表示个体学习器在样本$x$上的不一致性。\n\n而集成的泛化误差可以表示为$E=\\overline{E} - \\overline{A}$（$\\overline{E} = \\sum\\limits_{i=1}^Tw_iE_i$，$\\overline{A}=\\sum_{i=1}^Tw_iA_i$，推导过程略过），这说明个体学习器误差越低，多样性越大，集成的效果越好。\n\n增加个体学习器多样性的方法：\n- 数据样本扰动：通常基于采样法，产生不同的数据子集。但是有些学习器对数据样本量的扰动不敏感（如线性学习器，支持向量机，朴素贝叶斯，k近邻，这些学习器称为稳定基学习器）\n- 输入属性扰动：抽取属性子集来训练学习器。不适合属性较少的数据。\n- 输出表示扰动：将样本的类别标记稍作改动，例如随机改变一些标记，或者进行编码调制，例如ECOC。\n- 算法参数扰动：随机设置不同的参数，例如隐层神经元个数、初始连接权值等。\n\n# 聚类\n\n## 原型聚类\n又称基于原型的聚类方法，此类算法假设聚类结构可以通过一组原型来刻画，通常先对原型进行初始化，然后对原型进行迭代更新求解。不同的原型表示、不同的求解方式就会产生不同的算法\n\n### k均值算法\n给定样本集$D=\\{x_1,x_2,...,x_m\\}$，聚类结果$C = \\{C_1, C_2, ...,C_k\\}$，定义平方误差$E = \\sum\\limits_{i=1}^k \\sum\\limits_{x\\in C_i} ||x-\\mu_i||^2_2$，其中$\\mu_i = \\frac{1}{C_i}\\sum\\limits_{x \\in C_i}x$表示簇$C_i$的均值向量。\n\nk均值算法针对平方误差$E$进行优化，使用贪心策略，通过迭代优化方式来进行：\n\n1、初始化$k$个初始均值向量$\\mu_1, \\mu_2, ..., \\mu_k$。\n\n2、计算每个样本到$k$个均值向量的值，将每个样本划入最近的均值向量对应的簇中，得到一个划分。\n\n3、使用划分好的簇计算新的均值向量。\n\n4、如果没有均值向量被大幅更新或者达到了最大迭代次数，那么停止，否则从第2步继续循环。\n\n### 学习向量量化（Learning Vector Quantization，LVQ）\n和k均值算法类似，但是学习向量量化在学习过程中还利用样本的监督信息来辅助聚类：\n\n1、初始化原型向量$\\{p_1, p_2, ..., p_q\\}$\n\n2、随机选取样本$x_j$，计算$x_j$和每个原型向量的距离，并找出距离最小对应的原型向量。\n\n3、如果$x_j$的标签和原型向量的标签相同，则使用$p=p + \\eta (x_j - p)$来对选出的原型向量进行更新，否则使用$p = p - \\eta (x_j - p)$来对选出的原型向量进行更新\n\n4、达到最大迭代轮数或者更新幅度很小则停止更新，否则从第2步继续循环。\n\n其中$\\eta \\in (0, 1)$表示学习速率，在迭代停止之后，对于任意样本$x$，可以将其划分到与其距离最近的原型向量所代表的的簇中。\n\n### 高斯混合聚类（Mixture-of-Gaussian）\n假设数据服从高斯混合分布$p_M(x) = \\sum\\limits_{i=1}^k \\alpha_i p(x|\\mu_i, \\Sigma_i), \\sum\\limits_{i=1}^k \\alpha_i=1$。\n\n令随机变量$z_j,j\\in \\{1,2,...,k\\}$表示样本$x_j$预测类别，其先验概率$p(z_j = i) = \\alpha_i$，则$p_M(z_j = i|x_j) = \\frac{\\alpha_ip(x_i|\\mu_i,\\Sigma_i)}{\\sum\\limits_{l=1}^k \\alpha_lp(x_j|\\mu_l, \\Sigma_l)}$\n\n当高斯混合分布已知时，即可将样本划分成$k$个簇，其标记为$\\mathop{\\arg\\max}\\limits_i p_M(z_j = i|x_j)$\n\n估计这样的高斯混合分布可以使用极大似然法，令对数似然$LL(p_M) = \\sum\\limits_{j=1}^m ln(\\sum\\limits_{i=1}^k \\alpha_ip(x_i|\\mu_i,\\Sigma_i))$，使用EM算法即可求解。\n\n## 密度聚类\n### DBSCAN\n密度聚类算法的代表是DBSCAN算法，使用一组邻域参数$(\\epsilon, MinPts)$来刻画样本分布的紧密程度。\n\n- $\\epsilon$-邻域：$N_\\epsilon(x_j) = \\{x_i | x_i \\in D, dist(x_j,x_i)\\le \\epsilon\\}$，其中$D$表示全部数据集合。\n- 核心对象：满足$|N_\\epsilon(x_j)| \\ge MinPts$的$x_j$称为核心对象，其中$|N_\\epsilon(x_j)|$表示$x_j$的$\\epsilon$-邻域中的样本数量。\n- 密度直达：$x_j$是一个核心对象，且$x_i$在$x_j$的$\\epsilon$-邻域中，则$x_i$由$x_j$密度直达。\n- 密度可达：如果存在样本序列$p_1, p_2, ..., p_n, p_1 = x_j, p_n = x_i$使得$p_{m+1}$由$p_m$密度直达，则称$x_i$由$x_j$密度可达。\n- 密度相连：如果存在$x_k$使得$x_j$和$x_i$均可由$x_k$密度可达，则称$x_i$和$x_j$密度相连。\n\n基于以上概念，DBSCAN将簇定义为由密度可达关系到处的最大密度相连样本集合。\n\nDBSCAN的聚类过程如下：\n\n1、找到所有的核心对象。\n\n2、循环随机取一个还没有访问过的核心对象，将其所有密度相连的样本生成一个簇并标记为已访问，如果没有未访问的核心对象，则停止循环\n\n3、仍未访问的样本被视为噪声样本。\n\n## 层次聚类\n层次聚类方法试图在不同层次上对数据进行划分，形成树形的聚类结构。\n\n### AGNES\n\nAGNES是一种自底向上的层次聚类算法，首先将所有样本单独看做一个簇，然后每次迭代找到距离最近的两个簇进行合并，直到簇数量等于指定数量。\n\n这里的关键是如何定义两个簇的距离，主要有三种方式，使用三种距离计算的AGNES分别被称为：\n- 最大距离：单链接算法\n- 最小距离：全链接算法\n- 平均距离：均链接算法\n\n# 降维与度量学习\n\n## k近邻（k-Nearest Neighbor）学习\nk近邻方法没有训练过程，在给定测试样本时，直接使用训练样本中与其最靠近的$k$个样本，基于这$k$个样本的信息来对测试样本进行预测。\n\nk近邻方法是懒惰学习（lazy learning）的一个代表，而那些在训练阶段就对样本进行学习处理的方法称为急切学习（eager learning）\n\n## 低维嵌入\n大部分时候，观测到的数据是高维数据，但与学习任务密切相关的很可能是高维空间中的一个低维嵌入。\n\n### 多维缩放（Multiple Dimensional Scaling，MDS）\n多维缩放的思路是找到一个低维空间，使得在这个低维空间中的欧氏距离和原始空间中的距离相等。\n\n假如原始空间的维度为$d$，所有数据的距离矩阵$D\\in R^{m \\times m}$，其中$m$为样本数量，$d_{ij}$表示样本$x_i$和$x_j$的距离，降维后的数据$z \\in R^{d'}$，所有样本表示为$Z\\in R^{d' \\times m}$。\n\n令$B = Z^T Z \\in R^{m\\times m}$是降维后的内积矩阵，有$b_{ij} = z_i^T z_j$\n\n则$d_{ij}^2 = ||z_i||^2 + ||z_j||^2 - 2z_i^T z_j = b_{ii} + b_{jj} - 2b_{ij}$\n\n令降维后的样本$Z$被中心化，即$\\sum\\limits_{i=1}^m z_i = \\mathbf{0}$，可得到$\\sum\\limits_{i=1}^m b_{ij} = \\sum\\limits_{j=1}^m b_{ij} = 0$\n\n因此：\n\n$$\n\\sum\\limits_{i=1}^md_{ij}^2 = tr(B) + mb_{jj}\\\\\n\\sum\\limits_{j=1}^md_{ij}^2 = tr(B) + mb_{ii}\\\\\n\\sum\\limits_{i=1}^m\\sum\\limits_{j=1}^md_{ij}^2 = 2m\\ tr(B)\\\\\ntr(B) = \\sum\\limits_{i=1}^m b_{ii}\n$$\n\n则有：\n\n$$\n\\begin{aligned}\n    b_{ij} &= \\frac{b_{ii} + b_{jj} - d_{ij}^2}{2}\\\\\n    &=\\frac{1}{2m}(\\sum\\limits_{i=1}^md_{ij}^2) + \\frac{1}{2m}(\\sum\\limits_{j=1}^md_{ij}^2) - \\frac{1}{2m^2} \\sum\\limits_{i=1}^m\\sum\\limits_{j=1}^md_{ij}^2 - \\frac{d_{ij}^2}{2}\n\\end{aligned}\n$$\n\n这样就能根据原来的距离矩阵求出内积矩阵$B$，再对$B$进行特征分解得到$B=V \\Lambda V^T$，其中$\\Lambda = diag(\\lambda_1, ..., \\lambda_d)$为特征值构成的对角阵，$\\lambda_1 \\ge \\lambda_2\\ge ...\\ge \\lambda_d$，为了降维，我们可以取其中$d'$个非零值，构成对角矩阵$\\Lambda_\\star$与其对应的特征向量矩阵$V_\\star$，则$Z = \\Lambda_\\star ^{\\frac{1}{2}} V_\\star^T \\in R^{d' \\times m}$\n\n## 主成分分析（PCA）\n如果希望使用一个超平面来对数据进行表示，那么可以从两个方面去考虑：\n- 最近重构性：样本点到这个超平面的距离都足够近\n- 最大可分性：样本点在这个超平面上的投影尽可能分开\n\n但是两个方面的考虑最终都会得到PCA的等价推导，即PCA既保证了最近重构性也保证了最大可分性。\n\n假定数据$x_i \\in R^d$已经进行过中心化，即$\\sum_i x_i = \\mathbf{0}$，现在使用一组标准正交基对$x_i$进行投影，得到$z_i = Wx_i, W = \\begin{bmatrix}w_1^T\\\\ w_2^T\\\\ \\vdots\\\\ w_{d'}^T \\end{bmatrix}\\in R^{d' \\times d}, w_i^T w_j = \\begin{cases}1 & i=j\\\\ 0& i\\ne j\\end{cases}$，其中$z_{ij} = w_j^Tx_i$，如果使用$z_i$来还原$x_i$则得到$\\hat{x}_i = \\sum\\limits_{j=1}^{d'}z_{ij}w_j = W^T z_i$。\n\n如果从最近重构性来考虑，我们希望最小化$\\sum\\limits_{i=1}^m ||\\hat{x}_i - x_i||^2_2$，即：\n$$\n\\begin{aligned}\n    &\\min \\sum\\limits_{i=1}^m ||\\hat{x}_i - x_i||^2_2\\\\\n    &=\\min \\sum\\limits_{i=1}^m || \\sum\\limits_{j=1}^{d'}z_{ij}w_j - x_i||^2_2\\\\\n    &=\\min \\sum\\limits_{i=1}^m || W^Tz_i - x_i||^2_2\\\\\n    &=\\min \\sum\\limits_{i=1}^m (z_i^TWW^Tz_i - z_i^TWx_i - x^T_iW^Tz_i + x^T_i x_i)\\\\\n    &=\\min \\sum\\limits_{i=1}^m (z_i^Tz_i - z_i^TWx_i - x^T_iW^Tz_i)\\\\\n    &=\\min \\sum\\limits_{i=1}^m tr(z_i^Tz_i - z_i^TWx_i - x^T_iW^Tz_i)\\\\\n    &=\\min \\sum\\limits_{i=1}^m tr(z_i^Tz_i - 2z_i^TWx_i)\\\\\n    &=\\min \\sum\\limits_{i=1}^m -tr(z_i^T z_i)\\\\\n    &=\\min -tr(Z^T Z),\\ Z = \\begin{bmatrix}z_1 &z_2 & \\cdots &z_m\\end{bmatrix} = WX,\\ X = \\begin{bmatrix}x_1 &x_2 & \\cdots &x_m\\end{bmatrix}\\\\\n    &=\\min -tr(X^TW^TWX)\\\\\n    &=\\min -tr(WXX^TW^T)\\\\\n\\end{aligned}\n$$\n\n因此我们需要解决的问题就是：\n$$\n\\begin{aligned}\n    \\min\\ & -tr(WXX^TW^T)\\\\\n    s.t.\\ & WW^T = I_{d'}\n\\end{aligned}\n$$\n\n另一方面，如果从最大可分性来考虑，我们希望最大化$z_i$之间的方差$\\sum\\limits_{i=1}^m (z_i - \\frac{1}{m}\\sum\\limits_{j=1}^mz_j)^2$\n\n$$\n\\begin{aligned}\n    &\\max \\sum\\limits_{i=1}^m ||z_i - \\frac{1}{m}\\sum\\limits_{j=1}^mz_j||_2^2\\\\\n    &=\\max \\sum\\limits_{i=1}^m ||z_i - \\frac{1}{m}\\sum\\limits_{j=1}^mWx_j||_2^2\\\\\n    &=\\max \\sum\\limits_{i=1}^m ||z_i - \\frac{1}{m}W\\sum\\limits_{j=1}^mx_j||_2^2\\\\\n    &=\\max \\sum\\limits_{i=1}^m ||z_i||_2^2\\\\\n    &=\\max \\sum\\limits_{i=1}^m (z^T_iz_i)\\\\\n    &=\\max\\ tr(Z^TZ)\\\\\n    &=\\max\\ tr(X^TW^TWX)\\\\\n    &=\\min\\ -tr(X^TW^TWX)\\\\\n    &=\\min\\ -tr(WXX^TW^T)\\\\\n\\end{aligned}\n$$\n\n因此我们需要解决的问题就是：\n$$\n\\begin{aligned}\n    \\min\\ & -tr(WXX^TW^T)\\\\\n    s.t.\\ & WW^T = I_{d'}\\\\\n\\end{aligned}\n$$\n\n由此可见，在$\\sum x_i = 0$的情况下，从两个方面得到的结果完全相同。\n\n求解PCA可以使用拉格朗日法，首先得到拉格朗日函数$L(W) = -tr(X^TW^TWX) + \\lambda (WW^T - I_{d'}), \\lambda \\ge 0$\n\n$$\n\\begin{aligned}\n    dL(W) &= -tr(WXX^TdW^T + dWXX^TW^T) + \\lambda tr(dW W^T + WdW^T)\\\\\n    &= -tr(2XX^TW^TdW - 2\\lambda W^TdW)\\\\\n    \\\\\n    \\frac{\\partial dL(W)}{\\partial W} &= 2\\lambda W - 2WXX^T\n\\end{aligned}\n$$\n\n令$\\frac{\\partial dL(W)}{\\partial W} = 0$可得$\\lambda W^T = XX^TW^T$，即求出协方差矩阵$XX^T$的特征向量即可构成$W^T$，在这个过程中，可以舍弃一部分特征向量，只取特征值最大的$d'$个特征向量，即可将数据维度从$d$维缩减到$d'$维。\n\n如果将$X$进行奇异值分解，则有$X=D\\Sigma V^T$，$XX^T = D \\Sigma \\Sigma^T D^T$，其中$D$是$X$的左奇异矩阵，也就是$XX^T$的特征矩阵，如果令$W=D^T$可以得到$Z=D^TX=D^TD\\Sigma V^T = \\Sigma V^T$，因此求出$X^TX$的特征矩阵也可以求出$Z$。\n\n## 核化线性降维\n\n### 核主成分分析（KPCA）\n\nPCA是一种线性降维方式，在降维时假设从高维空间到低维空间的映射是线性的，其线性映射由$W$确定，且有$\\lambda W^T = XX^TW^T = (\\sum\\limits_{i=1}^m x_i x^T_i)W^T$，假设在高维空间$\\phi(x)$中进行PCA，则有$\\lambda W^T = (\\sum\\limits_{i=1}^m \\phi(x_i) \\phi(x_i)^T)W^T$，有如下推导：\n\n$$\n\\begin{aligned}\n    \\lambda W^T &= (\\sum\\limits_{i=1}^m \\phi(x_i) \\phi(x_i)^T)W^T\\\\\n    &= (\\sum\\limits_{i=1}^m \\phi(x_i) \\phi(x_i)^T)W^T\\\\\n    &= \\sum\\limits_{i=1}^m \\phi(x_i)\\phi(x_i)^TW^T\\\\\n    &= \\sum\\limits_{i=1}^m\\phi(x_i)\\alpha_i,\\ \\alpha_i = \\phi(x_i)^TW^T\\\\\n\\end{aligned}\n$$\n\n由于$\\phi$函数无法明确求出，因此引入核函数。\n$$\n\\begin{aligned}\n    \\lambda \\phi(x_j)^TW^T &= \\phi(x_j)^T\\sum\\limits_{i=1}^m\\phi(x_i)\\alpha_i\\\\\n    \\lambda A&=KA\\\\\n\\end{aligned}\n$$\n\n其中$K$是核矩阵$K_{ij} = \\phi(x_i)^T\\phi(x_j)$，$A = \\begin{bmatrix}\\alpha_1, \\alpha_2, ..., \\alpha_m\\end{bmatrix}$\n\n则$z_{ij} = w_j^T \\phi(x_i) = \\sum\\limits_{k=1}^m\\alpha_{k,j}\\phi(x_k)^T\\phi(x_i)=\\sum\\limits_{k=1}^m\\alpha_{k,j}K_{ki}$表示降维之后$x_i$对应向量的第$j$个分量。\n\n其中W可以由$\\phi(X)^T\\phi(X)$的特征矩阵求出，但在计算$z$时，由于每个分量需要求和$\\sum\\limits_{k=1}^m\\alpha_{k,j}$，计算量非常大。\n\n## 流形学习\n流形是指在局部与欧式空间统配的空间，如果数据是嵌入在高维空间中的低维流形，则可以利用其局部与欧式空间同胚的性质，使用局部的欧式距离来计算数据样本之间的距离。\n\n### 等度量映射（Isometric Mapping，Isomap）\n将低维嵌入流形上两点的距离定义为“测地线”距离，即两个样本沿着流形的最短距离，测地线距离的计算可以将邻近点之间进行连接，然后转换计算近邻连接图上两点之间的最短路径问题（Dijkstra算法或者Floyd算法）。得到距离矩阵之后，可以使用多维缩放算法（MDS）来进行降维。\n\n邻近图的构建一般有两种做法，一个是指定最近的$k$个点作为邻近点，这样得到的邻近图称为$k$邻近图，另一个是指定距离阈值$\\epsilon$，小于$\\epsilon$的点被认为是邻近点，这样得到的邻近图称为$\\epsilon$邻近图，两种方式各有优劣。\n\n### 局部线性嵌入（Locally Linear Embedding，LLE）\nIsomap试图保持邻近样本之间的距离，而LLE试图保持邻域内样本的线性关系，假定样本点$x_i$可以通过$x_j,x_k,x_l$线性组合而得到，即$x_i = w_{ij}x_j+w_{ik}x_k+w_{il}x_l$\n\n首先对于每个样本$x_i$，找到其邻近下标集合$Q_i$，然后计算$Q_i$对$x_i$的线性重构系数：\n$$\n\\begin{aligned}\n    \\min\\limits_{w_1, w_2, ..., w_m}\\ &\\sum\\limits_{i=1}^m ||x_i - \\sum\\limits_{j\\in Q_i}w_{ij} x_j||_2^2\\\\\n    s.t.\\ &\\sum\\limits_{j\\in Q_i} w_{ij} = 1\n\\end{aligned}\n$$\n\n求得$w_{ij}$之后，$x_i$对应的低维空间坐标$z_i = \\min\\limits_{z_1, z_2, ..., z_m} \\sum\\limits_{i=1}^m ||z_i - \\sum\\limits_{j\\in Q_i} w_{ij} z_j||_2^2$。\n\n令$Z = \\begin{bmatrix}z_1 & z_2 & \\cdots & z_m\\end{bmatrix} \\in R^{d' \\times m}, W_{ij} = w_{ij}$\n\n则确定$W$之后，$Z$可以通过：\n$$\n\\begin{aligned}\n    \\min\\limits_Z\\ &tr(Z(\\mathbf{I} - W)^T(\\mathbf{I} - W)Z^T)\\\\\n    s.t.\\ &ZZ^T = \\mathbf{I}\n\\end{aligned}\n$$\n来求得，即对$(\\mathbf{I} - W)^T(\\mathbf{I} - W)$进行特征分解，取最小的$d'$个特征值对应的特征向量构成$Z^T$\n\n## 度量学习\n对高维数据的降维主要是希望找到一个合适的低维空间使得此空间中学习能比原始空间性能更好，度量学习的思路是尝试学习出一个距离度量。\n\n对于两个$d$维样本$x_i$，$x_j$，其欧氏距离的平方$||x_i - x_j||^2_2 = dist_{ij,1} + dist_{ij,2} + \\cdots + dist_{ij,d}$，其中$dist_{ij,k}$表示在第$k$维上的距离。\n\n如果假定不同属性的重要性不同，则可以引入权重$w$，$||x_i - x_j||^2_2 = w_1 dist_{ij,1} + w_2 dist_{ij,2} + \\cdots + w_d dist_{ij,d} = (x_i - x_j)^T W (x_i - x_j)$，其中$W = diag(w), w_i \\ge 0$\n\n如果令$W$不再是一个对角矩阵，而是让其等于一个半正定对称矩阵$M$，则可以定义马氏距离$dist_{mah}^2(x_i, x_j) = (x_i - x_j)^TM(x_i - x_j) = ||x_i - x_j||^2_M$，则可以对这个$M$进行学习，得到满足要求的距离表达。\n\n在近邻成分分析（Neighbourhood Component Analysis, NCA）中可以用对$M$进行训练，提高其分类正确率。\n\n又例如根据一些领域知识已知某些样本相似（必连约束集合），另外一些样本不相似（勿连约束及合），则可以对$M$进行训练，使得必连约束集合中的样本距离尽可能小，而勿连约束集合中的样本的尽可能大。\n\n不管以任何方式训练得到的$M$，都可以对M进行特征值分解，然后去掉一部分特征向量，得到降维矩阵，用于数据的降维。","slug":"学习笔记/《机器学习》读书笔记","published":1,"updated":"2020-08-31T06:39:20.761Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckjxpc3t0003f44mq2c3h4nii","content":"<h1 id=\"线性模型\">线性模型</h1>\r\n<h2 id=\"基于均方误差最小化来进行求解的方法称为最小二乘法\">基于均方误差最小化来进行求解的方法称为最小二乘法</h2>\r\n<h2 id=\"用最小二乘法来优化线性回归\">用最小二乘法来优化线性回归</h2>\r\n<p>线性回归的目标是学习函数<span class=\"math inline\">\\(f(X) = Xw\\)</span>，使得<span class=\"math inline\">\\(f(X) \\approx Y\\)</span>，其中<span class=\"math inline\">\\(X=\\begin{bmatrix}x_1 &amp; 1\\\\ x_2 &amp; 1\\\\ \\vdots &amp; \\vdots \\\\ x_m &amp; 1 \\end{bmatrix}\\)</span>，<span class=\"math inline\">\\(Y=\\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_m\\end{bmatrix}\\)</span>，<span class=\"math inline\">\\(w \\in R^{(n+1) \\times 1}\\)</span>，<span class=\"math inline\">\\(n\\)</span>是数据特征维数。</p>\r\n<p>如果用均方误差作为损失函数<span class=\"math inline\">\\(L = (Xw-Y)^T (Xw-Y)\\)</span>，那么问题可以描述为</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n&amp;\\arg \\min_{w} (Xw-Y)^T (Xw-Y)\\\\\r\n\\end{aligned}\r\n\\]</span> 直接求导： <span class=\"math display\">\\[\r\n\\frac{\\partial L}{\\partial w} = \\frac{\\partial (w^T X^T X w - Y^TXw -XwY - Y^TY)}{\\partial w} = 2X^T(Xw-Y)\\\\\r\n\\]</span> 令： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n&amp;\\frac{\\partial L}{\\partial w} = 0\\\\\r\n&amp;\\Rightarrow w = (X^TX)^{-1}X^TY\r\n\\end{aligned}\r\n\\]</span> 即得到使用均方误差的线性回归问题的解。</p>\r\n<h2 id=\"对数几率回归逻辑回归-logistic-regression\">对数几率回归（逻辑回归， Logistic Regression）</h2>\r\n<p>对数几率回归即用线性回归去拟合对数几率<span class=\"math inline\">\\(ln\\frac{y}{1-y} = w^T x + b\\)</span></p>\r\n<p>对数几率回归也等价于<span class=\"math inline\">\\(y=\\frac{1}{1+e^{-z}}, z = w^T x + b\\)</span></p>\r\n<p>若将<span class=\"math inline\">\\(y\\)</span>视为后验概率，则<span class=\"math inline\">\\(ln \\frac{P(y=1|x)}{P(y=0|x)} = z, z=w^Tx + b\\)</span></p>\r\n<p>显然有<span class=\"math inline\">\\(P(y=1|x) = \\frac{e^z}{1 + e^z}, P(y=0|x)=\\frac{1}{1 + e^z}\\)</span></p>\r\n<p>使用极大似然法求解对数几率回归：<span class=\"math inline\">\\(\\mathop{\\arg\\max}\\limits_{w, b}\\prod\\limits_{i=1}^m P(y=y_i| x_i) \\Rightarrow \\mathop{\\arg\\max}\\limits_{w, b}\\sum\\limits_{i=1}^m ln(P(y=y_i|x_i))\\)</span></p>\r\n<p>令<span class=\"math inline\">\\(\\beta = \\begin{bmatrix}w ^ T&amp; b\\end{bmatrix} ^ T \\in R^{n+1}, Y=\\begin{bmatrix}y_1&amp;y_2&amp;\\dots&amp;y_m\\end{bmatrix} ^ T, X=\\begin{bmatrix} x_1&amp;x_2&amp;\\dots&amp;x_m\\\\1&amp;1&amp;\\dots&amp;1\\end{bmatrix},x_i \\in R^n, X \\in R^{(n+1) \\times m}\\)</span>，其中<span class=\"math inline\">\\(m\\)</span>是数据量。</p>\r\n<p>使用极大似然法求解对数几率回归可以重写为： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n&amp;\\mathop{\\arg\\max}\\limits_{\\beta} l(Z)\\\\\r\n&amp;Z = X^T \\beta\\\\\r\n&amp;l(Z) = Y^Tln\\frac{e^Z}{\\mathbf{1} + e^Z} + (\\mathbf{1}-Y)^Tln\\frac{\\mathbf{1}}{\\mathbf{1}+e^Z}\\\\\r\n&amp;=Y^TZ - ln(\\mathbf{1}+e^Z)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>使用牛顿法，第<span class=\"math inline\">\\(t\\)</span>次更新为<span class=\"math inline\">\\(\\beta^{t+1} \\leftarrow \\beta ^ t - (\\triangledown_2l)^{-1}\\frac{\\partial l}{\\partial \\beta}\\)</span></p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\ndl &amp;= Y^TdZ - \\mathbf{1}^T\\frac{e^Z}{\\mathbf{1}+e^Z} \\odot dZ\\\\\r\n&amp;=Y^TdZ -\\mathbf{1}^T \\hat{P}_1 \\odot dZ, \\hat{P} = \\begin{bmatrix} P(y=1|x_1) &amp; P(y=1|x_2)&amp; \\dots &amp; P(y=1|x_m)\\end{bmatrix}^T\\\\\r\n&amp;=Y^TX^Td\\beta - \\mathbf{1}^T \\hat{P}_1 \\odot (X^Td\\beta)\\\\\r\n&amp;=Y^TX^Td\\beta - (\\mathbf{1} \\odot \\hat{P}_1)^TX^Td\\beta\\\\\r\n&amp;=(Y^T-\\hat{P}_1^T)X^Td\\beta\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>所以<span class=\"math inline\">\\(\\frac{\\partial l}{\\partial \\beta} = X(Y-\\hat{P}_1)\\)</span></p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    d(\\frac{\\partial l}{\\partial \\beta}) &amp;= d(X(Y-\\hat{P}_1))\\\\\r\n    &amp;=Xd\\hat{P}_1\\\\\r\n    &amp;=Xd\\frac{e^Z}{\\mathbf{1}+e^Z}\\\\\r\n    &amp;=X(\\frac{1}{1+e^Z}\\odot\\frac{e^Z}{1+e^Z} \\odot dZ)\\\\\r\n    &amp;=X(\\hat{P}_0 \\odot \\hat{P}_1 \\odot (X^Td\\beta))\\\\\r\n    &amp;=X diag(\\hat{P}_0) diag(\\hat{P}_1) X^Td\\beta,\\ diag(\\hat{P}_0) = \\begin{bmatrix}\r\n        P(y=0|x_1)&amp;\\cdots&amp;0\\\\\r\n        \\vdots&amp;\\ddots&amp;\\vdots\\\\\r\n        0&amp;\\cdots&amp;P(y=0|x_m)\r\n    \\end{bmatrix}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>所以<span class=\"math inline\">\\(\\frac{\\partial^2 l}{\\partial \\beta \\partial \\beta^T} = \\frac{\\partial^2 l}{\\partial \\beta^T \\partial \\beta} = Xdiag(\\hat{P}_1) diag(\\hat{P}_0)X^T\\)</span></p>\r\n<p>即如果用牛顿法来求解极大似然对数几率回归，第<span class=\"math inline\">\\(t\\)</span>次更新为</p>\r\n<p><span class=\"math display\">\\[\r\n\\beta^{t+1} \\leftarrow \\beta ^ t - (Xdiag(\\hat{P}_1) diag(\\hat{P}_0)X^T)^{-1} X(Y-\\hat{P}_1)\r\n\\]</span></p>\r\n<h2 id=\"线性判别分析\">线性判别分析</h2>\r\n<p>线性判别分析也叫“Fisher判别分析”，其思想非常朴素：试图找到一个低维空间（直线），可以表示为<span class=\"math inline\">\\(y=wx\\)</span>(这个表达式中的<span class=\"math inline\">\\(y\\)</span>表示<span class=\"math inline\">\\(x\\)</span>投影到这个空间（直线）后和原点的距离)，使得当样本投影到该直线时不同类别样本的距离尽可能远，而相同类别的数据尽可能接近，预测时按照样本的投影位置对其进行分类。</p>\r\n<p>以两类数据<span class=\"math inline\">\\(x_1, x_2\\)</span>为例，设<span class=\"math inline\">\\(\\mu_1,\\mu_2,\\Sigma_1,\\Sigma_2\\)</span>分别表示两类数据的均值和方差，则投影之后的均值和方差为<span class=\"math inline\">\\(w\\mu_1,w\\mu_2,w^T\\Sigma_1w,w^T\\Sigma_2w\\)</span>，因为直线是一维空间，所以这些均为实数，投影之后的类内距离可以用方差来衡量，这里使用<span class=\"math inline\">\\(w^T\\Sigma_1w + w^T\\Sigma_2w\\)</span>来度量投影之后的类内距离，而类间距离可以写成<span class=\"math inline\">\\(||w\\mu_2 - w\\mu_1||_2^2\\)</span>，同时考虑两种距离，给出希望最大化的目标函数如下。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\nJ &amp;= \\frac{||w^T\\mu_2 - w^T\\mu_1||_2^2}{w^T\\Sigma_1w + w^T\\Sigma_2w}\\\\\r\n&amp;= \\frac{w^T(\\mu_2 - \\mu_1)(\\mu_2 - \\mu_1)^Tw}{w^T(\\Sigma_1 + \\Sigma_2)w}\r\n\\end{aligned}\r\n\\]</span> 定义类内散度矩阵<span class=\"math inline\">\\(S_w = \\Sigma_1 + \\Sigma_2\\)</span>，类间散度矩阵<span class=\"math inline\">\\(S_b = (\\mu_2 - \\mu_1)(\\mu_2 - \\mu_1)^T\\)</span>，上面的优化目标可以简写为如下。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    J = \\frac{w^TS_bw}{w^TS_ww}\r\n\\end{aligned}\r\n\\]</span> 这个优化目标又称为<span class=\"math inline\">\\(S_b\\)</span>和<span class=\"math inline\">\\(S_w\\)</span>的广义瑞利商，注意到分子分母中都有<span class=\"math inline\">\\(w\\)</span>的二次项，因此和<span class=\"math inline\">\\(w\\)</span>大小无关，只和w方向有关，所以优化问题可以写成下式。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n\\min_w&amp; \\quad-w^TS_bw\\\\\r\ns.t.&amp; \\quad w^TS_ww = 1\r\n\\end{aligned}\r\n\\]</span> 用拉格朗日乘子法进行优化，求解<span class=\"math inline\">\\(S_bw = \\lambda S_ww\\)</span>，因<span class=\"math inline\">\\(S_bw\\)</span>方向和<span class=\"math inline\">\\(\\mu_2 - \\mu_1\\)</span>相同，因此令<span class=\"math inline\">\\(S_bw = \\lambda(\\mu_2 - \\mu_1)\\)</span>，代入求解，可以得到<span class=\"math inline\">\\(w = S_w^{-1}(\\mu_2 - \\mu_1)\\)</span>。</p>\r\n<p>将数据进行降维，使得类内散度最小，类间散度最大，本质上是有监督的降维。</p>\r\n<h2 id=\"多分类\">多分类</h2>\r\n<p>可以将多分类问题拆解为二分类问题，拆解策略有三种：一对一（OvO）、一对其余（OvR）、多对多（MvM）</p>\r\n<p>在MvM中，最常用的是纠错输出码（Error Correcting Output Codes，ECOC）,有<span class=\"math inline\">\\(C_1C_2C_3...C_n\\)</span>共<span class=\"math inline\">\\(n\\)</span>个类别，每个样本属于其中的一种，训练m个二分类器<span class=\"math inline\">\\(f_1, f_2, ..., f_m\\)</span>，每个分类器将一些类作为正类，另一些类作为负类，这样对于某个类别的样本，理想情况是<span class=\"math inline\">\\(m\\)</span>个分类器对其进行预测的输出组成的0,1串，构成一种长度为<span class=\"math inline\">\\(m\\)</span>的固定的类别组合串，<span class=\"math inline\">\\(n\\)</span>个类就有<span class=\"math inline\">\\(n\\)</span>种组合，但在预测时，对一个样本预测得到的输出串，可能不在<span class=\"math inline\">\\(n\\)</span>个类的<span class=\"math inline\">\\(n\\)</span>种组合中，这时，计算预测输出串和每个类别组合串的距离（海明距离或者欧式距离），将样本判定为距离最小的那个类别组合串对应的类别。</p>\r\n<h2 id=\"类别不平衡\">类别不平衡</h2>\r\n<p>解决办法主要有三种： - 再缩放（再平衡），根据样本数量移动判定阈值或者缩放预测概率。 - 欠采样，将样本量过多的类别进行采样，减少该类别的样本数量，再拿去训练，但是这个方法容易丢失数据中的信息，最好是分成多个模型，每个模型使用该类别的一部分数据。 - 过采样，将样本量过少的类别样本进行重复，然后训练，但是这个方法容易严重过拟合，一个办法是用两个该类别样本进行插值，生成新的该类别样本。</p>\r\n<h1 id=\"决策树\">决策树</h1>\r\n<h2 id=\"信息熵\">信息熵</h2>\r\n<p>样本集合<span class=\"math inline\">\\(D\\)</span>中第<span class=\"math inline\">\\(k\\)</span>类样本所占比例为<span class=\"math inline\">\\(p_k\\)</span>，则信息熵定义为<span class=\"math inline\">\\(Ent(D) = -\\sum\\limits_{k=1}^C p_k\\log_2p_k\\)</span>，其中<span class=\"math inline\">\\(C\\)</span>为类别个数。</p>\r\n<h2 id=\"信息熵增益\">信息熵增益</h2>\r\n<p>假设离散属性<span class=\"math inline\">\\(a\\)</span>有<span class=\"math inline\">\\(v\\)</span>个取值：<span class=\"math inline\">\\(a_1, a_2, ..., a_v\\)</span>，可以将当前数据集合分成<span class=\"math inline\">\\(V\\)</span>个子集：<span class=\"math inline\">\\(D_1, D_2, ..., D^V\\)</span>，那么信息熵增益定义为<span class=\"math inline\">\\(Gain(D,a)=Ent(D)-\\sum\\limits_{v=1}^V\\frac{|D^v|}{|D|}Ent(D^v)\\)</span></p>\r\n<p>决策树构造过程，即每次选择一个信息熵增益最大的属性<span class=\"math inline\">\\(a\\)</span>，将数据划分为<span class=\"math inline\">\\(V\\)</span>个子集，然后在每个子集中进行同样的操作（不能选择已经使用过的属性），若无属性可用或者当前子集类别相同，则当前子集标记为类别数量最多的类别，停止继续划分。</p>\r\n<h2 id=\"增益率\">增益率</h2>\r\n<p>信息熵增益的定义导致其对数量较多的<span class=\"math inline\">\\(D^v\\)</span>更加敏感，因此又提出了增益率的概念：<span class=\"math inline\">\\(Gain_ratio(D,a)=\\frac{Gain(D,a)}{IV(a)}\\)</span>，其中，<span class=\"math inline\">\\(IV(a)=-\\sum\\limits_{v=1}^V \\frac{|D^v|}{|D|} \\log_2\\frac{|D^v|}{|D|}\\)</span>，称为属性<span class=\"math inline\">\\(a\\)</span>的固有值。</p>\r\n<h2 id=\"基尼指数\">基尼指数</h2>\r\n<p>基尼值定义为<span class=\"math inline\">\\(Gini(D) = \\sum\\limits_{k=1}^C\\sum\\limits_{k&#39; \\ne k}p_k p_{k&#39;} = 1-\\sum\\limits_{k=1}^Cp_k^2\\)</span>，其反映了在<span class=\"math inline\">\\(D\\)</span>中随机抽取两个样本，属于同一类别的概率。 和信息熵增益类似，定义基尼指数为<span class=\"math inline\">\\(Gini_index(D, a) = \\sum\\limits_{v=1}^V\\frac{|D^v|}{|D|}Gini(D^v)\\)</span>，构造决策树时的最优划分属性选择也可以使用能够使基尼指数最小的属性。</p>\r\n<h1 id=\"决策树的剪枝\">决策树的剪枝</h1>\r\n<h2 id=\"预剪枝\">预剪枝</h2>\r\n<p>在生成决策树的过程中，利用验证集验证当前划分是否能提升泛化效果，如果不能，则停止继续划分，直接将当前样本集作为决策树叶节点，并指定数量最多的类别做为叶节点类别。预剪枝训练时间短，但是由于其是贪心策略，容易引起欠拟合。</p>\r\n<h2 id=\"后剪枝\">后剪枝</h2>\r\n<p>在决策树生成完成之后，自底向上的对非叶节点进行考察，观察将该节点直接指定为叶节点是否能提升验证集泛化效果，如果能则将此节点的子树一起合并为一个叶节点。后剪枝的训练时间比预剪枝长很多，但是其欠拟合风险小，而且泛化性能往往优于预剪枝。</p>\r\n<h1 id=\"包含连续值的决策树\">包含连续值的决策树</h1>\r\n<p>如果连续属性<span class=\"math inline\">\\(a\\)</span>在<span class=\"math inline\">\\(D\\)</span>中出现<span class=\"math inline\">\\(n\\)</span>个取值，则将其从小到大排序为<span class=\"math inline\">\\(\\begin{bmatrix}a_1, a_2, ... a_n\\end{bmatrix}\\)</span>，这样产生<span class=\"math inline\">\\(n-1\\)</span>个离散值<span class=\"math inline\">\\(T_a = \\{\\frac{a_i + a_{i+1}}{2}|1 \\le i \\le n-1\\}\\)</span> 则<span class=\"math inline\">\\(Gain(D,a)= \\max\\limits_{t \\in T_a}Gain(D, a, t)\\)</span>，其中<span class=\"math inline\">\\(Gain(D, a, t)\\)</span>表示将<span class=\"math inline\">\\(a\\)</span>属性使用<span class=\"math inline\">\\(t\\)</span>划分为两部分，这样，连续值的决策树构造和离散值类似了，不过连续值属性在构造决策树的过程中可以使用多次。</p>\r\n<h1 id=\"属性缺失的处理\">属性缺失的处理</h1>\r\n<p>令<span class=\"math inline\">\\(\\tilde{D}\\)</span>是所有没有缺失属性a的样本集合，对于样本<span class=\"math inline\">\\(x\\)</span>，有样本权重<span class=\"math inline\">\\(w_x\\)</span>，定义如下参数。 <span class=\"math display\">\\[\r\n\\rho = \\frac{\\sum_{x\\in \\tilde{D}} w_x}{\\sum_{x\\in D}w_x}\\\\\r\n\\tilde{p}_k = \\frac{\\sum_{x\\in \\tilde{D}_k w_x}}{\\sum_{x\\in \\tilde{D}}w_x}, (1\\le k \\le C)\\\\\r\n\\tilde{r}_v = \\frac{\\sum_{x\\in \\tilde{D}^v}w_x}{\\sum_{x \\in \\tilde{D}} w_x}, (1 \\le v \\le V)\r\n\\]</span> 显然，<span class=\"math inline\">\\(\\rho\\)</span>表示属性无缺失样本所占比例，<span class=\"math inline\">\\(\\tilde{p}_k\\)</span>表示属性无缺失样本中第<span class=\"math inline\">\\(k\\)</span>类所占比例，<span class=\"math inline\">\\(\\tilde{r}_v\\)</span>表示属性无缺失样本中在属性<span class=\"math inline\">\\(a\\)</span>上取值<span class=\"math inline\">\\(a^v\\)</span>的样本比例。</p>\r\n<p>由此推广信息增益为： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\nGain(D, a) &amp;= \\rho \\times Gain(\\tilde{D}, a)\\\\\r\n&amp;=\\rho \\times (Ent(\\tilde{D}) - \\sum\\limits_{v=1}^V \\tilde{r}_v Ent(\\tilde{D}^v))\r\n\\end{aligned}\r\n\\]</span> 其中： <span class=\"math display\">\\[\r\nEnt(\\tilde{D}) = -\\sum\\limits_{k=1}^C \\tilde{p}_k log_2 \\tilde{p}_k\r\n\\]</span> 这样解决了最优划分的属性选择问题，在构造子树时，如果样本<span class=\"math inline\">\\(x\\)</span>在属性<span class=\"math inline\">\\(a\\)</span>上的取值已知，那么<span class=\"math inline\">\\(x\\)</span>划分到相应子节点，且权重保持为<span class=\"math inline\">\\(w_x\\)</span>，如果属性<span class=\"math inline\">\\(a\\)</span>未知，则将<span class=\"math inline\">\\(s\\)</span>划分入所有的子节点，且权重调整为<span class=\"math inline\">\\(\\tilde{r}_v w_x\\)</span>。</p>\r\n<h2 id=\"多变量决策树\">多变量决策树</h2>\r\n<p>叶节点不再针对某个属性，而是针对属性的线性组合进行划分。</p>\r\n<h1 id=\"神经网络\">神经网络</h1>\r\n<h2 id=\"感知机\">感知机</h2>\r\n<p>两层神经元，输入层（没有权重，直接前馈数据）和输出层，输出层是M-P神经元（阈值逻辑单元），感知机只能拟合线性可分的数据，否则其学习过程将变得震荡，难以收敛。</p>\r\n<h2 id=\"bp算法\">BP算法</h2>\r\n<p>对于<span class=\"math inline\">\\(l\\)</span>层神经网络,输入<span class=\"math inline\">\\(x \\in R^n\\)</span>，标签<span class=\"math inline\">\\(y \\in R^c\\)</span>，第<span class=\"math inline\">\\(i\\)</span>层权重表示为<span class=\"math inline\">\\(w_i \\in R^{O_i \\times I_i}, I_1 = n，O_l = c\\)</span>，第<span class=\"math inline\">\\(i\\)</span>层偏移表示为<span class=\"math inline\">\\(b_i \\in R^{O_i}\\)</span>，第<span class=\"math inline\">\\(i\\)</span>层激活函数表示为<span class=\"math inline\">\\(\\sigma_i\\)</span>，这一般是个逐元素函数，第<span class=\"math inline\">\\(i\\)</span>层输入即第<span class=\"math inline\">\\(i-1\\)</span>层的输出，表示为<span class=\"math inline\">\\(l_{i-1}\\)</span>，其中<span class=\"math inline\">\\(l_0 = x, z_i = w_i l_{i-1} + b_i, l_i = \\sigma_i(z_i)\\)</span></p>\r\n<p>loss函数记为<span class=\"math inline\">\\(E(l_l, y)\\)</span>，BP算法每次更新<span class=\"math inline\">\\(w_i = w_i - \\eta \\frac{\\partial E}{\\partial w_i}\\)</span>， <span class=\"math inline\">\\(b_i = b_i - \\eta \\frac{\\partial E}{\\partial b_i}\\)</span>，即让参数像梯度最小的方向前进。</p>\r\n<p>首先定义<span class=\"math inline\">\\(E\\)</span>对<span class=\"math inline\">\\(l_l\\)</span>的偏导为<span class=\"math inline\">\\(\\frac{\\partial E}{\\partial l_l} = E&#39;\\)</span>，这个值由loss函数决定。 因此 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\ndE &amp;= E&#39;^Tdl_l\\\\\r\n&amp;=E&#39;^T(\\sigma_l&#39;(z_l) \\odot (dz_l))\\\\\r\n&amp;=E&#39;^Tdiag(\\sigma_l&#39;(z_l))da_l\\\\\r\n\\Rightarrow \\frac{\\partial E}{\\partial z_l} &amp;= diag(\\sigma_l&#39;(z_l))E&#39;\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>这里把<span class=\"math inline\">\\(\\frac{\\partial E}{\\partial z_l}\\)</span>记作<span class=\"math inline\">\\(\\delta_l\\)</span></p>\r\n<p>因为： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    da_i &amp;= w_idl_{i-1}\\\\\r\n    &amp;= w_i(\\sigma_i&#39;(z_{i-1}) \\odot (dz_{i-1}))\\\\\r\n    &amp;=w_idiag(\\sigma_{i-1}&#39;(z_{i-1}))dz_{i-1}\\\\\r\n    \\Rightarrow \\frac{\\partial z_i}{\\partial z_{i-1}} &amp;= diag(\\sigma_{i-1}&#39;(z_{i-1}))w_i^T\\\\\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>所以定义: <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\delta_i &amp;= \\frac{\\partial E}{\\partial z_i},\\ i=1,2,...,l-1\\\\\r\n    \\Rightarrow \\delta_{i-1} &amp;= \\frac{\\partial z_i}{\\partial z_{i-1}}\\frac{\\partial E}{\\partial z_i},\\ i=2,...,l\\\\\r\n    &amp;= diag(\\sigma_{i-1}&#39;(z_{i-1}))w_i^T\\delta_i\\\\\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>现在再来考虑<span class=\"math inline\">\\(E\\)</span>对<span class=\"math inline\">\\(w_{l-k}\\)</span>的导数： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    dE &amp;= \\frac{\\partial E}{\\partial z_{l-k}}^Tdz_{l-k}\\\\\r\n    &amp;= \\delta_{l-k}^T(dw_{l-k}l_{l-k-1} + db_{l-k})\\\\\r\n    &amp;= tr(\\delta_{l-k}^Tdw_{l-k}l_{l-k-1} + \\delta_{l-k}^Tdb_{l-k})\\\\\r\n    &amp;= tr(l_{l-k-1}\\delta_{l-k}^Tdw_{l-k} + \\delta_{l-k}^Tdb_{l-k})\\\\\r\n    \\Rightarrow \\frac{\\partial E}{\\partial w_{l-k}} &amp;= \\delta_{l-k}l_{l-k-1}^T\\\\\r\n    \\Rightarrow \\frac{\\partial E}{\\partial b_{l-k}} &amp;= \\delta_{l-k}\r\n\\end{aligned}\r\n\\]</span> 这里的变换属于标量对矩阵求导<span class=\"math inline\">\\(d f = tr((\\frac{\\partial f}{\\partial X}) ^ T dX)\\)</span>，且用到了迹的一个性质：<span class=\"math inline\">\\(tr(A B) = tr(B A)\\)</span>，其中<span class=\"math inline\">\\(A\\)</span>和<span class=\"math inline\">\\(B^T\\)</span>大小相同</p>\r\n<p>全连接层的BP算法看起来很复杂，其实非常简单，只要使用以下几个等式即可求出任一层的权重和偏置的导数： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\delta_l = \\frac{\\partial E}{\\partial z_l} &amp;= diag(\\sigma_l&#39;(z_l))E&#39;\\\\\r\n    \\delta_{i} = \\frac{\\partial E}{\\partial z_i} &amp;= diag(\\sigma_i&#39;(z_i))w_{i+1}^T\\delta_{i+1},\\ i=1,2,...,l-1\\\\\r\n    \\frac{\\partial E}{\\partial w_i} &amp;= \\delta_il_{i-1}^T,\\ i=1,2,...,l\\\\\r\n    \\frac{\\partial E}{\\partial b_i} &amp;= \\delta_i,\\ i=1,2,...,l\r\n\\end{aligned}\r\n\\]</span></p>\r\n<h2 id=\"rbfradial-basis-function径向基函数网络\">RBF（Radial Basis Function，径向基函数）网络</h2>\r\n<p>RBF网络是指使用径向基函数作为隐层激活函数的单隐层前馈神经网络<span class=\"math inline\">\\(\\phi(x) = \\sum\\limits_{i=1}^q w_i\\rho(x, c_i)\\)</span>，常用的高斯径向基函数<span class=\"math inline\">\\(\\rho(x, c_i) = e^{-\\beta_i||x-c_i||^2}\\)</span>，其中<span class=\"math inline\">\\(c_i,w_i\\)</span>分别表示第<span class=\"math inline\">\\(i\\)</span>个神经元的中心和权重。</p>\r\n<p><span class=\"math inline\">\\(c_i\\)</span>可以使用随机采样或者聚类来获得，而其他参数<span class=\"math inline\">\\(w_i, \\beta_i\\)</span>由BP算法等方法来获得。</p>\r\n<h1 id=\"svm\">SVM</h1>\r\n<p>支持向量机的相关内容可以见我的另一篇文章<a href=\"#\">Post not found: SVM学习笔记 SVM学习笔记</a>，这里不再重新做笔记。</p>\r\n<h1 id=\"贝叶斯分类器\">贝叶斯分类器</h1>\r\n<h2 id=\"条件风险\">条件风险</h2>\r\n<p>条件风险<span class=\"math inline\">\\(R(c_i|x) = \\sum\\limits_{j=1}^N \\lambda_{ij}P(c_j|x)\\)</span>其中<span class=\"math inline\">\\(\\lambda_{ij}\\)</span>表示将<span class=\"math inline\">\\(j\\)</span>类样本分类为<span class=\"math inline\">\\(i\\)</span>时的损失。</p>\r\n<p>机器学习的任务是寻找一个判定准则<span class=\"math inline\">\\(h:x \\rightarrow y\\)</span>以最小化总体风险<span class=\"math inline\">\\(\\min\\limits_{h} R(h)=E_x[R(h(x)|x)]\\)</span>，即在每个样本上选择<span class=\"math inline\">\\(h^\\star(x) = \\mathop{\\arg\\max}\\limits_c R(c|x)\\)</span>，这样的分类器<span class=\"math inline\">\\(h(x)\\)</span>被称为贝叶斯最优分类器。与之对应的总体风险<span class=\"math inline\">\\(R(h^\\star)\\)</span>被称为贝叶斯风险，<span class=\"math inline\">\\(1-R(h^\\star)\\)</span>是分类器能达到的最好性能，即通过机器学习能产生的模型精度理论上限。</p>\r\n<p>如果<span class=\"math inline\">\\(\\lambda_{ij} = \\begin{cases}1&amp;i \\ne j\\\\0&amp;i = j\\end{cases}\\)</span>，那么条件风险将变成<span class=\"math inline\">\\(R(c|x) = 1-P(c|x)\\)</span>，于是最小化分类错误率的贝叶斯最优分类器变成<span class=\"math inline\">\\(h^\\star(x) = \\mathop{\\arg\\max}\\limits_c P(c|x)\\)</span>即最大化后验概率。</p>\r\n<h2 id=\"生成式与判别式模型\">生成式与判别式模型</h2>\r\n<p>在使用贝叶斯分类器时，需要获取后验概率<span class=\"math inline\">\\(P(c|x)\\)</span>，但是非常难以直接获取，因此有两种方式：</p>\r\n<p>第一种是直接对<span class=\"math inline\">\\(P(c|x)\\)</span>进行建模，称为判别式方法。</p>\r\n<p>第二种是生成式模型，考虑<span class=\"math inline\">\\(P(c|x) = \\frac{P(x, c)}{P(x)}=\\frac{P(c)P(x|c)}{p(x)}\\)</span>，其中<span class=\"math inline\">\\(P(c)\\)</span>称为类先验概率，<span class=\"math inline\">\\(P(x|c)\\)</span>是样本<span class=\"math inline\">\\(x\\)</span>相对于类别<span class=\"math inline\">\\(c\\)</span>的类条件概率（似然），<span class=\"math inline\">\\(P(c)\\)</span>可以在样本量足够大时用各类样本出现的频率来估计（大数定律），但是<span class=\"math inline\">\\(P(x|c)\\)</span>非常难估计，因为这涉及到关于<span class=\"math inline\">\\(x\\)</span>的所有属性的联合概率，很难直接用样本出现的频率来进行估计。</p>\r\n<h2 id=\"极大似然估计\">极大似然估计</h2>\r\n<p>再生成式模型中，估计<span class=\"math inline\">\\(P(x|c)\\)</span>的一种策略是首先确定其满足某种确定的概率分布形式，假设<span class=\"math inline\">\\(P(x|c)\\)</span>被参数向量<span class=\"math inline\">\\(\\theta_c\\)</span>唯一确定，因此<span class=\"math inline\">\\(P(x|c)\\)</span>可以被记为<span class=\"math inline\">\\(P(x|\\theta_c)\\)</span>，概率模型的训练过程就是参数估计的过程，参数估计有两种方案：</p>\r\n<ul>\r\n<li>参数虽然未知，但是却是客观存在的固定值（频率主义学派）</li>\r\n<li>参数也是一种未观察到的随机变量，其本身也满足一定的分布，因此需要先假设参数服从一个先验分布，然后基于观测数据来计算参数的后验分布（贝叶斯学派）</li>\r\n</ul>\r\n<p>极大似然估计属于频率主义学派，将参数当成未知的固定值来处理。首先令<span class=\"math inline\">\\(D_c\\)</span>表示训练集<span class=\"math inline\">\\(D\\)</span>的第<span class=\"math inline\">\\(c\\)</span>类样本集合，并假设这些样本独立同分布，因此其似然可以表示为<span class=\"math inline\">\\(P(D_c|\\theta_c) = \\prod\\limits_{x \\in D_c}P(x|\\theta_c)\\)</span>，极大化似然，就能找到参数<span class=\"math inline\">\\(\\theta_c\\)</span>。</p>\r\n<p>似然的表达式中有连乘，容易造成下溢，因此通常使用对数似然<span class=\"math inline\">\\(\\log P(D_c|\\theta_c) = \\sum\\limits_{x \\in D_c} \\log P(x|\\theta_c)\\)</span>。</p>\r\n<p>像极大似然法这种参数化的方法的准确性严重依赖于对<span class=\"math inline\">\\(P(x|\\theta_c)\\)</span>分布的假设，在实际应用中需要利用应用任务的经验知识，才能得到比较好的分类器。</p>\r\n<h2 id=\"朴素贝叶斯分类器\">朴素贝叶斯分类器</h2>\r\n<p>在生成式模型中，<span class=\"math inline\">\\(P(c|x) = \\frac{P(x, c)}{P(x)}=\\frac{P(c)P(x|c)}{P(x)}\\)</span>，这里估计<span class=\"math inline\">\\(P(x|c)\\)</span>的困难在于类条件概率<span class=\"math inline\">\\(P(x|c)\\)</span>是<span class=\"math inline\">\\(x\\)</span>所有属性的联合分布，难以从有限的训练样本中估计得到。</p>\r\n<p>朴素贝叶斯分类器采用属性条件独立性假设：对所有已知类别，样本<span class=\"math inline\">\\(x\\)</span>的所有属性相互独立。</p>\r\n<p>因此<span class=\"math inline\">\\(P(c|x) = \\frac{P(c)P(x|c)}{P(x)} = \\frac{P(c)}{P(x)} \\prod\\limits_{i=1}^dP(x_i|c)\\)</span>，其中<span class=\"math inline\">\\(d\\)</span>为样本的属性数量，<span class=\"math inline\">\\(x_i\\)</span>表示样本<span class=\"math inline\">\\(x\\)</span>的第<span class=\"math inline\">\\(i\\)</span>个属性值。</p>\r\n<p>对于所有类别来说，<span class=\"math inline\">\\(P(x)\\)</span>相同，因此基于<span class=\"math inline\">\\(\\lambda_{ij} = \\begin{cases}1&amp;i \\ne j\\\\0&amp;i = j\\end{cases}\\)</span>，<span class=\"math inline\">\\(h^\\star(x) = \\mathop{\\arg\\max}\\limits_c P(c|x)\\)</span>即最大化后验概率的朴素贝叶斯分类器就可以表达为<span class=\"math inline\">\\(h_{nb}(x) = \\mathop{\\arg\\max}\\limits_c P(c)\\prod\\limits_{i=1}^dP(x_i|c)\\)</span></p>\r\n<p>对于类先验概率，可以从训练集中使用<span class=\"math inline\">\\(P(c) = \\frac{|D_c|}{|D|}\\)</span>估计。</p>\r\n<p>对于离散属性，估计<span class=\"math inline\">\\(P(x_i|c)\\)</span>的方式常用<span class=\"math inline\">\\(P(x_i|c) = \\frac{|D_{c,x_i}|}{|D_c|}\\)</span>，其中<span class=\"math inline\">\\(D_{c,x_i}\\)</span>表示<span class=\"math inline\">\\(D_c\\)</span>中第<span class=\"math inline\">\\(i\\)</span>个属性取值为<span class=\"math inline\">\\(x_i\\)</span>的样本集合。</p>\r\n<p>对于连续属性，则可以使用概率密度函数，假定<span class=\"math inline\">\\(P(x_i|c)\\)</span>服从某种分布，然后对其进行估计。</p>\r\n<p>在离散属性的处理上，有个问题是：如果训练集中某种属性在类别<span class=\"math inline\">\\(c\\)</span>上没有出现，或者类别<span class=\"math inline\">\\(c\\)</span>在训练集上没有出现，则<span class=\"math inline\">\\(P(c)\\prod\\limits_{i=1}^dP(x_i|c)\\)</span>直接就为0了，因此需要进行修正平滑处理。</p>\r\n<p>常用的是拉普拉斯修正，即将<span class=\"math inline\">\\(P(c) = \\frac{|D_c|}{|D|}\\)</span>更改为<span class=\"math inline\">\\(P(c) = \\frac{|D_c|+1}{|D|+N}\\)</span>，其中N表示类别个数, 将<span class=\"math inline\">\\(P(x_i|c) = \\frac{|D_{c,x_i}|}{|D_c|}\\)</span>更改为<span class=\"math inline\">\\(P(x_i|c) = \\frac{|D_{c,x_i}| + 1}{|D_c| + N_i}\\)</span>，其中<span class=\"math inline\">\\(N_i\\)</span>表示第<span class=\"math inline\">\\(i\\)</span>个属性的可取值个数。这样可以避免因训练集样本不充分而导致的概率估计为零的问题。</p>\r\n<h2 id=\"半朴素贝叶斯分类器\">半朴素贝叶斯分类器</h2>\r\n<p>由于属性条件独立性假设很难成立，因此尝试对条件独立性假设进行一定程度的放松。</p>\r\n<p>例如独依赖估计（One-Dependent Estimator，ODE），即每个属性在类别之外最多依赖一个其他属性：<span class=\"math inline\">\\(P(c|x)\\propto P(c) \\prod\\limits_{i=1}^dP(x_i|c,pa_i)\\)</span>，其中<span class=\"math inline\">\\(pa_i\\)</span>为属性<span class=\"math inline\">\\(x_i\\)</span>所依赖的属性，称为<span class=\"math inline\">\\(x_i\\)</span>的父属性。</p>\r\n<p>问题的关键在于如何确定父属性，最直接的办法是假设所有属性都依赖于同一个父属性，称为超父（super-parent）由此形成了SPODE(Super-Parent ODE)方法。</p>\r\n<h1 id=\"集成学习\">集成学习</h1>\r\n<p>构建并结合多个学习器来完成学习任务。一般是先产生一组个体学习器，然后使用某种策略将他们结合。</p>\r\n<p>根据个体学习器的生成方式，集成学习方法大致可以分为两类： - 个体学习器之间存在强依赖关系，必须串行生成，例如：Boosting。 - 个体学习器之间不存在强依赖关系，可以同时并行生成，例如：Bagging和随机森林（Random Forest）</p>\r\n<h2 id=\"boosting\">Boosting</h2>\r\n<p>先训练一个基学习器，然后使用基学习器对训练样本分布进行调整，使得基学习器预测错误的样本在后续得到更多关注，然后进行下一个基学习器的训练，直到学习器数量达到指定数量。</p>\r\n<p>Boosting中最著名的是AdaBoosting，可以理解为加性模型，即使用基学习器的线性组合<span class=\"math inline\">\\(H(x) = \\sum\\limits_{t=1}^T\\alpha_t h_t (x)\\)</span>来最小化指数损失函数<span class=\"math inline\">\\(l_{exp}(H|D) = E_{x \\sim D}[e^{-f(x)H(x)}]\\)</span>，其中<span class=\"math inline\">\\(h_i\\)</span>表示第<span class=\"math inline\">\\(i\\)</span>个基分类器，<span class=\"math inline\">\\(f(x)\\)</span>表示真实函数<span class=\"math inline\">\\(f(x) = y\\)</span>。</p>\r\n<p>AdaBoosting只适用于二分类任务。</p>\r\n<h2 id=\"bagging\">Bagging</h2>\r\n<p>在原始数据中每次有放回的采样<span class=\"math inline\">\\(m\\)</span>个样本，组成一个子训练集，最终得到共<span class=\"math inline\">\\(T\\)</span>个子训练集，用每个子训练集训练一个基学习器，再将所有基学习器进行结合。</p>\r\n<p>假设基学习器的计算复杂度为<span class=\"math inline\">\\(O(m)\\)</span>，Bagging的复杂度大致为<span class=\"math inline\">\\(T(O(m) + O(s))\\)</span>，其中<span class=\"math inline\">\\(O(s)\\)</span>为投票和采样的复杂度，一般非常小，而<span class=\"math inline\">\\(T\\)</span>是个比较小的常数，因此Bagging的训练复杂度和训练一个学习器的复杂程度同阶，非常高效。</p>\r\n<h2 id=\"bagging和boosting的区别\">Bagging和Boosting的区别</h2>\r\n<h2 id=\"随机森林random-forestrf\">随机森林（Random Forest，RF）</h2>\r\n<p>以决策树为基学习器的Bagging集成基础上，引入随机属性选择。在构造决策树过程中，选择最优划分属性时，可以在当前可用属性的随机子集中进行选择。</p>\r\n<h2 id=\"集成学习的结合策略\">集成学习的结合策略</h2>\r\n<ul>\r\n<li>平均法</li>\r\n<li>投票法</li>\r\n<li>学习法</li>\r\n</ul>\r\n<p>stacking是学习法的一个例子：先用原始训练集训练出初级学习器，然后将初级学习器的输出作为输入特征，训练一个次级学习器，用于结合初级学习器输出得到最终输出。最好在初级学习器的验证集上对次级学习器进行训练。</p>\r\n<h2 id=\"多样性\">多样性</h2>\r\n<p>如果用<span class=\"math inline\">\\(h_1,h_2,...,h_T\\)</span>通过加权平均法来集成得到<span class=\"math inline\">\\(H\\)</span>来估计真实函数<span class=\"math inline\">\\(f:R^d \\rightarrow R\\)</span>，则对样本<span class=\"math inline\">\\(x\\)</span>，定义学习器<span class=\"math inline\">\\(h_i\\)</span>的分歧为：<span class=\"math inline\">\\(A(h_i|x) = (h(x_i) - H(x))^2\\)</span>。集成的分歧表示为<span class=\"math inline\">\\(\\overline{A}(H|x) = \\sum\\limits_{i=1}^T w_i A(h_i|x)=\\sum\\limits_{i=1}^Tw_i(h_i(x) - H(x))^2\\)</span>。分歧表示个体学习器在样本<span class=\"math inline\">\\(x\\)</span>上的不一致性。</p>\r\n<p>而集成的泛化误差可以表示为<span class=\"math inline\">\\(E=\\overline{E} - \\overline{A}\\)</span>（<span class=\"math inline\">\\(\\overline{E} = \\sum\\limits_{i=1}^Tw_iE_i\\)</span>，<span class=\"math inline\">\\(\\overline{A}=\\sum_{i=1}^Tw_iA_i\\)</span>，推导过程略过），这说明个体学习器误差越低，多样性越大，集成的效果越好。</p>\r\n<p>增加个体学习器多样性的方法： - 数据样本扰动：通常基于采样法，产生不同的数据子集。但是有些学习器对数据样本量的扰动不敏感（如线性学习器，支持向量机，朴素贝叶斯，k近邻，这些学习器称为稳定基学习器） - 输入属性扰动：抽取属性子集来训练学习器。不适合属性较少的数据。 - 输出表示扰动：将样本的类别标记稍作改动，例如随机改变一些标记，或者进行编码调制，例如ECOC。 - 算法参数扰动：随机设置不同的参数，例如隐层神经元个数、初始连接权值等。</p>\r\n<h1 id=\"聚类\">聚类</h1>\r\n<h2 id=\"原型聚类\">原型聚类</h2>\r\n<p>又称基于原型的聚类方法，此类算法假设聚类结构可以通过一组原型来刻画，通常先对原型进行初始化，然后对原型进行迭代更新求解。不同的原型表示、不同的求解方式就会产生不同的算法</p>\r\n<h3 id=\"k均值算法\">k均值算法</h3>\r\n<p>给定样本集<span class=\"math inline\">\\(D=\\{x_1,x_2,...,x_m\\}\\)</span>，聚类结果<span class=\"math inline\">\\(C = \\{C_1, C_2, ...,C_k\\}\\)</span>，定义平方误差<span class=\"math inline\">\\(E = \\sum\\limits_{i=1}^k \\sum\\limits_{x\\in C_i} ||x-\\mu_i||^2_2\\)</span>，其中<span class=\"math inline\">\\(\\mu_i = \\frac{1}{C_i}\\sum\\limits_{x \\in C_i}x\\)</span>表示簇<span class=\"math inline\">\\(C_i\\)</span>的均值向量。</p>\r\n<p>k均值算法针对平方误差<span class=\"math inline\">\\(E\\)</span>进行优化，使用贪心策略，通过迭代优化方式来进行：</p>\r\n<p>1、初始化<span class=\"math inline\">\\(k\\)</span>个初始均值向量<span class=\"math inline\">\\(\\mu_1, \\mu_2, ..., \\mu_k\\)</span>。</p>\r\n<p>2、计算每个样本到<span class=\"math inline\">\\(k\\)</span>个均值向量的值，将每个样本划入最近的均值向量对应的簇中，得到一个划分。</p>\r\n<p>3、使用划分好的簇计算新的均值向量。</p>\r\n<p>4、如果没有均值向量被大幅更新或者达到了最大迭代次数，那么停止，否则从第2步继续循环。</p>\r\n<h3 id=\"学习向量量化learning-vector-quantizationlvq\">学习向量量化（Learning Vector Quantization，LVQ）</h3>\r\n<p>和k均值算法类似，但是学习向量量化在学习过程中还利用样本的监督信息来辅助聚类：</p>\r\n<p>1、初始化原型向量<span class=\"math inline\">\\(\\{p_1, p_2, ..., p_q\\}\\)</span></p>\r\n<p>2、随机选取样本<span class=\"math inline\">\\(x_j\\)</span>，计算<span class=\"math inline\">\\(x_j\\)</span>和每个原型向量的距离，并找出距离最小对应的原型向量。</p>\r\n<p>3、如果<span class=\"math inline\">\\(x_j\\)</span>的标签和原型向量的标签相同，则使用<span class=\"math inline\">\\(p=p + \\eta (x_j - p)\\)</span>来对选出的原型向量进行更新，否则使用<span class=\"math inline\">\\(p = p - \\eta (x_j - p)\\)</span>来对选出的原型向量进行更新</p>\r\n<p>4、达到最大迭代轮数或者更新幅度很小则停止更新，否则从第2步继续循环。</p>\r\n<p>其中<span class=\"math inline\">\\(\\eta \\in (0, 1)\\)</span>表示学习速率，在迭代停止之后，对于任意样本<span class=\"math inline\">\\(x\\)</span>，可以将其划分到与其距离最近的原型向量所代表的的簇中。</p>\r\n<h3 id=\"高斯混合聚类mixture-of-gaussian\">高斯混合聚类（Mixture-of-Gaussian）</h3>\r\n<p>假设数据服从高斯混合分布<span class=\"math inline\">\\(p_M(x) = \\sum\\limits_{i=1}^k \\alpha_i p(x|\\mu_i, \\Sigma_i), \\sum\\limits_{i=1}^k \\alpha_i=1\\)</span>。</p>\r\n<p>令随机变量<span class=\"math inline\">\\(z_j,j\\in \\{1,2,...,k\\}\\)</span>表示样本<span class=\"math inline\">\\(x_j\\)</span>预测类别，其先验概率<span class=\"math inline\">\\(p(z_j = i) = \\alpha_i\\)</span>，则<span class=\"math inline\">\\(p_M(z_j = i|x_j) = \\frac{\\alpha_ip(x_i|\\mu_i,\\Sigma_i)}{\\sum\\limits_{l=1}^k \\alpha_lp(x_j|\\mu_l, \\Sigma_l)}\\)</span></p>\r\n<p>当高斯混合分布已知时，即可将样本划分成<span class=\"math inline\">\\(k\\)</span>个簇，其标记为<span class=\"math inline\">\\(\\mathop{\\arg\\max}\\limits_i p_M(z_j = i|x_j)\\)</span></p>\r\n<p>估计这样的高斯混合分布可以使用极大似然法，令对数似然<span class=\"math inline\">\\(LL(p_M) = \\sum\\limits_{j=1}^m ln(\\sum\\limits_{i=1}^k \\alpha_ip(x_i|\\mu_i,\\Sigma_i))\\)</span>，使用EM算法即可求解。</p>\r\n<h2 id=\"密度聚类\">密度聚类</h2>\r\n<h3 id=\"dbscan\">DBSCAN</h3>\r\n<p>密度聚类算法的代表是DBSCAN算法，使用一组邻域参数<span class=\"math inline\">\\((\\epsilon, MinPts)\\)</span>来刻画样本分布的紧密程度。</p>\r\n<ul>\r\n<li><span class=\"math inline\">\\(\\epsilon\\)</span>-邻域：<span class=\"math inline\">\\(N_\\epsilon(x_j) = \\{x_i | x_i \\in D, dist(x_j,x_i)\\le \\epsilon\\}\\)</span>，其中<span class=\"math inline\">\\(D\\)</span>表示全部数据集合。</li>\r\n<li>核心对象：满足<span class=\"math inline\">\\(|N_\\epsilon(x_j)| \\ge MinPts\\)</span>的<span class=\"math inline\">\\(x_j\\)</span>称为核心对象，其中<span class=\"math inline\">\\(|N_\\epsilon(x_j)|\\)</span>表示<span class=\"math inline\">\\(x_j\\)</span>的<span class=\"math inline\">\\(\\epsilon\\)</span>-邻域中的样本数量。</li>\r\n<li>密度直达：<span class=\"math inline\">\\(x_j\\)</span>是一个核心对象，且<span class=\"math inline\">\\(x_i\\)</span>在<span class=\"math inline\">\\(x_j\\)</span>的<span class=\"math inline\">\\(\\epsilon\\)</span>-邻域中，则<span class=\"math inline\">\\(x_i\\)</span>由<span class=\"math inline\">\\(x_j\\)</span>密度直达。</li>\r\n<li>密度可达：如果存在样本序列<span class=\"math inline\">\\(p_1, p_2, ..., p_n, p_1 = x_j, p_n = x_i\\)</span>使得<span class=\"math inline\">\\(p_{m+1}\\)</span>由<span class=\"math inline\">\\(p_m\\)</span>密度直达，则称<span class=\"math inline\">\\(x_i\\)</span>由<span class=\"math inline\">\\(x_j\\)</span>密度可达。</li>\r\n<li>密度相连：如果存在<span class=\"math inline\">\\(x_k\\)</span>使得<span class=\"math inline\">\\(x_j\\)</span>和<span class=\"math inline\">\\(x_i\\)</span>均可由<span class=\"math inline\">\\(x_k\\)</span>密度可达，则称<span class=\"math inline\">\\(x_i\\)</span>和<span class=\"math inline\">\\(x_j\\)</span>密度相连。</li>\r\n</ul>\r\n<p>基于以上概念，DBSCAN将簇定义为由密度可达关系到处的最大密度相连样本集合。</p>\r\n<p>DBSCAN的聚类过程如下：</p>\r\n<p>1、找到所有的核心对象。</p>\r\n<p>2、循环随机取一个还没有访问过的核心对象，将其所有密度相连的样本生成一个簇并标记为已访问，如果没有未访问的核心对象，则停止循环</p>\r\n<p>3、仍未访问的样本被视为噪声样本。</p>\r\n<h2 id=\"层次聚类\">层次聚类</h2>\r\n<p>层次聚类方法试图在不同层次上对数据进行划分，形成树形的聚类结构。</p>\r\n<h3 id=\"agnes\">AGNES</h3>\r\n<p>AGNES是一种自底向上的层次聚类算法，首先将所有样本单独看做一个簇，然后每次迭代找到距离最近的两个簇进行合并，直到簇数量等于指定数量。</p>\r\n<p>这里的关键是如何定义两个簇的距离，主要有三种方式，使用三种距离计算的AGNES分别被称为： - 最大距离：单链接算法 - 最小距离：全链接算法 - 平均距离：均链接算法</p>\r\n<h1 id=\"降维与度量学习\">降维与度量学习</h1>\r\n<h2 id=\"k近邻k-nearest-neighbor学习\">k近邻（k-Nearest Neighbor）学习</h2>\r\n<p>k近邻方法没有训练过程，在给定测试样本时，直接使用训练样本中与其最靠近的<span class=\"math inline\">\\(k\\)</span>个样本，基于这<span class=\"math inline\">\\(k\\)</span>个样本的信息来对测试样本进行预测。</p>\r\n<p>k近邻方法是懒惰学习（lazy learning）的一个代表，而那些在训练阶段就对样本进行学习处理的方法称为急切学习（eager learning）</p>\r\n<h2 id=\"低维嵌入\">低维嵌入</h2>\r\n<p>大部分时候，观测到的数据是高维数据，但与学习任务密切相关的很可能是高维空间中的一个低维嵌入。</p>\r\n<h3 id=\"多维缩放multiple-dimensional-scalingmds\">多维缩放（Multiple Dimensional Scaling，MDS）</h3>\r\n<p>多维缩放的思路是找到一个低维空间，使得在这个低维空间中的欧氏距离和原始空间中的距离相等。</p>\r\n<p>假如原始空间的维度为<span class=\"math inline\">\\(d\\)</span>，所有数据的距离矩阵<span class=\"math inline\">\\(D\\in R^{m \\times m}\\)</span>，其中<span class=\"math inline\">\\(m\\)</span>为样本数量，<span class=\"math inline\">\\(d_{ij}\\)</span>表示样本<span class=\"math inline\">\\(x_i\\)</span>和<span class=\"math inline\">\\(x_j\\)</span>的距离，降维后的数据<span class=\"math inline\">\\(z \\in R^{d&#39;}\\)</span>，所有样本表示为<span class=\"math inline\">\\(Z\\in R^{d&#39; \\times m}\\)</span>。</p>\r\n<p>令<span class=\"math inline\">\\(B = Z^T Z \\in R^{m\\times m}\\)</span>是降维后的内积矩阵，有<span class=\"math inline\">\\(b_{ij} = z_i^T z_j\\)</span></p>\r\n<p>则<span class=\"math inline\">\\(d_{ij}^2 = ||z_i||^2 + ||z_j||^2 - 2z_i^T z_j = b_{ii} + b_{jj} - 2b_{ij}\\)</span></p>\r\n<p>令降维后的样本<span class=\"math inline\">\\(Z\\)</span>被中心化，即<span class=\"math inline\">\\(\\sum\\limits_{i=1}^m z_i = \\mathbf{0}\\)</span>，可得到<span class=\"math inline\">\\(\\sum\\limits_{i=1}^m b_{ij} = \\sum\\limits_{j=1}^m b_{ij} = 0\\)</span></p>\r\n<p>因此：</p>\r\n<p><span class=\"math display\">\\[\r\n\\sum\\limits_{i=1}^md_{ij}^2 = tr(B) + mb_{jj}\\\\\r\n\\sum\\limits_{j=1}^md_{ij}^2 = tr(B) + mb_{ii}\\\\\r\n\\sum\\limits_{i=1}^m\\sum\\limits_{j=1}^md_{ij}^2 = 2m\\ tr(B)\\\\\r\ntr(B) = \\sum\\limits_{i=1}^m b_{ii}\r\n\\]</span></p>\r\n<p>则有：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    b_{ij} &amp;= \\frac{b_{ii} + b_{jj} - d_{ij}^2}{2}\\\\\r\n    &amp;=\\frac{1}{2m}(\\sum\\limits_{i=1}^md_{ij}^2) + \\frac{1}{2m}(\\sum\\limits_{j=1}^md_{ij}^2) - \\frac{1}{2m^2} \\sum\\limits_{i=1}^m\\sum\\limits_{j=1}^md_{ij}^2 - \\frac{d_{ij}^2}{2}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>这样就能根据原来的距离矩阵求出内积矩阵<span class=\"math inline\">\\(B\\)</span>，再对<span class=\"math inline\">\\(B\\)</span>进行特征分解得到<span class=\"math inline\">\\(B=V \\Lambda V^T\\)</span>，其中<span class=\"math inline\">\\(\\Lambda = diag(\\lambda_1, ..., \\lambda_d)\\)</span>为特征值构成的对角阵，<span class=\"math inline\">\\(\\lambda_1 \\ge \\lambda_2\\ge ...\\ge \\lambda_d\\)</span>，为了降维，我们可以取其中<span class=\"math inline\">\\(d&#39;\\)</span>个非零值，构成对角矩阵<span class=\"math inline\">\\(\\Lambda_\\star\\)</span>与其对应的特征向量矩阵<span class=\"math inline\">\\(V_\\star\\)</span>，则<span class=\"math inline\">\\(Z = \\Lambda_\\star ^{\\frac{1}{2}} V_\\star^T \\in R^{d&#39; \\times m}\\)</span></p>\r\n<h2 id=\"主成分分析pca\">主成分分析（PCA）</h2>\r\n<p>如果希望使用一个超平面来对数据进行表示，那么可以从两个方面去考虑： - 最近重构性：样本点到这个超平面的距离都足够近 - 最大可分性：样本点在这个超平面上的投影尽可能分开</p>\r\n<p>但是两个方面的考虑最终都会得到PCA的等价推导，即PCA既保证了最近重构性也保证了最大可分性。</p>\r\n<p>假定数据<span class=\"math inline\">\\(x_i \\in R^d\\)</span>已经进行过中心化，即<span class=\"math inline\">\\(\\sum_i x_i = \\mathbf{0}\\)</span>，现在使用一组标准正交基对<span class=\"math inline\">\\(x_i\\)</span>进行投影，得到<span class=\"math inline\">\\(z_i = Wx_i, W = \\begin{bmatrix}w_1^T\\\\ w_2^T\\\\ \\vdots\\\\ w_{d&#39;}^T \\end{bmatrix}\\in R^{d&#39; \\times d}, w_i^T w_j = \\begin{cases}1 &amp; i=j\\\\ 0&amp; i\\ne j\\end{cases}\\)</span>，其中<span class=\"math inline\">\\(z_{ij} = w_j^Tx_i\\)</span>，如果使用<span class=\"math inline\">\\(z_i\\)</span>来还原<span class=\"math inline\">\\(x_i\\)</span>则得到<span class=\"math inline\">\\(\\hat{x}_i = \\sum\\limits_{j=1}^{d&#39;}z_{ij}w_j = W^T z_i\\)</span>。</p>\r\n<p>如果从最近重构性来考虑，我们希望最小化<span class=\"math inline\">\\(\\sum\\limits_{i=1}^m ||\\hat{x}_i - x_i||^2_2\\)</span>，即： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    &amp;\\min \\sum\\limits_{i=1}^m ||\\hat{x}_i - x_i||^2_2\\\\\r\n    &amp;=\\min \\sum\\limits_{i=1}^m || \\sum\\limits_{j=1}^{d&#39;}z_{ij}w_j - x_i||^2_2\\\\\r\n    &amp;=\\min \\sum\\limits_{i=1}^m || W^Tz_i - x_i||^2_2\\\\\r\n    &amp;=\\min \\sum\\limits_{i=1}^m (z_i^TWW^Tz_i - z_i^TWx_i - x^T_iW^Tz_i + x^T_i x_i)\\\\\r\n    &amp;=\\min \\sum\\limits_{i=1}^m (z_i^Tz_i - z_i^TWx_i - x^T_iW^Tz_i)\\\\\r\n    &amp;=\\min \\sum\\limits_{i=1}^m tr(z_i^Tz_i - z_i^TWx_i - x^T_iW^Tz_i)\\\\\r\n    &amp;=\\min \\sum\\limits_{i=1}^m tr(z_i^Tz_i - 2z_i^TWx_i)\\\\\r\n    &amp;=\\min \\sum\\limits_{i=1}^m -tr(z_i^T z_i)\\\\\r\n    &amp;=\\min -tr(Z^T Z),\\ Z = \\begin{bmatrix}z_1 &amp;z_2 &amp; \\cdots &amp;z_m\\end{bmatrix} = WX,\\ X = \\begin{bmatrix}x_1 &amp;x_2 &amp; \\cdots &amp;x_m\\end{bmatrix}\\\\\r\n    &amp;=\\min -tr(X^TW^TWX)\\\\\r\n    &amp;=\\min -tr(WXX^TW^T)\\\\\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>因此我们需要解决的问题就是： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\min\\ &amp; -tr(WXX^TW^T)\\\\\r\n    s.t.\\ &amp; WW^T = I_{d&#39;}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>另一方面，如果从最大可分性来考虑，我们希望最大化<span class=\"math inline\">\\(z_i\\)</span>之间的方差<span class=\"math inline\">\\(\\sum\\limits_{i=1}^m (z_i - \\frac{1}{m}\\sum\\limits_{j=1}^mz_j)^2\\)</span></p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    &amp;\\max \\sum\\limits_{i=1}^m ||z_i - \\frac{1}{m}\\sum\\limits_{j=1}^mz_j||_2^2\\\\\r\n    &amp;=\\max \\sum\\limits_{i=1}^m ||z_i - \\frac{1}{m}\\sum\\limits_{j=1}^mWx_j||_2^2\\\\\r\n    &amp;=\\max \\sum\\limits_{i=1}^m ||z_i - \\frac{1}{m}W\\sum\\limits_{j=1}^mx_j||_2^2\\\\\r\n    &amp;=\\max \\sum\\limits_{i=1}^m ||z_i||_2^2\\\\\r\n    &amp;=\\max \\sum\\limits_{i=1}^m (z^T_iz_i)\\\\\r\n    &amp;=\\max\\ tr(Z^TZ)\\\\\r\n    &amp;=\\max\\ tr(X^TW^TWX)\\\\\r\n    &amp;=\\min\\ -tr(X^TW^TWX)\\\\\r\n    &amp;=\\min\\ -tr(WXX^TW^T)\\\\\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>因此我们需要解决的问题就是： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\min\\ &amp; -tr(WXX^TW^T)\\\\\r\n    s.t.\\ &amp; WW^T = I_{d&#39;}\\\\\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>由此可见，在<span class=\"math inline\">\\(\\sum x_i = 0\\)</span>的情况下，从两个方面得到的结果完全相同。</p>\r\n<p>求解PCA可以使用拉格朗日法，首先得到拉格朗日函数<span class=\"math inline\">\\(L(W) = -tr(X^TW^TWX) + \\lambda (WW^T - I_{d&#39;}), \\lambda \\ge 0\\)</span></p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    dL(W) &amp;= -tr(WXX^TdW^T + dWXX^TW^T) + \\lambda tr(dW W^T + WdW^T)\\\\\r\n    &amp;= -tr(2XX^TW^TdW - 2\\lambda W^TdW)\\\\\r\n    \\\\\r\n    \\frac{\\partial dL(W)}{\\partial W} &amp;= 2\\lambda W - 2WXX^T\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>令<span class=\"math inline\">\\(\\frac{\\partial dL(W)}{\\partial W} = 0\\)</span>可得<span class=\"math inline\">\\(\\lambda W^T = XX^TW^T\\)</span>，即求出协方差矩阵<span class=\"math inline\">\\(XX^T\\)</span>的特征向量即可构成<span class=\"math inline\">\\(W^T\\)</span>，在这个过程中，可以舍弃一部分特征向量，只取特征值最大的<span class=\"math inline\">\\(d&#39;\\)</span>个特征向量，即可将数据维度从<span class=\"math inline\">\\(d\\)</span>维缩减到<span class=\"math inline\">\\(d&#39;\\)</span>维。</p>\r\n<p>如果将<span class=\"math inline\">\\(X\\)</span>进行奇异值分解，则有<span class=\"math inline\">\\(X=D\\Sigma V^T\\)</span>，<span class=\"math inline\">\\(XX^T = D \\Sigma \\Sigma^T D^T\\)</span>，其中<span class=\"math inline\">\\(D\\)</span>是<span class=\"math inline\">\\(X\\)</span>的左奇异矩阵，也就是<span class=\"math inline\">\\(XX^T\\)</span>的特征矩阵，如果令<span class=\"math inline\">\\(W=D^T\\)</span>可以得到<span class=\"math inline\">\\(Z=D^TX=D^TD\\Sigma V^T = \\Sigma V^T\\)</span>，因此求出<span class=\"math inline\">\\(X^TX\\)</span>的特征矩阵也可以求出<span class=\"math inline\">\\(Z\\)</span>。</p>\r\n<h2 id=\"核化线性降维\">核化线性降维</h2>\r\n<h3 id=\"核主成分分析kpca\">核主成分分析（KPCA）</h3>\r\n<p>PCA是一种线性降维方式，在降维时假设从高维空间到低维空间的映射是线性的，其线性映射由<span class=\"math inline\">\\(W\\)</span>确定，且有<span class=\"math inline\">\\(\\lambda W^T = XX^TW^T = (\\sum\\limits_{i=1}^m x_i x^T_i)W^T\\)</span>，假设在高维空间<span class=\"math inline\">\\(\\phi(x)\\)</span>中进行PCA，则有<span class=\"math inline\">\\(\\lambda W^T = (\\sum\\limits_{i=1}^m \\phi(x_i) \\phi(x_i)^T)W^T\\)</span>，有如下推导：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\lambda W^T &amp;= (\\sum\\limits_{i=1}^m \\phi(x_i) \\phi(x_i)^T)W^T\\\\\r\n    &amp;= (\\sum\\limits_{i=1}^m \\phi(x_i) \\phi(x_i)^T)W^T\\\\\r\n    &amp;= \\sum\\limits_{i=1}^m \\phi(x_i)\\phi(x_i)^TW^T\\\\\r\n    &amp;= \\sum\\limits_{i=1}^m\\phi(x_i)\\alpha_i,\\ \\alpha_i = \\phi(x_i)^TW^T\\\\\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>由于<span class=\"math inline\">\\(\\phi\\)</span>函数无法明确求出，因此引入核函数。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\lambda \\phi(x_j)^TW^T &amp;= \\phi(x_j)^T\\sum\\limits_{i=1}^m\\phi(x_i)\\alpha_i\\\\\r\n    \\lambda A&amp;=KA\\\\\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>其中<span class=\"math inline\">\\(K\\)</span>是核矩阵<span class=\"math inline\">\\(K_{ij} = \\phi(x_i)^T\\phi(x_j)\\)</span>，<span class=\"math inline\">\\(A = \\begin{bmatrix}\\alpha_1, \\alpha_2, ..., \\alpha_m\\end{bmatrix}\\)</span></p>\r\n<p>则<span class=\"math inline\">\\(z_{ij} = w_j^T \\phi(x_i) = \\sum\\limits_{k=1}^m\\alpha_{k,j}\\phi(x_k)^T\\phi(x_i)=\\sum\\limits_{k=1}^m\\alpha_{k,j}K_{ki}\\)</span>表示降维之后<span class=\"math inline\">\\(x_i\\)</span>对应向量的第<span class=\"math inline\">\\(j\\)</span>个分量。</p>\r\n<p>其中W可以由<span class=\"math inline\">\\(\\phi(X)^T\\phi(X)\\)</span>的特征矩阵求出，但在计算<span class=\"math inline\">\\(z\\)</span>时，由于每个分量需要求和<span class=\"math inline\">\\(\\sum\\limits_{k=1}^m\\alpha_{k,j}\\)</span>，计算量非常大。</p>\r\n<h2 id=\"流形学习\">流形学习</h2>\r\n<p>流形是指在局部与欧式空间统配的空间，如果数据是嵌入在高维空间中的低维流形，则可以利用其局部与欧式空间同胚的性质，使用局部的欧式距离来计算数据样本之间的距离。</p>\r\n<h3 id=\"等度量映射isometric-mappingisomap\">等度量映射（Isometric Mapping，Isomap）</h3>\r\n<p>将低维嵌入流形上两点的距离定义为“测地线”距离，即两个样本沿着流形的最短距离，测地线距离的计算可以将邻近点之间进行连接，然后转换计算近邻连接图上两点之间的最短路径问题（Dijkstra算法或者Floyd算法）。得到距离矩阵之后，可以使用多维缩放算法（MDS）来进行降维。</p>\r\n<p>邻近图的构建一般有两种做法，一个是指定最近的<span class=\"math inline\">\\(k\\)</span>个点作为邻近点，这样得到的邻近图称为<span class=\"math inline\">\\(k\\)</span>邻近图，另一个是指定距离阈值<span class=\"math inline\">\\(\\epsilon\\)</span>，小于<span class=\"math inline\">\\(\\epsilon\\)</span>的点被认为是邻近点，这样得到的邻近图称为<span class=\"math inline\">\\(\\epsilon\\)</span>邻近图，两种方式各有优劣。</p>\r\n<h3 id=\"局部线性嵌入locally-linear-embeddinglle\">局部线性嵌入（Locally Linear Embedding，LLE）</h3>\r\n<p>Isomap试图保持邻近样本之间的距离，而LLE试图保持邻域内样本的线性关系，假定样本点<span class=\"math inline\">\\(x_i\\)</span>可以通过<span class=\"math inline\">\\(x_j,x_k,x_l\\)</span>线性组合而得到，即<span class=\"math inline\">\\(x_i = w_{ij}x_j+w_{ik}x_k+w_{il}x_l\\)</span></p>\r\n<p>首先对于每个样本<span class=\"math inline\">\\(x_i\\)</span>，找到其邻近下标集合<span class=\"math inline\">\\(Q_i\\)</span>，然后计算<span class=\"math inline\">\\(Q_i\\)</span>对<span class=\"math inline\">\\(x_i\\)</span>的线性重构系数： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\min\\limits_{w_1, w_2, ..., w_m}\\ &amp;\\sum\\limits_{i=1}^m ||x_i - \\sum\\limits_{j\\in Q_i}w_{ij} x_j||_2^2\\\\\r\n    s.t.\\ &amp;\\sum\\limits_{j\\in Q_i} w_{ij} = 1\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>求得<span class=\"math inline\">\\(w_{ij}\\)</span>之后，<span class=\"math inline\">\\(x_i\\)</span>对应的低维空间坐标<span class=\"math inline\">\\(z_i = \\min\\limits_{z_1, z_2, ..., z_m} \\sum\\limits_{i=1}^m ||z_i - \\sum\\limits_{j\\in Q_i} w_{ij} z_j||_2^2\\)</span>。</p>\r\n<p>令<span class=\"math inline\">\\(Z = \\begin{bmatrix}z_1 &amp; z_2 &amp; \\cdots &amp; z_m\\end{bmatrix} \\in R^{d&#39; \\times m}, W_{ij} = w_{ij}\\)</span></p>\r\n<p>则确定<span class=\"math inline\">\\(W\\)</span>之后，<span class=\"math inline\">\\(Z\\)</span>可以通过： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\min\\limits_Z\\ &amp;tr(Z(\\mathbf{I} - W)^T(\\mathbf{I} - W)Z^T)\\\\\r\n    s.t.\\ &amp;ZZ^T = \\mathbf{I}\r\n\\end{aligned}\r\n\\]</span> 来求得，即对<span class=\"math inline\">\\((\\mathbf{I} - W)^T(\\mathbf{I} - W)\\)</span>进行特征分解，取最小的<span class=\"math inline\">\\(d&#39;\\)</span>个特征值对应的特征向量构成<span class=\"math inline\">\\(Z^T\\)</span></p>\r\n<h2 id=\"度量学习\">度量学习</h2>\r\n<p>对高维数据的降维主要是希望找到一个合适的低维空间使得此空间中学习能比原始空间性能更好，度量学习的思路是尝试学习出一个距离度量。</p>\r\n<p>对于两个<span class=\"math inline\">\\(d\\)</span>维样本<span class=\"math inline\">\\(x_i\\)</span>，<span class=\"math inline\">\\(x_j\\)</span>，其欧氏距离的平方<span class=\"math inline\">\\(||x_i - x_j||^2_2 = dist_{ij,1} + dist_{ij,2} + \\cdots + dist_{ij,d}\\)</span>，其中<span class=\"math inline\">\\(dist_{ij,k}\\)</span>表示在第<span class=\"math inline\">\\(k\\)</span>维上的距离。</p>\r\n<p>如果假定不同属性的重要性不同，则可以引入权重<span class=\"math inline\">\\(w\\)</span>，<span class=\"math inline\">\\(||x_i - x_j||^2_2 = w_1 dist_{ij,1} + w_2 dist_{ij,2} + \\cdots + w_d dist_{ij,d} = (x_i - x_j)^T W (x_i - x_j)\\)</span>，其中<span class=\"math inline\">\\(W = diag(w), w_i \\ge 0\\)</span></p>\r\n<p>如果令<span class=\"math inline\">\\(W\\)</span>不再是一个对角矩阵，而是让其等于一个半正定对称矩阵<span class=\"math inline\">\\(M\\)</span>，则可以定义马氏距离<span class=\"math inline\">\\(dist_{mah}^2(x_i, x_j) = (x_i - x_j)^TM(x_i - x_j) = ||x_i - x_j||^2_M\\)</span>，则可以对这个<span class=\"math inline\">\\(M\\)</span>进行学习，得到满足要求的距离表达。</p>\r\n<p>在近邻成分分析（Neighbourhood Component Analysis, NCA）中可以用对<span class=\"math inline\">\\(M\\)</span>进行训练，提高其分类正确率。</p>\r\n<p>又例如根据一些领域知识已知某些样本相似（必连约束集合），另外一些样本不相似（勿连约束及合），则可以对<span class=\"math inline\">\\(M\\)</span>进行训练，使得必连约束集合中的样本距离尽可能小，而勿连约束集合中的样本的尽可能大。</p>\r\n<p>不管以任何方式训练得到的<span class=\"math inline\">\\(M\\)</span>，都可以对M进行特征值分解，然后去掉一部分特征向量，得到降维矩阵，用于数据的降维。</p>\r\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"线性模型\">线性模型</h1>\r\n<h2 id=\"基于均方误差最小化来进行求解的方法称为最小二乘法\">基于均方误差最小化来进行求解的方法称为最小二乘法</h2>\r\n<h2 id=\"用最小二乘法来优化线性回归\">用最小二乘法来优化线性回归</h2>\r\n<p>线性回归的目标是学习函数<span class=\"math inline\">\\(f(X) = Xw\\)</span>，使得<span class=\"math inline\">\\(f(X) \\approx Y\\)</span>，其中<span class=\"math inline\">\\(X=\\begin{bmatrix}x_1 &amp; 1\\\\ x_2 &amp; 1\\\\ \\vdots &amp; \\vdots \\\\ x_m &amp; 1 \\end{bmatrix}\\)</span>，<span class=\"math inline\">\\(Y=\\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_m\\end{bmatrix}\\)</span>，<span class=\"math inline\">\\(w \\in R^{(n+1) \\times 1}\\)</span>，<span class=\"math inline\">\\(n\\)</span>是数据特征维数。</p>\r\n<p>如果用均方误差作为损失函数<span class=\"math inline\">\\(L = (Xw-Y)^T (Xw-Y)\\)</span>，那么问题可以描述为</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n&amp;\\arg \\min_{w} (Xw-Y)^T (Xw-Y)\\\\\r\n\\end{aligned}\r\n\\]</span> 直接求导： <span class=\"math display\">\\[\r\n\\frac{\\partial L}{\\partial w} = \\frac{\\partial (w^T X^T X w - Y^TXw -XwY - Y^TY)}{\\partial w} = 2X^T(Xw-Y)\\\\\r\n\\]</span> 令： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n&amp;\\frac{\\partial L}{\\partial w} = 0\\\\\r\n&amp;\\Rightarrow w = (X^TX)^{-1}X^TY\r\n\\end{aligned}\r\n\\]</span> 即得到使用均方误差的线性回归问题的解。</p>\r\n<h2 id=\"对数几率回归逻辑回归-logistic-regression\">对数几率回归（逻辑回归， Logistic Regression）</h2>\r\n<p>对数几率回归即用线性回归去拟合对数几率<span class=\"math inline\">\\(ln\\frac{y}{1-y} = w^T x + b\\)</span></p>\r\n<p>对数几率回归也等价于<span class=\"math inline\">\\(y=\\frac{1}{1+e^{-z}}, z = w^T x + b\\)</span></p>\r\n<p>若将<span class=\"math inline\">\\(y\\)</span>视为后验概率，则<span class=\"math inline\">\\(ln \\frac{P(y=1|x)}{P(y=0|x)} = z, z=w^Tx + b\\)</span></p>\r\n<p>显然有<span class=\"math inline\">\\(P(y=1|x) = \\frac{e^z}{1 + e^z}, P(y=0|x)=\\frac{1}{1 + e^z}\\)</span></p>\r\n<p>使用极大似然法求解对数几率回归：<span class=\"math inline\">\\(\\mathop{\\arg\\max}\\limits_{w, b}\\prod\\limits_{i=1}^m P(y=y_i| x_i) \\Rightarrow \\mathop{\\arg\\max}\\limits_{w, b}\\sum\\limits_{i=1}^m ln(P(y=y_i|x_i))\\)</span></p>\r\n<p>令<span class=\"math inline\">\\(\\beta = \\begin{bmatrix}w ^ T&amp; b\\end{bmatrix} ^ T \\in R^{n+1}, Y=\\begin{bmatrix}y_1&amp;y_2&amp;\\dots&amp;y_m\\end{bmatrix} ^ T, X=\\begin{bmatrix} x_1&amp;x_2&amp;\\dots&amp;x_m\\\\1&amp;1&amp;\\dots&amp;1\\end{bmatrix},x_i \\in R^n, X \\in R^{(n+1) \\times m}\\)</span>，其中<span class=\"math inline\">\\(m\\)</span>是数据量。</p>\r\n<p>使用极大似然法求解对数几率回归可以重写为： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n&amp;\\mathop{\\arg\\max}\\limits_{\\beta} l(Z)\\\\\r\n&amp;Z = X^T \\beta\\\\\r\n&amp;l(Z) = Y^Tln\\frac{e^Z}{\\mathbf{1} + e^Z} + (\\mathbf{1}-Y)^Tln\\frac{\\mathbf{1}}{\\mathbf{1}+e^Z}\\\\\r\n&amp;=Y^TZ - ln(\\mathbf{1}+e^Z)\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>使用牛顿法，第<span class=\"math inline\">\\(t\\)</span>次更新为<span class=\"math inline\">\\(\\beta^{t+1} \\leftarrow \\beta ^ t - (\\triangledown_2l)^{-1}\\frac{\\partial l}{\\partial \\beta}\\)</span></p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\ndl &amp;= Y^TdZ - \\mathbf{1}^T\\frac{e^Z}{\\mathbf{1}+e^Z} \\odot dZ\\\\\r\n&amp;=Y^TdZ -\\mathbf{1}^T \\hat{P}_1 \\odot dZ, \\hat{P} = \\begin{bmatrix} P(y=1|x_1) &amp; P(y=1|x_2)&amp; \\dots &amp; P(y=1|x_m)\\end{bmatrix}^T\\\\\r\n&amp;=Y^TX^Td\\beta - \\mathbf{1}^T \\hat{P}_1 \\odot (X^Td\\beta)\\\\\r\n&amp;=Y^TX^Td\\beta - (\\mathbf{1} \\odot \\hat{P}_1)^TX^Td\\beta\\\\\r\n&amp;=(Y^T-\\hat{P}_1^T)X^Td\\beta\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>所以<span class=\"math inline\">\\(\\frac{\\partial l}{\\partial \\beta} = X(Y-\\hat{P}_1)\\)</span></p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    d(\\frac{\\partial l}{\\partial \\beta}) &amp;= d(X(Y-\\hat{P}_1))\\\\\r\n    &amp;=Xd\\hat{P}_1\\\\\r\n    &amp;=Xd\\frac{e^Z}{\\mathbf{1}+e^Z}\\\\\r\n    &amp;=X(\\frac{1}{1+e^Z}\\odot\\frac{e^Z}{1+e^Z} \\odot dZ)\\\\\r\n    &amp;=X(\\hat{P}_0 \\odot \\hat{P}_1 \\odot (X^Td\\beta))\\\\\r\n    &amp;=X diag(\\hat{P}_0) diag(\\hat{P}_1) X^Td\\beta,\\ diag(\\hat{P}_0) = \\begin{bmatrix}\r\n        P(y=0|x_1)&amp;\\cdots&amp;0\\\\\r\n        \\vdots&amp;\\ddots&amp;\\vdots\\\\\r\n        0&amp;\\cdots&amp;P(y=0|x_m)\r\n    \\end{bmatrix}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>所以<span class=\"math inline\">\\(\\frac{\\partial^2 l}{\\partial \\beta \\partial \\beta^T} = \\frac{\\partial^2 l}{\\partial \\beta^T \\partial \\beta} = Xdiag(\\hat{P}_1) diag(\\hat{P}_0)X^T\\)</span></p>\r\n<p>即如果用牛顿法来求解极大似然对数几率回归，第<span class=\"math inline\">\\(t\\)</span>次更新为</p>\r\n<p><span class=\"math display\">\\[\r\n\\beta^{t+1} \\leftarrow \\beta ^ t - (Xdiag(\\hat{P}_1) diag(\\hat{P}_0)X^T)^{-1} X(Y-\\hat{P}_1)\r\n\\]</span></p>\r\n<h2 id=\"线性判别分析\">线性判别分析</h2>\r\n<p>线性判别分析也叫“Fisher判别分析”，其思想非常朴素：试图找到一个低维空间（直线），可以表示为<span class=\"math inline\">\\(y=wx\\)</span>(这个表达式中的<span class=\"math inline\">\\(y\\)</span>表示<span class=\"math inline\">\\(x\\)</span>投影到这个空间（直线）后和原点的距离)，使得当样本投影到该直线时不同类别样本的距离尽可能远，而相同类别的数据尽可能接近，预测时按照样本的投影位置对其进行分类。</p>\r\n<p>以两类数据<span class=\"math inline\">\\(x_1, x_2\\)</span>为例，设<span class=\"math inline\">\\(\\mu_1,\\mu_2,\\Sigma_1,\\Sigma_2\\)</span>分别表示两类数据的均值和方差，则投影之后的均值和方差为<span class=\"math inline\">\\(w\\mu_1,w\\mu_2,w^T\\Sigma_1w,w^T\\Sigma_2w\\)</span>，因为直线是一维空间，所以这些均为实数，投影之后的类内距离可以用方差来衡量，这里使用<span class=\"math inline\">\\(w^T\\Sigma_1w + w^T\\Sigma_2w\\)</span>来度量投影之后的类内距离，而类间距离可以写成<span class=\"math inline\">\\(||w\\mu_2 - w\\mu_1||_2^2\\)</span>，同时考虑两种距离，给出希望最大化的目标函数如下。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\nJ &amp;= \\frac{||w^T\\mu_2 - w^T\\mu_1||_2^2}{w^T\\Sigma_1w + w^T\\Sigma_2w}\\\\\r\n&amp;= \\frac{w^T(\\mu_2 - \\mu_1)(\\mu_2 - \\mu_1)^Tw}{w^T(\\Sigma_1 + \\Sigma_2)w}\r\n\\end{aligned}\r\n\\]</span> 定义类内散度矩阵<span class=\"math inline\">\\(S_w = \\Sigma_1 + \\Sigma_2\\)</span>，类间散度矩阵<span class=\"math inline\">\\(S_b = (\\mu_2 - \\mu_1)(\\mu_2 - \\mu_1)^T\\)</span>，上面的优化目标可以简写为如下。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    J = \\frac{w^TS_bw}{w^TS_ww}\r\n\\end{aligned}\r\n\\]</span> 这个优化目标又称为<span class=\"math inline\">\\(S_b\\)</span>和<span class=\"math inline\">\\(S_w\\)</span>的广义瑞利商，注意到分子分母中都有<span class=\"math inline\">\\(w\\)</span>的二次项，因此和<span class=\"math inline\">\\(w\\)</span>大小无关，只和w方向有关，所以优化问题可以写成下式。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n\\min_w&amp; \\quad-w^TS_bw\\\\\r\ns.t.&amp; \\quad w^TS_ww = 1\r\n\\end{aligned}\r\n\\]</span> 用拉格朗日乘子法进行优化，求解<span class=\"math inline\">\\(S_bw = \\lambda S_ww\\)</span>，因<span class=\"math inline\">\\(S_bw\\)</span>方向和<span class=\"math inline\">\\(\\mu_2 - \\mu_1\\)</span>相同，因此令<span class=\"math inline\">\\(S_bw = \\lambda(\\mu_2 - \\mu_1)\\)</span>，代入求解，可以得到<span class=\"math inline\">\\(w = S_w^{-1}(\\mu_2 - \\mu_1)\\)</span>。</p>\r\n<p>将数据进行降维，使得类内散度最小，类间散度最大，本质上是有监督的降维。</p>\r\n<h2 id=\"多分类\">多分类</h2>\r\n<p>可以将多分类问题拆解为二分类问题，拆解策略有三种：一对一（OvO）、一对其余（OvR）、多对多（MvM）</p>\r\n<p>在MvM中，最常用的是纠错输出码（Error Correcting Output Codes，ECOC）,有<span class=\"math inline\">\\(C_1C_2C_3...C_n\\)</span>共<span class=\"math inline\">\\(n\\)</span>个类别，每个样本属于其中的一种，训练m个二分类器<span class=\"math inline\">\\(f_1, f_2, ..., f_m\\)</span>，每个分类器将一些类作为正类，另一些类作为负类，这样对于某个类别的样本，理想情况是<span class=\"math inline\">\\(m\\)</span>个分类器对其进行预测的输出组成的0,1串，构成一种长度为<span class=\"math inline\">\\(m\\)</span>的固定的类别组合串，<span class=\"math inline\">\\(n\\)</span>个类就有<span class=\"math inline\">\\(n\\)</span>种组合，但在预测时，对一个样本预测得到的输出串，可能不在<span class=\"math inline\">\\(n\\)</span>个类的<span class=\"math inline\">\\(n\\)</span>种组合中，这时，计算预测输出串和每个类别组合串的距离（海明距离或者欧式距离），将样本判定为距离最小的那个类别组合串对应的类别。</p>\r\n<h2 id=\"类别不平衡\">类别不平衡</h2>\r\n<p>解决办法主要有三种： - 再缩放（再平衡），根据样本数量移动判定阈值或者缩放预测概率。 - 欠采样，将样本量过多的类别进行采样，减少该类别的样本数量，再拿去训练，但是这个方法容易丢失数据中的信息，最好是分成多个模型，每个模型使用该类别的一部分数据。 - 过采样，将样本量过少的类别样本进行重复，然后训练，但是这个方法容易严重过拟合，一个办法是用两个该类别样本进行插值，生成新的该类别样本。</p>\r\n<h1 id=\"决策树\">决策树</h1>\r\n<h2 id=\"信息熵\">信息熵</h2>\r\n<p>样本集合<span class=\"math inline\">\\(D\\)</span>中第<span class=\"math inline\">\\(k\\)</span>类样本所占比例为<span class=\"math inline\">\\(p_k\\)</span>，则信息熵定义为<span class=\"math inline\">\\(Ent(D) = -\\sum\\limits_{k=1}^C p_k\\log_2p_k\\)</span>，其中<span class=\"math inline\">\\(C\\)</span>为类别个数。</p>\r\n<h2 id=\"信息熵增益\">信息熵增益</h2>\r\n<p>假设离散属性<span class=\"math inline\">\\(a\\)</span>有<span class=\"math inline\">\\(v\\)</span>个取值：<span class=\"math inline\">\\(a_1, a_2, ..., a_v\\)</span>，可以将当前数据集合分成<span class=\"math inline\">\\(V\\)</span>个子集：<span class=\"math inline\">\\(D_1, D_2, ..., D^V\\)</span>，那么信息熵增益定义为<span class=\"math inline\">\\(Gain(D,a)=Ent(D)-\\sum\\limits_{v=1}^V\\frac{|D^v|}{|D|}Ent(D^v)\\)</span></p>\r\n<p>决策树构造过程，即每次选择一个信息熵增益最大的属性<span class=\"math inline\">\\(a\\)</span>，将数据划分为<span class=\"math inline\">\\(V\\)</span>个子集，然后在每个子集中进行同样的操作（不能选择已经使用过的属性），若无属性可用或者当前子集类别相同，则当前子集标记为类别数量最多的类别，停止继续划分。</p>\r\n<h2 id=\"增益率\">增益率</h2>\r\n<p>信息熵增益的定义导致其对数量较多的<span class=\"math inline\">\\(D^v\\)</span>更加敏感，因此又提出了增益率的概念：<span class=\"math inline\">\\(Gain_ratio(D,a)=\\frac{Gain(D,a)}{IV(a)}\\)</span>，其中，<span class=\"math inline\">\\(IV(a)=-\\sum\\limits_{v=1}^V \\frac{|D^v|}{|D|} \\log_2\\frac{|D^v|}{|D|}\\)</span>，称为属性<span class=\"math inline\">\\(a\\)</span>的固有值。</p>\r\n<h2 id=\"基尼指数\">基尼指数</h2>\r\n<p>基尼值定义为<span class=\"math inline\">\\(Gini(D) = \\sum\\limits_{k=1}^C\\sum\\limits_{k&#39; \\ne k}p_k p_{k&#39;} = 1-\\sum\\limits_{k=1}^Cp_k^2\\)</span>，其反映了在<span class=\"math inline\">\\(D\\)</span>中随机抽取两个样本，属于同一类别的概率。 和信息熵增益类似，定义基尼指数为<span class=\"math inline\">\\(Gini_index(D, a) = \\sum\\limits_{v=1}^V\\frac{|D^v|}{|D|}Gini(D^v)\\)</span>，构造决策树时的最优划分属性选择也可以使用能够使基尼指数最小的属性。</p>\r\n<h1 id=\"决策树的剪枝\">决策树的剪枝</h1>\r\n<h2 id=\"预剪枝\">预剪枝</h2>\r\n<p>在生成决策树的过程中，利用验证集验证当前划分是否能提升泛化效果，如果不能，则停止继续划分，直接将当前样本集作为决策树叶节点，并指定数量最多的类别做为叶节点类别。预剪枝训练时间短，但是由于其是贪心策略，容易引起欠拟合。</p>\r\n<h2 id=\"后剪枝\">后剪枝</h2>\r\n<p>在决策树生成完成之后，自底向上的对非叶节点进行考察，观察将该节点直接指定为叶节点是否能提升验证集泛化效果，如果能则将此节点的子树一起合并为一个叶节点。后剪枝的训练时间比预剪枝长很多，但是其欠拟合风险小，而且泛化性能往往优于预剪枝。</p>\r\n<h1 id=\"包含连续值的决策树\">包含连续值的决策树</h1>\r\n<p>如果连续属性<span class=\"math inline\">\\(a\\)</span>在<span class=\"math inline\">\\(D\\)</span>中出现<span class=\"math inline\">\\(n\\)</span>个取值，则将其从小到大排序为<span class=\"math inline\">\\(\\begin{bmatrix}a_1, a_2, ... a_n\\end{bmatrix}\\)</span>，这样产生<span class=\"math inline\">\\(n-1\\)</span>个离散值<span class=\"math inline\">\\(T_a = \\{\\frac{a_i + a_{i+1}}{2}|1 \\le i \\le n-1\\}\\)</span> 则<span class=\"math inline\">\\(Gain(D,a)= \\max\\limits_{t \\in T_a}Gain(D, a, t)\\)</span>，其中<span class=\"math inline\">\\(Gain(D, a, t)\\)</span>表示将<span class=\"math inline\">\\(a\\)</span>属性使用<span class=\"math inline\">\\(t\\)</span>划分为两部分，这样，连续值的决策树构造和离散值类似了，不过连续值属性在构造决策树的过程中可以使用多次。</p>\r\n<h1 id=\"属性缺失的处理\">属性缺失的处理</h1>\r\n<p>令<span class=\"math inline\">\\(\\tilde{D}\\)</span>是所有没有缺失属性a的样本集合，对于样本<span class=\"math inline\">\\(x\\)</span>，有样本权重<span class=\"math inline\">\\(w_x\\)</span>，定义如下参数。 <span class=\"math display\">\\[\r\n\\rho = \\frac{\\sum_{x\\in \\tilde{D}} w_x}{\\sum_{x\\in D}w_x}\\\\\r\n\\tilde{p}_k = \\frac{\\sum_{x\\in \\tilde{D}_k w_x}}{\\sum_{x\\in \\tilde{D}}w_x}, (1\\le k \\le C)\\\\\r\n\\tilde{r}_v = \\frac{\\sum_{x\\in \\tilde{D}^v}w_x}{\\sum_{x \\in \\tilde{D}} w_x}, (1 \\le v \\le V)\r\n\\]</span> 显然，<span class=\"math inline\">\\(\\rho\\)</span>表示属性无缺失样本所占比例，<span class=\"math inline\">\\(\\tilde{p}_k\\)</span>表示属性无缺失样本中第<span class=\"math inline\">\\(k\\)</span>类所占比例，<span class=\"math inline\">\\(\\tilde{r}_v\\)</span>表示属性无缺失样本中在属性<span class=\"math inline\">\\(a\\)</span>上取值<span class=\"math inline\">\\(a^v\\)</span>的样本比例。</p>\r\n<p>由此推广信息增益为： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\nGain(D, a) &amp;= \\rho \\times Gain(\\tilde{D}, a)\\\\\r\n&amp;=\\rho \\times (Ent(\\tilde{D}) - \\sum\\limits_{v=1}^V \\tilde{r}_v Ent(\\tilde{D}^v))\r\n\\end{aligned}\r\n\\]</span> 其中： <span class=\"math display\">\\[\r\nEnt(\\tilde{D}) = -\\sum\\limits_{k=1}^C \\tilde{p}_k log_2 \\tilde{p}_k\r\n\\]</span> 这样解决了最优划分的属性选择问题，在构造子树时，如果样本<span class=\"math inline\">\\(x\\)</span>在属性<span class=\"math inline\">\\(a\\)</span>上的取值已知，那么<span class=\"math inline\">\\(x\\)</span>划分到相应子节点，且权重保持为<span class=\"math inline\">\\(w_x\\)</span>，如果属性<span class=\"math inline\">\\(a\\)</span>未知，则将<span class=\"math inline\">\\(s\\)</span>划分入所有的子节点，且权重调整为<span class=\"math inline\">\\(\\tilde{r}_v w_x\\)</span>。</p>\r\n<h2 id=\"多变量决策树\">多变量决策树</h2>\r\n<p>叶节点不再针对某个属性，而是针对属性的线性组合进行划分。</p>\r\n<h1 id=\"神经网络\">神经网络</h1>\r\n<h2 id=\"感知机\">感知机</h2>\r\n<p>两层神经元，输入层（没有权重，直接前馈数据）和输出层，输出层是M-P神经元（阈值逻辑单元），感知机只能拟合线性可分的数据，否则其学习过程将变得震荡，难以收敛。</p>\r\n<h2 id=\"bp算法\">BP算法</h2>\r\n<p>对于<span class=\"math inline\">\\(l\\)</span>层神经网络,输入<span class=\"math inline\">\\(x \\in R^n\\)</span>，标签<span class=\"math inline\">\\(y \\in R^c\\)</span>，第<span class=\"math inline\">\\(i\\)</span>层权重表示为<span class=\"math inline\">\\(w_i \\in R^{O_i \\times I_i}, I_1 = n，O_l = c\\)</span>，第<span class=\"math inline\">\\(i\\)</span>层偏移表示为<span class=\"math inline\">\\(b_i \\in R^{O_i}\\)</span>，第<span class=\"math inline\">\\(i\\)</span>层激活函数表示为<span class=\"math inline\">\\(\\sigma_i\\)</span>，这一般是个逐元素函数，第<span class=\"math inline\">\\(i\\)</span>层输入即第<span class=\"math inline\">\\(i-1\\)</span>层的输出，表示为<span class=\"math inline\">\\(l_{i-1}\\)</span>，其中<span class=\"math inline\">\\(l_0 = x, z_i = w_i l_{i-1} + b_i, l_i = \\sigma_i(z_i)\\)</span></p>\r\n<p>loss函数记为<span class=\"math inline\">\\(E(l_l, y)\\)</span>，BP算法每次更新<span class=\"math inline\">\\(w_i = w_i - \\eta \\frac{\\partial E}{\\partial w_i}\\)</span>， <span class=\"math inline\">\\(b_i = b_i - \\eta \\frac{\\partial E}{\\partial b_i}\\)</span>，即让参数像梯度最小的方向前进。</p>\r\n<p>首先定义<span class=\"math inline\">\\(E\\)</span>对<span class=\"math inline\">\\(l_l\\)</span>的偏导为<span class=\"math inline\">\\(\\frac{\\partial E}{\\partial l_l} = E&#39;\\)</span>，这个值由loss函数决定。 因此 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\ndE &amp;= E&#39;^Tdl_l\\\\\r\n&amp;=E&#39;^T(\\sigma_l&#39;(z_l) \\odot (dz_l))\\\\\r\n&amp;=E&#39;^Tdiag(\\sigma_l&#39;(z_l))da_l\\\\\r\n\\Rightarrow \\frac{\\partial E}{\\partial z_l} &amp;= diag(\\sigma_l&#39;(z_l))E&#39;\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>这里把<span class=\"math inline\">\\(\\frac{\\partial E}{\\partial z_l}\\)</span>记作<span class=\"math inline\">\\(\\delta_l\\)</span></p>\r\n<p>因为： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    da_i &amp;= w_idl_{i-1}\\\\\r\n    &amp;= w_i(\\sigma_i&#39;(z_{i-1}) \\odot (dz_{i-1}))\\\\\r\n    &amp;=w_idiag(\\sigma_{i-1}&#39;(z_{i-1}))dz_{i-1}\\\\\r\n    \\Rightarrow \\frac{\\partial z_i}{\\partial z_{i-1}} &amp;= diag(\\sigma_{i-1}&#39;(z_{i-1}))w_i^T\\\\\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>所以定义: <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\delta_i &amp;= \\frac{\\partial E}{\\partial z_i},\\ i=1,2,...,l-1\\\\\r\n    \\Rightarrow \\delta_{i-1} &amp;= \\frac{\\partial z_i}{\\partial z_{i-1}}\\frac{\\partial E}{\\partial z_i},\\ i=2,...,l\\\\\r\n    &amp;= diag(\\sigma_{i-1}&#39;(z_{i-1}))w_i^T\\delta_i\\\\\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>现在再来考虑<span class=\"math inline\">\\(E\\)</span>对<span class=\"math inline\">\\(w_{l-k}\\)</span>的导数： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    dE &amp;= \\frac{\\partial E}{\\partial z_{l-k}}^Tdz_{l-k}\\\\\r\n    &amp;= \\delta_{l-k}^T(dw_{l-k}l_{l-k-1} + db_{l-k})\\\\\r\n    &amp;= tr(\\delta_{l-k}^Tdw_{l-k}l_{l-k-1} + \\delta_{l-k}^Tdb_{l-k})\\\\\r\n    &amp;= tr(l_{l-k-1}\\delta_{l-k}^Tdw_{l-k} + \\delta_{l-k}^Tdb_{l-k})\\\\\r\n    \\Rightarrow \\frac{\\partial E}{\\partial w_{l-k}} &amp;= \\delta_{l-k}l_{l-k-1}^T\\\\\r\n    \\Rightarrow \\frac{\\partial E}{\\partial b_{l-k}} &amp;= \\delta_{l-k}\r\n\\end{aligned}\r\n\\]</span> 这里的变换属于标量对矩阵求导<span class=\"math inline\">\\(d f = tr((\\frac{\\partial f}{\\partial X}) ^ T dX)\\)</span>，且用到了迹的一个性质：<span class=\"math inline\">\\(tr(A B) = tr(B A)\\)</span>，其中<span class=\"math inline\">\\(A\\)</span>和<span class=\"math inline\">\\(B^T\\)</span>大小相同</p>\r\n<p>全连接层的BP算法看起来很复杂，其实非常简单，只要使用以下几个等式即可求出任一层的权重和偏置的导数： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\delta_l = \\frac{\\partial E}{\\partial z_l} &amp;= diag(\\sigma_l&#39;(z_l))E&#39;\\\\\r\n    \\delta_{i} = \\frac{\\partial E}{\\partial z_i} &amp;= diag(\\sigma_i&#39;(z_i))w_{i+1}^T\\delta_{i+1},\\ i=1,2,...,l-1\\\\\r\n    \\frac{\\partial E}{\\partial w_i} &amp;= \\delta_il_{i-1}^T,\\ i=1,2,...,l\\\\\r\n    \\frac{\\partial E}{\\partial b_i} &amp;= \\delta_i,\\ i=1,2,...,l\r\n\\end{aligned}\r\n\\]</span></p>\r\n<h2 id=\"rbfradial-basis-function径向基函数网络\">RBF（Radial Basis Function，径向基函数）网络</h2>\r\n<p>RBF网络是指使用径向基函数作为隐层激活函数的单隐层前馈神经网络<span class=\"math inline\">\\(\\phi(x) = \\sum\\limits_{i=1}^q w_i\\rho(x, c_i)\\)</span>，常用的高斯径向基函数<span class=\"math inline\">\\(\\rho(x, c_i) = e^{-\\beta_i||x-c_i||^2}\\)</span>，其中<span class=\"math inline\">\\(c_i,w_i\\)</span>分别表示第<span class=\"math inline\">\\(i\\)</span>个神经元的中心和权重。</p>\r\n<p><span class=\"math inline\">\\(c_i\\)</span>可以使用随机采样或者聚类来获得，而其他参数<span class=\"math inline\">\\(w_i, \\beta_i\\)</span>由BP算法等方法来获得。</p>\r\n<h1 id=\"svm\">SVM</h1>\r\n<p>支持向量机的相关内容可以见我的另一篇文章<a href=\"#\">Post not found: SVM学习笔记 SVM学习笔记</a>，这里不再重新做笔记。</p>\r\n<h1 id=\"贝叶斯分类器\">贝叶斯分类器</h1>\r\n<h2 id=\"条件风险\">条件风险</h2>\r\n<p>条件风险<span class=\"math inline\">\\(R(c_i|x) = \\sum\\limits_{j=1}^N \\lambda_{ij}P(c_j|x)\\)</span>其中<span class=\"math inline\">\\(\\lambda_{ij}\\)</span>表示将<span class=\"math inline\">\\(j\\)</span>类样本分类为<span class=\"math inline\">\\(i\\)</span>时的损失。</p>\r\n<p>机器学习的任务是寻找一个判定准则<span class=\"math inline\">\\(h:x \\rightarrow y\\)</span>以最小化总体风险<span class=\"math inline\">\\(\\min\\limits_{h} R(h)=E_x[R(h(x)|x)]\\)</span>，即在每个样本上选择<span class=\"math inline\">\\(h^\\star(x) = \\mathop{\\arg\\max}\\limits_c R(c|x)\\)</span>，这样的分类器<span class=\"math inline\">\\(h(x)\\)</span>被称为贝叶斯最优分类器。与之对应的总体风险<span class=\"math inline\">\\(R(h^\\star)\\)</span>被称为贝叶斯风险，<span class=\"math inline\">\\(1-R(h^\\star)\\)</span>是分类器能达到的最好性能，即通过机器学习能产生的模型精度理论上限。</p>\r\n<p>如果<span class=\"math inline\">\\(\\lambda_{ij} = \\begin{cases}1&amp;i \\ne j\\\\0&amp;i = j\\end{cases}\\)</span>，那么条件风险将变成<span class=\"math inline\">\\(R(c|x) = 1-P(c|x)\\)</span>，于是最小化分类错误率的贝叶斯最优分类器变成<span class=\"math inline\">\\(h^\\star(x) = \\mathop{\\arg\\max}\\limits_c P(c|x)\\)</span>即最大化后验概率。</p>\r\n<h2 id=\"生成式与判别式模型\">生成式与判别式模型</h2>\r\n<p>在使用贝叶斯分类器时，需要获取后验概率<span class=\"math inline\">\\(P(c|x)\\)</span>，但是非常难以直接获取，因此有两种方式：</p>\r\n<p>第一种是直接对<span class=\"math inline\">\\(P(c|x)\\)</span>进行建模，称为判别式方法。</p>\r\n<p>第二种是生成式模型，考虑<span class=\"math inline\">\\(P(c|x) = \\frac{P(x, c)}{P(x)}=\\frac{P(c)P(x|c)}{p(x)}\\)</span>，其中<span class=\"math inline\">\\(P(c)\\)</span>称为类先验概率，<span class=\"math inline\">\\(P(x|c)\\)</span>是样本<span class=\"math inline\">\\(x\\)</span>相对于类别<span class=\"math inline\">\\(c\\)</span>的类条件概率（似然），<span class=\"math inline\">\\(P(c)\\)</span>可以在样本量足够大时用各类样本出现的频率来估计（大数定律），但是<span class=\"math inline\">\\(P(x|c)\\)</span>非常难估计，因为这涉及到关于<span class=\"math inline\">\\(x\\)</span>的所有属性的联合概率，很难直接用样本出现的频率来进行估计。</p>\r\n<h2 id=\"极大似然估计\">极大似然估计</h2>\r\n<p>再生成式模型中，估计<span class=\"math inline\">\\(P(x|c)\\)</span>的一种策略是首先确定其满足某种确定的概率分布形式，假设<span class=\"math inline\">\\(P(x|c)\\)</span>被参数向量<span class=\"math inline\">\\(\\theta_c\\)</span>唯一确定，因此<span class=\"math inline\">\\(P(x|c)\\)</span>可以被记为<span class=\"math inline\">\\(P(x|\\theta_c)\\)</span>，概率模型的训练过程就是参数估计的过程，参数估计有两种方案：</p>\r\n<ul>\r\n<li>参数虽然未知，但是却是客观存在的固定值（频率主义学派）</li>\r\n<li>参数也是一种未观察到的随机变量，其本身也满足一定的分布，因此需要先假设参数服从一个先验分布，然后基于观测数据来计算参数的后验分布（贝叶斯学派）</li>\r\n</ul>\r\n<p>极大似然估计属于频率主义学派，将参数当成未知的固定值来处理。首先令<span class=\"math inline\">\\(D_c\\)</span>表示训练集<span class=\"math inline\">\\(D\\)</span>的第<span class=\"math inline\">\\(c\\)</span>类样本集合，并假设这些样本独立同分布，因此其似然可以表示为<span class=\"math inline\">\\(P(D_c|\\theta_c) = \\prod\\limits_{x \\in D_c}P(x|\\theta_c)\\)</span>，极大化似然，就能找到参数<span class=\"math inline\">\\(\\theta_c\\)</span>。</p>\r\n<p>似然的表达式中有连乘，容易造成下溢，因此通常使用对数似然<span class=\"math inline\">\\(\\log P(D_c|\\theta_c) = \\sum\\limits_{x \\in D_c} \\log P(x|\\theta_c)\\)</span>。</p>\r\n<p>像极大似然法这种参数化的方法的准确性严重依赖于对<span class=\"math inline\">\\(P(x|\\theta_c)\\)</span>分布的假设，在实际应用中需要利用应用任务的经验知识，才能得到比较好的分类器。</p>\r\n<h2 id=\"朴素贝叶斯分类器\">朴素贝叶斯分类器</h2>\r\n<p>在生成式模型中，<span class=\"math inline\">\\(P(c|x) = \\frac{P(x, c)}{P(x)}=\\frac{P(c)P(x|c)}{P(x)}\\)</span>，这里估计<span class=\"math inline\">\\(P(x|c)\\)</span>的困难在于类条件概率<span class=\"math inline\">\\(P(x|c)\\)</span>是<span class=\"math inline\">\\(x\\)</span>所有属性的联合分布，难以从有限的训练样本中估计得到。</p>\r\n<p>朴素贝叶斯分类器采用属性条件独立性假设：对所有已知类别，样本<span class=\"math inline\">\\(x\\)</span>的所有属性相互独立。</p>\r\n<p>因此<span class=\"math inline\">\\(P(c|x) = \\frac{P(c)P(x|c)}{P(x)} = \\frac{P(c)}{P(x)} \\prod\\limits_{i=1}^dP(x_i|c)\\)</span>，其中<span class=\"math inline\">\\(d\\)</span>为样本的属性数量，<span class=\"math inline\">\\(x_i\\)</span>表示样本<span class=\"math inline\">\\(x\\)</span>的第<span class=\"math inline\">\\(i\\)</span>个属性值。</p>\r\n<p>对于所有类别来说，<span class=\"math inline\">\\(P(x)\\)</span>相同，因此基于<span class=\"math inline\">\\(\\lambda_{ij} = \\begin{cases}1&amp;i \\ne j\\\\0&amp;i = j\\end{cases}\\)</span>，<span class=\"math inline\">\\(h^\\star(x) = \\mathop{\\arg\\max}\\limits_c P(c|x)\\)</span>即最大化后验概率的朴素贝叶斯分类器就可以表达为<span class=\"math inline\">\\(h_{nb}(x) = \\mathop{\\arg\\max}\\limits_c P(c)\\prod\\limits_{i=1}^dP(x_i|c)\\)</span></p>\r\n<p>对于类先验概率，可以从训练集中使用<span class=\"math inline\">\\(P(c) = \\frac{|D_c|}{|D|}\\)</span>估计。</p>\r\n<p>对于离散属性，估计<span class=\"math inline\">\\(P(x_i|c)\\)</span>的方式常用<span class=\"math inline\">\\(P(x_i|c) = \\frac{|D_{c,x_i}|}{|D_c|}\\)</span>，其中<span class=\"math inline\">\\(D_{c,x_i}\\)</span>表示<span class=\"math inline\">\\(D_c\\)</span>中第<span class=\"math inline\">\\(i\\)</span>个属性取值为<span class=\"math inline\">\\(x_i\\)</span>的样本集合。</p>\r\n<p>对于连续属性，则可以使用概率密度函数，假定<span class=\"math inline\">\\(P(x_i|c)\\)</span>服从某种分布，然后对其进行估计。</p>\r\n<p>在离散属性的处理上，有个问题是：如果训练集中某种属性在类别<span class=\"math inline\">\\(c\\)</span>上没有出现，或者类别<span class=\"math inline\">\\(c\\)</span>在训练集上没有出现，则<span class=\"math inline\">\\(P(c)\\prod\\limits_{i=1}^dP(x_i|c)\\)</span>直接就为0了，因此需要进行修正平滑处理。</p>\r\n<p>常用的是拉普拉斯修正，即将<span class=\"math inline\">\\(P(c) = \\frac{|D_c|}{|D|}\\)</span>更改为<span class=\"math inline\">\\(P(c) = \\frac{|D_c|+1}{|D|+N}\\)</span>，其中N表示类别个数, 将<span class=\"math inline\">\\(P(x_i|c) = \\frac{|D_{c,x_i}|}{|D_c|}\\)</span>更改为<span class=\"math inline\">\\(P(x_i|c) = \\frac{|D_{c,x_i}| + 1}{|D_c| + N_i}\\)</span>，其中<span class=\"math inline\">\\(N_i\\)</span>表示第<span class=\"math inline\">\\(i\\)</span>个属性的可取值个数。这样可以避免因训练集样本不充分而导致的概率估计为零的问题。</p>\r\n<h2 id=\"半朴素贝叶斯分类器\">半朴素贝叶斯分类器</h2>\r\n<p>由于属性条件独立性假设很难成立，因此尝试对条件独立性假设进行一定程度的放松。</p>\r\n<p>例如独依赖估计（One-Dependent Estimator，ODE），即每个属性在类别之外最多依赖一个其他属性：<span class=\"math inline\">\\(P(c|x)\\propto P(c) \\prod\\limits_{i=1}^dP(x_i|c,pa_i)\\)</span>，其中<span class=\"math inline\">\\(pa_i\\)</span>为属性<span class=\"math inline\">\\(x_i\\)</span>所依赖的属性，称为<span class=\"math inline\">\\(x_i\\)</span>的父属性。</p>\r\n<p>问题的关键在于如何确定父属性，最直接的办法是假设所有属性都依赖于同一个父属性，称为超父（super-parent）由此形成了SPODE(Super-Parent ODE)方法。</p>\r\n<h1 id=\"集成学习\">集成学习</h1>\r\n<p>构建并结合多个学习器来完成学习任务。一般是先产生一组个体学习器，然后使用某种策略将他们结合。</p>\r\n<p>根据个体学习器的生成方式，集成学习方法大致可以分为两类： - 个体学习器之间存在强依赖关系，必须串行生成，例如：Boosting。 - 个体学习器之间不存在强依赖关系，可以同时并行生成，例如：Bagging和随机森林（Random Forest）</p>\r\n<h2 id=\"boosting\">Boosting</h2>\r\n<p>先训练一个基学习器，然后使用基学习器对训练样本分布进行调整，使得基学习器预测错误的样本在后续得到更多关注，然后进行下一个基学习器的训练，直到学习器数量达到指定数量。</p>\r\n<p>Boosting中最著名的是AdaBoosting，可以理解为加性模型，即使用基学习器的线性组合<span class=\"math inline\">\\(H(x) = \\sum\\limits_{t=1}^T\\alpha_t h_t (x)\\)</span>来最小化指数损失函数<span class=\"math inline\">\\(l_{exp}(H|D) = E_{x \\sim D}[e^{-f(x)H(x)}]\\)</span>，其中<span class=\"math inline\">\\(h_i\\)</span>表示第<span class=\"math inline\">\\(i\\)</span>个基分类器，<span class=\"math inline\">\\(f(x)\\)</span>表示真实函数<span class=\"math inline\">\\(f(x) = y\\)</span>。</p>\r\n<p>AdaBoosting只适用于二分类任务。</p>\r\n<h2 id=\"bagging\">Bagging</h2>\r\n<p>在原始数据中每次有放回的采样<span class=\"math inline\">\\(m\\)</span>个样本，组成一个子训练集，最终得到共<span class=\"math inline\">\\(T\\)</span>个子训练集，用每个子训练集训练一个基学习器，再将所有基学习器进行结合。</p>\r\n<p>假设基学习器的计算复杂度为<span class=\"math inline\">\\(O(m)\\)</span>，Bagging的复杂度大致为<span class=\"math inline\">\\(T(O(m) + O(s))\\)</span>，其中<span class=\"math inline\">\\(O(s)\\)</span>为投票和采样的复杂度，一般非常小，而<span class=\"math inline\">\\(T\\)</span>是个比较小的常数，因此Bagging的训练复杂度和训练一个学习器的复杂程度同阶，非常高效。</p>\r\n<h2 id=\"bagging和boosting的区别\">Bagging和Boosting的区别</h2>\r\n<h2 id=\"随机森林random-forestrf\">随机森林（Random Forest，RF）</h2>\r\n<p>以决策树为基学习器的Bagging集成基础上，引入随机属性选择。在构造决策树过程中，选择最优划分属性时，可以在当前可用属性的随机子集中进行选择。</p>\r\n<h2 id=\"集成学习的结合策略\">集成学习的结合策略</h2>\r\n<ul>\r\n<li>平均法</li>\r\n<li>投票法</li>\r\n<li>学习法</li>\r\n</ul>\r\n<p>stacking是学习法的一个例子：先用原始训练集训练出初级学习器，然后将初级学习器的输出作为输入特征，训练一个次级学习器，用于结合初级学习器输出得到最终输出。最好在初级学习器的验证集上对次级学习器进行训练。</p>\r\n<h2 id=\"多样性\">多样性</h2>\r\n<p>如果用<span class=\"math inline\">\\(h_1,h_2,...,h_T\\)</span>通过加权平均法来集成得到<span class=\"math inline\">\\(H\\)</span>来估计真实函数<span class=\"math inline\">\\(f:R^d \\rightarrow R\\)</span>，则对样本<span class=\"math inline\">\\(x\\)</span>，定义学习器<span class=\"math inline\">\\(h_i\\)</span>的分歧为：<span class=\"math inline\">\\(A(h_i|x) = (h(x_i) - H(x))^2\\)</span>。集成的分歧表示为<span class=\"math inline\">\\(\\overline{A}(H|x) = \\sum\\limits_{i=1}^T w_i A(h_i|x)=\\sum\\limits_{i=1}^Tw_i(h_i(x) - H(x))^2\\)</span>。分歧表示个体学习器在样本<span class=\"math inline\">\\(x\\)</span>上的不一致性。</p>\r\n<p>而集成的泛化误差可以表示为<span class=\"math inline\">\\(E=\\overline{E} - \\overline{A}\\)</span>（<span class=\"math inline\">\\(\\overline{E} = \\sum\\limits_{i=1}^Tw_iE_i\\)</span>，<span class=\"math inline\">\\(\\overline{A}=\\sum_{i=1}^Tw_iA_i\\)</span>，推导过程略过），这说明个体学习器误差越低，多样性越大，集成的效果越好。</p>\r\n<p>增加个体学习器多样性的方法： - 数据样本扰动：通常基于采样法，产生不同的数据子集。但是有些学习器对数据样本量的扰动不敏感（如线性学习器，支持向量机，朴素贝叶斯，k近邻，这些学习器称为稳定基学习器） - 输入属性扰动：抽取属性子集来训练学习器。不适合属性较少的数据。 - 输出表示扰动：将样本的类别标记稍作改动，例如随机改变一些标记，或者进行编码调制，例如ECOC。 - 算法参数扰动：随机设置不同的参数，例如隐层神经元个数、初始连接权值等。</p>\r\n<h1 id=\"聚类\">聚类</h1>\r\n<h2 id=\"原型聚类\">原型聚类</h2>\r\n<p>又称基于原型的聚类方法，此类算法假设聚类结构可以通过一组原型来刻画，通常先对原型进行初始化，然后对原型进行迭代更新求解。不同的原型表示、不同的求解方式就会产生不同的算法</p>\r\n<h3 id=\"k均值算法\">k均值算法</h3>\r\n<p>给定样本集<span class=\"math inline\">\\(D=\\{x_1,x_2,...,x_m\\}\\)</span>，聚类结果<span class=\"math inline\">\\(C = \\{C_1, C_2, ...,C_k\\}\\)</span>，定义平方误差<span class=\"math inline\">\\(E = \\sum\\limits_{i=1}^k \\sum\\limits_{x\\in C_i} ||x-\\mu_i||^2_2\\)</span>，其中<span class=\"math inline\">\\(\\mu_i = \\frac{1}{C_i}\\sum\\limits_{x \\in C_i}x\\)</span>表示簇<span class=\"math inline\">\\(C_i\\)</span>的均值向量。</p>\r\n<p>k均值算法针对平方误差<span class=\"math inline\">\\(E\\)</span>进行优化，使用贪心策略，通过迭代优化方式来进行：</p>\r\n<p>1、初始化<span class=\"math inline\">\\(k\\)</span>个初始均值向量<span class=\"math inline\">\\(\\mu_1, \\mu_2, ..., \\mu_k\\)</span>。</p>\r\n<p>2、计算每个样本到<span class=\"math inline\">\\(k\\)</span>个均值向量的值，将每个样本划入最近的均值向量对应的簇中，得到一个划分。</p>\r\n<p>3、使用划分好的簇计算新的均值向量。</p>\r\n<p>4、如果没有均值向量被大幅更新或者达到了最大迭代次数，那么停止，否则从第2步继续循环。</p>\r\n<h3 id=\"学习向量量化learning-vector-quantizationlvq\">学习向量量化（Learning Vector Quantization，LVQ）</h3>\r\n<p>和k均值算法类似，但是学习向量量化在学习过程中还利用样本的监督信息来辅助聚类：</p>\r\n<p>1、初始化原型向量<span class=\"math inline\">\\(\\{p_1, p_2, ..., p_q\\}\\)</span></p>\r\n<p>2、随机选取样本<span class=\"math inline\">\\(x_j\\)</span>，计算<span class=\"math inline\">\\(x_j\\)</span>和每个原型向量的距离，并找出距离最小对应的原型向量。</p>\r\n<p>3、如果<span class=\"math inline\">\\(x_j\\)</span>的标签和原型向量的标签相同，则使用<span class=\"math inline\">\\(p=p + \\eta (x_j - p)\\)</span>来对选出的原型向量进行更新，否则使用<span class=\"math inline\">\\(p = p - \\eta (x_j - p)\\)</span>来对选出的原型向量进行更新</p>\r\n<p>4、达到最大迭代轮数或者更新幅度很小则停止更新，否则从第2步继续循环。</p>\r\n<p>其中<span class=\"math inline\">\\(\\eta \\in (0, 1)\\)</span>表示学习速率，在迭代停止之后，对于任意样本<span class=\"math inline\">\\(x\\)</span>，可以将其划分到与其距离最近的原型向量所代表的的簇中。</p>\r\n<h3 id=\"高斯混合聚类mixture-of-gaussian\">高斯混合聚类（Mixture-of-Gaussian）</h3>\r\n<p>假设数据服从高斯混合分布<span class=\"math inline\">\\(p_M(x) = \\sum\\limits_{i=1}^k \\alpha_i p(x|\\mu_i, \\Sigma_i), \\sum\\limits_{i=1}^k \\alpha_i=1\\)</span>。</p>\r\n<p>令随机变量<span class=\"math inline\">\\(z_j,j\\in \\{1,2,...,k\\}\\)</span>表示样本<span class=\"math inline\">\\(x_j\\)</span>预测类别，其先验概率<span class=\"math inline\">\\(p(z_j = i) = \\alpha_i\\)</span>，则<span class=\"math inline\">\\(p_M(z_j = i|x_j) = \\frac{\\alpha_ip(x_i|\\mu_i,\\Sigma_i)}{\\sum\\limits_{l=1}^k \\alpha_lp(x_j|\\mu_l, \\Sigma_l)}\\)</span></p>\r\n<p>当高斯混合分布已知时，即可将样本划分成<span class=\"math inline\">\\(k\\)</span>个簇，其标记为<span class=\"math inline\">\\(\\mathop{\\arg\\max}\\limits_i p_M(z_j = i|x_j)\\)</span></p>\r\n<p>估计这样的高斯混合分布可以使用极大似然法，令对数似然<span class=\"math inline\">\\(LL(p_M) = \\sum\\limits_{j=1}^m ln(\\sum\\limits_{i=1}^k \\alpha_ip(x_i|\\mu_i,\\Sigma_i))\\)</span>，使用EM算法即可求解。</p>\r\n<h2 id=\"密度聚类\">密度聚类</h2>\r\n<h3 id=\"dbscan\">DBSCAN</h3>\r\n<p>密度聚类算法的代表是DBSCAN算法，使用一组邻域参数<span class=\"math inline\">\\((\\epsilon, MinPts)\\)</span>来刻画样本分布的紧密程度。</p>\r\n<ul>\r\n<li><span class=\"math inline\">\\(\\epsilon\\)</span>-邻域：<span class=\"math inline\">\\(N_\\epsilon(x_j) = \\{x_i | x_i \\in D, dist(x_j,x_i)\\le \\epsilon\\}\\)</span>，其中<span class=\"math inline\">\\(D\\)</span>表示全部数据集合。</li>\r\n<li>核心对象：满足<span class=\"math inline\">\\(|N_\\epsilon(x_j)| \\ge MinPts\\)</span>的<span class=\"math inline\">\\(x_j\\)</span>称为核心对象，其中<span class=\"math inline\">\\(|N_\\epsilon(x_j)|\\)</span>表示<span class=\"math inline\">\\(x_j\\)</span>的<span class=\"math inline\">\\(\\epsilon\\)</span>-邻域中的样本数量。</li>\r\n<li>密度直达：<span class=\"math inline\">\\(x_j\\)</span>是一个核心对象，且<span class=\"math inline\">\\(x_i\\)</span>在<span class=\"math inline\">\\(x_j\\)</span>的<span class=\"math inline\">\\(\\epsilon\\)</span>-邻域中，则<span class=\"math inline\">\\(x_i\\)</span>由<span class=\"math inline\">\\(x_j\\)</span>密度直达。</li>\r\n<li>密度可达：如果存在样本序列<span class=\"math inline\">\\(p_1, p_2, ..., p_n, p_1 = x_j, p_n = x_i\\)</span>使得<span class=\"math inline\">\\(p_{m+1}\\)</span>由<span class=\"math inline\">\\(p_m\\)</span>密度直达，则称<span class=\"math inline\">\\(x_i\\)</span>由<span class=\"math inline\">\\(x_j\\)</span>密度可达。</li>\r\n<li>密度相连：如果存在<span class=\"math inline\">\\(x_k\\)</span>使得<span class=\"math inline\">\\(x_j\\)</span>和<span class=\"math inline\">\\(x_i\\)</span>均可由<span class=\"math inline\">\\(x_k\\)</span>密度可达，则称<span class=\"math inline\">\\(x_i\\)</span>和<span class=\"math inline\">\\(x_j\\)</span>密度相连。</li>\r\n</ul>\r\n<p>基于以上概念，DBSCAN将簇定义为由密度可达关系到处的最大密度相连样本集合。</p>\r\n<p>DBSCAN的聚类过程如下：</p>\r\n<p>1、找到所有的核心对象。</p>\r\n<p>2、循环随机取一个还没有访问过的核心对象，将其所有密度相连的样本生成一个簇并标记为已访问，如果没有未访问的核心对象，则停止循环</p>\r\n<p>3、仍未访问的样本被视为噪声样本。</p>\r\n<h2 id=\"层次聚类\">层次聚类</h2>\r\n<p>层次聚类方法试图在不同层次上对数据进行划分，形成树形的聚类结构。</p>\r\n<h3 id=\"agnes\">AGNES</h3>\r\n<p>AGNES是一种自底向上的层次聚类算法，首先将所有样本单独看做一个簇，然后每次迭代找到距离最近的两个簇进行合并，直到簇数量等于指定数量。</p>\r\n<p>这里的关键是如何定义两个簇的距离，主要有三种方式，使用三种距离计算的AGNES分别被称为： - 最大距离：单链接算法 - 最小距离：全链接算法 - 平均距离：均链接算法</p>\r\n<h1 id=\"降维与度量学习\">降维与度量学习</h1>\r\n<h2 id=\"k近邻k-nearest-neighbor学习\">k近邻（k-Nearest Neighbor）学习</h2>\r\n<p>k近邻方法没有训练过程，在给定测试样本时，直接使用训练样本中与其最靠近的<span class=\"math inline\">\\(k\\)</span>个样本，基于这<span class=\"math inline\">\\(k\\)</span>个样本的信息来对测试样本进行预测。</p>\r\n<p>k近邻方法是懒惰学习（lazy learning）的一个代表，而那些在训练阶段就对样本进行学习处理的方法称为急切学习（eager learning）</p>\r\n<h2 id=\"低维嵌入\">低维嵌入</h2>\r\n<p>大部分时候，观测到的数据是高维数据，但与学习任务密切相关的很可能是高维空间中的一个低维嵌入。</p>\r\n<h3 id=\"多维缩放multiple-dimensional-scalingmds\">多维缩放（Multiple Dimensional Scaling，MDS）</h3>\r\n<p>多维缩放的思路是找到一个低维空间，使得在这个低维空间中的欧氏距离和原始空间中的距离相等。</p>\r\n<p>假如原始空间的维度为<span class=\"math inline\">\\(d\\)</span>，所有数据的距离矩阵<span class=\"math inline\">\\(D\\in R^{m \\times m}\\)</span>，其中<span class=\"math inline\">\\(m\\)</span>为样本数量，<span class=\"math inline\">\\(d_{ij}\\)</span>表示样本<span class=\"math inline\">\\(x_i\\)</span>和<span class=\"math inline\">\\(x_j\\)</span>的距离，降维后的数据<span class=\"math inline\">\\(z \\in R^{d&#39;}\\)</span>，所有样本表示为<span class=\"math inline\">\\(Z\\in R^{d&#39; \\times m}\\)</span>。</p>\r\n<p>令<span class=\"math inline\">\\(B = Z^T Z \\in R^{m\\times m}\\)</span>是降维后的内积矩阵，有<span class=\"math inline\">\\(b_{ij} = z_i^T z_j\\)</span></p>\r\n<p>则<span class=\"math inline\">\\(d_{ij}^2 = ||z_i||^2 + ||z_j||^2 - 2z_i^T z_j = b_{ii} + b_{jj} - 2b_{ij}\\)</span></p>\r\n<p>令降维后的样本<span class=\"math inline\">\\(Z\\)</span>被中心化，即<span class=\"math inline\">\\(\\sum\\limits_{i=1}^m z_i = \\mathbf{0}\\)</span>，可得到<span class=\"math inline\">\\(\\sum\\limits_{i=1}^m b_{ij} = \\sum\\limits_{j=1}^m b_{ij} = 0\\)</span></p>\r\n<p>因此：</p>\r\n<p><span class=\"math display\">\\[\r\n\\sum\\limits_{i=1}^md_{ij}^2 = tr(B) + mb_{jj}\\\\\r\n\\sum\\limits_{j=1}^md_{ij}^2 = tr(B) + mb_{ii}\\\\\r\n\\sum\\limits_{i=1}^m\\sum\\limits_{j=1}^md_{ij}^2 = 2m\\ tr(B)\\\\\r\ntr(B) = \\sum\\limits_{i=1}^m b_{ii}\r\n\\]</span></p>\r\n<p>则有：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    b_{ij} &amp;= \\frac{b_{ii} + b_{jj} - d_{ij}^2}{2}\\\\\r\n    &amp;=\\frac{1}{2m}(\\sum\\limits_{i=1}^md_{ij}^2) + \\frac{1}{2m}(\\sum\\limits_{j=1}^md_{ij}^2) - \\frac{1}{2m^2} \\sum\\limits_{i=1}^m\\sum\\limits_{j=1}^md_{ij}^2 - \\frac{d_{ij}^2}{2}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>这样就能根据原来的距离矩阵求出内积矩阵<span class=\"math inline\">\\(B\\)</span>，再对<span class=\"math inline\">\\(B\\)</span>进行特征分解得到<span class=\"math inline\">\\(B=V \\Lambda V^T\\)</span>，其中<span class=\"math inline\">\\(\\Lambda = diag(\\lambda_1, ..., \\lambda_d)\\)</span>为特征值构成的对角阵，<span class=\"math inline\">\\(\\lambda_1 \\ge \\lambda_2\\ge ...\\ge \\lambda_d\\)</span>，为了降维，我们可以取其中<span class=\"math inline\">\\(d&#39;\\)</span>个非零值，构成对角矩阵<span class=\"math inline\">\\(\\Lambda_\\star\\)</span>与其对应的特征向量矩阵<span class=\"math inline\">\\(V_\\star\\)</span>，则<span class=\"math inline\">\\(Z = \\Lambda_\\star ^{\\frac{1}{2}} V_\\star^T \\in R^{d&#39; \\times m}\\)</span></p>\r\n<h2 id=\"主成分分析pca\">主成分分析（PCA）</h2>\r\n<p>如果希望使用一个超平面来对数据进行表示，那么可以从两个方面去考虑： - 最近重构性：样本点到这个超平面的距离都足够近 - 最大可分性：样本点在这个超平面上的投影尽可能分开</p>\r\n<p>但是两个方面的考虑最终都会得到PCA的等价推导，即PCA既保证了最近重构性也保证了最大可分性。</p>\r\n<p>假定数据<span class=\"math inline\">\\(x_i \\in R^d\\)</span>已经进行过中心化，即<span class=\"math inline\">\\(\\sum_i x_i = \\mathbf{0}\\)</span>，现在使用一组标准正交基对<span class=\"math inline\">\\(x_i\\)</span>进行投影，得到<span class=\"math inline\">\\(z_i = Wx_i, W = \\begin{bmatrix}w_1^T\\\\ w_2^T\\\\ \\vdots\\\\ w_{d&#39;}^T \\end{bmatrix}\\in R^{d&#39; \\times d}, w_i^T w_j = \\begin{cases}1 &amp; i=j\\\\ 0&amp; i\\ne j\\end{cases}\\)</span>，其中<span class=\"math inline\">\\(z_{ij} = w_j^Tx_i\\)</span>，如果使用<span class=\"math inline\">\\(z_i\\)</span>来还原<span class=\"math inline\">\\(x_i\\)</span>则得到<span class=\"math inline\">\\(\\hat{x}_i = \\sum\\limits_{j=1}^{d&#39;}z_{ij}w_j = W^T z_i\\)</span>。</p>\r\n<p>如果从最近重构性来考虑，我们希望最小化<span class=\"math inline\">\\(\\sum\\limits_{i=1}^m ||\\hat{x}_i - x_i||^2_2\\)</span>，即： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    &amp;\\min \\sum\\limits_{i=1}^m ||\\hat{x}_i - x_i||^2_2\\\\\r\n    &amp;=\\min \\sum\\limits_{i=1}^m || \\sum\\limits_{j=1}^{d&#39;}z_{ij}w_j - x_i||^2_2\\\\\r\n    &amp;=\\min \\sum\\limits_{i=1}^m || W^Tz_i - x_i||^2_2\\\\\r\n    &amp;=\\min \\sum\\limits_{i=1}^m (z_i^TWW^Tz_i - z_i^TWx_i - x^T_iW^Tz_i + x^T_i x_i)\\\\\r\n    &amp;=\\min \\sum\\limits_{i=1}^m (z_i^Tz_i - z_i^TWx_i - x^T_iW^Tz_i)\\\\\r\n    &amp;=\\min \\sum\\limits_{i=1}^m tr(z_i^Tz_i - z_i^TWx_i - x^T_iW^Tz_i)\\\\\r\n    &amp;=\\min \\sum\\limits_{i=1}^m tr(z_i^Tz_i - 2z_i^TWx_i)\\\\\r\n    &amp;=\\min \\sum\\limits_{i=1}^m -tr(z_i^T z_i)\\\\\r\n    &amp;=\\min -tr(Z^T Z),\\ Z = \\begin{bmatrix}z_1 &amp;z_2 &amp; \\cdots &amp;z_m\\end{bmatrix} = WX,\\ X = \\begin{bmatrix}x_1 &amp;x_2 &amp; \\cdots &amp;x_m\\end{bmatrix}\\\\\r\n    &amp;=\\min -tr(X^TW^TWX)\\\\\r\n    &amp;=\\min -tr(WXX^TW^T)\\\\\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>因此我们需要解决的问题就是： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\min\\ &amp; -tr(WXX^TW^T)\\\\\r\n    s.t.\\ &amp; WW^T = I_{d&#39;}\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>另一方面，如果从最大可分性来考虑，我们希望最大化<span class=\"math inline\">\\(z_i\\)</span>之间的方差<span class=\"math inline\">\\(\\sum\\limits_{i=1}^m (z_i - \\frac{1}{m}\\sum\\limits_{j=1}^mz_j)^2\\)</span></p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    &amp;\\max \\sum\\limits_{i=1}^m ||z_i - \\frac{1}{m}\\sum\\limits_{j=1}^mz_j||_2^2\\\\\r\n    &amp;=\\max \\sum\\limits_{i=1}^m ||z_i - \\frac{1}{m}\\sum\\limits_{j=1}^mWx_j||_2^2\\\\\r\n    &amp;=\\max \\sum\\limits_{i=1}^m ||z_i - \\frac{1}{m}W\\sum\\limits_{j=1}^mx_j||_2^2\\\\\r\n    &amp;=\\max \\sum\\limits_{i=1}^m ||z_i||_2^2\\\\\r\n    &amp;=\\max \\sum\\limits_{i=1}^m (z^T_iz_i)\\\\\r\n    &amp;=\\max\\ tr(Z^TZ)\\\\\r\n    &amp;=\\max\\ tr(X^TW^TWX)\\\\\r\n    &amp;=\\min\\ -tr(X^TW^TWX)\\\\\r\n    &amp;=\\min\\ -tr(WXX^TW^T)\\\\\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>因此我们需要解决的问题就是： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\min\\ &amp; -tr(WXX^TW^T)\\\\\r\n    s.t.\\ &amp; WW^T = I_{d&#39;}\\\\\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>由此可见，在<span class=\"math inline\">\\(\\sum x_i = 0\\)</span>的情况下，从两个方面得到的结果完全相同。</p>\r\n<p>求解PCA可以使用拉格朗日法，首先得到拉格朗日函数<span class=\"math inline\">\\(L(W) = -tr(X^TW^TWX) + \\lambda (WW^T - I_{d&#39;}), \\lambda \\ge 0\\)</span></p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    dL(W) &amp;= -tr(WXX^TdW^T + dWXX^TW^T) + \\lambda tr(dW W^T + WdW^T)\\\\\r\n    &amp;= -tr(2XX^TW^TdW - 2\\lambda W^TdW)\\\\\r\n    \\\\\r\n    \\frac{\\partial dL(W)}{\\partial W} &amp;= 2\\lambda W - 2WXX^T\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>令<span class=\"math inline\">\\(\\frac{\\partial dL(W)}{\\partial W} = 0\\)</span>可得<span class=\"math inline\">\\(\\lambda W^T = XX^TW^T\\)</span>，即求出协方差矩阵<span class=\"math inline\">\\(XX^T\\)</span>的特征向量即可构成<span class=\"math inline\">\\(W^T\\)</span>，在这个过程中，可以舍弃一部分特征向量，只取特征值最大的<span class=\"math inline\">\\(d&#39;\\)</span>个特征向量，即可将数据维度从<span class=\"math inline\">\\(d\\)</span>维缩减到<span class=\"math inline\">\\(d&#39;\\)</span>维。</p>\r\n<p>如果将<span class=\"math inline\">\\(X\\)</span>进行奇异值分解，则有<span class=\"math inline\">\\(X=D\\Sigma V^T\\)</span>，<span class=\"math inline\">\\(XX^T = D \\Sigma \\Sigma^T D^T\\)</span>，其中<span class=\"math inline\">\\(D\\)</span>是<span class=\"math inline\">\\(X\\)</span>的左奇异矩阵，也就是<span class=\"math inline\">\\(XX^T\\)</span>的特征矩阵，如果令<span class=\"math inline\">\\(W=D^T\\)</span>可以得到<span class=\"math inline\">\\(Z=D^TX=D^TD\\Sigma V^T = \\Sigma V^T\\)</span>，因此求出<span class=\"math inline\">\\(X^TX\\)</span>的特征矩阵也可以求出<span class=\"math inline\">\\(Z\\)</span>。</p>\r\n<h2 id=\"核化线性降维\">核化线性降维</h2>\r\n<h3 id=\"核主成分分析kpca\">核主成分分析（KPCA）</h3>\r\n<p>PCA是一种线性降维方式，在降维时假设从高维空间到低维空间的映射是线性的，其线性映射由<span class=\"math inline\">\\(W\\)</span>确定，且有<span class=\"math inline\">\\(\\lambda W^T = XX^TW^T = (\\sum\\limits_{i=1}^m x_i x^T_i)W^T\\)</span>，假设在高维空间<span class=\"math inline\">\\(\\phi(x)\\)</span>中进行PCA，则有<span class=\"math inline\">\\(\\lambda W^T = (\\sum\\limits_{i=1}^m \\phi(x_i) \\phi(x_i)^T)W^T\\)</span>，有如下推导：</p>\r\n<p><span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\lambda W^T &amp;= (\\sum\\limits_{i=1}^m \\phi(x_i) \\phi(x_i)^T)W^T\\\\\r\n    &amp;= (\\sum\\limits_{i=1}^m \\phi(x_i) \\phi(x_i)^T)W^T\\\\\r\n    &amp;= \\sum\\limits_{i=1}^m \\phi(x_i)\\phi(x_i)^TW^T\\\\\r\n    &amp;= \\sum\\limits_{i=1}^m\\phi(x_i)\\alpha_i,\\ \\alpha_i = \\phi(x_i)^TW^T\\\\\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>由于<span class=\"math inline\">\\(\\phi\\)</span>函数无法明确求出，因此引入核函数。 <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\lambda \\phi(x_j)^TW^T &amp;= \\phi(x_j)^T\\sum\\limits_{i=1}^m\\phi(x_i)\\alpha_i\\\\\r\n    \\lambda A&amp;=KA\\\\\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>其中<span class=\"math inline\">\\(K\\)</span>是核矩阵<span class=\"math inline\">\\(K_{ij} = \\phi(x_i)^T\\phi(x_j)\\)</span>，<span class=\"math inline\">\\(A = \\begin{bmatrix}\\alpha_1, \\alpha_2, ..., \\alpha_m\\end{bmatrix}\\)</span></p>\r\n<p>则<span class=\"math inline\">\\(z_{ij} = w_j^T \\phi(x_i) = \\sum\\limits_{k=1}^m\\alpha_{k,j}\\phi(x_k)^T\\phi(x_i)=\\sum\\limits_{k=1}^m\\alpha_{k,j}K_{ki}\\)</span>表示降维之后<span class=\"math inline\">\\(x_i\\)</span>对应向量的第<span class=\"math inline\">\\(j\\)</span>个分量。</p>\r\n<p>其中W可以由<span class=\"math inline\">\\(\\phi(X)^T\\phi(X)\\)</span>的特征矩阵求出，但在计算<span class=\"math inline\">\\(z\\)</span>时，由于每个分量需要求和<span class=\"math inline\">\\(\\sum\\limits_{k=1}^m\\alpha_{k,j}\\)</span>，计算量非常大。</p>\r\n<h2 id=\"流形学习\">流形学习</h2>\r\n<p>流形是指在局部与欧式空间统配的空间，如果数据是嵌入在高维空间中的低维流形，则可以利用其局部与欧式空间同胚的性质，使用局部的欧式距离来计算数据样本之间的距离。</p>\r\n<h3 id=\"等度量映射isometric-mappingisomap\">等度量映射（Isometric Mapping，Isomap）</h3>\r\n<p>将低维嵌入流形上两点的距离定义为“测地线”距离，即两个样本沿着流形的最短距离，测地线距离的计算可以将邻近点之间进行连接，然后转换计算近邻连接图上两点之间的最短路径问题（Dijkstra算法或者Floyd算法）。得到距离矩阵之后，可以使用多维缩放算法（MDS）来进行降维。</p>\r\n<p>邻近图的构建一般有两种做法，一个是指定最近的<span class=\"math inline\">\\(k\\)</span>个点作为邻近点，这样得到的邻近图称为<span class=\"math inline\">\\(k\\)</span>邻近图，另一个是指定距离阈值<span class=\"math inline\">\\(\\epsilon\\)</span>，小于<span class=\"math inline\">\\(\\epsilon\\)</span>的点被认为是邻近点，这样得到的邻近图称为<span class=\"math inline\">\\(\\epsilon\\)</span>邻近图，两种方式各有优劣。</p>\r\n<h3 id=\"局部线性嵌入locally-linear-embeddinglle\">局部线性嵌入（Locally Linear Embedding，LLE）</h3>\r\n<p>Isomap试图保持邻近样本之间的距离，而LLE试图保持邻域内样本的线性关系，假定样本点<span class=\"math inline\">\\(x_i\\)</span>可以通过<span class=\"math inline\">\\(x_j,x_k,x_l\\)</span>线性组合而得到，即<span class=\"math inline\">\\(x_i = w_{ij}x_j+w_{ik}x_k+w_{il}x_l\\)</span></p>\r\n<p>首先对于每个样本<span class=\"math inline\">\\(x_i\\)</span>，找到其邻近下标集合<span class=\"math inline\">\\(Q_i\\)</span>，然后计算<span class=\"math inline\">\\(Q_i\\)</span>对<span class=\"math inline\">\\(x_i\\)</span>的线性重构系数： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\min\\limits_{w_1, w_2, ..., w_m}\\ &amp;\\sum\\limits_{i=1}^m ||x_i - \\sum\\limits_{j\\in Q_i}w_{ij} x_j||_2^2\\\\\r\n    s.t.\\ &amp;\\sum\\limits_{j\\in Q_i} w_{ij} = 1\r\n\\end{aligned}\r\n\\]</span></p>\r\n<p>求得<span class=\"math inline\">\\(w_{ij}\\)</span>之后，<span class=\"math inline\">\\(x_i\\)</span>对应的低维空间坐标<span class=\"math inline\">\\(z_i = \\min\\limits_{z_1, z_2, ..., z_m} \\sum\\limits_{i=1}^m ||z_i - \\sum\\limits_{j\\in Q_i} w_{ij} z_j||_2^2\\)</span>。</p>\r\n<p>令<span class=\"math inline\">\\(Z = \\begin{bmatrix}z_1 &amp; z_2 &amp; \\cdots &amp; z_m\\end{bmatrix} \\in R^{d&#39; \\times m}, W_{ij} = w_{ij}\\)</span></p>\r\n<p>则确定<span class=\"math inline\">\\(W\\)</span>之后，<span class=\"math inline\">\\(Z\\)</span>可以通过： <span class=\"math display\">\\[\r\n\\begin{aligned}\r\n    \\min\\limits_Z\\ &amp;tr(Z(\\mathbf{I} - W)^T(\\mathbf{I} - W)Z^T)\\\\\r\n    s.t.\\ &amp;ZZ^T = \\mathbf{I}\r\n\\end{aligned}\r\n\\]</span> 来求得，即对<span class=\"math inline\">\\((\\mathbf{I} - W)^T(\\mathbf{I} - W)\\)</span>进行特征分解，取最小的<span class=\"math inline\">\\(d&#39;\\)</span>个特征值对应的特征向量构成<span class=\"math inline\">\\(Z^T\\)</span></p>\r\n<h2 id=\"度量学习\">度量学习</h2>\r\n<p>对高维数据的降维主要是希望找到一个合适的低维空间使得此空间中学习能比原始空间性能更好，度量学习的思路是尝试学习出一个距离度量。</p>\r\n<p>对于两个<span class=\"math inline\">\\(d\\)</span>维样本<span class=\"math inline\">\\(x_i\\)</span>，<span class=\"math inline\">\\(x_j\\)</span>，其欧氏距离的平方<span class=\"math inline\">\\(||x_i - x_j||^2_2 = dist_{ij,1} + dist_{ij,2} + \\cdots + dist_{ij,d}\\)</span>，其中<span class=\"math inline\">\\(dist_{ij,k}\\)</span>表示在第<span class=\"math inline\">\\(k\\)</span>维上的距离。</p>\r\n<p>如果假定不同属性的重要性不同，则可以引入权重<span class=\"math inline\">\\(w\\)</span>，<span class=\"math inline\">\\(||x_i - x_j||^2_2 = w_1 dist_{ij,1} + w_2 dist_{ij,2} + \\cdots + w_d dist_{ij,d} = (x_i - x_j)^T W (x_i - x_j)\\)</span>，其中<span class=\"math inline\">\\(W = diag(w), w_i \\ge 0\\)</span></p>\r\n<p>如果令<span class=\"math inline\">\\(W\\)</span>不再是一个对角矩阵，而是让其等于一个半正定对称矩阵<span class=\"math inline\">\\(M\\)</span>，则可以定义马氏距离<span class=\"math inline\">\\(dist_{mah}^2(x_i, x_j) = (x_i - x_j)^TM(x_i - x_j) = ||x_i - x_j||^2_M\\)</span>，则可以对这个<span class=\"math inline\">\\(M\\)</span>进行学习，得到满足要求的距离表达。</p>\r\n<p>在近邻成分分析（Neighbourhood Component Analysis, NCA）中可以用对<span class=\"math inline\">\\(M\\)</span>进行训练，提高其分类正确率。</p>\r\n<p>又例如根据一些领域知识已知某些样本相似（必连约束集合），另外一些样本不相似（勿连约束及合），则可以对<span class=\"math inline\">\\(M\\)</span>进行训练，使得必连约束集合中的样本距离尽可能小，而勿连约束集合中的样本的尽可能大。</p>\r\n<p>不管以任何方式训练得到的<span class=\"math inline\">\\(M\\)</span>，都可以对M进行特征值分解，然后去掉一部分特征向量，得到降维矩阵，用于数据的降维。</p>\r\n"}],"PostAsset":[{"_id":"source/_posts/学习笔记/BatchNormalization学习笔记/normalization.png","slug":"normalization.png","post":"ckjxpc3ra000044mqfmzib5d6","modified":0,"renderable":0},{"_id":"source/_posts/学习笔记/CNN感受野/CNN操作示意.png","slug":"CNN操作示意.png","post":"ckjxpc3rh000344mq3gxf88ft","modified":0,"renderable":0},{"_id":"source/_posts/学习笔记/CNN感受野/VGG模型配置.png","slug":"VGG模型配置.png","post":"ckjxpc3rh000344mq3gxf88ft","modified":0,"renderable":0},{"_id":"source/_posts/学习笔记/CNN感受野/感受野权重示意图.png","slug":"感受野权重示意图.png","post":"ckjxpc3rh000344mq3gxf88ft","modified":0,"renderable":0},{"_id":"source/_posts/学习笔记/CNN感受野/有效感受野示意.png","slug":"有效感受野示意.png","post":"ckjxpc3rh000344mq3gxf88ft","modified":0,"renderable":0},{"_id":"source/_posts/学习笔记/CNN相关知识/Conv.gif","slug":"Conv.gif","post":"ckjxpc3rj000844mq0i0q3iwk","modified":0,"renderable":0},{"_id":"source/_posts/学习笔记/ResNet总结/exp_result.png","slug":"exp_result.png","post":"ckjxpc3rl000d44mq7mar3up3","modified":0,"renderable":0},{"_id":"source/_posts/学习笔记/ResNet总结/Residual_block.png","slug":"Residual_block.png","post":"ckjxpc3rl000d44mq7mar3up3","modified":0,"renderable":0},{"_id":"source/_posts/学习笔记/ResNet总结/Residual_block_v2.png","slug":"Residual_block_v2.png","post":"ckjxpc3rl000d44mq7mar3up3","modified":0,"renderable":0},{"_id":"source/_posts/学习笔记/attention机制总结/Attention计算方式.png","slug":"Attention计算方式.png","post":"ckjxpc3ro000i44mq549cd5m4","modified":0,"renderable":0},{"_id":"source/_posts/学习笔记/attention机制总结/CBAM_detail.png","slug":"CBAM_detail.png","post":"ckjxpc3ro000i44mq549cd5m4","modified":0,"renderable":0},{"_id":"source/_posts/学习笔记/attention机制总结/Non_local_NN.png","slug":"Non_local_NN.png","post":"ckjxpc3ro000i44mq549cd5m4","modified":0,"renderable":0},{"_id":"source/_posts/学习笔记/attention机制总结/Resnet_CBAM.png","slug":"Resnet_CBAM.png","post":"ckjxpc3ro000i44mq549cd5m4","modified":0,"renderable":0},{"_id":"source/_posts/学习笔记/attention机制总结/SEInceptionNet_SEResNet.png","slug":"SEInceptionNet_SEResNet.png","post":"ckjxpc3ro000i44mq549cd5m4","modified":0,"renderable":0},{"_id":"source/_posts/学习笔记/attention机制总结/Squeeze_and_Excitation.png","slug":"Squeeze_and_Excitation.png","post":"ckjxpc3ro000i44mq549cd5m4","modified":0,"renderable":0},{"_id":"source/_posts/学习笔记/各种优化算法总结/Momentum和NAG的运行差别.png","slug":"Momentum和NAG的运行差别.png","post":"ckjxpc3rt000t44mqhtda54gb","modified":0,"renderable":0},{"_id":"source/_posts/学习笔记/各种优化算法总结/狭长形状的损失函数下Momentum的运行示意图.png","slug":"狭长形状的损失函数下Momentum的运行示意图.png","post":"ckjxpc3rt000t44mqhtda54gb","modified":0,"renderable":0},{"_id":"source/_posts/学习笔记/各种优化算法总结/狭长形状的损失函数下SGD的运行示意图.png","slug":"狭长形状的损失函数下SGD的运行示意图.png","post":"ckjxpc3rt000t44mqhtda54gb","modified":0,"renderable":0},{"_id":"source/_posts/学习笔记/学习笔记-2019-04/center_ness.png","slug":"center_ness.png","post":"ckjxpc3rv001044mqaf3w7yqp","modified":0,"renderable":0},{"_id":"source/_posts/学习笔记/标签噪声的处理/sModel.png","slug":"sModel.png","post":"ckjxpc3s1001d44mq6twsc1ix","modified":0,"renderable":0},{"_id":"source/_posts/学习笔记/标签噪声的处理/噪声标签和真实标签的关系.png","slug":"噪声标签和真实标签的关系.png","post":"ckjxpc3s1001d44mq6twsc1ix","modified":0,"renderable":0},{"_id":"source/_posts/学习笔记/标签噪声的处理/标签修正框架.png","slug":"标签修正框架.png","post":"ckjxpc3s1001d44mq6twsc1ix","modified":0,"renderable":0},{"_id":"source/_posts/学习笔记/正则化方法/l1正则化效果示意.png","slug":"l1正则化效果示意.png","post":"ckjxpc3s2001g44mqdgrma3xn","modified":0,"renderable":0},{"_id":"source/_posts/环境搭建/在win10上搭建python3和tensorflow-gpu环境/Anaconda_install.png","slug":"Anaconda_install.png","post":"ckjxpc3s6001r44mq05cy8v51","modified":0,"renderable":0},{"_id":"source/_posts/环境搭建/在win10上搭建python3和tensorflow-gpu环境/cuda_dir.png","slug":"cuda_dir.png","post":"ckjxpc3s6001r44mq05cy8v51","modified":0,"renderable":0},{"_id":"source/_posts/环境搭建/在win10上搭建python3和tensorflow-gpu环境/cuda_download.png","slug":"cuda_download.png","post":"ckjxpc3s6001r44mq05cy8v51","modified":0,"renderable":0},{"_id":"source/_posts/环境搭建/在win10上搭建python3和tensorflow-gpu环境/cuda_version.png","slug":"cuda_version.png","post":"ckjxpc3s6001r44mq05cy8v51","modified":0,"renderable":0},{"_id":"source/_posts/环境搭建/在win10上搭建python3和tensorflow-gpu环境/install_tensorflow-gpu.png","slug":"install_tensorflow-gpu.png","post":"ckjxpc3s6001r44mq05cy8v51","modified":0,"renderable":0},{"_id":"source/_posts/环境搭建/在win10上搭建python3和tensorflow-gpu环境/test.png","slug":"test.png","post":"ckjxpc3s6001r44mq05cy8v51","modified":0,"renderable":0},{"_id":"source/_posts/论文阅读/论文翻译之《Perceptual-Losses-for-Real-Time-Style-Transferand-Super-Resolution》/fig1.png","slug":"fig1.png","post":"ckjxpc3s7001u44mq3qtn5s11","modified":0,"renderable":0},{"_id":"source/_posts/论文阅读/论文翻译之《Perceptual-Losses-for-Real-Time-Style-Transferand-Super-Resolution》/fig2.png","slug":"fig2.png","post":"ckjxpc3s7001u44mq3qtn5s11","modified":0,"renderable":0},{"_id":"source/_posts/论文阅读/论文阅读《ACNet——Strengthening-the-Kernel-Skeletons-for-Powerful-CNN-via-AsymmetricConvolution-Blocks》/ACB_evaluating.png","slug":"ACB_evaluating.png","post":"ckjxpc3s8001x44mq8zfy5bd8","modified":0,"renderable":0},{"_id":"source/_posts/论文阅读/论文阅读《ACNet——Strengthening-the-Kernel-Skeletons-for-Powerful-CNN-via-AsymmetricConvolution-Blocks》/ACB_training.png","slug":"ACB_training.png","post":"ckjxpc3s8001x44mq8zfy5bd8","modified":0,"renderable":0},{"_id":"source/_posts/论文阅读/论文阅读《ACNet——Strengthening-the-Kernel-Skeletons-for-Powerful-CNN-via-AsymmetricConvolution-Blocks》/BN_fusion.png","slug":"BN_fusion.png","post":"ckjxpc3s8001x44mq8zfy5bd8","modified":0,"renderable":0},{"_id":"source/_posts/论文阅读/论文阅读《CondenseNet-An-Efficient-DenseNet-using-Learned-Group-Convolutions》/GroupConv_Learn_Test.png","slug":"GroupConv_Learn_Test.png","post":"ckjxpc3s9002044mq8crq844j","modified":0,"renderable":0},{"_id":"source/_posts/论文阅读/论文阅读《CondenseNet-An-Efficient-DenseNet-using-Learned-Group-Convolutions》/分组个数实验.png","slug":"分组个数实验.png","post":"ckjxpc3s9002044mq8crq844j","modified":0,"renderable":0},{"_id":"source/_posts/论文阅读/论文阅读《CondenseNet-An-Efficient-DenseNet-using-Learned-Group-Convolutions》/特征凝聚因子实验.png","slug":"特征凝聚因子实验.png","post":"ckjxpc3s9002044mq8crq844j","modified":0,"renderable":0},{"_id":"source/_posts/论文阅读/论文阅读《CondenseNet-An-Efficient-DenseNet-using-Learned-Group-Convolutions》/特征凝聚因子效率对比.png","slug":"特征凝聚因子效率对比.png","post":"ckjxpc3s9002044mq8crq844j","modified":0,"renderable":0},{"_id":"source/_posts/论文阅读/论文阅读《CondenseNet-An-Efficient-DenseNet-using-Learned-Group-Convolutions》/训练loss和学习速率.png","slug":"训练loss和学习速率.png","post":"ckjxpc3s9002044mq8crq844j","modified":0,"renderable":0},{"_id":"source/_posts/论文阅读/论文阅读《CornerNet-Detecting-Objects-as-Paired-Keypoints》/CornerPooling.png","slug":"CornerPooling.png","post":"ckjxpc3sa002244mqbfhh1fdm","modified":0,"renderable":0},{"_id":"source/_posts/论文阅读/论文阅读《Deep-Bilateral-Learning-for-Real-Time-Image-Enhancement》/模型整体结构.png","slug":"模型整体结构.png","post":"ckjxpc3sb002544mq12tv2ev0","modified":0,"renderable":0},{"_id":"source/_posts/论文阅读/论文阅读《Expectation-Maximization-Attention-Networks-for-Semantic-Segmentation》/EMA_Unit.png","slug":"EMA_Unit.png","post":"ckjxpc3sd002a44mq16bgcb9i","modified":0,"renderable":0},{"_id":"source/_posts/论文阅读/论文阅读《Improving-neural-networks-by-preventingco-adaptation-of-feature-detectors》/feature_map_activate_show.png","slug":"feature_map_activate_show.png","post":"ckjxpc3sn002t44mq3rp61ggi","modified":0,"renderable":0},{"_id":"source/_posts/论文阅读/论文阅读《Improving-neural-networks-by-preventingco-adaptation-of-feature-detectors》/feature_map_show.png","slug":"feature_map_show.png","post":"ckjxpc3sn002t44mq3rp61ggi","modified":0,"renderable":0},{"_id":"source/_posts/论文阅读/论文阅读《Multi-Task-Learning-Using-Uncertainty-to-Weigh-Lossesfor-Scene-Geometry-and-Semantics》/paper_result.png","slug":"paper_result.png","post":"ckjxpc3sq002w44mq79uf4tc2","modified":0,"renderable":0},{"_id":"source/_posts/论文阅读/论文阅读《Learning-a-Discriminative-Feature-Network-for-Semantic-Segmentation》/total_model.png","slug":"total_model.png","post":"ckjxpc3sn002u44mq07sm90dv","modified":0,"renderable":0},{"_id":"source/_posts/论文阅读/论文阅读《Reducing-the-Dimensionality-of-Data-with-Neural-Networks》/RBM_and_multilayer_model.png","slug":"RBM_and_multilayer_model.png","post":"ckjxpc3sr002y44mq3ml0h7ip","modified":0,"renderable":0},{"_id":"source/_posts/学习笔记/GAN概述/AdaIN模型结构示意图.png","slug":"AdaIN模型结构示意图.png","post":"ckjxpc3sr003044mq2qk9babf","modified":0,"renderable":0},{"_id":"source/_posts/学习笔记/GAN概述/GAN模型结构.png","slug":"GAN模型结构.png","post":"ckjxpc3sr003044mq2qk9babf","modified":0,"renderable":0},{"_id":"source/_posts/学习笔记/GAN概述/基于联合双边学习的图像风格转换模型结构示意图.png","slug":"基于联合双边学习的图像风格转换模型结构示意图.png","post":"ckjxpc3sr003044mq2qk9babf","modified":0,"renderable":0},{"_id":"source/_posts/学习笔记/学习笔记-2019-01/DenseNet模型结构.png","slug":"DenseNet模型结构.png","post":"ckjxpc3su003444mqethl3gbe","modified":0,"renderable":0},{"_id":"source/_posts/学习笔记/学习笔记-2019-01/ROI_Pooling输入.png","slug":"ROI_Pooling输入.png","post":"ckjxpc3su003444mqethl3gbe","modified":0,"renderable":0},{"_id":"source/_posts/学习笔记/学习笔记-2019-01/ROI_Pooling输出.png","slug":"ROI_Pooling输出.png","post":"ckjxpc3su003444mqethl3gbe","modified":0,"renderable":0},{"_id":"source/_posts/学习笔记/学习笔记-2019-01/区域建议网络给出的位置.png","slug":"区域建议网络给出的位置.png","post":"ckjxpc3su003444mqethl3gbe","modified":0,"renderable":0},{"_id":"source/_posts/学习笔记/学习笔记-2019-01/按照设定的输出大小进行划分.png","slug":"按照设定的输出大小进行划分.png","post":"ckjxpc3su003444mqethl3gbe","modified":0,"renderable":0},{"_id":"source/_posts/学习笔记/学习笔记-2019-01/空洞卷积感受野示意图.png","slug":"空洞卷积感受野示意图.png","post":"ckjxpc3su003444mqethl3gbe","modified":0,"renderable":0},{"_id":"source/_posts/学习笔记/学习笔记-2019-02/l1正则化效果示意.png","slug":"l1正则化效果示意.png","post":"ckjxpc3sv003644mqh1ed3kwn","modified":0,"renderable":0},{"_id":"source/_posts/学习笔记/学习笔记-2019-02/支持向量机超平面示意.png","slug":"支持向量机超平面示意.png","post":"ckjxpc3sv003644mqh1ed3kwn","modified":0,"renderable":0},{"_id":"source/_posts/学习笔记/学习笔记-2019-02/约束条件和目标函数的等值线.png","slug":"约束条件和目标函数的等值线.png","post":"ckjxpc3sv003644mqh1ed3kwn","modified":0,"renderable":0},{"_id":"source/_posts/学习笔记/目标检测/Cascade_R-CNN.png","slug":"Cascade_R-CNN.png","post":"ckjxpc3sw003844mqagq5dkgl","modified":0,"renderable":0},{"_id":"source/_posts/学习笔记/目标检测/FastRCNN.png","slug":"FastRCNN.png","post":"ckjxpc3sw003844mqagq5dkgl","modified":0,"renderable":0},{"_id":"source/_posts/学习笔记/目标检测/FPN.png","slug":"FPN.png","post":"ckjxpc3sw003844mqagq5dkgl","modified":0,"renderable":0},{"_id":"source/_posts/学习笔记/目标检测/SPP.png","slug":"SPP.png","post":"ckjxpc3sw003844mqagq5dkgl","modified":0,"renderable":0},{"_id":"source/_posts/学习笔记/目标检测/YOLOv1.png","slug":"YOLOv1.png","post":"ckjxpc3sw003844mqagq5dkgl","modified":0,"renderable":0},{"_id":"source/_posts/学习笔记/目标检测/YOLOv2_exp.png","slug":"YOLOv2_exp.png","post":"ckjxpc3sw003844mqagq5dkgl","modified":0,"renderable":0},{"_id":"source/_posts/学习笔记/目标检测/YOLOv2_predict.png","slug":"YOLOv2_predict.png","post":"ckjxpc3sw003844mqagq5dkgl","modified":0,"renderable":0},{"_id":"source/_posts/学习笔记/目标检测/YOLOv3_exp.png","slug":"YOLOv3_exp.png","post":"ckjxpc3sw003844mqagq5dkgl","modified":0,"renderable":0},{"_id":"source/_posts/学习笔记/目标检测/YOLOv3结构示意.png","slug":"YOLOv3结构示意.png","post":"ckjxpc3sw003844mqagq5dkgl","modified":0,"renderable":0}],"PostCategory":[{"post_id":"ckjxpc3rp000k44mq9sdrdxmp","category_id":"ckjxpc3rr000o44mq44xa1188","_id":"ckjxpc3rw001144mqhfxrf03q"},{"post_id":"ckjxpc3s6001r44mq05cy8v51","category_id":"ckjxpc3s8001v44mqc3776oyj","_id":"ckjxpc3sc002844mq6ghg8lza"}],"PostTag":[{"post_id":"ckjxpc3rh000444mq5o941xoa","tag_id":"ckjxpc3rg000244mq4l2jhbz0","_id":"ckjxpc3rj000744mq1wws4srr"},{"post_id":"ckjxpc3ri000544mq65dob8pu","tag_id":"ckjxpc3rg000244mq4l2jhbz0","_id":"ckjxpc3rk000944mq3affc6x1"},{"post_id":"ckjxpc3rj000844mq0i0q3iwk","tag_id":"ckjxpc3rg000244mq4l2jhbz0","_id":"ckjxpc3rl000c44mqap2d3x0w"},{"post_id":"ckjxpc3ra000044mqfmzib5d6","tag_id":"ckjxpc3rg000244mq4l2jhbz0","_id":"ckjxpc3rm000e44mq1ipqh1r8"},{"post_id":"ckjxpc3ra000044mqfmzib5d6","tag_id":"ckjxpc3rj000644mq4994af86","_id":"ckjxpc3rn000h44mqe14d8yug"},{"post_id":"ckjxpc3rl000d44mq7mar3up3","tag_id":"ckjxpc3rg000244mq4l2jhbz0","_id":"ckjxpc3rp000j44mq08749vlb"},{"post_id":"ckjxpc3re000144mq5b8hgmw4","tag_id":"ckjxpc3rl000b44mqd27lcogu","_id":"ckjxpc3rq000m44mq5d33fifh"},{"post_id":"ckjxpc3rm000f44mq4wmo0spb","tag_id":"ckjxpc3rl000b44mqd27lcogu","_id":"ckjxpc3rs000p44mqdma5drqb"},{"post_id":"ckjxpc3ro000i44mq549cd5m4","tag_id":"ckjxpc3rg000244mq4l2jhbz0","_id":"ckjxpc3rt000s44mq58br3h5o"},{"post_id":"ckjxpc3rh000344mq3gxf88ft","tag_id":"ckjxpc3rg000244mq4l2jhbz0","_id":"ckjxpc3ru000u44mq7jx17f8a"},{"post_id":"ckjxpc3rp000k44mq9sdrdxmp","tag_id":"ckjxpc3rg000244mq4l2jhbz0","_id":"ckjxpc3ru000w44mq00qb2055"},{"post_id":"ckjxpc3rk000a44mq7i11hr7x","tag_id":"ckjxpc3rl000b44mqd27lcogu","_id":"ckjxpc3rv000z44mqgze24otr"},{"post_id":"ckjxpc3rs000q44mqc1wq6akq","tag_id":"ckjxpc3rl000b44mqd27lcogu","_id":"ckjxpc3rw001244mqevs38awe"},{"post_id":"ckjxpc3rt000t44mqhtda54gb","tag_id":"ckjxpc3rg000244mq4l2jhbz0","_id":"ckjxpc3rx001544mq7spb29c5"},{"post_id":"ckjxpc3ru000v44mq83lddka9","tag_id":"ckjxpc3rl000b44mqd27lcogu","_id":"ckjxpc3rz001744mqeasrepej"},{"post_id":"ckjxpc3rq000n44mq895uc74p","tag_id":"ckjxpc3rg000244mq4l2jhbz0","_id":"ckjxpc3s1001a44mq3l3c7qg2"},{"post_id":"ckjxpc3rq000n44mq895uc74p","tag_id":"ckjxpc3rt000r44mq78utbfs8","_id":"ckjxpc3s1001c44mq73zn4kif"},{"post_id":"ckjxpc3rq000n44mq895uc74p","tag_id":"ckjxpc3rv000y44mq4hyh899s","_id":"ckjxpc3s2001f44mq50jvgn3c"},{"post_id":"ckjxpc3rx001644mq8ni57qil","tag_id":"ckjxpc3rx001444mqd1qv6r7c","_id":"ckjxpc3s3001h44mqfr9h3h7e"},{"post_id":"ckjxpc3ru000x44mq2hraefy9","tag_id":"ckjxpc3rx001444mqd1qv6r7c","_id":"ckjxpc3s4001j44mq9i26aybp"},{"post_id":"ckjxpc3s0001844mq0q329zm6","tag_id":"ckjxpc3rl000b44mqd27lcogu","_id":"ckjxpc3s5001m44mq071u5tiz"},{"post_id":"ckjxpc3rv001044mqaf3w7yqp","tag_id":"ckjxpc3rx001444mqd1qv6r7c","_id":"ckjxpc3s5001o44mqcjbi043z"},{"post_id":"ckjxpc3s1001d44mq6twsc1ix","tag_id":"ckjxpc3rg000244mq4l2jhbz0","_id":"ckjxpc3s6001q44mqhvj9gpwx"},{"post_id":"ckjxpc3s2001g44mqdgrma3xn","tag_id":"ckjxpc3rg000244mq4l2jhbz0","_id":"ckjxpc3s7001t44mq01zp133c"},{"post_id":"ckjxpc3rw001344mq1rdsghw2","tag_id":"ckjxpc3rx001444mqd1qv6r7c","_id":"ckjxpc3s8001w44mq7djq53kb"},{"post_id":"ckjxpc3s5001p44mq3smgc2y6","tag_id":"ckjxpc3rv000y44mq4hyh899s","_id":"ckjxpc3s9001z44mq68fn7bxc"},{"post_id":"ckjxpc3s3001i44mq670t6leq","tag_id":"ckjxpc3s4001l44mqhnw8ed7q","_id":"ckjxpc3sa002144mq637w0w9e"},{"post_id":"ckjxpc3s4001k44mqfmo0h5cp","tag_id":"ckjxpc3s7001s44mq6nyo5oki","_id":"ckjxpc3sb002444mqdbv849z4"},{"post_id":"ckjxpc3s5001n44mqec742c9a","tag_id":"ckjxpc3s9001y44mq92wi1kh2","_id":"ckjxpc3sc002644mq8l3s6uyj"},{"post_id":"ckjxpc3s6001r44mq05cy8v51","tag_id":"ckjxpc3sb002344mq7tpe20vt","_id":"ckjxpc3se002d44mq7d5xgpic"},{"post_id":"ckjxpc3s6001r44mq05cy8v51","tag_id":"ckjxpc3sd002944mqak6vafg0","_id":"ckjxpc3se002e44mq4cjhcn9j"},{"post_id":"ckjxpc3s6001r44mq05cy8v51","tag_id":"ckjxpc3se002b44mqeejz7g4t","_id":"ckjxpc3se002g44mqgyux4rbn"},{"post_id":"ckjxpc3s7001u44mq3qtn5s11","tag_id":"ckjxpc3se002c44mq57100lvk","_id":"ckjxpc3se002h44mq4pjt3el5"},{"post_id":"ckjxpc3s8001x44mq8zfy5bd8","tag_id":"ckjxpc3se002f44mq6q5a88gi","_id":"ckjxpc3sf002j44mq3j0c40al"},{"post_id":"ckjxpc3s9002044mq8crq844j","tag_id":"ckjxpc3se002f44mq6q5a88gi","_id":"ckjxpc3sf002l44mqey9e8xuk"},{"post_id":"ckjxpc3sa002244mqbfhh1fdm","tag_id":"ckjxpc3se002f44mq6q5a88gi","_id":"ckjxpc3sg002n44mq0zzf2132"},{"post_id":"ckjxpc3sb002544mq12tv2ev0","tag_id":"ckjxpc3se002f44mq6q5a88gi","_id":"ckjxpc3sg002p44mq7zlr1bhk"},{"post_id":"ckjxpc3sc002744mq0ssb1g1g","tag_id":"ckjxpc3se002f44mq6q5a88gi","_id":"ckjxpc3sh002r44mqepnf0rbd"},{"post_id":"ckjxpc3sd002a44mq16bgcb9i","tag_id":"ckjxpc3se002f44mq6q5a88gi","_id":"ckjxpc3sh002s44mq25rq4ssh"},{"post_id":"ckjxpc3sn002t44mq3rp61ggi","tag_id":"ckjxpc3se002f44mq6q5a88gi","_id":"ckjxpc3sq002v44mqf6ak5qe1"},{"post_id":"ckjxpc3sn002u44mq07sm90dv","tag_id":"ckjxpc3se002f44mq6q5a88gi","_id":"ckjxpc3sr002x44mqdm1ig8ym"},{"post_id":"ckjxpc3sq002w44mq79uf4tc2","tag_id":"ckjxpc3se002f44mq6q5a88gi","_id":"ckjxpc3sr002z44mq6l2zcpo1"},{"post_id":"ckjxpc3sr002y44mq3ml0h7ip","tag_id":"ckjxpc3se002f44mq6q5a88gi","_id":"ckjxpc3st003144mqbce8gbb5"},{"post_id":"ckjxpc3sr003044mq2qk9babf","tag_id":"ckjxpc3rg000244mq4l2jhbz0","_id":"ckjxpc3su003344mqflgie015"},{"post_id":"ckjxpc3su003444mqethl3gbe","tag_id":"ckjxpc3rx001444mqd1qv6r7c","_id":"ckjxpc3sw003744mqd6e4bpaj"},{"post_id":"ckjxpc3sv003644mqh1ed3kwn","tag_id":"ckjxpc3rx001444mqd1qv6r7c","_id":"ckjxpc3sx003944mqdwxfcenv"},{"post_id":"ckjxpc3sw003844mqagq5dkgl","tag_id":"ckjxpc3rg000244mq4l2jhbz0","_id":"ckjxpc3sy003b44mq2vntdw9x"},{"post_id":"ckjxpc3st003244mq00zk9xcn","tag_id":"ckjxpc3sv003544mqambu50c5","_id":"ckjxpc3sy003d44mq0ge70oz7"},{"post_id":"ckjxpc3sy003a44mqatxv0ec1","tag_id":"ckjxpc3sy003c44mqc9uh4fa5","_id":"ckjxpc3sz003e44mq9c2h8dij"},{"post_id":"ckjxpc3t0003f44mq2c3h4nii","tag_id":"ckjxpc3rl000b44mqd27lcogu","_id":"ckjxpc3t0003g44mq6vnj2f96"},{"post_id":"ckjxpc3t0003f44mq2c3h4nii","tag_id":"ckjxpc3rt000r44mq78utbfs8","_id":"ckjxpc3t1003h44mq9g574h55"},{"post_id":"ckjxpc3t0003f44mq2c3h4nii","tag_id":"ckjxpc3rv000y44mq4hyh899s","_id":"ckjxpc3t1003i44mq7dlb69ru"}],"Tag":[{"name":"深度学习","_id":"ckjxpc3rg000244mq4l2jhbz0"},{"name":"论文笔记","_id":"ckjxpc3rj000644mq4994af86"},{"name":"机器学习","_id":"ckjxpc3rl000b44mqd27lcogu"},{"name":"读书笔记","_id":"ckjxpc3rt000r44mq78utbfs8"},{"name":"杂项","_id":"ckjxpc3rv000y44mq4hyh899s"},{"name":"学习笔记，杂项","_id":"ckjxpc3rx001444mqd1qv6r7c"},{"name":"线性代数","_id":"ckjxpc3s4001l44mqhnw8ed7q"},{"name":"环境搭建","_id":"ckjxpc3s7001s44mq6nyo5oki"},{"name":"求导，矩阵，向量","_id":"ckjxpc3s9001y44mq92wi1kh2"},{"name":"tensorflow-gpu","_id":"ckjxpc3sb002344mq7tpe20vt"},{"name":"python3","_id":"ckjxpc3sd002944mqak6vafg0"},{"name":"win10","_id":"ckjxpc3se002b44mqeejz7g4t"},{"name":"论文翻译","_id":"ckjxpc3se002c44mq57100lvk"},{"name":"论文阅读","_id":"ckjxpc3se002f44mq6q5a88gi"},{"name":"凸优化","_id":"ckjxpc3sv003544mqambu50c5"},{"name":"算法","_id":"ckjxpc3sy003c44mqc9uh4fa5"}]}}